{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMi0xMC0yN1QxNzoyNDowNC0wNjowMM4ARIFh"
    },
    "edges": [
      {
        "node": {
          "title": "Multiapp mesh transfer with adaptivity",
          "author": {
            "login": "xueyang94"
          },
          "bodyText": "I have one mainapp that transfers a few variables from one subapp using FullSolveMultiApp. The transfer only occurs after the subapp is finished. In the subapp, I use mesh Adaptivity starting at initial step. The subapp domain is smaller than the mainapp domain.  How to transfer the subapp mesh after adaptivity? Thanks.",
          "url": "https://github.com/idaholab/moose/discussions/22417",
          "updatedAt": "2022-11-15T19:31:52Z",
          "publishedAt": "2022-10-16T20:12:48Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nwe don\u2019t transfer meshes. However if you want to refine the main app the same way as the subapp you can transfer the indicator and use it for the criterion for adaptivity\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/22417#discussioncomment-3891778",
                  "updatedAt": "2022-10-17T00:14:33Z",
                  "publishedAt": "2022-10-17T00:14:32Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "the trickiness might be to order the transfer and the adaptivity correctly.\nMaybe if you run the mulitapp and do the transfer on timestep_end, then run the adaptivity on timestep_begin that would work for you?",
                          "url": "https://github.com/idaholab/moose/discussions/22417#discussioncomment-3891893",
                          "updatedAt": "2022-10-17T00:58:53Z",
                          "publishedAt": "2022-10-17T00:58:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "xueyang94"
                          },
                          "bodyText": "Hello Guillaume, you have described what I am doing currently. I transfer the indicator (bnds) from the subapp to mainapp after the subapp is finished, and run the adaptivity of mainapp on timestep_begin. However, since the mainapp does not start with the finest mesh of the subapp, as the mesh refines, the mainapp would crash due to memory.\nI want the mainapp to start with the finest mesh so that at least the crash would happen at the first timestep. The mainapp adaptivity block uses ValueRangeMarker on both the transferred indicator (bnds) and the variables in the mainapp. However, at the initial timestep of mainapp, only its own variables are refined, while bnds is at its coarsest. This is strange to me why bnds is not refined just like other variables.",
                          "url": "https://github.com/idaholab/moose/discussions/22417#discussioncomment-3994000",
                          "updatedAt": "2022-10-28T15:14:07Z",
                          "publishedAt": "2022-10-28T15:14:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "xueyang94"
                          },
                          "bodyText": "The issue can be visualized using the input files I supplied in Discussion #22342. The meshes of subapp and mainapp at timestep zero are below. There are three phases in the mainapp from left to right. The subapp simulates the left-most phase but with two grains on top of each other. Although mesh cannot be transferred as you mentioned, I would expect the left phase in the mainapp to have a refined horizontal grain boundary because the bnds indicator from subapp is refined together with other variables in the mainapp at timestep zero.\nSubapp at timestep zero:\n\nMainapp at timestep zero:",
                          "url": "https://github.com/idaholab/moose/discussions/22417#discussioncomment-3994239",
                          "updatedAt": "2022-10-28T15:37:50Z",
                          "publishedAt": "2022-10-28T15:37:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I ran your inputs and I think I have a better understanding of what you want but I think there are a few issues:\n\nin the main app I'm seeing etaa0 and etaa1 used as markers, not bnds.\nthe mesh of the main app will never see the updated value from the transfer of bnds from the main app because when the transfer is performed, it does not have nodes in the center/middle of the square.\nOnce the mesh is refined, it could, but at INITIAL when the transfer is performed it does not.\nI made ny be even, so that there's nodes on the horizontal middle of the left box, and got the expected bnds value in the center.\n\nbut even then, looking at the FEProblemBase.C order of execution of things in initialSetup():\n\nmarkers are on line 750\nadaptivity is on line 850\nmultiapps and transfers are on line 1055\n\nso adaptivity is ran before the multiapps are, the values transfered and so on. So the marker would not be used on INITIAL.\nSo your options:\n\nmake a PR to MOOSE moving adaptivity after the first round of multiapps. I doubt anyone relies on the opposite\nrun a fake time step with no physics at the beginning of your main app\nhave a very very small first time step with a coarsened mesh. In a very small time step in the real world, nothing happens, and same in MOOSE.",
                          "url": "https://github.com/idaholab/moose/discussions/22417#discussioncomment-3998327",
                          "updatedAt": "2022-10-28T21:59:51Z",
                          "publishedAt": "2022-10-28T21:59:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "xueyang94"
                          },
                          "bodyText": "Thanks for the reply. I used an updated input file where bnds was transferred instead of grain variables. The transfer does not have nodes in the center of the square, and it only have nodes on the edge of the square? Not sure how this transfer works.\nIf the adaptivity is ran before the multiapps are, I would imagine the markers being used on INITIAL because the marker is associated with adaptivity.",
                          "url": "https://github.com/idaholab/moose/discussions/22417#discussioncomment-4020903",
                          "updatedAt": "2022-10-31T17:17:52Z",
                          "publishedAt": "2022-10-31T17:17:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "ok for updated input\nThe meshfunction or shapeevaluation transfer evaluates the source variable at the locations of the target mesh.\nYou do not have any mesh node at the right location in the target mesh at initialization\nyes, markers are also pre-multiapp",
                          "url": "https://github.com/idaholab/moose/discussions/22417#discussioncomment-4021831",
                          "updatedAt": "2022-10-31T19:36:13Z",
                          "publishedAt": "2022-10-31T19:36:12Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "CZM used with HeatConduction",
          "author": {
            "login": "echo1115"
          },
          "bodyText": "Hi,\nI made a case using CZM and HeatConduction modules. In the case, an object (x\u2208[0,1],y\u2208[0,1],z\u2208[0,4]) is consisted of 4 blocks. The mesh of block1-block2 and block2-block3 are broken using BreakMeshByBlockGenerator, block0-block1 are always connected.\n\nAs for the BCs,\nMechanical BCs:  the boundaries x0 (x=0), y0 (y=0), z0 (z=0) are fixed, a tension displacement is applied at z1 (z=4).\nThermal BCs: a heatflux is applied at z0 (z=0), and a DirichletBC is applied at z1 (z=4).\nBesides, to ensure the temperature is continuous between block1-block2, block2-block3, I  used ThermalContact and applied a large\ngap_conductance.\nHere is the figure of temperature calculation result. I ploted the temperature along z axis,\nbut why the temperature is not decrease along z axis\uff1f\n\nThe input shows here\n[Mesh]\n  [./gen]\n    type = GeneratedMeshGenerator\n    dim = 3\n    nx = 1\n    ny = 1\n    nz = 40\n    zmin = 0\n    zmax = 4\n  [../]\n  [block1]\n    type = SubdomainBoundingBoxGenerator\n    block_id = 0\n    bottom_left = '0 0 0'\n    top_right = '1 1 1'\n    input = gen\n  []\n  [block2]\n    type = SubdomainBoundingBoxGenerator\n    block_id = 1\n    bottom_left = '0 0 1'\n    top_right = '1 1 2'\n    input = block1\n  []\n  [block3]\n    type = SubdomainBoundingBoxGenerator\n    block_id = 2\n    bottom_left = '0 0 2'\n    top_right = '1 1 3'\n    input = block2\n  []\n  [block4]\n    type = SubdomainBoundingBoxGenerator\n    block_id = 3\n    bottom_left = '0 0 3'\n    top_right = '1 1 4'\n    input = block3\n  []\n  [./split]\n    type = BreakMeshByBlockGenerator\n    input = block4 #subdomain_id\n    split_interface = true\n    surrounding_blocks = 2 # added\n    add_interface_on_two_sides = true # added\n  []\n  [add_side_sets]\n    input = split\n    type = SideSetsFromNormalsGenerator\n    normals = '0 -1  0\n               0  1  0\n               -1 0  0\n               1  0  0\n               0  0 -1\n               0  0  1'\n    fixed_normal = true\n    new_boundary = 'y0 y1 x0 x1 z0 z1'\n  []\n[]\n\n[GlobalParams]\n  displacements = 'disp_x disp_y disp_z'\n[]\n\n[Variables] # added\n  [./temp]\n    order = FIRST\n    family = LAGRANGE\n    initial_condition = 500\n  [../]\n  [./disp_x]\n    order = FIRST\n    family = LAGRANGE\n  [../]\n  [./disp_y]\n    order = FIRST\n    family = LAGRANGE\n  [../]\n  [./disp_z]\n    order = FIRST\n    family = LAGRANGE\n  [../]\n[]\n\n[Functions]\n  [./stretch]\n    type = PiecewiseLinear\n    x = '0 1'\n    y = '0 100'\n  [../]\n[]\n\n[Kernels]  # added\n  [./heat]\n    type = HeatConduction\n    variable = temp\n  [../]\n  [./heat_dt]\n    type = HeatConductionTimeDerivative\n    variable = temp\n  [../]\n[]\n\n[Modules]\n  [./TensorMechanics]\n    [./Master]\n      [./all]\n        strain = FINITE\n        add_variables = true\n        use_finite_deform_jacobian = true\n        use_automatic_differentiation = true\n        generate_output = 'stress_xx stress_yy stress_zz stress_xy stress_yz stress_xz'\n      [../]\n    [../]\n  [../]\n[]\n\n[Modules/TensorMechanics/CohesiveZoneMaster]\n  [./czm_ik_012]\n    boundary = ' Block1_Block2 Block2_Block3'  # modified\n    base_name = 'czm_b012'\n  [../]\n[]\n\n[Materials]\n  # cohesive materials\n  [./czm_3dc]\n    type = SalehaniIrani3DCTraction\n    boundary = ' Block1_Block2 Block2_Block3'  # modified\n    normal_gap_at_maximum_normal_traction = 1\n    tangential_gap_at_maximum_shear_traction = 0.5\n    maximum_normal_traction = 500\n    maximum_shear_traction = 300\n    base_name = 'czm_b012'\n  [../]\n  # bulk materials\n  [./stress]\n    type = ADComputeFiniteStrainElasticStress\n  [../]\n  [./elasticity_tensor]\n    type = ADComputeIsotropicElasticityTensor\n    youngs_modulus = 200e4\n    poissons_ratio = 0.3\n  [../]\n  # added: thermal materials\n  [./density]\n    type = Density\n    density = 3180\n  [../]\n  [./k_cp]\n    type = HeatConductionMaterial\n    thermal_conductivity = 4\n    specific_heat = 620.0\n  [../]\n[]\n\n[BCs]\n  [./fix_x]\n    type = DirichletBC\n    preset = true\n    value = 0.0\n    boundary = x0\n    variable = disp_x\n  [../]\n  [./fix_y]\n    type = DirichletBC\n    preset = true\n    value = 0.0\n    boundary = y0\n    variable = disp_y\n  [../]\n  [./fix_z]\n    type = DirichletBC\n    preset = true\n    value = 0.0\n    boundary = z0\n    variable = disp_z\n  [../]\n  [./back_z]\n    type = FunctionNeumannBC\n    boundary = z1\n    variable = disp_z\n    use_displaced_mesh = false\n    function = stretch\n  [../]\n\n  # added: thermal bc\n  [./innerHeatFlux]\n    type = FunctionNeumannBC\n    boundary = z0\n    variable = temp\n    function = 1.0e4\n  [../]\n  [./outerTemp_z1]\n     type = DirichletBC\n     boundary = z1\n     variable = temp\n     value = 500.0\n  [../]\n[]\n\n[ThermalContact] # added\n  [thermal_contact]\n    type = GapHeatTransfer\n    variable = temp\n    primary = 'Block1_Block2 Block2_Block3'\n    secondary = 'Block2_Block1 Block3_Block2'\n    emissivity_primary = 0\n    emissivity_secondary = 0\n    gap_conductance = 1.0e9\n  []\n[]\n\n[Constraints]\n  [x1]\n    type = EqualValueBoundaryConstraint\n    variable = disp_x\n    secondary = 'x1'    # boundary\n    penalty = 1e6\n  []\n  [y1]\n    type = EqualValueBoundaryConstraint\n    variable = disp_y\n    secondary = 'y1'    # boundary\n    penalty = 1e6\n  []\n[]\n\n[Preconditioning]\n  [./SMP]\n    type = SMP\n    full = true\n  [../]\n[]\n\n[Executioner]\n  type = Transient\n  solve_type = 'PJFNK'\n  petsc_options_iname = '-ksp_gmres_restart -pc_type -pc_hypre_type -pc_hypre_boomeramg_max_iter'\n  petsc_options_value = '201                hypre    boomeramg      4'\n\n  line_search = 'none'\n  nl_rel_tol = 1e-8\n  l_tol = 1e-3\n  l_max_its = 100\n\n  start_time = 0.0\n  dt = 0.25\n  dtmin = 0.25\n  num_steps = 10\n[]\n\n[Outputs]\n  exodus = true\n[]",
          "url": "https://github.com/idaholab/moose/discussions/22528",
          "updatedAt": "2022-10-31T16:50:14Z",
          "publishedAt": "2022-10-28T02:24:09Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "lindsayad"
                  },
                  "bodyText": "@jiangwen84 @arovinelli would you guys be able to help or have some suggestions for this user?",
                  "url": "https://github.com/idaholab/moose/discussions/22528#discussioncomment-3998196",
                  "updatedAt": "2022-10-28T21:35:40Z",
                  "publishedAt": "2022-10-28T21:35:40Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "It's not really a CZM question. This happens when the characteristic time for heat transfer is way larger than the simulation time. The characteristic time can be expressed as\ntau = L^2 rho c / k\nwhere L is the domain size, rho the density, c the specific heat, and k the conduction coef.\nI would first check to see if the material properties you used are realistic. If they are, then it suggests the time scale you might be interested is much larger than what you were using.\nAlternatively, you can remove the time derivative kernel for temperature. Consequently, the solution you get will be the steady state temperature, from which you can check if the conjugate heat transfer is working as expected.",
                          "url": "https://github.com/idaholab/moose/discussions/22528#discussioncomment-3999931",
                          "updatedAt": "2022-10-29T02:02:01Z",
                          "publishedAt": "2022-10-29T02:02:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "echo1115"
                          },
                          "bodyText": "@hugary1995 Thanks for your reply. Yes it's not a CZM question, this problem exists even though I don't break the mesh and omit the CZM modules. But when I reduce the size of mesh (not applied BreakMeshByBlockGenerator) to 1e-6 times, the temperature decreases along Z axis, since the characteristic time 'tau' you mentioned decreases a lot.",
                          "url": "https://github.com/idaholab/moose/discussions/22528#discussioncomment-4005280",
                          "updatedAt": "2022-10-29T14:05:10Z",
                          "publishedAt": "2022-10-29T14:05:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "Glad it's working. Yeah, mesh refinement is another way to resolve this.",
                          "url": "https://github.com/idaholab/moose/discussions/22528#discussioncomment-4005305",
                          "updatedAt": "2022-10-29T14:12:10Z",
                          "publishedAt": "2022-10-29T14:12:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "If you decide to use ThermalContact to enforce the continuity,  you can set ''quadrature=true'' which is a better option for paired interface like CZM you used here.\nIn addtion to ThermalContact option,  you  an also consider use CoupledPenaltyInterfaceDiffusion or ThinLayerHeatTransfer.",
                          "url": "https://github.com/idaholab/moose/discussions/22528#discussioncomment-4005364",
                          "updatedAt": "2022-10-29T14:22:51Z",
                          "publishedAt": "2022-10-29T14:22:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "echo1115"
                          },
                          "bodyText": "Thanks for all your suggestions! @hugary1995 @jiangwen84",
                          "url": "https://github.com/idaholab/moose/discussions/22528#discussioncomment-4006788",
                          "updatedAt": "2022-10-29T17:02:27Z",
                          "publishedAt": "2022-10-29T17:02:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Thank you @hugary1995 for all your help on the Discussions board! It is much appreciated",
                          "url": "https://github.com/idaholab/moose/discussions/22528#discussioncomment-4019511",
                          "updatedAt": "2022-10-31T14:51:46Z",
                          "publishedAt": "2022-10-31T14:51:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "@echo1115 can we mark this as answered?",
                          "url": "https://github.com/idaholab/moose/discussions/22528#discussioncomment-4019515",
                          "updatedAt": "2022-10-31T14:52:12Z",
                          "publishedAt": "2022-10-31T14:52:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "echo1115"
                          },
                          "bodyText": "@lindsayad yes, this can be marked as answered. Thank you a lot.",
                          "url": "https://github.com/idaholab/moose/discussions/22528#discussioncomment-4020322",
                          "updatedAt": "2022-10-31T16:11:46Z",
                          "publishedAt": "2022-10-31T16:11:45Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Same question:Tensor Mechanics and Heat Conduction coupling",
          "author": {
            "login": "DYLDYLDYL"
          },
          "bodyText": "Hi\uff01\nI am  trying to make a code using Tensor Mechanics Action to calculate the  thermal expansion with a temperature transient. When I do the simulation with only Heat Conduction or only Tensor Mechanics, both work, just when I try to put them together it does not converge.\nThanks in advance for your help!\n[GlobalParams]\n  displacements = 'disp_x disp_y disp_z'\n[]\n\n[Mesh]\nfile = fuelm.e\n  []\n\n[Variables]\n  [T]\n  initial_condition = 300.0\n  []\n[]\n\n[Kernels]\n  [heat_conduction]\n    type = HeatConduction\n    variable = T\n  []\n  [heat_source]\n  type = HeatSource\n  function = volumetric_heat\n  variable = T\n[]\n[]\n\n[Modules/TensorMechanics/Master]\n  [all]\n    add_variables = true\n    automatic_eigenstrain_names = true\n    generate_output = 'vonmises_stress'\n  []\n[]\n\n[BCs]\n  [./bottom]\n    type = DirichletBC\n    boundary = 1\n    variable = T\n    value = 300\n  [../]\n    [./top]\n    type = DirichletBC\n    boundary = 3\n    variable = T\n    value = 300\n  [../]\n    [bottom_z]\n    type = DirichletBC\n    variable = disp_z\n    boundary = 1\n    value = 0\n  []\n[]\n\n[Materials]\n  [./density]\n    type = GenericConstantMaterial\n    prop_names = 'density '\n    prop_values = '10980.0 '\n  [../]\n  [./hcm]\n    type = HeatConductionMaterial\n    specific_heat = 216\n    thermal_conductivity = 7.5\n  [../]\n    [elasticity]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 2.013e11\n    poissons_ratio = 0.316\n  []\n  [expansion1]\n    type = ComputeThermalExpansionEigenstrain\n    temperature = T\n    thermal_expansion_coeff = 9.802e-4\n    stress_free_temperature = 300\n    eigenstrain_name = thermal_expansion\n  []\n  \n    [stress]\n    type = ComputeLinearElasticStress\n  []\n[]\n\n[Functions]\n  [./volumetric_heat]\n     type = ParsedFunction\n     value = 5.96e+5*sin(alpha*pi*z)*t*0.2\n    vars = 'alpha'\n    vals = '5'\n  [../]\n[]\n\n[Executioner]\n  type = Transient\n  solve_type = 'NEWTON'\n  end_time = 5\n  dt = 1\n[]\n\n[Outputs]\n  exodus = true\n[]\n\n 427 Linear |R| = 8.321297e-09\n    428 Linear |R| = 8.243846e-09\n    429 Linear |R| = 8.168653e-09\n    430 Linear |R| = 8.109369e-09\nNonlinear solve did not converge due to DIVERGED_LINE_SEARCH iterations 0\n Solve Did NOT Converge!\n  Finished Solving                                                                       [ 81.61 s] [  325 MB]\nAborting as solve did not converge\n\nTime Step 1, time = 0.5, dt = 0.5\n 0 Nonlinear |R| = 4.066117e-04\n\n    Computing Jacobian.                                                                  [ 11.48 s] [  -20 MB]\n      0 Linear |R| = 4.066117e-04\n      1 Linear |R| = 3.610152e-04\n      2 Linear |R| = 3.524526e-04\n      3 Linear |R| = 3.472519e-04",
          "url": "https://github.com/idaholab/moose/discussions/22541",
          "updatedAt": "2022-11-15T19:20:23Z",
          "publishedAt": "2022-10-30T09:24:02Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "js-jixu"
                  },
                  "bodyText": "Maybe you can use some preconditioners to get the problem converged. For example, add the following in the [Executioner] block:\n    petsc_options_iname = '-pc_type -pc_factor_shift_type -pc_factor_mat_solver_type'\n    petsc_options_value = 'lu       NONZERO               superlu_dist'",
                  "url": "https://github.com/idaholab/moose/discussions/22541#discussioncomment-4011502",
                  "updatedAt": "2022-10-30T10:59:15Z",
                  "publishedAt": "2022-10-30T10:59:14Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "DYLDYLDYL"
                          },
                          "bodyText": "Thanks!\nHi, it`s still unconverged.\nTime Step 1, time = 1, dt = 1\n 0 Nonlinear |R| = 8.132233e-04\n\n    Computing Jacobian.                                                                  [ 11.24 s] [  321 MB]\n      0 Linear |R| = 8.132233e-04\n      1 Linear |R| = 2.434529e-14\nNonlinear solve did not converge due to DIVERGED_LINE_SEARCH iterations 0\n Solve Did NOT Converge!\n  Finished Solving                                                                       [ 74.64 s] [ 4620 MB]\nAborting as solve did not converge\n\nI hope you can add my wechat\uff1a13147778609\uff01",
                          "url": "https://github.com/idaholab/moose/discussions/22541#discussioncomment-4011997",
                          "updatedAt": "2022-10-30T13:07:44Z",
                          "publishedAt": "2022-10-30T13:07:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\ncan you turn off the linear search for now?\nIn executioner:\nline_search = none\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/22541#discussioncomment-4012145",
                          "updatedAt": "2022-10-30T13:42:01Z",
                          "publishedAt": "2022-10-30T13:42:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "DYLDYLDYL"
                          },
                          "bodyText": "Thanks\uff01\nIt`s still unconverged.\n48 Nonlinear |R| = 1.250104e-08\n\n    Computing Jacobian.                                                                  [ 11.43 s] [    0 MB]\n      0 Linear |R| = 1.250104e-08\n      1 Linear |R| = 2.749728e-14\n49 Nonlinear |R| = 1.208844e-08\n\n    Computing Jacobian.                                                                  [ 11.23 s] [    0 MB]\n      0 Linear |R| = 1.208844e-08\n      1 Linear |R| = 1.312762e-14\n50 Nonlinear |R| = 1.287461e-08\nNonlinear solve did not converge due to DIVERGED_MAX_IT iterations 50\n Solve Did NOT Converge!\n  Finished Solving                                                                       [2380.17 s] [ 6211 MB]\nAborting as solve did not converge\n\nTime Step 1, time = 0.5, dt = 0.5\n 0 Nonlinear |R| = 4.066117e-04",
                          "url": "https://github.com/idaholab/moose/discussions/22541#discussioncomment-4014676",
                          "updatedAt": "2022-10-31T01:46:21Z",
                          "publishedAt": "2022-10-31T01:46:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Can you try turning on automatic scaling?\nIs 1e-8 for the residual not converged? The default settings for accepting solutions are not applicable to every physics",
                          "url": "https://github.com/idaholab/moose/discussions/22541#discussioncomment-4015096",
                          "updatedAt": "2022-10-31T03:24:34Z",
                          "publishedAt": "2022-10-31T03:24:33Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "DYLDYLDYL"
                          },
                          "bodyText": "Thanks a lot \uff01Your answer solves my problem perfectly \uff01",
                          "url": "https://github.com/idaholab/moose/discussions/22541#discussioncomment-4017585",
                          "updatedAt": "2022-10-31T10:59:54Z",
                          "publishedAt": "2022-10-31T10:59:54Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Iterative Schwarz method for domain decomposition",
          "author": {
            "login": "salaudeen-ya"
          },
          "bodyText": "Hello,\nI have a 1D domain to be decomposed into two subdomains by solving the Poisson equation (at least) using the Additive Schwarz Algorithm.\nMy enquiries:\n\nCan this be solved with MOOSE?\nHow do I go about it? (The modules, multiapp solver, transfers etc).\n\nI have a little understanding of MOOSE as I have been able to use it solve some IB basic FEM analysis of Diffusion-Reaction-Source equation.\nI would be glad to have as much as as possible.\nThank you!\n----The Equation---\n\n---The Algorithm-------",
          "url": "https://github.com/idaholab/moose/discussions/22516",
          "updatedAt": "2022-11-15T18:06:21Z",
          "publishedAt": "2022-10-27T17:34:46Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nWe support using asm preconditioning from petsc if you want to try that with a simple Poisson problem\nThere are examples for poisson in the tutorial and the petsc options are set in the executioner block\nhttps://mooseframework.inl.gov/source/executioners/Steady.html\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/22516#discussioncomment-3984684",
                  "updatedAt": "2022-10-27T17:59:44Z",
                  "publishedAt": "2022-10-27T17:59:43Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "otherwise if you want to write your own custom iterative method, then this is possible in MOOSE as well",
                          "url": "https://github.com/idaholab/moose/discussions/22516#discussioncomment-3984693",
                          "updatedAt": "2022-10-27T18:00:39Z",
                          "publishedAt": "2022-10-27T18:00:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "salaudeen-ya"
                          },
                          "bodyText": "Hello, thank you for your response.\nI understand I can use the petsc in the executioner block of the input file. I am actually looking for how to do this with Multi-App System where I have two different systems that run iteratively, and only transfer the solution at the interface. The emphasis is on using Mullti-App  (with different input files) and transfer so as to have the same solution as when the problem is solved with a single input file.\nFor instance, I would like to solve this problem between [-1,0] using two subdomains [-1,0] and [0,1] with interface at point (0,0). It admits DirichletBC on the left and NeumanBC on the right. The idea is to enforce u and v at the interface, and to make the flux of the two variables to be the negative of the other at the interface. For some reasons, I amnot getting the results as expected and my first domain (sub1) is returning zero values for the flux and u. So far, I have the following trials:\n---- Sub 1 ---------\n[Mesh]\n  type = GeneratedMesh\n  dim = 1\n  nx = 64\n  xmin = -1\n  xmax = 0\n[]\n\n[Variables]\n  [u]\n    order = FIRST\n    family = LAGRANGE\n  []\n[]\n\n[AuxVariables]\n  [flux_u]\n    order = CONSTANT\n    family = MONOMIAL\n  \n  []\n[]\n\n[Kernels]\n  [diffusion]\n    type = MatDiffusion\n    variable = u\n    diffusivity = 2\n  []\n[]\n\n[AuxKernels]\n  [fluxKernel]\n    type = DiffusionFluxAux\n    diffusivity = 2\n    variable = flux_u\n    diffusion_variable = u\n    component = x\n  []\n[]\n\n[BCs]\n  [leftBC] \n    type = DirichletBC\n    variable = u\n    boundary = 'left'\n    value = 0\n  []\n\n  [rightBC_Neumann] \n    type = FunctionNeumannBC\n    variable = u\n    boundary = 'right' \n    function =  BC_u_right_Neumann\n    #preset = true\n  []\n\n  [rightBC_Dirichlet] \n    type = FunctionDirichletBC\n    variable = u\n    boundary = 'right' \n    function = BC_u_right_Dirichlet\n    preset = true\n  []\n[]\n\n\n[Postprocessors]\n  [right_uflux]\n    type = PointValue\n    variable = 'flux_u'\n    point = '0 0 0'\n   []\n  [right_uval]\n    type = PointValue\n    variable = 'u'\n    point = '0 0 0'\n   []\n  [vval_left]\n    type = Receiver\n  []\n  [v_flux_left]\n    type = Receiver\n  []\n[]\n\n[Functions]\n  [BC_u_right_Neumann]\n    type = ParsedFunction\n    value = '-a'\n    vars = 'a'\n    vals = 'v_flux_left'\n  []\n  [BC_u_right_Dirichlet]\n    type = ParsedFunction\n    value = 'b'\n    vars = 'b'\n    vals = 'vval_left'\n  []\n[]\n\n[Executioner]\n  type = Steady\n  solve_type = 'PJFNK'\n\n  fixed_point_max_its = 20\n  nl_abs_tol = 1e-10\n  fixed_point_rel_tol = 1e-16\n  fixed_point_abs_tol = 1e-10\n\n  petsc_options_iname = '-pc_type -pc_hypre_type'\n  petsc_options_value = 'hypre boomeramg'\n[]\n\n[Outputs]\n  exodus = true\n  #csv = true\n  console = true\n[]\n\n\n[MultiApps]\n  [sub]\n    type = FullSolveMultiApp\n    execute_on = 'timestep_begin'\n    positions = '0 0 0'\n    input_files = sub2.i \n  []\n[]\n\n\n[Transfers]\n  [flux_ut]\n    type = MultiAppPostprocessorTransfer\n    to_multi_app = sub\n    from_postprocessor = right_uflux\n    to_postprocessor = u_flux_right\n  []\n  [uval_t]\n    type = MultiAppPostprocessorTransfer\n    to_multi_app = sub\n    from_postprocessor = right_uval\n    to_postprocessor = uval_right\n  []\n  [flux_vt]\n    type = MultiAppPostprocessorTransfer\n    from_multi_app = sub\n    reduction_type = average\n    from_postprocessor = left_vflux\n    to_postprocessor = v_flux_left\n  []\n  [vval_t]\n    type = MultiAppPostprocessorTransfer\n    from_multi_app = sub\n    reduction_type = average\n    from_postprocessor = left_vval\n    to_postprocessor = vval_left\n  [] \n[]\n\n-----Sub 2 -----\n[Mesh]\n  type = GeneratedMesh\n  dim = 1\n  nx = 64\n  xmin = 0\n  xmax = 1\n\n[]\n\n[Variables]\n  [v]\n    order = FIRST\n    family = LAGRANGE\n  []\n[]\n\n[AuxVariables]\n  [flux_v]\n    order = CONSTANT\n    family = MONOMIAL\n  []\n[]\n\n[Kernels]\n  [diffusion]\n    type = MatDiffusion\n    variable = v\n    diffusivity = 2\n  []\n[]\n\n[AuxKernels]\n  [fluxKernel]\n    type = DiffusionFluxAux\n    diffusivity = 2\n    variable = flux_v\n    diffusion_variable = v\n    component = x\n  []\n[]\n\n[BCs]\n  [rightBC] \n    type = NeumannBC\n    variable = v\n    boundary = 'right'\n    value = 3\n  []\n\n  [leftBC_Neumann] \n    type = FunctionNeumannBC\n    variable = v\n    boundary = 'left' \n    function = BC_vleft_Neumann\n  []\n\n  [leftBC_Dirichlet] \n    type = FunctionDirichletBC\n    variable = v\n    boundary = 'left' \n    function = BC_vleft_Dirichlet\n    preset = true\n  []\n\n[]\n\n[Postprocessors]\n  [left_vflux]\n    type = PointValue\n    variable = 'flux_v'\n    point = '0 0 0'\n   []\n  [left_vval]\n    type = PointValue\n    variable = 'v'\n    point = '0 0 0'\n   []\n  [uval_right]\n    type = Receiver\n  []\n  [u_flux_right]\n    type = Receiver\n  []\n[]\n\n[Functions]\n  [BC_vleft_Neumann]\n    type = ParsedFunction\n    value = '-a'\n    vars = 'a'\n    vals = 'u_flux_right'\n\n  []\n  [BC_vleft_Dirichlet]\n    type = ParsedFunction\n    value = 'b'\n    vars = 'b'\n    vals = 'uval_right'\n  []\n[]\n\n\n[Executioner]\n  type = Steady\n  solve_type = 'PJFNK'\n\n  fixed_point_max_its = 20\n  nl_abs_tol = 1e-10\n  fixed_point_rel_tol = 1e-16\n  fixed_point_abs_tol = 1e-10\n\n  petsc_options_iname = '-pc_type -pc_hypre_type'\n  petsc_options_value = 'hypre boomeramg'\n[]\n\n[Outputs]\n  exodus = true\n  #csv = true\n  console = true\n[]\n\n----- The reference domain (when solved as a single domain) -------\n[Mesh]\n  type = GeneratedMesh\n  dim = 1\n  nx = 64\n  xmin = -1\n  xmax = 1\n\n[]\n\n[Variables]\n  [v]\n    order = FIRST\n    family = LAGRANGE\n  []\n[]\n\n[AuxVariables]\n  [flux_v]\n    order = CONSTANT\n    family = MONOMIAL\n  []\n[]\n\n[Kernels]\n  [diffusion]\n    type = MatDiffusion\n    variable = v\n    diffusivity = 2\n  []\n[]\n\n[AuxKernels]\n  [fluxKernel]\n    type = DiffusionFluxAux\n    diffusivity = 2\n    variable = flux_v\n    diffusion_variable = v\n    component = x\n  []\n[]\n\n[BCs]\n  [rightBC] \n    type = NeumannBC\n    variable = v\n    boundary = 'right'\n    value = 3\n  []\n\n  [leftBC] \n    type = DirichletBC\n    variable = v\n    boundary = 'left'\n    value = 0\n  []\n\n[]\n\n\n[Executioner]\n  type = Steady\n  solve_type = 'PJFNK'\n[]\n\n[Outputs]\n  exodus = true\n  #csv = true\n  console = true\n[]\n\n\n\nThank you! I hope someone can help spot the problem.",
                          "url": "https://github.com/idaholab/moose/discussions/22516#discussioncomment-3993206",
                          "updatedAt": "2022-10-28T14:15:36Z",
                          "publishedAt": "2022-10-28T13:51:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so on the right of subapp1 you have a dirichlet and a neumann BC.\nIs that desired? Are you trying to have like a robin BC ?",
                          "url": "https://github.com/idaholab/moose/discussions/22516#discussioncomment-3995656",
                          "updatedAt": "2022-10-28T17:21:45Z",
                          "publishedAt": "2022-10-28T17:21:45Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "salaudeen-ya"
                  },
                  "bodyText": "Yes. Exactly. And I've tried the VacuumBC too which doesn't give me the\ndesired results.\n\n\nHow do I go about doing that?\n\u2026\nOn Fri, Oct 28, 2022, 1:21 PM Guillaume Giudicelli ***@***.***> wrote:\n so on the right of subapp1 you have a dirichlet and a neumann BC.\n Is that desired? Are you trying to have like a robin BC ?\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#22516 (reply in thread)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/AJCC7Q3XPJKOBFRM6J2LBPDWFQDTJANCNFSM6AAAAAARQJLYW4>\n .\n You are receiving this because you authored the thread.Message ID:\n ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/22516#discussioncomment-3996194",
                  "updatedAt": "2022-10-28T18:14:24Z",
                  "publishedAt": "2022-10-28T18:14:23Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I m actually not sure, I just dont think it's with Dirichlet + Neumann that we make those. This should just set the value of the variable, and the flux, as if you had two equations at the boundary.\nWe have robinBCs in the electromagnetics module, they are fairly complicated though. We have other RobinBC in the test object but they dont look right\n@cticenhour do you know how to make the simple case RobinBC ?",
                          "url": "https://github.com/idaholab/moose/discussions/22516#discussioncomment-3996459",
                          "updatedAt": "2022-10-28T18:30:59Z",
                          "publishedAt": "2022-10-28T18:30:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "cticenhour"
                          },
                          "bodyText": "Yeah, we don't add the impact of Dirichlet + Neumann like we do with kernels. In fact, I believe Dirichlet actually manipulates the matrix by zeroing out matrix rows corresponding to the discretized variable at those boundary locations.\nThere is a very simple (maybe too simple) RobinBC in the Squirrel application (a dependency of Zapdos) - see the link here. This only takes in a 1D \"velocity\" value, but could serve as a guide basis for a more complicated custom IntegratedBC object that you could create to fit your scenario.",
                          "url": "https://github.com/idaholab/moose/discussions/22516#discussioncomment-3997085",
                          "updatedAt": "2022-10-28T19:59:47Z",
                          "publishedAt": "2022-10-28T19:59:45Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "I may be completely misunderstanding the discussion here. But a ConvectiveHeatFluxBC is the simplest Robin BC based on the canonical definition: https://en.m.wikipedia.org/wiki/Robin_boundary_condition",
                          "url": "https://github.com/idaholab/moose/discussions/22516#discussioncomment-3997853",
                          "updatedAt": "2022-10-28T20:55:21Z",
                          "publishedAt": "2022-10-28T20:55:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "salaudeen-ya"
                          },
                          "bodyText": "I'll take a look at all the suggestions and see what I can make of it.\nThank you all..!",
                          "url": "https://github.com/idaholab/moose/discussions/22516#discussioncomment-3999980",
                          "updatedAt": "2022-10-29T02:23:50Z",
                          "publishedAt": "2022-10-29T02:23:50Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "salaudeen-ya"
                  },
                  "bodyText": "I'm tried to get a Robin on the right of sub1 and left of sub2, which is\nthe interface. Not really a direct dirichlet or Neumann on that interface.\n\nOn Fri, Oct 28, 2022, 2:14 PM Yusuf A. SALAUDEEN <\n***@***.***> wrote:\n\u2026\n Yes. Exactly. And I've tried the VacuumBC too which doesn't give me the\n desired results.\n\n\n How do I go about doing that?\n\n On Fri, Oct 28, 2022, 1:21 PM Guillaume Giudicelli <\n ***@***.***> wrote:\n\n> so on the right of subapp1 you have a dirichlet and a neumann BC.\n> Is that desired? Are you trying to have like a robin BC ?\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <#22516 (reply in thread)>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AJCC7Q3XPJKOBFRM6J2LBPDWFQDTJANCNFSM6AAAAAARQJLYW4>\n> .\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***>\n>",
                  "url": "https://github.com/idaholab/moose/discussions/22516#discussioncomment-3996248",
                  "updatedAt": "2022-10-28T18:16:30Z",
                  "publishedAt": "2022-10-28T18:16:29Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "salaudeen-ya"
                  },
                  "bodyText": "I was wondering if I could take a look at the ConvectiveHeatFluxBC file but\nit's registered in Heat Conduction App and not in MOOSE App.\n\nHow can I get that please?\n\nThanks.\n\u2026\nOn Fri, Oct 28, 2022, 4:55 PM Gary (Tianchen) Hu ***@***.***> wrote:\n I may be completely misunderstanding the discussion here. But a\n ConvectiveHeatFluxBC is the simplest Robin BC based on the canonical\n definition: https://en.m.wikipedia.org/wiki/Robin_boundary_condition\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#22516 (reply in thread)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/AJCC7QZ2VJ7XI2IISDE7EGDWFQ4UJANCNFSM6AAAAAARQJLYW4>\n .\n You are receiving this because you authored the thread.Message ID:\n ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/22516#discussioncomment-4014524",
                  "updatedAt": "2022-10-31T01:08:13Z",
                  "publishedAt": "2022-10-31T01:08:12Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "You can include that module in your application by modifying the makefile.\n\nIf you just want to look it s in modules/heat_conduction/src/bcs\n\u2026\n Le 30 oct. 2022 \u00e0 20:08, Yusuf A. Salaudeen ***@***.***> a \u00e9crit :\n\n \ufeff\n I was wondering if I could take a look at the ConvectiveHeatFluxBC file but\n it's registered in Heat Conduction App and not in MOOSE App.\n\n How can I get that please?\n\n Thanks.\n\n On Fri, Oct 28, 2022, 4:55 PM Gary (Tianchen) Hu ***@***.***>\n wrote:\n\n > I may be completely misunderstanding the discussion here. But a\n > ConvectiveHeatFluxBC is the simplest Robin BC based on the canonical\n > definition: https://en.m.wikipedia.org/wiki/Robin_boundary_condition\n >\n > \u2014\n > Reply to this email directly, view it on GitHub\n > <#22516 (reply in thread)>,\n > or unsubscribe\n > <https://github.com/notifications/unsubscribe-auth/AJCC7QZ2VJ7XI2IISDE7EGDWFQ4UJANCNFSM6AAAAAARQJLYW4>\n > .\n > You are receiving this because you authored the thread.Message ID:\n > ***@***.***>\n >\n \u2014\n Reply to this email directly, view it on GitHub, or unsubscribe.\n You are receiving this because you commented.",
                  "url": "https://github.com/idaholab/moose/discussions/22516#discussioncomment-4014600",
                  "updatedAt": "2022-10-31T01:29:21Z",
                  "publishedAt": "2022-10-31T01:29:20Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "salaudeen-ya"
                  },
                  "bodyText": "Thank you.\n\nOn Sun, Oct 30, 2022, 9:29 PM Guillaume Giudicelli ***@***.***>\nwrote:\n\u2026\n You can include that module in your application by modifying the makefile.\n\n If you just want to look it s in modules/heat_conduction/src/bcs\n\n > Le 30 oct. 2022 \u00e0 20:08, Yusuf A. Salaudeen ***@***.***> a \u00e9crit :\n >\n > \ufeff\n > I was wondering if I could take a look at the ConvectiveHeatFluxBC file\n but\n > it's registered in Heat Conduction App and not in MOOSE App.\n >\n > How can I get that please?\n >\n > Thanks.\n >\n > On Fri, Oct 28, 2022, 4:55 PM Gary (Tianchen) Hu ***@***.***>\n > wrote:\n >\n > > I may be completely misunderstanding the discussion here. But a\n > > ConvectiveHeatFluxBC is the simplest Robin BC based on the canonical\n > > definition: https://en.m.wikipedia.org/wiki/Robin_boundary_condition\n > >\n > > \u2014\n > > Reply to this email directly, view it on GitHub\n > > <\n #22516 (reply in thread)\n >,\n > > or unsubscribe\n > > <\n https://github.com/notifications/unsubscribe-auth/AJCC7QZ2VJ7XI2IISDE7EGDWFQ4UJANCNFSM6AAAAAARQJLYW4\n >\n > > .\n > > You are receiving this because you authored the thread.Message ID:\n > > ***@***.***>\n > >\n > \u2014\n > Reply to this email directly, view it on GitHub, or unsubscribe.\n > You are receiving this because you commented.\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#22516 (comment)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/AJCC7Q2K6CSX3QRS63ZDHSLWF4OHVANCNFSM6AAAAAARQJLYW4>\n .\n You are receiving this because you authored the thread.Message ID:\n ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/22516#discussioncomment-4014955",
                  "updatedAt": "2022-10-31T02:51:43Z",
                  "publishedAt": "2022-10-31T02:51:43Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Error in compilation while plotting using python script",
          "author": {
            "login": "bha112"
          },
          "bodyText": "Hello,\nUsing this command in my desktop for compilation of the .json file\n **python ../../../python/make_histogram.py main_out.json* -v results:T_avg:value --xlabel 'Average Temperature'** \n\nTraceback (most recent call last):\n  File \"/home/lab_3/projects/moose/modules/stochastic_tools/examples/parameter_study/gold/../../../python/make_histogram.py\", line 17, in <module>  import mooseutils .\n\nModuleNotFoundError: No module named 'mooseutils'\n\nIts seems python script is not working (make_histogram.py).\nCould you please help me to solve the issue?\nBhavesh",
          "url": "https://github.com/idaholab/moose/discussions/22540",
          "updatedAt": "2022-10-31T11:45:27Z",
          "publishedAt": "2022-10-30T06:53:14Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou need to add moose/python to your pythonpath so python knows where to find mooseutils\nlike this:\nexport PYTHONPATH=$PYTHONPATH:/home/lab_3/projects/moose/python\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/22540#discussioncomment-4012201",
                  "updatedAt": "2022-10-30T13:56:30Z",
                  "publishedAt": "2022-10-30T13:56:29Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "bha112"
                          },
                          "bodyText": "Thanks for your response.",
                          "url": "https://github.com/idaholab/moose/discussions/22540#discussioncomment-4012475",
                          "updatedAt": "2022-10-30T15:03:41Z",
                          "publishedAt": "2022-10-30T15:03:40Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Best Transfer Option for Large Deformation Transfer",
          "author": {
            "login": "maxnezdyur"
          },
          "bodyText": "The images below present the problem I am having with the transfer. When there is a large deformation that transfer messes up. I tried a few different transfers, but I can't seem to get them to work. I am trying to transfer just a \"1\" from the solid onto the background mesh. This is to track the solid location within the fluid for an immersed FSI system I am trying to create. I made sure to use  displaced_source_mesh = true",
          "url": "https://github.com/idaholab/moose/discussions/22119",
          "updatedAt": "2022-11-15T19:17:33Z",
          "publishedAt": "2022-09-16T18:17:45Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Have you tried to set use_displaced_mesh to true in the transfer?",
                  "url": "https://github.com/idaholab/moose/discussions/22119#discussioncomment-3665058",
                  "updatedAt": "2022-09-16T18:42:40Z",
                  "publishedAt": "2022-09-16T18:42:40Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "Yes, that didn't change anything. Also to keep in mind, the parent app doesn't have any displacements just the sub-app, so I don't think use_displaced_mesh would do anything anyways. I also tried this by switching the parent and sub app and tried use_displaced_mesh  and that didn't help either.",
                          "url": "https://github.com/idaholab/moose/discussions/22119#discussioncomment-3665084",
                          "updatedAt": "2022-09-16T18:48:24Z",
                          "publishedAt": "2022-09-16T18:47:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I ll have to dig into it to check if we actually support subapp displacements in transfers.\nWhich transfer are you using? There are lots of options and some might support things better",
                          "url": "https://github.com/idaholab/moose/discussions/22119#discussioncomment-3665112",
                          "updatedAt": "2022-09-16T18:51:34Z",
                          "publishedAt": "2022-09-16T18:51:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "I used MultiAppShapeEvaluationTransfer for the pictures above. I wanted to use MultiAppProjectionTransfer specifically, but for that one, the \"indicator\" never moved in the background mesh.",
                          "url": "https://github.com/idaholab/moose/discussions/22119#discussioncomment-3665141",
                          "updatedAt": "2022-09-16T18:55:39Z",
                          "publishedAt": "2022-09-16T18:55:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "What do you mean by the indicator?\nCan you try GeometricInterpolation and NearestNode?",
                          "url": "https://github.com/idaholab/moose/discussions/22119#discussioncomment-3665222",
                          "updatedAt": "2022-09-16T19:12:49Z",
                          "publishedAt": "2022-09-16T19:12:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "The indicator is the variable that I am transferring from the solid to the background mesh. Both of the options you describe \"work\" but I do not want the background mesh that is not near the solid to have the indicator value of 1. Both of the options you gave would make the entire background mesh equal to 1, if I didn't use my \"hacky\" method. This \"hacky\" method makes it so the boundary of the solid has value 0 so the nearest node for a background node that is not inside the solid is on the boundary. Below shows it works, but this method has faults because of the \"hacky\" method.",
                          "url": "https://github.com/idaholab/moose/discussions/22119#discussioncomment-3665264",
                          "updatedAt": "2022-09-16T19:20:08Z",
                          "publishedAt": "2022-09-16T19:20:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I think you'd need block restriction to make it work without the accuracy issue\nvery few transfers have it, but you can try the ones in this PR which are yet to be merged\n#17417",
                          "url": "https://github.com/idaholab/moose/discussions/22119#discussioncomment-3665955",
                          "updatedAt": "2022-09-16T21:56:51Z",
                          "publishedAt": "2022-09-16T21:56:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "I am a bit confused. The background mesh is all 1 block. I need to use the transfer to determine which element gets changed to a new block to model the immersed solid. I can't block restrict the transfer because I do not which element is part of the solid or part of the real fluid before I transfer.",
                          "url": "https://github.com/idaholab/moose/discussions/22119#discussioncomment-3665984",
                          "updatedAt": "2022-09-16T22:03:39Z",
                          "publishedAt": "2022-09-16T22:03:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Is it possible for you to share a branch with your inputs (and any necessary code to run the inputs)?",
                          "url": "https://github.com/idaholab/moose/discussions/22119#discussioncomment-3666045",
                          "updatedAt": "2022-09-16T22:20:28Z",
                          "publishedAt": "2022-09-16T22:20:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "I\u2019ll create a minimal working example and send that over. I would have to share two branches and the current problem is solving the INS equations and that would be too long to run to debug anyway. Thanks for looking into this!",
                          "url": "https://github.com/idaholab/moose/discussions/22119#discussioncomment-3666071",
                          "updatedAt": "2022-09-16T22:27:57Z",
                          "publishedAt": "2022-09-16T22:27:57Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "While @lindsayad looks into the issue with transfers involving displaced mesh, one workaround for the time being is to run both apps on the undisplaced mesh, and use the displacements to advect the indicator function.",
                          "url": "https://github.com/idaholab/moose/discussions/22119#discussioncomment-3666185",
                          "updatedAt": "2022-09-16T23:10:32Z",
                          "publishedAt": "2022-09-16T23:10:31Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "maxnezdyur"
                  },
                  "bodyText": "Next time I will just share a branch, I didn't know I couldn't paste in a text file. In the parent file, I commented out a transfer that works with the method I described above but not perfectly.\nParent File\n[Mesh]\n  [gen]\n    type = GeneratedMeshGenerator\n    dim = 2\n    xmin = -1.0\n    xmax = 3.0\n    ymin = 0.0\n    ymax = 1.0\n    nx = 1000\n    ny = 250\n    elem_type = QUAD4\n  []\n[]\n[Problem]\nkernel_coverage_check = false\nmaterial_coverage_check = false\nskip_nl_system_check = false\n[]\n\n\n[AuxVariables]\n  [indicator]\n    order = FIRST\n    family = LAGRANGE\n    initial_condition = 0.0\n  []\n[]\n\n[Variables]\n[dummy]\n\n[]\n[]\n\n[Materials]\n[]\n\n[Executioner]\n  type = Transient\n  solve_type = 'NEWTON'\n  # Run for 100+ timesteps to reach stey state.\n\n  dt = 1e-2\n  end_time = 1\n  dtmin = 1.0e-7\n  # Note: -snes_ksp_ew seems to le to more nonlinear iterations, which isn't ideal\n  # when compute_jacobian() is so expensive for this problem.\n  petsc_options = '-snes_converged_reason -ksp_converged_reason'\n\n  # # Direct solver\n  petsc_options_iname = '-pc_type -pc_factor_shift_type -pc_factor_mat_solver_package'\n  petsc_options_value = 'lu NONZERO superlu_dist'\n\n  # petsc_options_iname = '-pc_type -pc_hypre_type'\n  # petsc_options_value = 'hypre boomeramg'\n  # residual_and_jacobian_together = true\n  line_search = 'bt'\n  nl_rel_tol = 1e-6\n  nl_abs_tol = 2e-7\n  nl_max_its = 5\n  l_max_its = 25\n\n  automatic_scaling = true\n  off_diagonals_in_auto_scaling = true\n  compute_scaling_once = false\n  # scaling_group_variables = 'velocity p'\n  [TimeIntegrator]\n    type = BDF2\n  []\n\n[]\n\n[Outputs]\n  # [exo]\n  #   type = Exodus\n  # []\n  exodus = true\n  print_linear_residuals = false\n[]\n\n[MultiApps]\n  [solid_domain]\n    type = TransientMultiApp\n    execute_on = \"INITIAL TIMESTEP_END\"\n    # positions = '0 0 0'\n    input_files = solid_multiapp.i\n    use_displaced_mesh = true\n    # sub_cycling = true\n    catch_up = true\n    max_catch_up_steps = 5\n    keep_solution_during_restore = true\n  []\n[]\n\n[Transfers]\n\n  # [push_indicator]\n  #   type = MultiAppGeometricInterpolationTransfer\n  #   # Transfer from the sub-app from this app\n  #   from_multi_app =  solid_domain\n  #   # The name of the variable in this app\n  #   source_variable = solid_indicator\n  #   # The name of the auxiliary variable in the sub-app\n  #   variable = indicator\n  #   displaced_source_mesh = true\n  #   use_displaced_mesh = true\n  #   num_points = 1\n  # []\n  [push_indicator]\n    type = MultiAppShapeEvaluationTransfer\n    # Transfer from the sub-app from this app\n    from_multi_app =  solid_domain\n    # The name of the variable in this app\n    source_variable = solid_indicator\n    # The name of the auxiliary variable in the sub-app\n    variable = indicator\n    displaced_source_mesh = true\n    use_displaced_mesh = true\n  []\n[]\n\n\nChild below\nbeta = 0.25\ngamma = 0.5\n\n[GlobalParams]\n  displacements = 'disp_x disp_y'\n[]\n\n[Mesh]\n  [gen]\n    type = GeneratedMeshGenerator\n    dim = 2\n    xmin = 0\n    ymin = 0\n    xmax = 0.2\n    ymax = 0.5\n    nx = 50\n    ny = 150\n    elem_type = QUAD4\n  []\n[]\n[Variables]\n  [disp_x]\n    order = FIRST\n  []\n  [disp_y]\n    order = FIRST\n  []\n[]\n\n[AuxVariables] # variables that are calculated for output\n\n  [solid_indicator]\n    order = FIRST\n    family = LAGRANGE\n    [AuxKernel]\n      type = ConstantAux\n      variable = solid_indicator\n      value = 0.0\n      boundary = 'left right top'\n      execute_on = 'INITIAL TIMESTEP_END'\n    []\n    initial_condition = 1.0\n  []\n[]\n\n[Modules/TensorMechanics/DynamicMaster]\n  [all]\n    # displacements = 'disp_x disp_y'\n    add_variables = true\n    # new_system = true\n    incremental = true\n    strain = FINITE\n    decomposition_method = EigenSolution\n    # hht_alpha = 0.25\n  []\n[]\n\n[Materials]\n  [elastic_tensor]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 10000.0\n    poissons_ratio = 0.3\n    use_displaced_mesh = true\n  []\n  [stress]\n    type = ComputeFiniteStrainElasticStress\n  []\n  [density]\n    type = GenericConstantMaterial\n    prop_names = 'density'\n    prop_values = '1'\n    use_displaced_mesh = true\n  []\n  [constant_stress]\n    type = GenericConstantRankTwoTensor\n    tensor_values = '100'\n    tensor_name = test_tensor\n  []\n[]\n\n[BCs]\n  [hold_x]\n    type = DirichletBC\n    boundary = bottom\n    variable = disp_x\n    value = 0\n    use_displaced_mesh = true\n    save_in = div_stess_x\n  []\n  [hold_y]\n    type = DirichletBC\n    boundary = bottom\n    variable = disp_y\n    value = 0\n    use_displaced_mesh = true\n    save_in = div_stess_y\n  []\n  [Pressure]\n    [push_left]\n      boundary = left\n      factor = 100\n    []\n  []\n[]\n\n[Preconditioning]\n  [SMP]\n    type = SMP\n    full = true\n  []\n[]\n[Executioner]\n  type = Transient\n  end_time = 10\n  dt = 1e-2\n  solve_type = 'NEWTON'\n  petsc_options = '-snes_converged_reason -ksp_converged_reason -snes_ksp_ew'\n  petsc_options_iname = '-pc_type -pc_factor_mat_solver_type -pc_factor_shift_type -pc_factor_shift_amount'\n  petsc_options_value = 'lu       superlu_dist                  NONZERO               1e-15'\n  nl_max_its = 40\n  l_max_its = 15\n  line_search = 'none'\n  nl_abs_tol = 1e-5\n  nl_rel_tol = 1e-4\n  automatic_scaling = true\n  [TimeIntegrator]\n    type = NewmarkBeta\n    beta = ${beta}\n    gamma = ${gamma}\n  []\n[]\n\n[Outputs]\n  exodus = true\n  print_linear_residuals = false\n  print_linear_converged_reason = false\n[]\n\n# [MultiApps]\n#   [fluid_domain]\n#     type = TransientMultiApp\n#     execute_on = \"INITIAL TIMESTEP_END\"\n#     positions = '0 0 0'\n#     input_files = real_fluid_multiapp.i\n#     use_displaced_mesh = true\n#   []\n# []\n\n# [Transfers]\n#   [push_indicator]\n#     type = MultiAppMeshFunctionTransfer\n#     # Transfer to the sub-app from this app\n#     to_multi_app = fluid_domain\n#     # The name of the variable in this app\n#     source_variable = solid_indicator\n#     # The name of the auxiliary variable in the sub-app\n#     variable = indicator\n#     displaced_source_mesh = true\n#     displaced_target_mesh = true\n#     bbox_factor = 0.5\n#     use_displaced_mesh = true\n#   []\n# []",
                  "url": "https://github.com/idaholab/moose/discussions/22119#discussioncomment-3671169",
                  "updatedAt": "2022-09-17T20:28:28Z",
                  "publishedAt": "2022-09-17T20:28:28Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Any update on this?",
                  "url": "https://github.com/idaholab/moose/discussions/22119#discussioncomment-3937874",
                  "updatedAt": "2022-10-21T23:59:08Z",
                  "publishedAt": "2022-10-21T23:59:07Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "I have not yet investigated",
                          "url": "https://github.com/idaholab/moose/discussions/22119#discussioncomment-3952087",
                          "updatedAt": "2022-10-24T15:04:06Z",
                          "publishedAt": "2022-10-24T15:04:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Ok, jotting some notes down here:\n\nlibMesh node ID 89365 (exodus node ID 89366) is an example of a to-node getting a 0 value when it shouldn't\nThis corresponds to the 4833 point that we want to query the from-mesh for (e.g. index 4832 in the incoming/outgoing points vector)\nWhen asking the mesh function to find the from-element containing that to-node we are getting back a null result, indicating that it thinks the point is out of the mesh\nWhen we get to the active nodes in TreeNode, we do get bounds_point but we do not get any elements that contains_point\n\nI don't really know why this would be a hard case ... these are linear elements so I would expect inverse mapping from the physical space to the reference space to essentially be exact.\n@roystgnr do you have any thoughts on what might be the issue here? Pasted below is a figure. The bottom is the sub-application field that we are transferring to the main application on top. You can see that we are getting these \"teeth\" in the transfer with the blue gaps in the teeth essentially indicating a failed transfer",
                          "url": "https://github.com/idaholab/moose/discussions/22119#discussioncomment-3989700",
                          "updatedAt": "2022-10-28T07:38:55Z",
                          "publishedAt": "2022-10-28T06:56:54Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "There is definitely some undesirable stuff happening here. More than enough to create an issue. Any further investigation will be discussed on the issue: #22534",
                          "url": "https://github.com/idaholab/moose/discussions/22119#discussioncomment-3996594",
                          "updatedAt": "2022-10-28T19:00:56Z",
                          "publishedAt": "2022-10-28T18:54:34Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Recovery of a multiapp simulation",
          "author": {
            "login": "xueyang94"
          },
          "bodyText": "Using FullSolveMultiApp, I have a main-app input file that calls one sub-app file. I also have a checkpoint in the output block of the main-app. My simulation crashed after the sub-app and one time step of the main-app finished. Therefore, the only checkpoint folder generated is 0000_mesh.cpr. When I use the recovery functionality by phase_field-opt -i main.i --recover , I got an error below. It seems like the sub-app (named joined_phase) is run again even though it has already finished executing before crashing. How to fix this error and continue the simulation? Thanks.\n*** Info ***\nUsing /scratch/wuxuey/inputs/nogrgr/nogrgr_main_out_cp/0000 for recovery.\ufffd[39m\n\ufffd[36mjoined_phase0: \ufffd[39mDeleting Remote Elements                                                                 [\ufffd[33m  7.37 s\ufffd[39m] [\ufffd[33m    1 MB\ufffd[39m]\n  Finished Instantiating Sub-Apps                                                        [\ufffd[33m 11.05 s\ufffd[39m] [\ufffd[33m   59 MB\ufffd[39m]\nFinished Setting Up                                                                      [\ufffd[33m 19.75 s\ufffd[39m] [\ufffd[33m  223 MB\ufffd[39m]\nFramework Information:\nMOOSE Version:           git commit baece37 on 2022-09-06\nLibMesh Version:         5fb5bc70cf1b8894b363e5678171ff90c10472bc\nPETSc Version:           3.16.5\nSLEPc Version:           3.16.2\nCurrent Time:            Sun Oct  9 11:56:10 2022\nExecutable Timestamp:    Tue Sep  6 18:56:07 2022\n\nParallelism:\n  Num Processors:          211\n  Num Threads:             1\n\nMesh: \n  Parallel Type:           distributed\n  Mesh Dimension:          2\n  Spatial Dimension:       2\n  Nodes:                   \n    Total:                 121043\n    Local:                 413\n    Min/Max/Avg:           303/3085/573\n  Elems:                   \n    Total:                 115710\n    Local:                 352\n    Min/Max/Avg:           331/2792/548\n  Num Subdomains:          1\n  Num Partitions:          1\n  Partitioner:             parmetis\n\nNonlinear System:\n  Num DOFs:                4115462\n  Num Local DOFs:          14042\n  Num Constrained DOFs:    319464\n  Local Constrained DOFs:  0\n  Variables:               { \"o\" \"cr\" \"mn\" \"fe\" \"o1\" ... \"lambda\" \"muo\" \"mucr\" \"mumn\" \"mufe\" } \n  Finite Element Types:    \"LAGRANGE\" \n  Approximation Orders:    \"FIRST\" \n\nAuxiliary System:\n  Num DOFs:                1762315\n  Num Local DOFs:          5585\n  Num Constrained DOFs:    46980\n  Local Constrained DOFs:  0\n  Variables:               \"va\" { \"bnds\" \"bnds_main\" \"etaa0\" \"etaa1\" \"etaa2\" } { \"etaa0_marker\" \"etaa1_marker\" \n                             \"etaa2_marker\" \"eta1_marker\" \"eta3_marker\" \"eta4_marker\" \"eta5_marker\" \"eta6_marker\" \n                             \"combo\" } \n  Finite Element Types:    \"MONOMIAL\" \"LAGRANGE\" \"MONOMIAL\" \n  Approximation Orders:    \"CONSTANT\" \"FIRST\" \"CONSTANT\" \n\nExecution Information:\n  Executioner:             Transient\n  TimeStepper:             IterationAdaptiveDT\n  Solver Mode:             NEWTON\n  MOOSE Preconditioner:    SMP\n\n\n    Restoring Restart Data\n      Finished Reading RestartableData                                                   [\ufffd[33m  1.19 s\ufffd[39m] [\ufffd[33m  197 MB\ufffd[39m]\n    Finished Restoring Restart Data                                                      [\ufffd[33m  1.19 s\ufffd[39m] [\ufffd[33m  197 MB\ufffd[39m]\n\ufffd[36mjoined_phase0: \ufffd[39mParallelism:\n\ufffd[36mjoined_phase0: \ufffd[39m  Num Processors:          211\n\ufffd[36mjoined_phase0: \ufffd[39m  Num Threads:             1\n\ufffd[36mjoined_phase0: \ufffd[39m\n\ufffd[36mjoined_phase0: \ufffd[39mMesh: \n\ufffd[36mjoined_phase0: \ufffd[39m  Parallel Type:           distributed\n\ufffd[36mjoined_phase0: \ufffd[39m  Mesh Dimension:          2\n\ufffd[36mjoined_phase0: \ufffd[39m  Spatial Dimension:       2\n\ufffd[36mjoined_phase0: \ufffd[39m  Nodes:                   \n\ufffd[36mjoined_phase0: \ufffd[39m    Total:                 2798980\n\ufffd[36mjoined_phase0: \ufffd[39m    Local:                 14029\n\ufffd[36mjoined_phase0: \ufffd[39m    Min/Max/Avg:           12187/14029/13265\n\ufffd[36mjoined_phase0: \ufffd[39m  Elems:                   \n\ufffd[36mjoined_phase0: \ufffd[39m    Total:                 2795580\n\ufffd[36mjoined_phase0: \ufffd[39m    Local:                 13665\n\ufffd[36mjoined_phase0: \ufffd[39m    Min/Max/Avg:           12135/13687/13249\n\ufffd[36mjoined_phase0: \ufffd[39m  Num Subdomains:          1\n\ufffd[36mjoined_phase0: \ufffd[39m  Num Partitions:          1\n\ufffd[36mjoined_phase0: \ufffd[39m  Partitioner:             parmetis\n\ufffd[36mjoined_phase0: \ufffd[39m\n\ufffd[36mjoined_phase0: \ufffd[39mNonlinear System:\n\ufffd[36mjoined_phase0: \ufffd[39m  Num DOFs:                8396940\n\ufffd[36mjoined_phase0: \ufffd[39m  Num Local DOFs:          42087\n\ufffd[36mjoined_phase0: \ufffd[39m  Variables:               { \"etaa0\" \"etaa1\" \"etaa2\" } \n\ufffd[36mjoined_phase0: \ufffd[39m  Finite Element Types:    \"LAGRANGE\" \n\ufffd[36mjoined_phase0: \ufffd[39m  Approximation Orders:    \"FIRST\" \n\ufffd[36mjoined_phase0: \ufffd[39m\n\ufffd[36mjoined_phase0: \ufffd[39mAuxiliary System:\n\ufffd[36mjoined_phase0: \ufffd[39m  Num DOFs:                2798980\n\ufffd[36mjoined_phase0: \ufffd[39m  Num Local DOFs:          14029\n\ufffd[36mjoined_phase0: \ufffd[39m  Variables:               \"bnds\" \n\ufffd[36mjoined_phase0: \ufffd[39m  Finite Element Types:    \"LAGRANGE\" \n\ufffd[36mjoined_phase0: \ufffd[39m  Approximation Orders:    \"FIRST\" \n\ufffd[36mjoined_phase0: \ufffd[39m\n\ufffd[36mjoined_phase0: \ufffd[39mExecution Information:\n\ufffd[36mjoined_phase0: \ufffd[39m  Executioner:             Transient\n\ufffd[36mjoined_phase0: \ufffd[39m  TimeStepper:             IterationAdaptiveDT\n\ufffd[36mjoined_phase0: \ufffd[39m  Solver Mode:             NEWTON\n\ufffd[36mjoined_phase0: \ufffd[39m  MOOSE Preconditioner:    SMP\n\ufffd[36mjoined_phase0: \ufffd[39m\n\n\ufffd[31m\n*** ERROR ***\nThe following error occurred in the object \"Executioner\", of type \"Transient\".\n\nInternal error in Transient executioner: _t_step is equal to 0 while recovering in init().\ufffd[39m\n\n[r3i5n25:mpi_rank_11][MPIDI_CH3_Abort] application called MPI_Abort(MPI_COMM_WORLD, 1) - process 11: Success (0)",
          "url": "https://github.com/idaholab/moose/discussions/22342",
          "updatedAt": "2022-11-15T19:17:20Z",
          "publishedAt": "2022-10-09T20:37:20Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nDoes it recover properly if there are a few more timesteps to run in the multiapp?\nWhat does your executioner block look like in the MultiApp?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/22342#discussioncomment-3836267",
                  "updatedAt": "2022-10-10T05:27:08Z",
                  "publishedAt": "2022-10-10T05:27:07Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "xueyang94"
                          },
                          "bodyText": "The sub-app and first step of main-app took 6 days to run. I don't have more time steps yet, but I have started another simulation. This is the executioner block in the main-app:\n[Executioner]\n  type = Transient\n  solve_type = NEWTON\n  petsc_options_iname = '-pc_type -pc_factor_mat_solver_package'\n  petsc_options_value = 'lu       superlu_dist'\n  scheme = bdf2\n  l_max_its = 15\n  l_tol = 1e-4\n  nl_max_its = 800\n  nl_rel_tol = 1e-9\n  nl_abs_tol = 1e-7\n  end_time = 321670 #1925 hours\n  [./TimeStepper]\n    type = IterationAdaptiveDT\n    dt = 1e-3\n    iteration_window = 2\n    optimal_iterations = 10\n    growth_factor = 1.1\n    cutback_factor = 0.75\n  [../]\n[\n```]",
                          "url": "https://github.com/idaholab/moose/discussions/22342#discussioncomment-3839132",
                          "updatedAt": "2022-10-10T13:01:43Z",
                          "publishedAt": "2022-10-10T13:01:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "ok and your MultiApps block in the main app?\nThe next step is to build a very reduced example",
                          "url": "https://github.com/idaholab/moose/discussions/22342#discussioncomment-3839584",
                          "updatedAt": "2022-10-10T13:46:31Z",
                          "publishedAt": "2022-10-10T13:46:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "xueyang94"
                          },
                          "bodyText": "This are the multiapp and transfer blocks in the main-app:\n[MultiApps]\n  [joined_phase]\n    type = FullSolveMultiApp\n    execute_on = 'INITIAL'\n    input_files = 'voronoi_sub.i'\n  []\n[]\n\n[Transfers]\n  [from_joined]\n    type = MultiAppMeshFunctionTransfer\n    source_variable = 'bnds etaa0 etaa1 etaa2'\n    variable = 'bnds etaa0 etaa1 etaa2'\n    from_multi_app = joined_phase\n  [../]\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/22342#discussioncomment-3840087",
                          "updatedAt": "2022-10-10T14:41:25Z",
                          "publishedAt": "2022-10-10T14:41:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "ok this all looks normal.\nWe should make a minimal working example then we can create an issue and find a resource to fix this",
                          "url": "https://github.com/idaholab/moose/discussions/22342#discussioncomment-3840113",
                          "updatedAt": "2022-10-10T14:43:53Z",
                          "publishedAt": "2022-10-10T14:43:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "xueyang94"
                          },
                          "bodyText": "I have a question. If only one timestep was finished, there should be only one checkpoint folder named 0000_mesh.cpr, correct? And if the first timestep did not finish, there's no checkpoint folder and we should just start over.",
                          "url": "https://github.com/idaholab/moose/discussions/22342#discussioncomment-3840353",
                          "updatedAt": "2022-10-10T15:08:16Z",
                          "publishedAt": "2022-10-10T15:08:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "not sure, checkpoint isnt meant to be used before a single time step for sure",
                          "url": "https://github.com/idaholab/moose/discussions/22342#discussioncomment-3840452",
                          "updatedAt": "2022-10-10T15:16:53Z",
                          "publishedAt": "2022-10-10T15:16:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I d like to have this in an issue. We need a minimal non-working example for this.\nWould you mind producing it? If you cant i can make one too",
                          "url": "https://github.com/idaholab/moose/discussions/22342#discussioncomment-3854632",
                          "updatedAt": "2022-10-11T21:40:38Z",
                          "publishedAt": "2022-10-11T21:40:37Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "xueyang94"
                          },
                          "bodyText": "Yes I will produce it. I will get back to you before next week.",
                          "url": "https://github.com/idaholab/moose/discussions/22342#discussioncomment-3862514",
                          "updatedAt": "2022-10-12T16:46:38Z",
                          "publishedAt": "2022-10-12T16:46:37Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "xueyang94"
                          },
                          "bodyText": "@GiudGiud I have created a mainapp and a subapp as below. It is a simple 3-phase 2-component KKS model with simple free energies. For testing, I terminate the mainapp after the subapp has finished, and the mainapp has run for a few steps. For recovery, I run the mainapp again with --recover flag. The recovery would crash. Could you please check if the recovery and transfer codes are correct? Then I can create an issue. Thanks.\nmainapp:\n[Executioner]\n  type = Transient\n  solve_type = NEWTON\n  petsc_options_iname = '-pc_type -pc_factor_mat_solver_package'\n  petsc_options_value = 'lu       superlu_dist'\n  scheme = bdf2\n  l_max_its = 15\n  l_tol = 1e-4\n  nl_max_its = 800\n  nl_rel_tol = 1e-9\n  nl_abs_tol = 1e-7\n  dt = 1e-3\n  num_steps = 10\n[]\n\n[Adaptivity]\n  [./Markers]\n    [./etaa0_marker]\n      type = ValueRangeMarker\n      variable = etaa0\n      upper_bound = 0.9\n      lower_bound = 0.1\n    [../]\n    [./etaa1_marker]\n      type = ValueRangeMarker\n      variable = etaa1\n      upper_bound = 0.9\n      lower_bound = 0.1\n    [../]\n    [./eta1_marker]\n      type = ValueRangeMarker\n      variable = eta1\n      upper_bound = 0.9\n      lower_bound = 0.1\n    [../]\n    [./eta2_marker]\n      type = ValueRangeMarker\n      variable = eta2\n      upper_bound = 0.9\n      lower_bound = 0.1\n    [../]\n    [./eta3_marker]\n      type = ValueRangeMarker\n      variable = eta3\n      upper_bound = 0.9\n      lower_bound = 0.1\n    [../]\n    [./combo]\n      type = ComboMarker\n      markers = 'etaa0_marker etaa1_marker eta1_marker eta2_marker eta3_marker'\n    [../]\n  [../]\nmarker = combo\ninitial_marker = combo\nrecompute_markers_during_cycles = true\ninitial_steps = 2\nmax_h_level = 2\n[]\n\n[Mesh]\n  [gmg]\n     type = DistributedRectilinearMeshGenerator\n     dim = 2\n     nx = 38\n     xmin = 0\n     xmax = 30\n     ny = 13\n     ymin = 0\n     ymax = 10\n     elem_type = QUAD4\n   []\n  parallel_type = DISTRIBUTED\n[]\n\n[MultiApps]\n  [joined_phase]\n    type = FullSolveMultiApp\n    execute_on = INITIAL\n    input_files = 'poly_sub.i'\n  []\n[]\n\n[Transfers]\n  [from_joined]\n    type = MultiAppMeshFunctionTransfer\n    source_variable = 'bnds etaa0 etaa1'\n    variable = 'bnds etaa0 etaa1'\n    from_multi_app = joined_phase\n    execute_on = INITIAL\n  [../]\n[]\n\n[Preconditioning]\n  [./full]\n    type = SMP\n    full = true\n  [../]\n[]\n\n[Outputs]\n  exodus = true\n  csv = true\n  [./out]\n    type = Checkpoint\n    num_files = 4\n    interval = 2\n  [../]\n[]\n\n[Variables]\n  [./eta1]\n  [../]\n  [./eta2]\n  [../]\n  [./eta3]\n  [../]\n  [./lambda]\n  [../]\n  [./c]\n  [../]\n  [./c1]\n    initial_condition = 0.3\n  [../]\n  [./c2]\n    initial_condition = 0.5\n  [../]\n  [./c3]\n    initial_condition = 0.8\n  [../]\n  [./mu]\n  [../]\n[]\n\n[AuxVariables]\n  [./bnds] # transferred from subapp\n  [../]\n  [./bnds_main]\n  [../]\n  [./etaa0]\n  [../]\n  [./etaa1]\n  [../]\n[]\n\n[AuxKernels]\n  [./bnds_main_aux]\n    type = BndsCalcAux\n    variable = bnds_main\n    v = 'etaa0 etaa1 eta2 eta3'\n    execute_on = 'initial timestep_end'\n  [../]\n[]\n\n[ICs]\n  [./eta1]\n    type = NestedBoundingBoxIC\n    variable = eta1\n    smaller_coordinate_corners = '-4 -4 0 '\n    larger_coordinate_corners = '10 14 0'\n    inside = '1'\n  [../]\n  [./eta2]\n    type = NestedBoundingBoxIC\n    variable = eta2\n    smaller_coordinate_corners = '10 -4 0 '\n    larger_coordinate_corners = '20 14 0'\n    inside = '1'\n  [../]\n  [./eta3]\n    type = NestedBoundingBoxIC\n    variable = eta3\n    smaller_coordinate_corners = '20 -4 0'\n    larger_coordinate_corners = '34 14 0'\n    inside = '1'\n  [../]\n  [./c]\n    type = NestedBoundingBoxIC\n    variable = c\n    smaller_coordinate_corners = '-4 -4 0 10 -4 0'\n    larger_coordinate_corners = '10 14 0 20 14 0'\n    inside = '0.3 0.5'\n    outside = 0.8\n  [../]\n[]\n\n[GlobalParams]\n  Fj_names  = 'F1 F2 F3'\n  hj_names  = 'h1 h2 h3'\n  wi = 0.02\n  int_width = 1\n  derivative_order = 2\n  evalerror_behavior = error\n  enable_ad_cache = false\n  enable_jit = false\n  epsilon = 1e-9 # used in lambda kernel, default\n[]\n\n[Materials]\n  [./const]\n    type = GenericConstantMaterial\n    prop_names =  'sigma   L  '\n    prop_values = '0.20   1e-2'\n  [../]\n  [./kappa]\n    type = ParsedMaterial\n    f_name = kappa\n    constant_names = 'lgb'\n    constant_expressions = '1'\n    material_property_names = 'sigma'\n    function = '3*sigma*lgb/4'\n  [../]\n  [./w]\n    type = ParsedMaterial\n    f_name = w\n    constant_names = 'lgb'\n    constant_expressions = '1'\n    material_property_names = 'sigma'\n    function = '6*sigma/lgb'\n  [../]\n\n  [./F1]\n    type = DerivativeParsedMaterial\n    f_name = F1\n    args = 'c1'\n    function = '(c1 - 0.3)^2'\n  [../]\n  [./F2]\n    type = DerivativeParsedMaterial\n    f_name = F2\n    args = 'c2'\n    function = '(c2 - 0.7)^2 - 10'\n  [../]\n  [./F3]\n    type = DerivativeParsedMaterial\n    f_name = F3\n    args = 'c3'\n    function = '(c3 - 0.4)^2'\n  [../]\n\n  [./h1]\n    type = SwitchingFunctionMaterial\n    eta = eta1\n    h_order = HIGH\n    function_name = h1\n  [../]\n  [./h2]\n    type = SwitchingFunctionMaterial\n    eta = eta2\n    h_order = HIGH\n    function_name = h2\n  [../]\n  [./h3]\n    type = SwitchingFunctionMaterial\n    eta = eta3\n    h_order = HIGH\n    function_name = h3\n  [../]\n\n  [./g]\n    type = DerivativeParsedMaterial\n    constant_names = 'gamma'\n    constant_expressions = '1.5'\n    f_name = g\n    function = '(eta1^4/4 - eta1^2/2) + (eta2^4/4 - eta2^2/2) + (eta3^4/4 - eta3^2/2) +\n                gamma * ((eta1*eta2)^2 + (eta1*eta3)^2 + (eta2*eta3)^2) + 1/4'\n    args = 'eta1 eta2 eta3'\n  [../]\n\n  [./M]\n    type = DerivativeParsedMaterial\n    f_name = M\n    material_property_names = 'h1(eta1) h2(eta2) h3(eta3)'\n    args = 'eta1 eta2 eta3 bnds'\n    constant_names =       'M1_bulk M1_gb  M2 M3'\n    constant_expressions = '10      1000   50 80'\n    function = 'if(bnds>0.9, h1*M1_bulk + h2*M2 + h3*M3, h1*M1_gb + h2*M2 + h3*M3)'\n  [../]\n[]\n\n[Kernels]\n  [./lambda_lagrange]\n    type = SwitchingFunctionConstraintLagrange\n    variable = lambda\n    etas =    'eta1 eta2 eta3'\n    h_names = 'h1   h2   h3'\n  [../]\n  [./eta1_lagrange]\n    type = SwitchingFunctionConstraintEta\n    variable = eta1\n    h_name = h1\n    lambda = lambda\n  [../]\n  [./eta2_lagrange]\n    type = SwitchingFunctionConstraintEta\n    variable = eta2\n    h_name = h2\n    lambda = lambda\n  [../]\n  [./eta3_lagrange]\n    type = SwitchingFunctionConstraintEta\n    variable = eta3\n    h_name = h3\n    lambda = lambda\n  [../]\n\n  [./phase_concentration]\n    type = KKSMultiPhaseConcentration\n    variable = c3\n    cj = 'c1 c2 c3'\n    etas = 'eta1 eta2 eta3'\n    c = c\n  [../]\n\n  [./chempot12]\n    type = KKSPhaseChemicalPotential\n    variable = c1\n    cb = c2\n    fa_name = F1\n    fb_name = F2\n  [../]\n  [./chempot23]\n    type = KKSPhaseChemicalPotential\n    variable = c2\n    cb = c3\n    fa_name = F2\n    fb_name = F3\n  [../]\n\n  # kernels for diffusion equation\n  [./CHBulk]\n    type = KKSSplitCHCRes\n    variable = c\n    w = mu\n    ca = c1\n    fa_name = F1\n  [../]\n  [./dcdt]\n    type = CoupledTimeDerivative\n    variable = mu\n    v = c\n  [../]\n  [./ckernel]\n    type = SplitCHWRes\n    variable = mu\n    mob_name = M\n    args = 'eta1 eta2 eta3'\n  [../]\n\n  # Kernels for Allen-Cahn equation\n  [./ACBulkF1]\n    type = KKSMultiACBulkF\n    variable  = eta1\n    gi_name   = g\n    eta_i     = eta1\n    args = 'eta2 eta3 c1 c2 c3'\n    mob_name = L\n  [../]\n  [./ACBulkC1]\n    type = KKSMultiACBulkC\n    variable  = eta1\n    cj_names  = 'c1 c2 c3'\n    eta_i     = eta1\n    args = 'eta2 eta3 c1 c2 c3'\n    mob_name = L\n  [../]\n  [./ACInterface1]\n    type = ACInterface\n    variable = eta1\n    kappa_name = kappa\n    mob_name = L\n    args      = 'eta2 eta3'\n  [../]\n  [./deta1dt]\n    type = TimeDerivative\n    variable = eta1\n  [../]\n\n  [./ACBulkF2]\n    type = KKSMultiACBulkF\n    variable  = eta2\n    gi_name   = g\n    eta_i     = eta2\n    args = 'eta1 eta3 c1 c2 c3'\n    mob_name = L\n  [../]\n  [./ACBulkC2]\n    type = KKSMultiACBulkC\n    variable  = eta2\n    cj_names  = 'c1 c2 c3'\n    eta_i     = eta2\n    args = 'eta1 eta3 c1 c2 c3'\n    mob_name = L\n  [../]\n  [./ACInterface2]\n    type = ACInterface\n    variable = eta2\n    kappa_name = kappa\n    mob_name = L\n    args      = 'eta1 eta3'\n  [../]\n  [./deta2dt]\n    type = TimeDerivative\n    variable = eta2\n  [../]\n\n  [./ACBulkF3]\n    type = KKSMultiACBulkF\n    variable  = eta3\n    gi_name   = g\n    eta_i     = eta3\n    args = 'eta1 eta2 c1 c2 c3'\n    mob_name = L\n  [../]\n  [./ACBulkC3]\n    type = KKSMultiACBulkC\n    variable  = eta3\n    cj_names  = 'c1 c2 c3'\n    eta_i     = eta3\n    args = 'eta1 eta2 c1 c2 c3'\n    mob_name = L\n  [../]\n  [./ACInterface3]\n    type = ACInterface\n    variable = eta3\n    kappa_name = kappa\n    mob_name = L\n    args      = 'eta1 eta2'\n  [../]\n  [./deta3dt]\n    type = TimeDerivative\n    variable = eta3\n  [../]\n[]\n\n\nsubapp:\n[Executioner]\n  type = Transient\n  solve_type = NEWTON\n  petsc_options_iname = '-pc_type -pc_factor_mat_solver_package'\n  petsc_options_value = 'lu       superlu_dist'\n  scheme = bdf2\n  l_max_its = 15\n  l_tol = 1e-4\n  nl_max_its = 800\n  nl_rel_tol = 1e-9\n  nl_abs_tol = 1e-7\n  num_steps = 50\n  dt = 1e-3\n[]\n\n[Adaptivity]\n  [./Markers]\n    [./etaa0_marker]\n      type = ValueRangeMarker\n      variable = etaa0\n      upper_bound = 0.9\n      lower_bound = 0.1\n    [../]\n    [./etaa1_marker]\n      type = ValueRangeMarker\n      variable = etaa1\n      upper_bound = 0.9\n      lower_bound = 0.1\n    [../]\n    [./combo]\n      type = ComboMarker\n      markers = 'etaa0_marker etaa1_marker'\n    [../]\n  [../]\nmarker = combo\nrecompute_markers_during_cycles = true\ninitial_steps = 2\nmax_h_level = 2\n[]\n\n[Mesh]\n  [gen]\n    type = DistributedRectilinearMeshGenerator\n    dim = 2\n    xmin = 0\n    xmax = 10\n    nx = 13\n    ymin = 0\n    ymax = 10\n    ny = 13\n    elem_type = QUAD4\n  []\n  parallel_type = DISTRIBUTED\n[]\n\n[Preconditioning]\n  [./full]\n    type = SMP\n    full = true\n  [../]\n[]\n\n[Outputs]\n  exodus = true\n  csv = true\n[]\n\n[Variables]\n  [./PolycrystalVariables]\n  [../]\n[]\n\n[ICs]\n  [./etaa0]\n    type = NestedBoundingBoxIC\n    variable = etaa0\n    smaller_coordinate_corners = '-2 -2 0'\n    larger_coordinate_corners = '12 5 0'\n    inside = 1\n    int_width = 1\n  [../]\n  [./etaa1]\n    type = NestedBoundingBoxIC\n    variable = etaa1\n    smaller_coordinate_corners = '-2 5 0'\n    larger_coordinate_corners = '12 12 0'\n    inside = 1\n    int_width = 1\n  [../]\n[]\n\n[GlobalParams]\n  op_num = 2\n  var_name_base = etaa\n[]\n\n[AuxVariables]\n  [./bnds]\n  [../]\n[]\n\n[AuxKernels]\n  [./bnds_aux]\n    type = BndsCalcAux\n    variable = bnds\n    execute_on = 'initial timestep_end'\n  [../]\n[]\n\n[Materials]\n  [./const]\n    type = GenericConstantMaterial\n    prop_names =  'gamma_asymm sigma    l    L'\n    prop_values = '1.5         0.2      1   1e-2'\n  [../]\n  [./kappa_op]\n    type = ParsedMaterial\n    f_name = kappa_op\n    material_property_names = 'sigma l'\n    function = '3*sigma*l/4'\n  [../]\n  [./mu]\n    type = ParsedMaterial\n    f_name = mu\n    material_property_names = 'sigma l'\n    function = '6*sigma/l'\n  [../]\n[]\n\n[Kernels]\n  [./PolycrystalKernel]\n  [../]\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/22342#discussioncomment-3892028",
                          "updatedAt": "2022-10-17T01:34:10Z",
                          "publishedAt": "2022-10-17T01:34:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "this is a little too complicated for a MWE.\nIn particular it d be good to remove adaptivity, unless it's actually part of the problem",
                          "url": "https://github.com/idaholab/moose/discussions/22342#discussioncomment-3900169",
                          "updatedAt": "2022-10-17T20:33:47Z",
                          "publishedAt": "2022-10-17T20:33:46Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Access out of bounds in MooseArray",
          "author": {
            "login": "heinono1"
          },
          "bodyText": "Hi,\nI am building a FV version of ferret. After successfully compiling and linking (and checking the input file) execution throws an error:\nAssertion `i < _size ' failed\nAccess out of bounds in MooseArray (i: 0 size: 0)\nat /home/olle/projects/moose/framework/build/header_symlinks/MooseArray.h, line 276\nI have pinned down that the problem is generated in the FV kernels I built. The FVTimeKernel works, but when I add a first extra kernel (FVMagneticExchangeCons.C), the execution error shows up. The kernel is modeled after the FVDiffusion.C kernel.\nI note that there were some postings about this error before, but not in the FV stuff.  Below is the input, and source and header files of the offending kernel. Any help will be greatly appreciated.\nInput file:\nalphadef = 0.25\n\n[Mesh]\n  [./mesh]\n    type = GeneratedMeshGenerator\n    dim = 3\n    nx = 80\n    ny = 40\n    nz = 12\n    xmin = -200\n    xmax = 200\n    ymin = -100\n    ymax = 100\n    zmin = -30\n#    block_id = 1\n#    block_name = brick\n  [../]\n[]\n\n\n#[GlobalParams]\n#  mag_x = mag_x\n#  mag_y = mag_y\n#  mag_z = mag_z\n#   Ae = Ae\n#   g0 = g0\n#   Ms = Ms\n#   alpha = alpha\n#[]\n\n[Materials]\n  ############################################################################\n  ##\n  ##       material constants used.\n  ## Materials are defined as ADGenericFunctorMaterial - have to convert to real\n  ## auxiliary variables to evaluate flux kernels\n  ##\n  ##34989.1\n  ############################################################################\n\n  [./constants] \n    type = ADGenericFunctorMaterial\n    prop_names = ' alpha           g0mu0Ms        g0       permittivity Ae      Ms' #   mu0'\n    prop_values = '${alphadef}     34989.1     34989.1       1.0        13.0   -1.0' #  1.0'\n#    block = '1'\n  [../]\n\n\n  [./a_long]\n    type = ADGenericFunctorMaterial\n    prop_names = 'alpha_long'\n    prop_values = '10.'\n#    block = '1' \n  [../]\n\n[]\n\n[Variables]\n  [./mag_x]\n    order = CONSTANT\n    family = MONOMIAL\n#    block = '1'\n    fv = true\n  [../]\n  [./mag_y]\n    order = CONSTANT\n    family = MONOMIAL\n#    block = '1'\n    fv = true\n  [../]\n  [./mag_z]\n    order = CONSTANT\n    family = MONOMIAL\n#    block= '1'\n    fv = true\n  [../]\n[]\n\n[AuxVariables]\n\n  [./mag_s]\n    type = MooseVariableFVReal\n#    block = '1'\n  [../]\n  [./Ae_elem]\n    type = MooseVariableFVReal\n#    block = '1'\n  [../]\n  [./Ms_elem]\n    type = MooseVariableFVReal\n#    block = '1'\n  [../]\n  [./alpha_elem]\n    type = MooseVariableFVReal\n#    block = '1'\n  [../]\n  [./alpha_long_elem]\n    type = MooseVariableFVReal\n#    block = '1'\n  [../]\n  [./g0_elem]\n    type = MooseVariableFVReal\n#    block = '1'\n  [../]\n[../]\n\n\n[AuxKernels]\n  [./mag_mag]\n    type = FVVectorMag\n    variable = mag_s\n    vector_x = mag_x\n    vector_y = mag_y\n    vector_z = mag_z\n    execute_on = 'initial timestep_end final'\n#    block = '1'\n  [../]\n  [./get_Ae]\n    type = ADFunctorElementalAux\n    functor = 'Ae'\n    variable = 'Ae_elem'\n#    block = '1'\n    execute_on = 'initial'\n  [../]\n  [./get_Ms]\n    type = ADFunctorElementalAux\n    functor = 'Ms'\n    variable = 'Ms_elem'\n#    block = '1'\n    execute_on = 'initial'\n  [../]\n  [./get_alpha]\n    type = ADFunctorElementalAux\n    functor = 'alpha'\n    variable = 'alpha_elem'\n#    block = '1'\n    execute_on = 'initial'\n  [../]\n    [./get_alpha_long]\n    type = ADFunctorElementalAux\n    functor = 'alpha_long'\n    variable = 'alpha_long_elem'\n#    block = '1'\n    execute_on = 'initial'\n  [../]\n  [./get_g0]\n    type = ADFunctorElementalAux\n    functor = 'g0'\n    variable = 'g0_elem'\n#    block = '1'\n    execute_on = 'initial'\n  [../]\n[]\n\n\n[FVKernels]\n  #---------------------------------------#\n  #                                       #\n  #          Time dependence              #\n  #                                       #\n  #---------------------------------------#\n\n  [./mag_x_time]\n    type = FVTimeKernel\n    variable = mag_x\n#    block = '1'\n  [../]\n  [./mag_y_time]\n    type = FVTimeKernel\n    variable = mag_y\n#    block = '1'\n  [../]\n  [./mag_z_time]\n    type = FVTimeKernel\n    variable = mag_z\n#    block = '1'\n  [../]\n\n  #---------------------------------------#\n  #                                       #\n  #    Local magnetic exchange            #\n  #                                       #\n  #---------------------------------------#\n  \n  [./exch_x_cons]\n    type = FVMagneticExchangeCons\n    variable = mag_x\n    component = 0\n#    Ae = Ae\n#    Ms = Ms\n#    g0 = g0\n#    alpha = alpha\n    mag_x = mag_x\n    mag_y = mag_y\n    mag_z = mag_z\n#    block = '1'\n  [../]\n#  [./exch_y_cons]\n#    type = FVMagneticExchangeCons\n#    variable = mag_y\n#    component = 1\n#    mag_x = mag_x\n#    mag_y = mag_y\n#    mag_z = mag_z\n#    block = '1'\n#  [../]\n#  [./exch_z_cons]\n#    type = FVMagneticExchangeCons\n#    variable = mag_z\n#    component = 2\n#    mag_x = mag_x\n#    mag_y = mag_y\n#    mag_z = mag_z\n#    block = '1'\n#  [../]\n  \n\n  #---------------------------------------#\n  #                                       #\n  #    demagnetization field              #\n  #                                       #\n  #---------------------------------------#\n\n\n[]\n\n\n[Postprocessors]\n   [./dt]\n     type = TimestepSize\n   [../]\n[]\n\n[Preconditioning]\n\n  #---------------------------------------#\n  #                                       #\n  #            Solver options             #\n  #                                       #\n  #---------------------------------------#\n\n  [./smp]\n    type = SMP\n    full = true\n    petsc_options_iname = ' -ksp_gmres_restart -snes_atol -snes_rtol -ksp_rtol -pc_type -sub_ksp_type -sub_pc_type -pc_asm_overlap'\n    petsc_options_value = '    31               1e-9      1e-9      1e-8        bjacobi      preonly       ilu           2'\n  [../]\n[]\n\n[Executioner]\n  type = Transient            \n  solve_type = 'NEWTON'\n\n  [./TimeIntegrator]\n    type = NewmarkBeta #LStableDirk4 #NewmarkBeta\n  [../]\n\n  dtmin = 1.e-7\n  dtmax = 2.e-5\n\n  [./TimeStepper]\n    type = IterationAdaptiveDT\n    optimal_iterations = 25  #usually 10\n    linear_iteration_ratio = 100\n    dt = 1e-8\n    growth_factor = 1.1\n    cutback_factor = 0.75\n  [../]\n  num_steps = 1500\n[../]\n\n[Outputs]\n  print_linear_residuals = false\n\n  [./out]\n    type = Exodus\n    file_base = brick\n    elemental_as_nodal = true\n    interval = 1\n  [../]\n  [./outCSV]\n    type = CSV\n    file_base = brick\n    interval = 1\n  [../]\n[]\n-----------------------------------------------------------------------------------------------------------------------\nSource file:\n#include \"FVMagneticExchangeCons.h\"\n#include \"RelationshipManager.h\"\n\nregisterMooseObject(\"FerretApp\", FVMagneticExchangeCons);\n\nInputParameters\nFVMagneticExchangeCons::validParams()\n{\n  InputParameters params = FVFluxKernel::validParams();\n  params.addClassDescription(\"Computes residual of conservative exchange torque\");\n  params.addRequiredParam<unsigned int>(\"component\", \"An integer corresponding to the direction in order parameter space\");\n  /*  params.addRequiredParam<MooseFunctorName>(\"alpha\", \"damping coefficient\");\n  params.addRequiredParam<MooseFunctorName>(\"g0\", \"g0 coefficient\");\n  params.addRequiredParam<MooseFunctorName>(\"Ms\", \"magnetization density\");\n  params.addRequiredParam<MooseFunctorName>(\"Ae\", \"exchange coupling\");*/\n  params.addRequiredCoupledVar(\"mag_x\", \"The x component of the constrained magnetic vector\");\n  params.addRequiredCoupledVar(\"mag_y\", \"The y component of the constrained magnetic vector\");\n  params.addRequiredCoupledVar(\"mag_z\", \"The z component of the constrained magnetic vector\");\n  //  MooseEnum coeff_interp_method(\"average\", \"harmonic\");\n  MooseEnum coeff_interp_method(\"average\");  \n  params.addParam<MooseEnum>(\n\t\t\t     \"coeff_interp_method\",\n\t\t\t     coeff_interp_method,\n\t\t\t     \"Switch that can select interpolation method for magnetization density.\"); \n  params.set<unsigned short>(\"ghost_layers\") = 2;\n  return params;\n}\n\nFVMagneticExchangeCons::FVMagneticExchangeCons(const InputParameters & params)\n  : FVFluxKernel(params), /*_coeff(getFunctor<ADReal>(\"coeff\")),*/\n   _component(getParam<unsigned int>(\"component\")),\n   _mag_x_var(coupled(\"mag_x\")),\n   _mag_y_var(coupled(\"mag_y\")),\n   _mag_z_var(coupled(\"mag_z\")),\n   _mag_x(coupledValue(\"mag_x\")),\n   _mag_y(coupledValue(\"mag_y\")),\n   _mag_z(coupledValue(\"mag_z\")),\n   _mag_x_grad(coupledGradient(\"mag_x\")),\n   _mag_y_grad(coupledGradient(\"mag_y\")),\n   _mag_z_grad(coupledGradient(\"mag_z\"))\n    /*   _alpha(getFunctor<ADReal>(\"alpha\")),   \n   _g0(getFunctor<ADReal>(\"g0\")),\n   _Ms(getFunctor<ADReal>(\"Ms\")),\n   _Ae(getFunctor<ADReal>(\"Ae\"))*/\n    /*   _alpha_elem(coupledValue(\"alpha_elem\")),   \n   _g0_elem(coupledValue(\"g0_elem\")),\n   _Ms_elem(coupledValue(\"Ms_elem\")),\n   _Ae_elem(coupledValue(\"Ae_elem\"))*/\n{\n#ifndef MOOSE_GLOBAL_AD_INDEXING\n  mooseError(\n      \"FVMagneticExchangeCons is not supported by local AD indexing. In order to use this object, please run \"\n      \"the configure script in the root MOOSE directory with the configure option \"\n      \"'--with-ad-indexing-type=global'. Note that global indexing is now the default \"\n      \"configuration for AD indexing type.\");\n#endif\n\n  const auto & interp_method = getParam<MooseEnum>(\"coeff_interp_method\");\n  //const auto _coeff_interp_method;\n  if (interp_method == \"average\")\n    const auto _coeff_interp_method = Moose::FV::InterpMethod::Average;\n  /*else if (interp_method == \"harmonic\")\n    _coeff_interp_method = Moose::FV::InterpMethod::HarmonicAverage;*/\n\n  if ((_var.faceInterpolationMethod() == Moose::FV::InterpMethod::SkewCorrectedAverage) &&\n      (_tid == 0))\n     adjustRMGhostLayers(std::max((unsigned short)(3), _pars.get<unsigned short>(\"ghost_layers\")));\n}\n\nADReal\nFVMagneticExchangeCons::computeQpResidual()\n{\n  //  auto mgradm = 0.;\n  if (_component == 0)\n    {\n      //      auto mgradm =\n      return\t_normal(0)*(_mag_y[_qp]*_mag_z_grad[_qp](0)-_mag_z[_qp]*_mag_y_grad[_qp](0)) +\n\t_normal(1)*(_mag_y[_qp]*_mag_z_grad[_qp](1)-_mag_z[_qp]*_mag_y_grad[_qp](1)) +\n\t_normal(2)*(_mag_y[_qp]*_mag_z_grad[_qp](2)-_mag_z[_qp]*_mag_y_grad[_qp](2));\n     }\n  else if (_component == 1)\n    {\n      //      auto mgradm =\n      return\t_normal(0)*(_mag_z[_qp]*_mag_x_grad[_qp](0)-_mag_x[_qp]*_mag_z_grad[_qp](0)) +\n\t_normal(1)*(_mag_z[_qp]*_mag_x_grad[_qp](1)-_mag_x[_qp]*_mag_z_grad[_qp](1)) +\n\t_normal(2)*(_mag_z[_qp]*_mag_x_grad[_qp](2)-_mag_x[_qp]*_mag_z_grad[_qp](2));\n     }\n    else if (_component == 2)\n    {\n      //      auto mgradm =\n      return\t_normal(0)*(_mag_x[_qp]*_mag_y_grad[_qp](0)-_mag_y[_qp]*_mag_x_grad[_qp](0)) +\n\t_normal(1)*(_mag_x[_qp]*_mag_y_grad[_qp](1)-_mag_y[_qp]*_mag_x_grad[_qp](1)) +\n\t_normal(2)*(_mag_x[_qp]*_mag_y_grad[_qp](2)-_mag_y[_qp]*_mag_x_grad[_qp](2));\n     }\n  //    else\n  return 0.;     \n  //      auto mgradm = 0.;\n  // Perform weighted-average or central differencing (CD) interpolation of coefficients\n   // Perform weighted-average or central differencing (CD) interpolation of constants\n  /*  ADReal M0k;\n  interpolate(_coeff_interp_method,M0k,_Ms(elemFromFace()),_Ms(neighborFromFace()),*_face_info, true);\n  ADReal Aek;\n  interpolate(_coeff_interp_method,Aek,_Ae(elemFromFace()),_Ae(neighborFromFace()),*_face_info, true);\n  ADReal g0k;\n  interpolate(_coeff_interp_method,g0k,_g0(elemFromFace()),_g0(neighborFromFace()),*_face_info, true);\n  ADReal alphak;\n  interpolate(_coeff_interp_method,alphak,_alpha(elemFromFace()),_alpha(neighborFromFace()),*_face_info, true);*/\n  \n\n  //  return 2.* Aek *g0k * mgradm/((1.+ Utility::pow<2>(alphak))*M0k);\n}\n\n\nHeader file:\n#pragma once\n\n#include \"FVFluxKernel.h\"\n\n/// FVDiffusion implements a standard diffusion term:\n///\n///     - strong form: \\nabla \\cdot k \\nabla u\n///\n///     - weak form: \\int_{A} k \\nabla u \\cdot \\vec{n} dA\n///\n/// It uses/requests a material property named \"coeff\" for k. An average of\n/// the elem and neighbor k-values (which should be face-values) is used to\n/// compute k on the face. Cross-diffusion correction factors are currently not\n/// implemented for the \"grad_u*n\" term.\nclass FVMagneticExchangeCons : public FVFluxKernel\n{\npublic:\n  static InputParameters validParams();\n  FVMagneticExchangeCons(const InputParameters & params);\n\nprotected:\n  virtual ADReal computeQpResidual() override;\n\n  private:\n  \n  const unsigned int _component;\n  const unsigned int _mag_x_var;\n  const unsigned int _mag_y_var;\n  const unsigned int _mag_z_var;\n  const VariableValue & _mag_x;\n  const VariableValue & _mag_y;\n  const VariableValue & _mag_z;\n  const VariableGradient & _mag_x_grad;\n  const VariableGradient & _mag_y_grad;\n  const VariableGradient & _mag_z_grad;\n  /*  const VariableValue & _alpha_elem;\n  const VariableValue & _g0_elem;\n  const VariableValue & _Ms_elem;\n  const VariableValue & _Ae_elem;*/\n  /*  const Moose::Functor<ADReal> & _alpha;\n  const Moose::Functor<ADReal> & _g0;\n  const Moose::Functor<ADReal> & _Ms;\n  const Moose::Functor<ADReal> & _Ae;*/\n    /// Decides if a geometric arithmetic or harmonic average is used for the\n  /// face interpolation of the diffusion coefficient.\n  Moose::FV::InterpMethod _coeff_interp_method;\n\n  //const Moose::Functor<ADReal> & _coeff;\n};",
          "url": "https://github.com/idaholab/moose/discussions/22464",
          "updatedAt": "2022-11-15T19:00:41Z",
          "publishedAt": "2022-10-21T20:10:02Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "unrelated but this needs to change from\n  if (interp_method == \"average\")\n    const auto _coeff_interp_method = Moose::FV::InterpMethod::Average;\n\nto this\n  if (interp_method == \"average\")\n    _coeff_interp_method = Moose::FV::InterpMethod::Average;\n\nalso if you are passing harmonic, you are not triggering this if at all, so _coeff_interp_method will not be initialized.\nNow what I think is the problem:\nNewmarkBeta is specialized to tensor mechanics. It's not useable in the general case, you need to be calculating special quantities and this is not set up in finite volume yet (it might not be necessary to set anything up for FV, but definitely for kernels).\nPlease use implicit Euler to form your model and then switch to higher order once you have something working",
                  "url": "https://github.com/idaholab/moose/discussions/22464#discussioncomment-3936947",
                  "updatedAt": "2022-10-21T20:22:45Z",
                  "publishedAt": "2022-10-21T20:19:20Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "heinono1"
                  },
                  "bodyText": "Hi Guillermo,\n\nOddly enough, with just FVTimeKernel and using NewmarkBeta that error did\nnot show up. Anyhow, I changed time integrator to ImplicitEuler - still\nsame error. So it does not seem like NewmarkBeta was causing the error - it\nseems to be caused by FVMagneticExchangeCons\n\u2026\nOn Fri, Oct 21, 2022 at 3:19 PM Guillaume Giudicelli < ***@***.***> wrote:\n unrelated but this needs to change from\n\n   if (interp_method == \"average\")\n     const auto _coeff_interp_method = Moose::FV::InterpMethod::Average;\n\n to this\n\n   if (interp_method == \"average\")\n     _coeff_interp_method = Moose::FV::InterpMethod::Average;\n\n also if you are passing harmonic, you are not triggering this if at all,\n so _coeff_interp_method will not be initialized.\n\n Now the problem:\n NewmarkBeta is specialized to tensor mechanics. It's not useable in the\n general case, you need to be calculating special quantities and this is not\n set up in finite volume yet.\n\n Please use implicit Euler to form your model and then switch to higher\n order once you have something working\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#22464 (comment)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/AEKZEFZFZLPDHECWKXC266LWEL3FFANCNFSM6AAAAAARLPGKXY>\n .\n You are receiving this because you authored the thread.Message ID:\n ***@***.***>\n\n\n-- \nOlle Heinonen\n***@***.***",
                  "url": "https://github.com/idaholab/moose/discussions/22464#discussioncomment-3937237",
                  "updatedAt": "2022-10-21T21:23:50Z",
                  "publishedAt": "2022-10-21T21:23:49Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I guess I was mistaken about the source. My advice still stands.\nSo you should be able to get a full backtrace on this error if you use a debugger\nhttps://mooseframework.inl.gov/application_development/debugging.html\nthen we ll know which line is problematic",
                          "url": "https://github.com/idaholab/moose/discussions/22464#discussioncomment-3937253",
                          "updatedAt": "2022-10-21T21:29:36Z",
                          "publishedAt": "2022-10-21T21:29:35Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "heinono1"
                  },
                  "bodyText": "Thanks. Yeah I changed the lines related to interp_method (I have not\npulled a newer dist that should have the harmonic average fixed). I'll get\non with the debugger\n\u2026\nOn Fri, Oct 21, 2022 at 4:29 PM Guillaume Giudicelli < ***@***.***> wrote:\n I guess I was mistaken about the source. My advice still stands.\n\n So you should be able to get a full backtrace on this error if you use a\n debugger\n https://mooseframework.inl.gov/application_development/debugging.html\n then we ll know which line is problematic\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#22464 (reply in thread)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/AEKZEF7AFKLMKVRXU3XN5M3WEMDMVANCNFSM6AAAAAARLPGKXY>\n .\n You are receiving this because you authored the thread.Message ID:\n ***@***.***>\n\n\n-- \nOlle Heinonen\n***@***.***",
                  "url": "https://github.com/idaholab/moose/discussions/22464#discussioncomment-3937272",
                  "updatedAt": "2022-10-21T21:34:31Z",
                  "publishedAt": "2022-10-21T21:34:30Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "heinono1"
                  },
                  "bodyText": "Hi Guillaume,\nThe error occurs in the FV kernel when I try to access the gradients of the\nnonlinear variables. Specifically, in the kernel (computeQpResidual), if I\nhave\nreturn _normal(0)*mag_z[_qp];\nthe codes run fine. If I change to\n\nreturn _normal(0)*mag_z[_qp]*_mag_z_grad[_qp](0);\n\nthen I get the error.\n\nOn Fri, Oct 21, 2022 at 4:34 PM Olle Heinonen ***@***.***>\nwrote:\n\u2026\n Thanks. Yeah I changed the lines related to interp_method (I have not\n pulled a newer dist that should have the harmonic average fixed). I'll get\n on with the debugger\n\n On Fri, Oct 21, 2022 at 4:29 PM Guillaume Giudicelli <\n ***@***.***> wrote:\n\n> I guess I was mistaken about the source. My advice still stands.\n>\n> So you should be able to get a full backtrace on this error if you use a\n> debugger\n> https://mooseframework.inl.gov/application_development/debugging.html\n> then we ll know which line is problematic\n>\n> \u2014\n> Reply to this email directly, view it on GitHub\n> <#22464 (reply in thread)>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AEKZEF7AFKLMKVRXU3XN5M3WEMDMVANCNFSM6AAAAAARLPGKXY>\n> .\n> You are receiving this because you authored the thread.Message ID:\n> ***@***.***>\n>\n\n\n --\n Olle Heinonen\n ***@***.***\n\n\n-- \nOlle Heinonen\n***@***.***",
                  "url": "https://github.com/idaholab/moose/discussions/22464#discussioncomment-3937416",
                  "updatedAt": "2022-10-21T22:04:15Z",
                  "publishedAt": "2022-10-21T22:04:15Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "ok I think you ll need to use the gradient routine on the variable rather than the coupleable interface (coupledGradient routine)\nThis is because we compute gradients in FV using Green-Gauss, not using a local quadrature point derivative.\nSee FVDiffusion for accessing the gradient using a functor.",
                          "url": "https://github.com/idaholab/moose/discussions/22464#discussioncomment-3937485",
                          "updatedAt": "2022-10-21T22:15:12Z",
                          "publishedAt": "2022-10-21T22:15:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I ll raise an issue, this should have been caught in an error message",
                          "url": "https://github.com/idaholab/moose/discussions/22464#discussioncomment-3937488",
                          "updatedAt": "2022-10-21T22:16:00Z",
                          "publishedAt": "2022-10-21T22:15:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You will want to use an element argument here, in FVDiffusion it s a face argument.",
                          "url": "https://github.com/idaholab/moose/discussions/22464#discussioncomment-3937568",
                          "updatedAt": "2022-10-21T22:33:03Z",
                          "publishedAt": "2022-10-21T22:33:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "I believe this was essentially resolved in another discussion correct? The key change being moving from coupledGradient to adCoupledGradient or using the gradient functor API? Can we regard this discussion as \"answered\" by the other discussion?",
                          "url": "https://github.com/idaholab/moose/discussions/22464#discussioncomment-3987362",
                          "updatedAt": "2022-10-28T00:54:33Z",
                          "publishedAt": "2022-10-28T00:54:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "I suppose we should error when a user attempts coupledGradient with an FV var \ud83e\udd14",
                          "url": "https://github.com/idaholab/moose/discussions/22464#discussioncomment-3987372",
                          "updatedAt": "2022-10-28T00:56:38Z",
                          "publishedAt": "2022-10-28T00:56:37Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "yeah I created an issue, though it needs to be updated to say coupledGradient because I did not know that was the issue",
                          "url": "https://github.com/idaholab/moose/discussions/22464#discussioncomment-3987749",
                          "updatedAt": "2022-10-28T02:04:51Z",
                          "publishedAt": "2022-10-28T02:04:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Ah ok. Captured in #22465",
                          "url": "https://github.com/idaholab/moose/discussions/22464#discussioncomment-3988620",
                          "updatedAt": "2022-10-28T05:19:47Z",
                          "publishedAt": "2022-10-28T05:19:46Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "heinono1"
                  },
                  "bodyText": "Thanks, Guillaume.\n\u2026\nOn Fri, Oct 21, 2022, 5:33 PM Guillaume Giudicelli ***@***.***> wrote:\n You will want to use an element argument here, in FVDiffusion it s a face\n argument.\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#22464 (reply in thread)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/AEKZEF7UPDLWB62TP62XPVTWEMK2TANCNFSM6AAAAAARLPGKXY>\n .\n You are receiving this because you authored the thread.Message ID:\n ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/22464#discussioncomment-3940161",
                  "updatedAt": "2022-10-22T14:32:45Z",
                  "publishedAt": "2022-10-22T14:32:45Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Which linux version of the system does the MOOSE support?",
          "author": {
            "login": "KangChenRui"
          },
          "bodyText": "hello!\nWhich linux version of the system does the MOOSE support?\nI am currently reinstalling the system.",
          "url": "https://github.com/idaholab/moose/discussions/22527",
          "updatedAt": "2022-11-15T19:00:14Z",
          "publishedAt": "2022-10-28T00:52:54Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "this page lists the requirements\nhttps://mooseframework.inl.gov/getting_started/installation/manual_installation_gcc.html\nwe dont keep track of that as closely as we generally always support linux",
                  "url": "https://github.com/idaholab/moose/discussions/22527#discussioncomment-3987748",
                  "updatedAt": "2022-10-28T02:14:24Z",
                  "publishedAt": "2022-10-28T02:04:09Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Out of memory",
          "author": {
            "login": "RECHOA"
          },
          "bodyText": "When i run a case, i met a error shows 'EXIT CODE: 9'. Learning from other discussions, it means running out of memory. But acturally, the computational memory of our HPC has been added to 2TB, which is the maximum memory it could provide. Here is a part of the terminal log:\n  Initializing\n    Updating Because Mesh Changed\n      Updating Mesh.                                                                     [\ufffd[33m 11.78 s\ufffd[39m] [\ufffd[33m    8 MB\ufffd[39m]\n    Finished Updating Because Mesh Changed                                               [\ufffd[33m 12.00 s\ufffd[39m] [\ufffd[33m   36 MB\ufffd[39m]\n.                                                                     [\ufffd[33m 15.24 s\ufffd[39m] [\ufffd[33m    0 MB\ufffd[39m]\n    Finished Updating Because Mesh Changed                                               [\ufffd[33m 15.55 s\ufffd[39m] [\ufffd[33m    0 MB\ufffd[39m]\n....\n      Updating Geometric Search\n        Updating Displaced GeometricSearch\n          Finding Nearest Nodes\n            Building Node To Elem Map                                                    [\ufffd[33m  9.36 s\ufffd[39m] [\ufffd[33m  358 MB\ufffd[39m]\n          Finished Finding Nearest Nodes                                                 [\ufffd[33m  9.37 s\ufffd[39m] [\ufffd[33m  358 MB\ufffd[39m]\n        Finished Updating Displaced GeometricSearch                                      [\ufffd[33m  9.60 s\ufffd[39m] [\ufffd[33m  363 MB\ufffd[39m]\n      Finished Updating Geometric Search                                                 [\ufffd[33m  9.60 s\ufffd[39m] [\ufffd[33m  363 MB\ufffd[39m]\n      Finished Building Node To Elem Map                                                 [\ufffd[33m  9.33 s\ufffd[39m] [\ufffd[33m  426 MB\ufffd[39m]\n    Still Initializing Equation Systems.........                                         [\ufffd[33m 96.82 s\ufffd[39m] [\ufffd[33m 1386 MB\ufffd[39m]\n    Initializing Displaced Equation System..........                                     [\ufffd[33m 62.36 s\ufffd[39m] [\ufffd[33m  717 MB\ufffd[39m]\n      Finished Updating Mesh                                                             [\ufffd[33m 11.17 s\ufffd[39m] [\ufffd[33m    3 MB\ufffd[39m]\n    Finished Updating Because Mesh Changed                                               [\ufffd[33m 11.47 s\ufffd[39m] [\ufffd[33m    3 MB\ufffd[39m]\n  Finished Initializing                                                                  [\ufffd[33m198.22 s\ufffd[39m] [\ufffd[33m 2143 MB\ufffd[39m]\n[DBG][ACT] TASK (\ufffd[33m            check_output\ufffd[39m) TYPE (\ufffd[33m               CheckOutputAction\ufffd[39m) NAME (\ufffd[33m                \ufffd[39m) Memory usage 7089MB\n[DBG][ACT] TASK (\ufffd[33m         check_integrity\ufffd[39m) TYPE (\ufffd[33m            CheckIntegrityAction\ufffd[39m) NAME (\ufffd[33m                \ufffd[39m) Memory usage 7089MB\n[DBG][ACT] TASK (\ufffd[33m               no_action\ufffd[39m) TYPE (\ufffd[33m                     EmptyAction\ufffd[39m) NAME (\ufffd[33m    BEEsPressure\ufffd[39m) Memory usage 7089MB\n[DBG][ACT] TASK (\ufffd[33mfinish_input_file_output\ufffd[39m) TYPE (\ufffd[33m                     EmptyAction\ufffd[39m) NAME (\ufffd[33m    BEEsPressure\ufffd[39m) Memory usage 7089MB\n[DBG][ACT] Finished executing all actions with memory usage 7089MB\n\nFinished Setting Up                                                                      [\ufffd[33m1289.15 s\ufffd[39m] [\ufffd[33m 7037 MB\ufffd[39m]\nFramework Information:\nMOOSE Version:           git commit ccf25ce on 2022-07-13\nLibMesh Version:         \nPETSc Version:           3.16.5\nSLEPc Version:           3.16.2\nCurrent Time:            Thu Oct 20 17:57:41 2022\nExecutable Timestamp:    Sat Oct  8 02:20:19 2022\n\nParallelism:\n  Num Processors:          60\n  Num Threads:             1\n\nMesh: \n  Parallel Type:           replicated\n  Mesh Dimension:          3\n  Spatial Dimension:       3\n  Nodes:                   \n    Total:                 1713784\n    Local:                 30461\n    Min/Max/Avg:           24821/30461/28563\n  Elems:                   \n    Total:                 4937408\n    Local:                 82442\n    Min/Max/Avg:           80273/88294/82290\n  Num Subdomains:          3143\n  Num Partitions:          60\n  Partitioner:             metis\n\nNonlinear System:\n  Num DOFs:                6855136\n  Num Local DOFs:          121844\n  Variables:               { \"temp\" \"disp_x\" \"disp_y\" \"disp_z\" } \n  Finite Element Types:    \"LAGRANGE\" \n  Approximation Orders:    \"FIRST\" \n\nAuxiliary System:\n  Num DOFs:                63691744\n  Num Local DOFs:          1068823\n  Variables:               { \"fast_neutron_fluence\" \"baf\" \"hoop_stress_bees\" \"hoop_strain_bees\" } \"rad_disp\" \n                             \"maxPrincipal_stress\" { \"creep_strain\" \"thermal_strain\" \"swell_strain\" \"Weibull_failure_probability\" \n                             \"Pdpenetration_depth\" \"qpoint_penetration\" \"paired_temp\" } { \"penetration\" \n                             \"contact_pressure\" \"nodal_area_pellet_clad_mechanical\" } \n  Finite Element Types:    \"MONOMIAL\" \"LAGRANGE\" \"MONOMIAL\" \"MONOMIAL\" \"LAGRANGE\" \n  Approximation Orders:    \"CONSTANT\" \"FIRST\" \"CONSTANT\" \"CONSTANT\" \"FIRST\" \n\nExecution Information:\n  Executioner:             Transient\n  TimeStepper:             FunctionDT\n  Solver Mode:             Preconditioned JFNK\n  MOOSE Preconditioner:    SMP\n\n\ufffd[31mLEGACY MODES ENABLED:\ufffd[39m\n This application uses the legacy material output option: material properties are output only on TIMESTEP_END, not INITIAL. To remove this message, set 'use_legacy_material_output' to false in this application. If there are gold output files that contain material property output for which output occurs on INITIAL, then these will generate diffs due to zero values being stored, and these tests should be re-golded.\n\ufffd[39m\n\n    Setting Up Materials\n      Computing Initial Material Values.....\n===================================================================================\n=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES\n=   PID 87204 RUNNING AT ga0304\n=   EXIT CODE: 9\n=   CLEANING UP REMAINING PROCESSES\n=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES\n===================================================================================\nYOUR APPLICATION TERMINATED WITH THE EXIT STRING: Killed (signal 9)\nThis typically refers to a problem with your application.\nPlease see the FAQ page for debugging suggestions\n\nAlso, I add the\n[Debug]\n  show_actions = true\n[]\n\nto to see more information about the memory cost .\nThe complete record of terminal log shows in the following file (result_log.txt):\nresult_log.txt\nThe Executioner setting is\n[Executioner]\n  type = Transient\n  petsc_options_iname = '-ksp_gmres_restart -pc_type -pc_hypre_type -pc_hypre_boomeramg_max_iter'\n  petsc_options_value = '201                hypre    boomeramg      4'\n  line_search = 'none'\n  solve_type = PJFNK\n  l_tol =      0.0001\n  l_max_its =  20\n  nl_rel_tol = 1e-06\n  nl_abs_tol = 1e-05\n  nl_max_its = 15\n  dtmin = 0.1\n  dtmax = 1.0e6\n  start_time = 0.0\n  end_time = 9.0e7\n  [./TimeStepper]\n    type = FunctionDT\n    function = dts\n    min_dt = 1.0\n  [../]\n []\n\nI don't know which part of my case causing the memory exceed the maximum, and could anyone give better petsc options setting in the Executioner ?\nThank you in advance.",
          "url": "https://github.com/idaholab/moose/discussions/22452",
          "updatedAt": "2022-11-15T19:00:08Z",
          "publishedAt": "2022-10-20T11:20:03Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou're at least at 420Gb used.\nCould you try distributed meshes (--distributed-mesh on the command line) to reduce the memory consumption?\nAre you running this on a cluster scheduler that could have killed it for other reasons?\npetsc options should not be an issue yet when computing materials\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/22452#discussioncomment-3924474",
                  "updatedAt": "2022-10-20T12:56:17Z",
                  "publishedAt": "2022-10-20T12:56:17Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "RECHOA"
                          },
                          "bodyText": "The computational memory of HPC has been added to 2TB, it is quite strange that this case failed but only 420Gb used.\nThe mesh file isn't created by using MOOSE built-in mesh block (acturally by Cubit), i'm not sure whether the --distributed-mesh could work, but i'll try it.\nI run this case in HPC for many times but it failed due to the same error. Besides, for similar cases with less number of elements, they could run successfully.",
                          "url": "https://github.com/idaholab/moose/discussions/22452#discussioncomment-3924606",
                          "updatedAt": "2022-10-20T13:12:48Z",
                          "publishedAt": "2022-10-20T13:12:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "RECHOA"
                          },
                          "bodyText": "I tried the --distributed-mesh command, but it got the same error. Besides, the memory usage  seems to have increased rather than decreased. Here is the new new_result_log.txt file:\nnew_result_log.txt\nDoes the memory usage show in the file reperesent the total meomry or the memory in one processor?",
                          "url": "https://github.com/idaholab/moose/discussions/22452#discussioncomment-3925115",
                          "updatedAt": "2022-10-20T14:08:02Z",
                          "publishedAt": "2022-10-20T14:08:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "RECHOA"
                          },
                          "bodyText": "I used the command watch \u201cfree -g\u201d to see the change of the memory usage, and I noticed that the memory usage was normal (about 400~500GB) during the case running  until it run to the step:\n  Setting Up Materials\n     Computing Initial Material Values.....\n\nat that time, the memory usage would increase dramatically (exceed 2TB), and then the case failed.\nWhat should i do to get over this problem?",
                          "url": "https://github.com/idaholab/moose/discussions/22452#discussioncomment-3925951",
                          "updatedAt": "2022-10-20T15:30:06Z",
                          "publishedAt": "2022-10-20T15:30:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "The previous log was showing 7Gb used, this one shows 6167MB so it's a little better with distributed meshes.\nWhy do you have so many materials? Seems like thousands of materials and auxkernels.\nWhy cant you use the same auxkernel or materials on all the blocks? What is different?\n3000 subdomains is a little much as well.",
                          "url": "https://github.com/idaholab/moose/discussions/22452#discussioncomment-3934061",
                          "updatedAt": "2022-10-21T14:08:06Z",
                          "publishedAt": "2022-10-21T14:08:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "RECHOA"
                          },
                          "bodyText": "There are a lot of subdomains (spheres) in my case. In each subdomain (sphere), its eigen strain is related to its location, since spherical coordinate system was created in every sphere. So there're many materials.",
                          "url": "https://github.com/idaholab/moose/discussions/22452#discussioncomment-3934164",
                          "updatedAt": "2022-10-21T14:21:11Z",
                          "publishedAt": "2022-10-21T14:21:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "cant it be the same material defined across multiple blocks?\nThe location is a parameter of the material?",
                          "url": "https://github.com/idaholab/moose/discussions/22452#discussioncomment-3934181",
                          "updatedAt": "2022-10-21T14:22:30Z",
                          "publishedAt": "2022-10-21T14:22:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "RECHOA"
                          },
                          "bodyText": "yes, when calcuting the eigen strain for each sphere, the location is a parameter.",
                          "url": "https://github.com/idaholab/moose/discussions/22452#discussioncomment-3934205",
                          "updatedAt": "2022-10-21T14:24:19Z",
                          "publishedAt": "2022-10-21T14:24:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I'd change that to use the same material everywhere, and have the location parameter be either:\n\na really long vector (temporarily)\nread from a text file\nKeeping the number of objects down will help tremendously with simulation time and memory cost.",
                          "url": "https://github.com/idaholab/moose/discussions/22452#discussioncomment-3934258",
                          "updatedAt": "2022-10-21T14:28:46Z",
                          "publishedAt": "2022-10-21T14:28:45Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "same for the auxkernels",
                          "url": "https://github.com/idaholab/moose/discussions/22452#discussioncomment-3934262",
                          "updatedAt": "2022-10-21T14:28:56Z",
                          "publishedAt": "2022-10-21T14:28:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "RECHOA"
                          },
                          "bodyText": "Sorry maybe i have misinformed you. Although the auxkernels are related to the location paramete, but i declared the same auxvariable.\nFor example, when calculting the radial displacement, i declared the same \"variable = rad_disp\":\n  [./raddispaux_73]\n    type = RadialDisplacementSphereAux\n    variable = rad_disp\n    block = '363 '\n    origin = '0.000270492 -0.00235077 -0.00270761'\n  [../]\n  [./raddispaux_74]\n    type = RadialDisplacementSphereAux\n    variable = rad_disp\n    block = '368'\n    origin = '-0.00152647 0.000133102 -0.00589633'\n  [../]\n  ...\n\nAs for the eigen strian, they are in the form like this:\n  [./EigenStrain_232]\n    type = ComputeMyMaterialEigenstrain\n    temperature = temp\n    eigenstrain_name = 'pyc_eigen_232'\n    block = '1160'\n    origin = '-0.000275106 -0.00174505 7.86264e-05'\n  [../]\n  [./_EigenStrain_233]\n    type = ComputeMyMaterialEigenstrain\n    temperature = temp\n    eigenstrain_name = 'pyc_eigen_233'\n    block = '1163 '\n    origin = '0.00161647 -0.0019067799999999998 0.00480855'\n  [../]\n  ...",
                          "url": "https://github.com/idaholab/moose/discussions/22452#discussioncomment-3934487",
                          "updatedAt": "2022-10-21T14:49:57Z",
                          "publishedAt": "2022-10-21T14:49:56Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "dschwen"
                  },
                  "bodyText": "Can you show your PBS script?",
                  "url": "https://github.com/idaholab/moose/discussions/22452#discussioncomment-3924962",
                  "updatedAt": "2022-10-20T13:56:09Z",
                  "publishedAt": "2022-10-20T13:51:50Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "RECHOA"
                          },
                          "bodyText": "The script is like this:\n#!/bin/bash\n#SBATCH -p amd_2T\n#SBATCH -N 1\n#SBATCH -n 64\nsource  activate /public1/home/scb3820/anaconda3/envs/moose/\nexport PATH=/public1/home/scb3820/new-projects/bees_new:$PATH\nmpiexec -n 60  ./*opt -i input_master.i",
                          "url": "https://github.com/idaholab/moose/discussions/22452#discussioncomment-3925048",
                          "updatedAt": "2022-10-20T14:00:36Z",
                          "publishedAt": "2022-10-20T14:00:35Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "do you need to request memory?\nfor example\n#SBATCH --mem=2G           # total memory per node",
                          "url": "https://github.com/idaholab/moose/discussions/22452#discussioncomment-3944506",
                          "updatedAt": "2022-10-23T16:05:00Z",
                          "publishedAt": "2022-10-23T16:04:59Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}