{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMS0wNi0xMVQwMjo0ODowOC0wNjowMM4AM-di"
    },
    "edges": [
      {
        "node": {
          "title": "set a value of porepressure of a particular block?",
          "author": {
            "login": "Traiwit"
          },
          "bodyText": "Hi guys,\nis it possible to set a value of pore-pressure of a particular block (sub-domain), maybe via [Materials] block?\nI know I can define the boundaries of a sub-domain and then set the boundary of that block to zero (below).\nHowever, when I have like 100+ sub-domains, my input file gets real ugly.\nNote: I am currently using PorousFlowFullySaturated action\nKind regards,\nTraiwit\n  [add_side_sets_block6]\n    type = SideSetsAroundSubdomainGenerator\n    block = 6\n    input = add_side_sets\n    fixed_normal = true\n    new_boundary = 'front6 top6 right6 back6 bottom6 left6'\n    variance = 0.5\n  []\n\n  [./water_grad_block6]\n     type = DirichletBC\n     variable = porepressure\n     boundary = 'front6 top6 right6 back6 bottom6 left6'\n     value = 0\n  [../]",
          "url": "https://github.com/idaholab/moose/discussions/18090",
          "updatedAt": "2024-09-10T08:05:39Z",
          "publishedAt": "2021-06-16T01:26:43Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "WilkAndy"
                  },
                  "bodyText": "Yep, usually input files are looooong when you have lots of subdomains because lots of things are going on.   i usually write a python script to write my input files, to avoid mistakes.",
                  "url": "https://github.com/idaholab/moose/discussions/18090#discussioncomment-875953",
                  "updatedAt": "2024-09-10T08:05:41Z",
                  "publishedAt": "2021-06-16T01:57:06Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "Hi @WilkAndy, this is probably the easiest way to deal with the input file. Thanks!",
                          "url": "https://github.com/idaholab/moose/discussions/18090#discussioncomment-881105",
                          "updatedAt": "2024-09-10T08:05:41Z",
                          "publishedAt": "2021-06-17T03:30:25Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "permcody"
                  },
                  "bodyText": "The other solution to this problem is to create a custom \"Action\". Actions allow you to programmatically setup your problem (add variables, kernels, bcs, other objects). You can then replace a long list of complicated blocks with a much smaller, higher-level input block for your application.",
                  "url": "https://github.com/idaholab/moose/discussions/18090#discussioncomment-880506",
                  "updatedAt": "2024-09-10T08:05:43Z",
                  "publishedAt": "2021-06-16T21:46:59Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "For this example, if it's 0 DirichletBC all the time for all pores, then you could:\n\nadd many blocks to SideSetsAroundSubdomainGenerator to have all the sidesets (can one sideset cover them all? I m not sure)\nadd all the boundaries (or just one if that worked) to the DirichletBC",
                  "url": "https://github.com/idaholab/moose/discussions/18090#discussioncomment-880561",
                  "updatedAt": "2024-09-10T08:05:59Z",
                  "publishedAt": "2021-06-16T22:11:36Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "@GiudGiud thank you for your reply, I was thinking about that too, but it wouldn't work if I want to change the properties of each block at each particular time.",
                          "url": "https://github.com/idaholab/moose/discussions/18090#discussioncomment-881110",
                          "updatedAt": "2024-09-10T08:06:02Z",
                          "publishedAt": "2021-06-17T03:32:07Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "MOOSE for HPC",
          "author": {
            "login": "adfboyd"
          },
          "bodyText": "Hello,\nI'm attempting to run some NS simulations on an HPC cluster that uses slurm scheduling, and I was wondering if anyone had any experience with what a job submission script should look like in this case. I installed Moose using conda in the normal way used for Linux systems.\nAlso any advice on the recommended flags to use in terms of MPIexec and threads in the context of HPC where we will hope to use several thousand cores would be very much appreciated!",
          "url": "https://github.com/idaholab/moose/discussions/17946",
          "updatedAt": "2022-06-10T13:51:19Z",
          "publishedAt": "2021-05-27T10:36:20Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "milljm"
                  },
                  "bodyText": "Conda isn't really a very optimized solution when running on thousands of cores. It would be best to get in touch with your HPC admins, and ask them how to use the MPI stack they created specially for their cluster.\nWe have some basic instructions on how to build this compiler stack if they truly do not have one available for you: https://mooseframework.inl.gov/getting_started/installation/hpc_install_moose.html\nYou can still use Conda. And it will work. But the MPI stack distributed with this product was only intended to run on a single node (aka: your workstation).\nAs for as how to create a SLURM launch script, I can only suggest the results of a google search: https://ubccr.freshdesk.com/support/solutions/articles/5000688140-submitting-a-slurm-job-script\nLots of things we can't fill in the blanks for though, since each HPC cluster is different (module load this, module load that, resource allocation, etc)",
                  "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-793336",
                  "updatedAt": "2022-06-10T13:51:19Z",
                  "publishedAt": "2021-05-27T14:31:14Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "adfboyd"
                          },
                          "bodyText": "Thanks, I wasn't aware of the drawbacks of conda. In terms of scripting, I was just more concerned with how best to use the -nthreads and -mpiexec flags with Moose, although of course I understand that this might be very hard to make any statements about in the general case too.",
                          "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-811825",
                          "updatedAt": "2022-06-10T13:51:25Z",
                          "publishedAt": "2021-06-01T16:12:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "permcody"
                          },
                          "bodyText": "@adfboyd - You'll likely want to talk to your system administrators for some advice. Typically, schedulers and their associated submission scripts are MPI aware so you'll focus on how many nodes/cores you want in the submission script and then run your application with something like mpiexec <path to my application> without any -n <cpus> option. Things can get more complex when running MPI+threading, but I'll advise against that unless you know you'll need it or have already mastered the pure MPI runs.",
                          "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-812044",
                          "updatedAt": "2022-06-10T13:51:23Z",
                          "publishedAt": "2021-06-01T16:59:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "I was just more concerned with how best to use the -nthreads and -mpiexec flags with Moose, although of course I understand that this might be very hard to make any statements about in the general case too.\n\nFor most of cases, I would suggest stay with pure MPIs unless your application has very specific structure where threading concurrency can be explored.\nIn my experience, the pure MPI often performs better than the hybrid",
                          "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-812463",
                          "updatedAt": "2022-06-10T13:51:23Z",
                          "publishedAt": "2021-06-01T18:39:25Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "adfboyd"
                  },
                  "bodyText": "Thank you for the advice. We're having a bit of trouble installing MOOSE as per the HPC instructions.\nThe commands that we are using to build MOOSE are:\n\n\ncd /work/e643/e643/dmse643/\nmkdir projects\ncd projects/\ngit clone https://github.com/idaholab/moose.git\ncd moose\ngit checkout master\nmodule restore PrgEnv-gnu\nmodule load cmake\nmodule load cray-python\nunset PETSC_DIR PETSC_ARCH\n./scripts/update_and_rebuild_petsc.sh\n./scripts/update_and_rebuild_libmesh.sh\ncd test\nmake -j 4\n\n\nLibmesh is built successfully (although it takes a long time) but the compilation of MOOSE fails because the linker cannot find libdsmml.la:\n\n\n/usr/bin/grep: /install/dsmml/opt/lib/libdsmml.la: No such file or directory\nand indeed there is no such file, however\n/opt/cray/pe/dsmml/0.1.2/dsmml/lib/libdsmml.la\nexists.\n\n\nWe do not yet know how to tell the build system where to find it, how can we do this?\nThanks again!",
                  "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-816987",
                  "updatedAt": "2022-06-10T13:51:24Z",
                  "publishedAt": "2021-06-02T17:02:49Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Do you need to add /opt/cray/pe/dsmml/0.1.2/dsmml/lib/ to your LIBRARY_PATH?",
                  "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-817041",
                  "updatedAt": "2022-06-10T13:51:28Z",
                  "publishedAt": "2021-06-02T17:14:36Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "adfboyd"
                          },
                          "bodyText": "Thank you for this suggestion. Unfortunately we're still having problems which we suspect are due to the system software - we're having this investigated at the moment.",
                          "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-820374",
                          "updatedAt": "2022-06-10T13:51:28Z",
                          "publishedAt": "2021-06-03T11:47:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Can you perform a module list after your module load events and list those modules? Also, an env afterwards would help as well.",
                          "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-821005",
                          "updatedAt": "2022-06-10T13:51:29Z",
                          "publishedAt": "2021-06-03T14:04:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "adfboyd"
                          },
                          "bodyText": "I have attached the output of 'module list' and 'env'\nOnce libmesh is installed incorrect paths to dsmml are specified in\nlibmesh/installed/lib/libmesh_opt.la\nlibmesh/installed/lib/libmesh_dbg.la\nlibmesh/installed/lib/libtimpi_oprof.la\nlibmesh/installed/lib/libtimpi_dbg.la\nlibmesh/installed/lib/libmetaphysicl.la\nlibmesh/installed/lib/libtimpi_opt.la\nlibmesh/installed/lib/libmesh_oprof.la\nFor example, in libmesh/installed/lib/libmesh_opt.la:\ndependency_libs='\n/work/e643/e643/dmse643/projects/moose/scripts/../libmesh/installed/lib/libnetcdf.la\n/work/e643/e643/dmse643/projects/moose/scripts/../libmesh/installed/lib/libtimpi_opt.la\n/install/dsmml/opt/lib/libdsmml.la\n/opt/gcc/10.1.0/snos/lib/../lib64/libgfortran.la\n/opt/gcc/10.1.0/snos/lib/../lib64/libquadmath.la\n/opt/gcc/10.1.0/snos/lib/../lib64/libstdc++.la\n-L/work/e643/e643/dmse643/projects/moose/petsc/arch-moose/lib\n-L/opt/cray/pe/dsmml/0.1.2/dsmml/lib -L/opt/gcc/10.1.0/snos/lib64\n-L/opt/gcc/10.1.0/snos/lib -lz -lslepc -lpetsc -lHYPRE -lcmumps -ldmumps\n-lsmumps -lzmumps -lmumps_common -lpord -lstrumpack -lscalapack\n-lsuperlu_dist -lflapack -lfblas -lptesmumps -lptscotchparmetis\n-lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lX11 -lparmetis\n-lmetis -lmpifort_gnu_91 -lrt /install/dsmml/opt/lib/libdsmml.la\n/opt/gcc/10.1.0/snos/lib/../lib64/libgfortran.la\n/opt/gcc/10.1.0/snos/lib/../lib64/libquadmath.la\n/opt/gcc/10.1.0/snos/lib/../lib64/libquadmath.la\n/opt/gcc/10.1.0/snos/lib/../lib64/libstdc++.la'\nThe last two lines of\n/opt/cray/pe/dsmml/0.1.2/dsmml/lib/libdsmml.la\nare\nDirectory that this library needs to be installe\nd in:\nlibdir='/install/dsmml/opt/lib'\nwhich may be the cause of the problem.\nmodule_and_env[3745].txt",
                          "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-821337",
                          "updatedAt": "2022-06-10T13:51:33Z",
                          "publishedAt": "2021-06-03T15:10:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "adfboyd"
                          },
                          "bodyText": "Apologies, the 3rd last line should not be a header, it was merely a hashtag indent.",
                          "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-821365",
                          "updatedAt": "2022-08-02T15:22:57Z",
                          "publishedAt": "2021-06-03T15:14:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I like to use markdown for preformatted text:\n```\npreformatted text goes here\n```\npreformatted text goes here\n\nYou can even declare the language for syntax highlighting:\n```bash\nfor i in `seq 1 9`;  do\necho $i\ndone\n```\nfor i in `seq 1 9`;  do\n  echo $i\ndone \nPython:\n```python\nfor i in []:\nprint(i)\n```\nfor i in []:\n  print(i)\nAnyways!!! I am not seeing anything stand out as an issue within your environment. I think you are on the right track about something off in link library instruction file (or whatever .la files are used for). Would it be possible to unload that one specific module?\nmodule unload cray-dsmml/0.1.2\nAnd then rebuild libMesh. MOOSE gets all (maybe most) of its compiler links/flags from libMesh.\nI think modulecmd is pretty lenient about unloading dependency modules. As in; it shouldn't unload anything else. But do check after unloading it to see if it only unloads that one specific module.\nThis may not be a solution, but I am curious as to the results. To speed up building libMesh, you can export the following variable to instruct the build script how many cores are available to build with:\nexport MOOSE_JOBS=n      # where n is the number of cores you are allowed to use\nEDIT:\nYou can also instruct libMesh to only build one method (by default we are asking you to build it three times opt devel oprof):\nexport METHODS='opt'\nwill only build optimized version",
                          "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-821538",
                          "updatedAt": "2022-08-02T15:22:57Z",
                          "publishedAt": "2021-06-03T15:43:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "adfboyd"
                          },
                          "bodyText": "It seems that\ngit clone https://github.com/idaholab/moose.git\ncd moose\ngit checkout master\nmodule restore PrgEnv-gnu\nmodule load cmake\nmodule load cray-python\nmodule unload cray-dsmml\nexport MOOSE_JOBS=8\nunset PETSC_DIR PETSC_ARCH\n./scripts/update_and_rebuild_petsc.sh\n./scripts/update_and_rebuild_libmesh.sh\ncd test\nmake -j 8\n\nworked as the build terminated with\nLinking Library /work/e643/e643/dmse643/projects/moose/framework/libmoose-opt.la...\nLinking Library /work/e643/e643/dmse643/projects/moose/test/lib/libmoose_test-opt.la...\nLinking Executable /work/e643/e643/dmse643/projects/moose/test/moose_test-opt...\n\nDoes this seem like a successful installation? Thank you again!",
                          "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-821938",
                          "updatedAt": "2022-08-02T15:22:58Z",
                          "publishedAt": "2021-06-03T17:14:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "It does... not sure what is wrong with that module. But, if it is not needed, I'd say continue to not use it!\nStill, do the tests pass?\ncd /work/e643/e643/dmse643/projects/moose/test\n./run_tests -j 8",
                          "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-821953",
                          "updatedAt": "2022-08-02T15:22:59Z",
                          "publishedAt": "2021-06-03T17:18:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "adfboyd"
                          },
                          "bodyText": "I have attached the output from an attempt to run the tests. It seems to have failed, any insights on what to do next would be very helpful. This was run as a batch job, but I did try just doing\n./run_tests -j 8\n\non the login node, that failed too.\nMoosetestoutputs.txt",
                          "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-834444",
                          "updatedAt": "2022-08-02T15:22:59Z",
                          "publishedAt": "2021-06-07T11:33:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Everything that tried to run failed due to a timeout. It feels as though this job was not allowed to run with the resources requested or something.\nBefore we try to make this work on the nodes, I suggest we try to make this work on the headnode first. What are the errors when you tried runing ./run_tests -j 8 on the headnode? Were they also timeout errors? Can you run one test manually and see what happens?\nPicking a random one that failed, I mean to say, can you run the same exact command that ./run_tests was trying to run:\n\ufffd[31mmesh_modifiers/add_extra_nodeset.extra/test: \ufffd[0mWorking Directory: /lus/cls01095/work/e643/e643/shared/mooseprojects/moose/test/tests/mesh_modifiers/add_extra_nodeset\n\ufffd[31mmesh_modifiers/add_extra_nodeset.extra/test: \ufffd[0mRunning command: /lus/cls01095/work/e643/e643/shared/mooseprojects/moose/test/moose_test-opt -i extra_nodeset_test.i --error --error-unused --error-override --no-gdb-backtrace\nThat is, establish your environment (load modules) and then run:\ncd /lus/cls01095/work/e643/e643/shared/mooseprojects/moose/test/tests/mesh_modifiers/add_extra_nodeset\n/lus/cls01095/work/e643/e643/shared/mooseprojects/moose/test/moose_test-opt -i extra_nodeset_test.i --error --error-unused --error-override --no-gdb-backtrace",
                          "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-835748",
                          "updatedAt": "2022-08-02T15:22:59Z",
                          "publishedAt": "2021-06-07T16:17:33Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "adfboyd"
                          },
                          "bodyText": "I ran\nmodule restore PrgEnv-gnu\nmodule load cray-python\ncd /lus/cls01095/work/e643/e643/shared/mooseprojects/moose/test/tests/mesh_modifiers/add_extra_nodeset\n/lus/cls01095/work/e643/e643/shared/mooseprojects/moose/test/moose_test-opt -i extra_nodeset_test.i --error --error-unused --error-override --no-gdb-backtrace\n\nThe errors were:\nMPICH ERROR [Rank 0] [job id unknown] [Mon Jun  7 17:51:15 2021] [unknown] [uan01] - Abort(590863) (rank 0 in comm 0): Fatal error in PMPI_Init_thread: Other MPI error, error stack:\nMPIR_Init_thread(632): \nMPID_Init(286).......:  PMI2 init failed: 1 \n\naborting job:\nFatal error in PMPI_Init_thread: Other MPI error, error stack:\nMPIR_Init_thread(632): \nMPID_Init(286).......:  PMI2 init failed: 1 \n\nIn general the errors I saw on the login node were not timeout errors - it would happen quickly, and \"Pool not running\"  was common.",
                          "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-835925",
                          "updatedAt": "2022-08-02T15:23:00Z",
                          "publishedAt": "2021-06-07T16:56:00Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "adfboyd"
                  },
                  "bodyText": "Thank you! I've arranged a meeting with a support team for the cluster next week, if we're still stuck he may be able to help out a bit.",
                  "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-858409",
                  "updatedAt": "2022-06-10T13:51:31Z",
                  "publishedAt": "2021-06-11T14:15:59Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "milljm"
                  },
                  "bodyText": "I have created an issue: #18099 to address supporting Slurm.",
                  "url": "https://github.com/idaholab/moose/discussions/17946#discussioncomment-878787",
                  "updatedAt": "2022-06-10T13:51:31Z",
                  "publishedAt": "2021-06-16T15:00:12Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Frozen solving environment step in installing moose in linux",
          "author": {
            "login": "Ali1990dashti"
          },
          "bodyText": "Dear community,\nSince yesterday I have bee tryin to install moose. I am usnig ubunu 20.04 and conda 4.8.3. I exactly followed explained instructions. But, when I try the line conda create --name moose moose-libmesh moose-tools  it stucks in the solving environment step. I tried to install it also in my base environemnt and also out of conda but the results was the same and I stuck in the same step. Uploaded fig also shows it I do appreciate any help to solve my issue.",
          "url": "https://github.com/idaholab/moose/discussions/18093",
          "updatedAt": "2023-01-14T02:23:34Z",
          "publishedAt": "2021-06-16T08:32:09Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Try\nconda create -n moose -q -y\nconda activate moose\nconda install moose-tools\nconda install moose-libmesh\nIt solves faster",
                  "url": "https://github.com/idaholab/moose/discussions/18093#discussioncomment-878407",
                  "updatedAt": "2023-01-14T02:23:39Z",
                  "publishedAt": "2021-06-16T14:27:22Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Ali1990dashti"
                          },
                          "bodyText": "Dear @GiudGiud ,\nThanks a lot.\nYour solution fixed my problem.",
                          "url": "https://github.com/idaholab/moose/discussions/18093#discussioncomment-878571",
                          "updatedAt": "2023-01-14T02:23:44Z",
                          "publishedAt": "2021-06-16T14:35:25Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Stochastic - error in SamplerReceiver",
          "author": {
            "login": "MatiasAllay"
          },
          "bodyText": "Hi MOOSE,\nI'm using the stochastic module to perform a SOBOL sensitivity analysis. However, I get the following error\n*** ERROR *** runner4: The following error occurred in the object \"stochastic\", of type \"SamplerReceiver\". runner4:  runner4: The desired parameter 'Materials/velocity_k/value' was not located for the 'stochastic' object, it either does not exist or has not been declared as controllable\nNot sure what is going on... I get the same error for every parameter. Any idea?\nCheers,\nMatias",
          "url": "https://github.com/idaholab/moose/discussions/18046",
          "updatedAt": "2022-06-12T21:39:27Z",
          "publishedAt": "2021-06-09T19:25:50Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "MatiasAllay"
                  },
                  "bodyText": "Hi MOOSE,\nMy questions was not really precise, I leave here a simplified version of my problem. I would appreciate if someone can take a look.\nI am sure is some stupid mistake!\nhttps://drive.google.com/drive/folders/1Fu-Yn80egK4Bep-PHbB5dE3G85_rKFWP?usp=sharing\nCheers,\nMatias",
                  "url": "https://github.com/idaholab/moose/discussions/18046#discussioncomment-853691",
                  "updatedAt": "2022-06-12T21:39:29Z",
                  "publishedAt": "2021-06-10T16:13:27Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "The first error message says that porosity is not declared as controllable. To make the control system work, the parameters need to be declared as controllable. See https://mooseframework.inl.gov/syntax/Controls/index.html\nAfter looking at PorousFlowPorosityConst, the porosity is a variable name and it probably cannot be declared as controllable variable.\n\nFor your case, I recommend you use `MultiAppCommandLineControl. See https://mooseframework.inl.gov/source/controls/MultiAppCommandLineControl.html\nIt is a more flexible way to set the parameters because parameter values are directly set through command line which does not require them to be controllable.",
                          "url": "https://github.com/idaholab/moose/discussions/18046#discussioncomment-853799",
                          "updatedAt": "2022-06-12T21:39:29Z",
                          "publishedAt": "2021-06-10T16:36:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "MatiasAllay"
                          },
                          "bodyText": "Hi,\nThanks for you reply. I took some time to perform some tests. The MultiAppCommandLineControl sounds like a excellent solution. However, when I apply it to my problem I get the error\nexecute_on: The sampler object, 'sobol', is being used by the 'cmdline' object, thus the 'execute_on' of the sampler must include 'PRE_MULTIAPP_SETUP'.\nBut apparently, the Sampler/Sobol does not have the flag execute_on (https://mooseframework.inl.gov/moose/source/samplers/SobolSampler.html). Any idea how can I solve this?\nThanks a lot!\nMatias",
                          "url": "https://github.com/idaholab/moose/discussions/18046#discussioncomment-873195",
                          "updatedAt": "2022-06-12T21:39:34Z",
                          "publishedAt": "2021-06-15T13:06:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "Comment out the following line, recompile, and run with execute_on set as recommend. I am not remembering why that was disabled, but we will see if that gets it working for you.\n\n  \n    \n      moose/modules/stochastic_tools/src/samplers/SobolSampler.C\n    \n    \n         Line 24\n      in\n      48fe2f2\n    \n  \n  \n    \n\n        \n          \n           params.suppressParameter<ExecFlagEnum>(\"execute_on\");",
                          "url": "https://github.com/idaholab/moose/discussions/18046#discussioncomment-873528",
                          "updatedAt": "2022-06-12T21:39:56Z",
                          "publishedAt": "2021-06-15T14:21:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "MatiasAllay"
                          },
                          "bodyText": "Thanks! That worked perfectly.\nMatias",
                          "url": "https://github.com/idaholab/moose/discussions/18046#discussioncomment-874026",
                          "updatedAt": "2022-12-27T23:19:52Z",
                          "publishedAt": "2021-06-15T15:59:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "Great, I will have to change that in MOOSE.",
                          "url": "https://github.com/idaholab/moose/discussions/18046#discussioncomment-876008",
                          "updatedAt": "2022-12-27T23:19:52Z",
                          "publishedAt": "2021-06-16T02:19:23Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Coupled Variable Name",
          "author": {
            "login": "rtaylo45"
          },
          "bodyText": "Im writing a kernel and that works with two coupled variables. I was wondering if there is a way to retrieve the name of a specific couple variable while inside the code for the kernel. For example, if my input file has this example kernel\n[kernels]\n  [exampleKernel]\n    type = ExampleKernel\n    variable = nonLinearVar\n    coupled_var_1 = example_coupled_var_1\n    coupled_var_2 = example_coupled_var_2\n  []\n[]\n\nIn my kernel i can use _var.name() to retrieve the name of the primal variable. But how would get the name of the associated inputs for coupled_var_1 and coupled_var_2? Something like _coupled_var_1.name() and _coupled_var_1.name(). From examples i have seen with multi-physics coupling, the type of the coupled variable is VariableValue which is not a type which I can extract a variable name from.",
          "url": "https://github.com/idaholab/moose/discussions/18084",
          "updatedAt": "2022-09-06T14:25:43Z",
          "publishedAt": "2021-06-14T16:52:28Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nI would do getFieldVar(\"parameter_name\", 0).name()  (might be ->name())\n0 is the component, I'm assuming you are not using array variables.\n\"parameter_name\" is \"coupled_var_i\" here\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/18084#discussioncomment-869548",
                  "updatedAt": "2022-09-06T14:25:50Z",
                  "publishedAt": "2021-06-14T16:56:31Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "dschwen"
                          },
                          "bodyText": "See here for example:\n\n  \n    \n      moose/modules/tensor_mechanics/src/materials/ComputeVariableEigenstrain.C\n    \n    \n         Line 36\n      in\n      06bc1f7\n    \n  \n  \n    \n\n        \n          \n           const VariableName & iname = getVar(\"args\", i)->name();",
                          "url": "https://github.com/idaholab/moose/discussions/18084#discussioncomment-869903",
                          "updatedAt": "2022-09-06T14:26:08Z",
                          "publishedAt": "2021-06-14T18:39:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "rtaylo45"
                          },
                          "bodyText": "Awesome, this works. Thanks for the help",
                          "url": "https://github.com/idaholab/moose/discussions/18084#discussioncomment-870314",
                          "updatedAt": "2022-09-06T14:26:19Z",
                          "publishedAt": "2021-06-14T20:24:31Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "MultiApp--Moose Test Issue",
          "author": {
            "login": "AhmedAlmetwally"
          },
          "bodyText": "Bug Description\nI am building a multiapp using Moose. I had the following issues while running the tests. Mostly related to MPI and Libmoose library.\nmesh/unique_ids.replicated_mesh .................................................. [min_cpus=2] FAILED (CRASH)\nsamplers/distribute.scale/execute ............................................................. FAILED (CRASH)\nperformance.multiprocess/mpi ................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nreporters/base.base .............................................................. [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/parallel_consistency.test ................................... [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/csv_reader.parallel ......................................... [min_cpus=2] FAILED (CRASH)\nsystem_interfaces.mpi .......................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\npostprocessors/num_residual_eval.test ............................................ [min_cpus=2] FAILED (CRASH)\noutputs/json/distributed.info/default ............................................ [min_cpus=2] FAILED (CRASH)\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject ........ [min_cpus=2] FAILED (CRASH)\nsystem_interfaces.partitioner/parmetis ........................................... [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/work_balance.work_balance/replicated ........................ [min_cpus=2] FAILED (CRASH)\nmesh/checkpoint.test_2 ........................................................... [min_cpus=2] FAILED (CRASH)\npreconditioners/hmg.hmg ........................................ [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nsamplers/base.parallel/mpi ....................................................... [min_cpus=2] FAILED (CRASH)\noutputs/xml.parallel/replicated .................................................. [min_cpus=3] FAILED (CRASH)\nsystem_interfaces.partitioner/ptscotch ........................................... [min_cpus=2] FAILED (CRASH)\nmisc/exception.parallel_exception_residual_transient_non_zero_rank ............... [min_cpus=2] FAILED (CRASH)\nmeshgenerators/meta_data_store.test_meta_data_with_use_split ..................... [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/parallel_consistency.broadcast .............................. [min_cpus=2] FAILED (CRASH)\nmesh/checkpoint.test_2a .......................................................... [min_cpus=2] FAILED (CRASH)\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel ................... [min_cpus=2] FAILED (CRASH)\ninterfacekernels/2d_interface.parallel_fdp_test .................................. [min_cpus=2] FAILED (CRASH)\nsystem_interfaces.solver/superlu ................................................. [min_cpus=2] FAILED (CRASH)\nbcs/periodic.testperiodic_vector ............................................................. FAILED (ERRMSG)\nmesh/custom_partitioner.group/custom_linear_partitioner .......................... [min_cpus=2] FAILED (CRASH)\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer ......................... [min_cpus=2] FAILED (CRASH)\nreporters/mesh_info.info/default ................................................. [min_cpus=2] FAILED (CRASH)\nsystem_interfaces.solver/mumps ................................................... [min_cpus=2] FAILED (CRASH)\nauxkernels/vector_postprocessor_visualization.test ............................... [min_cpus=3] FAILED (CRASH)\nbcs/dmg_periodic.check_one_step .................................................. [min_cpus=2] FAILED (CRASH)\noutputs/vtk.files/parallel ....................................................... [min_cpus=2] FAILED (CRASH)\nics/depend_on_uo.ic_depend_on_uo ................................................. [min_cpus=2] FAILED (CRASH)\nbcs/periodic.testperiodic_dp ..................................................... [min_cpus=2] FAILED (CRASH)\nfvkernels/mms/non-orthogonal.compact ............................................. [min_cpus=2] FAILED (CRASH)\nrestart/restartable_types.parallel/first ......................................... [min_cpus=2] FAILED (CRASH)\npreconditioners/hmg.hmg_3D ..................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nics/depend_on_uo.scalar_ic_from_uo ............................................... [min_cpus=2] FAILED (CRASH)\nmesh/mesh_only.mesh_only_checkpoint .............................................. [min_cpus=3] FAILED (CRASH)\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement ............. [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/csv_reader.tester_fail ..................... [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\ntransfers/multiapp_nearest_node_transfer.parallel ................................ [min_cpus=2] FAILED (CRASH)\nfvkernels/mms/non-orthogonal.extended ............................................ [min_cpus=2] FAILED (CRASH)\nmesh/custom_partitioner.group/custom_linear_partitioner_restart .................. [min_cpus=2] FAILED (CRASH)\noutputs/vtk.solution/diff_serial_mesh_parallel ................................... [min_cpus=2] FAILED (CRASH)\ninterfaces/random.parallel_verification .......................................... [min_cpus=2] FAILED (CRASH)\nmeshgenerators/distributed_rectilinear/partition.2D_3 ............................ [min_cpus=3] FAILED (CRASH)\nrestart/kernel_restartable.parallel_error/error1 ................................. [min_cpus=2] FAILED (CRASH)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor ................ [min_cpus=3] FAILED (CRASH)\noutputs/xml.parallel/distributed ................................................. [min_cpus=3] FAILED (CRASH)\nIssue #1\nmesh/unique_ids.distributed_mesh ................................................... [skipped dependency] SKIP\nmesh/unique_ids.replicated_mesh: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/mesh/unique_ids\nmesh/unique_ids.replicated_mesh: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i unique_ids.i --error --error-unused --error-override --no-gdb-backtrace\nmesh/unique_ids.replicated_mesh: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/unique_ids.replicated_mesh: MPIR_Init_thread(586)..............:\nmesh/unique_ids.replicated_mesh: MPID_Init(224).....................: channel initialization failed\nmesh/unique_ids.replicated_mesh: MPIDI_CH3_Init(105)................:\nmesh/unique_ids.replicated_mesh: MPID_nem_init(324).................:\nmesh/unique_ids.replicated_mesh: MPID_nem_tcp_init(175).............:\nmesh/unique_ids.replicated_mesh: MPID_nem_tcp_get_business_card(401):\nmesh/unique_ids.replicated_mesh: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/unique_ids.replicated_mesh: (unknown)(): Invalid group\nmesh/unique_ids.replicated_mesh:\nmesh/unique_ids.replicated_mesh:\nmesh/unique_ids.replicated_mesh: Exit Code: 8\nmesh/unique_ids.replicated_mesh: ################################################################################\nmesh/unique_ids.replicated_mesh: Tester failed, reason: CRASH\nmesh/unique_ids.replicated_mesh:\nmesh/unique_ids.replicated_mesh .................................................. [min_cpus=2] FAILED (CRASH)\nIssue #2\nvariables/fe_hermite_convergence.hermite_convergance/periodic ............................................. OK\nsamplers/distribute.scale/plot ..................................................... [skipped dependency] SKIP\nsamplers/distribute.scale/execute: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/samplers/distribute\nsamplers/distribute.scale/execute: Running command: /Users/almeag-mac/projects1/moose/test/tests/samplers/distribute/execute.py\nsamplers/distribute.scale/execute: mpiexec -n 1 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i distribute.i Outputs/file_base=distribute_1 Postprocessors/test/test_type=getGlobalSamples Samplers/sampler/num_rows=1\nsamplers/distribute.scale/execute: Traceback (most recent call last):\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/projects1/moose/test/tests/samplers/distribute/execute.py\", line 47, in \nsamplers/distribute.scale/execute:     execute('distribute.i', 'distribute_none', 1, args.processors, 'getGlobalSamples')\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/projects1/moose/test/tests/samplers/distribute/execute.py\", line 29, in execute\nsamplers/distribute.scale/execute:     local = pandas.read_csv('{}.csv'.format(file_base))\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 605, in read_csv\nsamplers/distribute.scale/execute:     return _read(filepath_or_buffer, kwds)\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 457, in _read\nsamplers/distribute.scale/execute:     parser = TextFileReader(filepath_or_buffer, **kwds)\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 814, in init\nsamplers/distribute.scale/execute:     self._engine = self._make_engine(self.engine)\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1045, in _make_engine\nsamplers/distribute.scale/execute:     return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1862, in init\nsamplers/distribute.scale/execute:     self._open_handles(src, kwds)\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1363, in _open_handles\nsamplers/distribute.scale/execute:     storage_options=kwds.get(\"storage_options\", None),\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/common.py\", line 647, in get_handle\nsamplers/distribute.scale/execute:     newline=\"\",\nsamplers/distribute.scale/execute: FileNotFoundError: [Errno 2] No such file or directory: 'distribute_1.csv'\nsamplers/distribute.scale/execute: mpiexec -n 1 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i distribute.i Outputs/file_base=distribute_1 Postprocessors/test/test_type=getGlobalSamples Samplers/sampler/num_rows=1\nsamplers/distribute.scale/execute: Traceback (most recent call last):\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/projects1/moose/test/tests/samplers/distribute/execute.py\", line 47, in \nsamplers/distribute.scale/execute:     execute('distribute.i', 'distribute_none', 1, args.processors, 'getGlobalSamples')\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/projects1/moose/test/tests/samplers/distribute/execute.py\", line 29, in execute\nsamplers/distribute.scale/execute:     local = pandas.read_csv('{}.csv'.format(file_base))\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 605, in read_csv\nsamplers/distribute.scale/execute:     return _read(filepath_or_buffer, kwds)\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 457, in _read\nsamplers/distribute.scale/execute:     parser = TextFileReader(filepath_or_buffer, **kwds)\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 814, in init\nsamplers/distribute.scale/execute:     self._engine = self._make_engine(self.engine)\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1045, in _make_engine\nsamplers/distribute.scale/execute:     return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1862, in init\nsamplers/distribute.scale/execute:     self._open_handles(src, kwds)\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/parsers.py\", line 1363, in _open_handles\nsamplers/distribute.scale/execute:     storage_options=kwds.get(\"storage_options\", None),\nsamplers/distribute.scale/execute:   File \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/site-packages/pandas/io/common.py\", line 647, in get_handle\nsamplers/distribute.scale/execute:     newline=\"\",\nsamplers/distribute.scale/execute: FileNotFoundError: [Errno 2] No such file or directory: 'distribute_1.csv'\nsamplers/distribute.scale/execute:\nsamplers/distribute.scale/execute:\nsamplers/distribute.scale/execute: Exit Code: 1\nsamplers/distribute.scale/execute: ################################################################################\nsamplers/distribute.scale/execute: Tester failed, reason: CRASH\nsamplers/distribute.scale/execute:\nsamplers/distribute.scale/execute ............................................................. FAILED (CRASH)\nIssue #3\nfvbcs/fv_neumannbc.fvbcs_internal ......................................................................... OK\nperformance.multiprocess/mpi: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/performance\nperformance.multiprocess/mpi: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i input.i --error --error-unused --error-override --no-gdb-backtrace\nperformance.multiprocess/mpi: Fatal error in MPI_Init_thread: Invalid group, error stack:\nperformance.multiprocess/mpi: MPIR_Init_thread(586)..............:\nperformance.multiprocess/mpi: MPID_Init(224).....................: channel initialization failed\nperformance.multiprocess/mpi: MPIDI_CH3_Init(105)................:\nperformance.multiprocess/mpi: MPID_nem_init(324).................:\nperformance.multiprocess/mpi: MPID_nem_tcp_init(175).............:\nperformance.multiprocess/mpi: MPID_nem_tcp_get_business_card(401):\nperformance.multiprocess/mpi: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nperformance.multiprocess/mpi: (unknown)(): Invalid group\nperformance.multiprocess/mpi: Fatal error in MPI_Init_thread: Invalid group, error stack:\nperformance.multiprocess/mpi: MPIR_Init_thread(586)..............:\nperformance.multiprocess/mpi: MPID_Init(224).....................: channel initialization failed\nperformance.multiprocess/mpi: MPIDI_CH3_Init(105)................:\nperformance.multiprocess/mpi: MPID_nem_init(324).................:\nperformance.multiprocess/mpi: MPID_nem_tcp_init(175).............:\nperformance.multiprocess/mpi: MPID_nem_tcp_get_business_card(401):\nperformance.multiprocess/mpi: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nperformance.multiprocess/mpi: (unknown)(): Invalid group\nperformance.multiprocess/mpi: ################################################################################\nperformance.multiprocess/mpi:\nperformance.multiprocess/mpi: Unable to match the following pattern against the program's output:\nperformance.multiprocess/mpi:\nperformance.multiprocess/mpi: Num Processors:\\s+2\\s+Num Threads:\\s+1\nperformance.multiprocess/mpi:\nperformance.multiprocess/mpi: ################################################################################\nperformance.multiprocess/mpi: Tester failed, reason: EXPECTED OUTPUT MISSING\nperformance.multiprocess/mpi:\nperformance.multiprocess/mpi ................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nrestrictable/block_api_test.block_undefined_var_block ..................................................... OK\nreporters/base.base: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/reporters/base\nreporters/base.base: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i base.i --allow-test-objects --error --error-unused --error-override --no-gdb-backtrace\nreporters/base.base: Fatal error in MPI_Init_thread: Invalid group, error stack:\nreporters/base.base: MPIR_Init_thread(586)..............:\nreporters/base.base: MPID_Init(224).....................: channel initialization failed\nreporters/base.base: MPIDI_CH3_Init(105)................:\nreporters/base.base: MPID_nem_init(324).................:\nreporters/base.base: MPID_nem_tcp_init(175).............:\nreporters/base.base: MPID_nem_tcp_get_business_card(401):\nreporters/base.base: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nreporters/base.base: (unknown)(): Invalid group\nreporters/base.base:\nreporters/base.base:\nreporters/base.base: Exit Code: 8\nreporters/base.base: ################################################################################\nreporters/base.base: Tester failed, reason: CRASH\nreporters/base.base:\nreporters/base.base .............................................................. [min_cpus=2] FAILED (CRASH)\nreporters/constant_reporter.errors/no_values .............................................................. OK\nIssue #4\nvectorpostprocessors/element_value_sampler.element_value_sampler/lagrange ................................. OK\nvectorpostprocessors/parallel_consistency.test: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/vectorpostprocessors/parallel_consistency\nvectorpostprocessors/parallel_consistency.test: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i parallel_consistency.i --error --error-unused --error-override --no-gdb-backtrace\nvectorpostprocessors/parallel_consistency.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.test: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.test: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.test: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/parallel_consistency.test: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.test: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.test: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.test: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/parallel_consistency.test: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.test: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.test: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.test: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/parallel_consistency.test: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.test: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.test: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.test: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/parallel_consistency.test: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.test:\nvectorpostprocessors/parallel_consistency.test:\nvectorpostprocessors/parallel_consistency.test: Exit Code: 8\nvectorpostprocessors/parallel_consistency.test: ################################################################################\nvectorpostprocessors/parallel_consistency.test: Tester failed, reason: CRASH\nvectorpostprocessors/parallel_consistency.test:\nvectorpostprocessors/parallel_consistency.test ................................... [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/intersection_points_along_line.intersecting_elems/3d ................................. OK\nvectorpostprocessors/csv_reader.parallel: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/vectorpostprocessors/csv_reader\nvectorpostprocessors/csv_reader.parallel: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i read.i UserObjects/tester/rank=1 Outputs/csv=false --error --error-unused --error-override --no-gdb-backtrace\nvectorpostprocessors/csv_reader.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/csv_reader.parallel: MPIR_Init_thread(586)..............:\nvectorpostprocessors/csv_reader.parallel: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/csv_reader.parallel: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/csv_reader.parallel: MPID_nem_init(324).................:\nvectorpostprocessors/csv_reader.parallel: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/csv_reader.parallel: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/csv_reader.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/csv_reader.parallel: (unknown)(): Invalid group\nvectorpostprocessors/csv_reader.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/csv_reader.parallel: MPIR_Init_thread(586)..............:\nvectorpostprocessors/csv_reader.parallel: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/csv_reader.parallel: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/csv_reader.parallel: MPID_nem_init(324).................:\nvectorpostprocessors/csv_reader.parallel: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/csv_reader.parallel: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/csv_reader.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/csv_reader.parallel: (unknown)(): Invalid group\nvectorpostprocessors/csv_reader.parallel:\nvectorpostprocessors/csv_reader.parallel:\nvectorpostprocessors/csv_reader.parallel: Exit Code: 8\nvectorpostprocessors/csv_reader.parallel: ################################################################################\nvectorpostprocessors/csv_reader.parallel: Tester failed, reason: CRASH\nvectorpostprocessors/csv_reader.parallel:\nvectorpostprocessors/csv_reader.parallel ......................................... [min_cpus=2] FAILED (CRASH)\ntime_integrators/implicit-euler.monomials ................................................................. OK\n\nIssue #5\nrestrictable/block_api_test.ids/blocks .................................................................... OK\nsystem_interfaces.mpi: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/system_interfaces\nsystem_interfaces.mpi: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i input.i --error --error-unused --error-override --no-gdb-backtrace\nsystem_interfaces.mpi: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.mpi: MPIR_Init_thread(586)..............:\nsystem_interfaces.mpi: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.mpi: MPIDI_CH3_Init(105)................:\nsystem_interfaces.mpi: MPID_nem_init(324).................:\nsystem_interfaces.mpi: MPID_nem_tcp_init(175).............:\nsystem_interfaces.mpi: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.mpi: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsystem_interfaces.mpi: (unknown)(): Invalid group\nsystem_interfaces.mpi: ################################################################################\nsystem_interfaces.mpi:\nsystem_interfaces.mpi: Unable to match the following pattern against the program's output:\nsystem_interfaces.mpi:\nsystem_interfaces.mpi: Num Processors:\\s+2\\s+Num Threads:\\s+1\nsystem_interfaces.mpi:\nsystem_interfaces.mpi: ################################################################################\nsystem_interfaces.mpi: Tester failed, reason: EXPECTED OUTPUT MISSING\nsystem_interfaces.mpi:\nsystem_interfaces.mpi .......................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\npreconditioners/fdp.jacobian_fdp_coloring_diagonal_test_fail .............................................. OK\nIssue #6\ntime_integrators/rk-2.jacobian/2d-quadratic_num-of-jacobian-calls ......................................... OK\npostprocessors/num_residual_eval.test: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/postprocessors/num_residual_eval\npostprocessors/num_residual_eval.test: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i num_residual_eval.i --error --error-unused --error-override --no-gdb-backtrace\npostprocessors/num_residual_eval.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\npostprocessors/num_residual_eval.test: MPIR_Init_thread(586)..............:\npostprocessors/num_residual_eval.test: MPID_Init(224).....................: channel initialization failed\npostprocessors/num_residual_eval.test: MPIDI_CH3_Init(105)................:\npostprocessors/num_residual_eval.test: MPID_nem_init(324).................:\npostprocessors/num_residual_eval.test: MPID_nem_tcp_init(175).............:\npostprocessors/num_residual_eval.test: MPID_nem_tcp_get_business_card(401):\npostprocessors/num_residual_eval.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\npostprocessors/num_residual_eval.test: (unknown)(): Invalid group\npostprocessors/num_residual_eval.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\npostprocessors/num_residual_eval.test: MPIR_Init_thread(586)..............:\npostprocessors/num_residual_eval.test: MPID_Init(224).....................: channel initialization failed\npostprocessors/num_residual_eval.test: MPIDI_CH3_Init(105)................:\npostprocessors/num_residual_eval.test: MPID_nem_init(324).................:\npostprocessors/num_residual_eval.test: MPID_nem_tcp_init(175).............:\npostprocessors/num_residual_eval.test: MPID_nem_tcp_get_business_card(401):\npostprocessors/num_residual_eval.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\npostprocessors/num_residual_eval.test: (unknown)(): Invalid group\npostprocessors/num_residual_eval.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\npostprocessors/num_residual_eval.test: MPIR_Init_thread(586)..............:\npostprocessors/num_residual_eval.test: MPID_Init(224).....................: channel initialization failed\npostprocessors/num_residual_eval.test: MPIDI_CH3_Init(105)................:\npostprocessors/num_residual_eval.test: MPID_nem_init(324).................:\npostprocessors/num_residual_eval.test: MPID_nem_tcp_init(175).............:\npostprocessors/num_residual_eval.test: MPID_nem_tcp_get_business_card(401):\npostprocessors/num_residual_eval.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\npostprocessors/num_residual_eval.test: (unknown)(): Invalid group\npostprocessors/num_residual_eval.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\npostprocessors/num_residual_eval.test: MPIR_Init_thread(586)..............:\npostprocessors/num_residual_eval.test: MPID_Init(224).....................: channel initialization failed\npostprocessors/num_residual_eval.test: MPIDI_CH3_Init(105)................:\npostprocessors/num_residual_eval.test: MPID_nem_init(324).................:\npostprocessors/num_residual_eval.test: MPID_nem_tcp_init(175).............:\npostprocessors/num_residual_eval.test: MPID_nem_tcp_get_business_card(401):\npostprocessors/num_residual_eval.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\npostprocessors/num_residual_eval.test: (unknown)(): Invalid group\npostprocessors/num_residual_eval.test:\npostprocessors/num_residual_eval.test:\npostprocessors/num_residual_eval.test: Exit Code: 8\npostprocessors/num_residual_eval.test: ################################################################################\npostprocessors/num_residual_eval.test: Tester failed, reason: CRASH\npostprocessors/num_residual_eval.test:\npostprocessors/num_residual_eval.test ............................................ [min_cpus=2] FAILED (CRASH)\npostprocessors/change_over_fixed_point.change_over_fixed_point_error/change_with_respect_to_initial_error_this OK\nIssue #7\noutputs/postprocessor.show_hide ........................................................................... OK\noutputs/json/distributed.info/default: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/outputs/json/distributed\noutputs/json/distributed.info/default: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i distributed.i --error --error-unused --error-override --no-gdb-backtrace\noutputs/json/distributed.info/default: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/json/distributed.info/default: MPIR_Init_thread(586)..............:\noutputs/json/distributed.info/default: MPID_Init(224).....................: channel initialization failed\noutputs/json/distributed.info/default: MPIDI_CH3_Init(105)................:\noutputs/json/distributed.info/default: MPID_nem_init(324).................:\noutputs/json/distributed.info/default: MPID_nem_tcp_init(175).............:\noutputs/json/distributed.info/default: MPID_nem_tcp_get_business_card(401):\noutputs/json/distributed.info/default: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/json/distributed.info/default: (unknown)(): Invalid group\noutputs/json/distributed.info/default: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/json/distributed.info/default: MPIR_Init_thread(586)..............:\noutputs/json/distributed.info/default: MPID_Init(224).....................: channel initialization failed\noutputs/json/distributed.info/default: MPIDI_CH3_Init(105)................:\noutputs/json/distributed.info/default: MPID_nem_init(324).................:\noutputs/json/distributed.info/default: MPID_nem_tcp_init(175).............:\noutputs/json/distributed.info/default: MPID_nem_tcp_get_business_card(401):\noutputs/json/distributed.info/default: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/json/distributed.info/default: (unknown)(): Invalid group\noutputs/json/distributed.info/default:\noutputs/json/distributed.info/default:\noutputs/json/distributed.info/default: Exit Code: 8\noutputs/json/distributed.info/default: ################################################################################\noutputs/json/distributed.info/default: Tester failed, reason: CRASH\noutputs/json/distributed.info/default:\noutputs/json/distributed.info/default ............................................ [min_cpus=2] FAILED (CRASH)\noutputs/intervals.sync_times .............................................................................. OK\nIssue #8\nbcs/periodic.orthogonal_pbc_on_square_test ................................................................ OK\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/userobjects/setup_interface_count\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i general.i --error --error-unused --error-override --no-gdb-backtrace\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Fatal error in MPI_Init_thread: Invalid group, error stack:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPIR_Init_thread(586)..............:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_Init(224).....................: channel initialization failed\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPIDI_CH3_Init(105)................:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_init(324).................:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_init(175).............:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_get_business_card(401):\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: (unknown)(): Invalid group\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Fatal error in MPI_Init_thread: Invalid group, error stack:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPIR_Init_thread(586)..............:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_Init(224).....................: channel initialization failed\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPIDI_CH3_Init(105)................:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_init(324).................:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_init(175).............:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_get_business_card(401):\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: (unknown)(): Invalid group\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Exit Code: 8\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: ################################################################################\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Tester failed, reason: CRASH\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject ........ [min_cpus=2] FAILED (CRASH)\nbcs/nodal_normals.small_sqaure ............................................................................ OK\nIssue #9\noutputs/misc.default_names ................................................................................ OK\nsystem_interfaces.partitioner/parmetis: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/system_interfaces\nsystem_interfaces.partitioner/parmetis: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i input.i --error --error-unused --error-override --no-gdb-backtrace\nsystem_interfaces.partitioner/parmetis: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.partitioner/parmetis: MPIR_Init_thread(586)..............:\nsystem_interfaces.partitioner/parmetis: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.partitioner/parmetis: MPIDI_CH3_Init(105)................:\nsystem_interfaces.partitioner/parmetis: MPID_nem_init(324).................:\nsystem_interfaces.partitioner/parmetis: MPID_nem_tcp_init(175).............:\nsystem_interfaces.partitioner/parmetis: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.partitioner/parmetis: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsystem_interfaces.partitioner/parmetis: (unknown)(): Invalid group\nsystem_interfaces.partitioner/parmetis: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.partitioner/parmetis: MPIR_Init_thread(586)..............:\nsystem_interfaces.partitioner/parmetis: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.partitioner/parmetis: MPIDI_CH3_Init(105)................:\nsystem_interfaces.partitioner/parmetis: MPID_nem_init(324).................:\nsystem_interfaces.partitioner/parmetis: MPID_nem_tcp_init(175).............:\nsystem_interfaces.partitioner/parmetis: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.partitioner/parmetis: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsystem_interfaces.partitioner/parmetis: (unknown)(): Invalid group\nsystem_interfaces.partitioner/parmetis:\nsystem_interfaces.partitioner/parmetis:\nsystem_interfaces.partitioner/parmetis: Exit Code: 8\nsystem_interfaces.partitioner/parmetis: ################################################################################\nsystem_interfaces.partitioner/parmetis: Tester failed, reason: CRASH\nsystem_interfaces.partitioner/parmetis:\nsystem_interfaces.partitioner/parmetis ........................................... [min_cpus=2] FAILED (CRASH)\noutputs/exodus.hide_output ................................................................................ OK\nIssue #10\nconstraints/equal_value_embedded_constraint.penalty/1D_3D ................................................. OK\nvectorpostprocessors/work_balance.work_balance/replicated: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/vectorpostprocessors/work_balance\nvectorpostprocessors/work_balance.work_balance/replicated: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i work_balance.i --error --error-unused --error-override --no-gdb-backtrace\nvectorpostprocessors/work_balance.work_balance/replicated: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/work_balance.work_balance/replicated: MPIR_Init_thread(586)..............:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/work_balance.work_balance/replicated: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_init(324).................:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/work_balance.work_balance/replicated: (unknown)(): Invalid group\nvectorpostprocessors/work_balance.work_balance/replicated: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/work_balance.work_balance/replicated: MPIR_Init_thread(586)..............:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/work_balance.work_balance/replicated: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_init(324).................:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/work_balance.work_balance/replicated: (unknown)(): Invalid group\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated: Exit Code: 8\nvectorpostprocessors/work_balance.work_balance/replicated: ################################################################################\nvectorpostprocessors/work_balance.work_balance/replicated: Tester failed, reason: CRASH\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated ........................ [min_cpus=2] FAILED (CRASH)\nmesh/checkpoint.test_2: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/mesh/checkpoint\nmesh/checkpoint.test_2: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i checkpoint_split.i Outputs/file_base=test_2 --use-split --split-file checkpoint_split_in --error --error-unused --error-override --no-gdb-backtrace\nmesh/checkpoint.test_2: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_2: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_2: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_2: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_2: MPID_nem_init(324).................:\nmesh/checkpoint.test_2: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_2: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_2: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/checkpoint.test_2: (unknown)(): Invalid group\nmesh/checkpoint.test_2: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_2: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_2: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_2: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_2: MPID_nem_init(324).................:\nmesh/checkpoint.test_2: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_2: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_2: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/checkpoint.test_2: (unknown)(): Invalid group\nmesh/checkpoint.test_2:\nmesh/checkpoint.test_2:\nmesh/checkpoint.test_2: Exit Code: 8\nmesh/checkpoint.test_2: ################################################################################\nmesh/checkpoint.test_2: Tester failed, reason: CRASH\nmesh/checkpoint.test_2:\nmesh/checkpoint.test_2 ........................................................... [min_cpus=2] FAILED (CRASH)\npostprocessors/num_iterations.methods/explicit_euler ...................................................... OK\nIssue #11\ntransfers/multiapp_mesh_function_transfer.errors/mismatch_exec_on ......................................... OK\npreconditioners/hmg.hmg: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/preconditioners/hmg\npreconditioners/hmg.hmg: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i diffusion_hmg.i --error --error-unused --error-override --no-gdb-backtrace\npreconditioners/hmg.hmg: Fatal error in MPI_Init_thread: Invalid group, error stack:\npreconditioners/hmg.hmg: MPIR_Init_thread(586)..............:\npreconditioners/hmg.hmg: MPID_Init(224).....................: channel initialization failed\npreconditioners/hmg.hmg: MPIDI_CH3_Init(105)................:\npreconditioners/hmg.hmg: MPID_nem_init(324).................:\npreconditioners/hmg.hmg: MPID_nem_tcp_init(175).............:\npreconditioners/hmg.hmg: MPID_nem_tcp_get_business_card(401):\npreconditioners/hmg.hmg: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\npreconditioners/hmg.hmg: (unknown)(): Invalid group\npreconditioners/hmg.hmg: Fatal error in MPI_Init_thread: Invalid group, error stack:\npreconditioners/hmg.hmg: MPIR_Init_thread(586)..............:\npreconditioners/hmg.hmg: MPID_Init(224).....................: channel initialization failed\npreconditioners/hmg.hmg: MPIDI_CH3_Init(105)................:\npreconditioners/hmg.hmg: MPID_nem_init(324).................:\npreconditioners/hmg.hmg: MPID_nem_tcp_init(175).............:\npreconditioners/hmg.hmg: MPID_nem_tcp_get_business_card(401):\npreconditioners/hmg.hmg: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\npreconditioners/hmg.hmg: (unknown)(): Invalid group\npreconditioners/hmg.hmg: ################################################################################\npreconditioners/hmg.hmg:\npreconditioners/hmg.hmg: Unable to match the following pattern against the program's output:\npreconditioners/hmg.hmg:\npreconditioners/hmg.hmg: using\\s+allatonce\\s+MatPtAP()\\s+implementation\npreconditioners/hmg.hmg:\npreconditioners/hmg.hmg: ################################################################################\npreconditioners/hmg.hmg: Tester failed, reason: EXPECTED OUTPUT MISSING\npreconditioners/hmg.hmg:\npreconditioners/hmg.hmg ........................................ [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nsamplers/base.parallel/mpi: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/samplers/base\nsamplers/base.parallel/mpi: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i mpi.i --allow-test-objects --error --error-unused --error-override --no-gdb-backtrace\nsamplers/base.parallel/mpi: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsamplers/base.parallel/mpi: MPIR_Init_thread(586)..............:\nsamplers/base.parallel/mpi: MPID_Init(224).....................: channel initialization failed\nsamplers/base.parallel/mpi: MPIDI_CH3_Init(105)................:\nsamplers/base.parallel/mpi: MPID_nem_init(324).................:\nsamplers/base.parallel/mpi: MPID_nem_tcp_init(175).............:\nsamplers/base.parallel/mpi: MPID_nem_tcp_get_business_card(401):\nsamplers/base.parallel/mpi: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsamplers/base.parallel/mpi: (unknown)(): Invalid group\nsamplers/base.parallel/mpi:\nsamplers/base.parallel/mpi:\nsamplers/base.parallel/mpi: Exit Code: 8\nsamplers/base.parallel/mpi: ################################################################################\nsamplers/base.parallel/mpi: Tester failed, reason: CRASH\nsamplers/base.parallel/mpi:\nsamplers/base.parallel/mpi ....................................................... [min_cpus=2] FAILED (CRASH)\ntransfers/multiapp_userobject_transfer.two_pipes .......................................................... OK\nIssue #11\nrestrictable/block_api_test.has/isBlockSubset ............................................................. OK\noutputs/xml.parallel/replicated: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/outputs/xml\noutputs/xml.parallel/replicated: Running command: mpiexec -n 3 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i xml.i --error --error-unused --error-override --no-gdb-backtrace\noutputs/xml.parallel/replicated: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/xml.parallel/replicated: MPIR_Init_thread(586)..............:\noutputs/xml.parallel/replicated: MPID_Init(224).....................: channel initialization failed\noutputs/xml.parallel/replicated: MPIDI_CH3_Init(105)................:\noutputs/xml.parallel/replicated: MPID_nem_init(324).................:\noutputs/xml.parallel/replicated: MPID_nem_tcp_init(175).............:\noutputs/xml.parallel/replicated: MPID_nem_tcp_get_business_card(401):\noutputs/xml.parallel/replicated: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/xml.parallel/replicated: (unknown)(): Invalid group\noutputs/xml.parallel/replicated:\noutputs/xml.parallel/replicated: ################################################################################\noutputs/xml.parallel/replicated: Tester failed, reason: CRASH\noutputs/xml.parallel/replicated:\noutputs/xml.parallel/replicated .................................................. [min_cpus=3] FAILED (CRASH)\ninterfacekernels/1d_interface.ik_save_in .................................................................. OK\nIssue #12\nreporters/base.errors/requested_different_type ............................................................ OK\nsystem_interfaces.partitioner/ptscotch: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/system_interfaces\nsystem_interfaces.partitioner/ptscotch: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i input.i --error --error-unused --error-override --no-gdb-backtrace\nsystem_interfaces.partitioner/ptscotch: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.partitioner/ptscotch: MPIR_Init_thread(586)..............:\nsystem_interfaces.partitioner/ptscotch: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.partitioner/ptscotch: MPIDI_CH3_Init(105)................:\nsystem_interfaces.partitioner/ptscotch: MPID_nem_init(324).................:\nsystem_interfaces.partitioner/ptscotch: MPID_nem_tcp_init(175).............:\nsystem_interfaces.partitioner/ptscotch: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.partitioner/ptscotch: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsystem_interfaces.partitioner/ptscotch: (unknown)(): Invalid group\nsystem_interfaces.partitioner/ptscotch:\nsystem_interfaces.partitioner/ptscotch:\nsystem_interfaces.partitioner/ptscotch: Exit Code: 8\nsystem_interfaces.partitioner/ptscotch: ################################################################################\nsystem_interfaces.partitioner/ptscotch: Tester failed, reason: CRASH\nsystem_interfaces.partitioner/ptscotch:\nsystem_interfaces.partitioner/ptscotch ........................................... [min_cpus=2] FAILED (CRASH)\ninterfacekernels/1d_interface.ik_save_in_other_side ....................................................... OK\nbcs/periodic.testperiodic ................................................................................. OK\noutputs/debug.show_material_properties_consumed ........................................................... OK\nics/random_ic_test.test_threaded .......................................................... [min_threads=2] OK\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/misc/exception\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i parallel_exception_residual_transient.i Kernels/exception/rank=1 --error --error-unused --error-override --no-gdb-backtrace\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPIR_Init_thread(586)..............:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_Init(224).....................: channel initialization failed\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPIDI_CH3_Init(105)................:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_init(324).................:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_init(175).............:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_get_business_card(401):\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: (unknown)(): Invalid group\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPIR_Init_thread(586)..............:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_Init(224).....................: channel initialization failed\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPIDI_CH3_Init(105)................:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_init(324).................:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_init(175).............:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_get_business_card(401):\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: (unknown)(): Invalid group\nmisc/exception.parallel_exception_residual_transient_non_zero_rank:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Exit Code: 8\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: ################################################################################\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Tester failed, reason: CRASH\nmisc/exception.parallel_exception_residual_transient_non_zero_rank:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank ............... [min_cpus=2] FAILED (CRASH)\nnodalkernels/constraint_enforcement.vi/rsls ............................................................... OK\nIssue #13\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/meshgenerators/meta_data_store\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i mesh_meta_data_store.i --use-split --split-file split2 --error --error-unused --error-override --no-gdb-backtrace\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPIR_Init_thread(586)..............:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPIDI_CH3_Init(105)................:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_nem_init(324).................:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_nem_tcp_init(175).............:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: (unknown)(): Invalid group\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPIR_Init_thread(586)..............:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPIDI_CH3_Init(105)................:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_nem_init(324).................:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_nem_tcp_init(175).............:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: (unknown)(): Invalid group\nmeshgenerators/meta_data_store.test_meta_data_with_use_split:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: Exit Code: 8\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: ################################################################################\nmeshgenerators/meta_data_store.test_meta_data_with_use_split: Tester failed, reason: CRASH\nmeshgenerators/meta_data_store.test_meta_data_with_use_split:\nmeshgenerators/meta_data_store.test_meta_data_with_use_split ..................... [min_cpus=2] FAILED (CRASH)\ntime_steppers/timesequence_stepper.restart_failure/timesequence_restart_failure_1 ......................... OK\nmultiapps/steffensen.variables_transient/app_end_transfers_end ............................................ OK\nmaterials/output.invalid_outputs .......................................................................... OK\nuserobjects/threaded_general_user_object.thread_copies_guo/th2 ............................ [min_threads=2] OK\nmesh/high_order_elems.test_prism6_refine .................................................................. OK\nvectorpostprocessors/parallel_consistency.broadcast: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/vectorpostprocessors/parallel_consistency\nvectorpostprocessors/parallel_consistency.broadcast: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i parallel_consistency.i AuxKernels/viewit/use_broadcast=true Outputs/file_base=broadcast_out --error --error-unused --error-override --no-gdb-backtrace\nvectorpostprocessors/parallel_consistency.broadcast: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.broadcast: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.broadcast: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/parallel_consistency.broadcast: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.broadcast: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.broadcast: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.broadcast: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/parallel_consistency.broadcast: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.broadcast: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.broadcast: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.broadcast: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/parallel_consistency.broadcast: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.broadcast: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.broadcast: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.broadcast: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.broadcast: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/parallel_consistency.broadcast: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.broadcast:\nvectorpostprocessors/parallel_consistency.broadcast:\nvectorpostprocessors/parallel_consistency.broadcast: Exit Code: 8\nvectorpostprocessors/parallel_consistency.broadcast: ################################################################################\nvectorpostprocessors/parallel_consistency.broadcast: Tester failed, reason: CRASH\nvectorpostprocessors/parallel_consistency.broadcast:\nvectorpostprocessors/parallel_consistency.broadcast .............................. [min_cpus=2] FAILED (CRASH)\nfvkernels/fv_simple_diffusion.unstructured-rz ............................................................. OK\nmesh/checkpoint.test_2a: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/mesh/checkpoint\nmesh/checkpoint.test_2a: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i checkpoint_split.i Outputs/file_base=test_2a --use-split --split-file checkpoint_split_in.cpa --error --error-unused --error-override --no-gdb-backtrace\nmesh/checkpoint.test_2a: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_2a: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_2a: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_2a: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_2a: MPID_nem_init(324).................:\nmesh/checkpoint.test_2a: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_2a: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_2a: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/checkpoint.test_2a: (unknown)(): Invalid group\nmesh/checkpoint.test_2a: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_2a: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_2a: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_2a: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_2a: MPID_nem_init(324).................:\nmesh/checkpoint.test_2a: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_2a: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_2a: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/checkpoint.test_2a: (unknown)(): Invalid group\nmesh/checkpoint.test_2a: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_2a: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_2a: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_2a: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_2a: MPID_nem_init(324).................:\nmesh/checkpoint.test_2a: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_2a: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_2a: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/checkpoint.test_2a: (unknown)(): Invalid group\nmesh/checkpoint.test_2a: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_2a: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_2a: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_2a: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_2a: MPID_nem_init(324).................:\nmesh/checkpoint.test_2a: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_2a: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_2a: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/checkpoint.test_2a: (unknown)(): Invalid group\nmesh/checkpoint.test_2a:\nmesh/checkpoint.test_2a:\nmesh/checkpoint.test_2a: Exit Code: 8\nmesh/checkpoint.test_2a: ################################################################################\nmesh/checkpoint.test_2a: Tester failed, reason: CRASH\nmesh/checkpoint.test_2a:\nmesh/checkpoint.test_2a .......................................................... [min_cpus=2] FAILED (CRASH)\nuserobjects/layered_average.layered_average/block_restricted .............................................. OK\npostprocessors/find_value_on_line.depth_exceeded .......................................................... OK\nconstraints/equal_value_embedded_constraint.penalty/3D_3D ................................................. OK\npostprocessors/num_iterations.methods/heun ................................................................ OK\ntransfers/reporter_transfer.clone_type/type_specified ................... [insufficient slots,min_cpus=6] SKIP\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/dgkernels/2d_diffusion_dg\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i no_mallocs_with_action.i Outputs/file_base=no_mallocs_with_action_parallel --error --error-unused --error-override --no-gdb-backtrace\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPIR_Init_thread(586)..............:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_Init(224).....................: channel initialization failed\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPIDI_CH3_Init(105)................:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_nem_init(324).................:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_nem_tcp_init(175).............:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_nem_tcp_get_business_card(401):\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: (unknown)(): Invalid group\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPIR_Init_thread(586)..............:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_Init(224).....................: channel initialization failed\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPIDI_CH3_Init(105)................:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_nem_init(324).................:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_nem_tcp_init(175).............:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_nem_tcp_get_business_card(401):\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: (unknown)(): Invalid group\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: Exit Code: 8\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: ################################################################################\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel: Tester failed, reason: CRASH\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel:\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel ................... [min_cpus=2] FAILED (CRASH)\ntime_integrators/convergence.explicit_midpoint/level2 ..................................................... OK\nproblems/eigen_problem/eigensolvers.coupled_system ........................................................ OK\ninterfacekernels/2d_interface.parallel_fdp_test: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/interfacekernels/2d_interface\ninterfacekernels/2d_interface.parallel_fdp_test: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i coupled_value_coupled_flux.i Preconditioning/smp/type=FDP --error-unused --error-override --no-gdb-backtrace\ninterfacekernels/2d_interface.parallel_fdp_test: Fatal error in MPI_Init_thread: Invalid group, error stack:\ninterfacekernels/2d_interface.parallel_fdp_test: MPIR_Init_thread(586)..............:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_Init(224).....................: channel initialization failed\ninterfacekernels/2d_interface.parallel_fdp_test: MPIDI_CH3_Init(105)................:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_init(324).................:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_init(175).............:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_get_business_card(401):\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ninterfacekernels/2d_interface.parallel_fdp_test: (unknown)(): Invalid group\ninterfacekernels/2d_interface.parallel_fdp_test: Fatal error in MPI_Init_thread: Invalid group, error stack:\ninterfacekernels/2d_interface.parallel_fdp_test: MPIR_Init_thread(586)..............:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_Init(224).....................: channel initialization failed\ninterfacekernels/2d_interface.parallel_fdp_test: MPIDI_CH3_Init(105)................:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_init(324).................:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_init(175).............:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_get_business_card(401):\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ninterfacekernels/2d_interface.parallel_fdp_test: (unknown)(): Invalid group\ninterfacekernels/2d_interface.parallel_fdp_test:\ninterfacekernels/2d_interface.parallel_fdp_test:\ninterfacekernels/2d_interface.parallel_fdp_test: Exit Code: 8\ninterfacekernels/2d_interface.parallel_fdp_test: ################################################################################\ninterfacekernels/2d_interface.parallel_fdp_test: Tester failed, reason: CRASH\ninterfacekernels/2d_interface.parallel_fdp_test:\ninterfacekernels/2d_interface.parallel_fdp_test .................................. [min_cpus=2] FAILED (CRASH)\nmaterials/derivative_material_interface.warn .............................................................. OK\nvariables/optionally_coupled.catch_out_of_bound_default_access/coupled .................................... OK\nsamplers/base.global_vs_local/base_1rank .................................................................. OK\nfunctions/solution_function.nonexistent_var_err ........................................................... OK\nfunctions/piecewise_multilinear.twoDa ..................................................................... OK\nrestrictable/block_api_test.mat/hasBlockMaterialProperty_false ............................................ OK\noutputs/checkpoint.block/recover_with_checkpoint_block .................................................... OK\nreporters/base.special_types .............................................................................. OK\noutputs/exodus.nodal_output ............................................................................... OK\nsystem_interfaces.solver/superlu: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/system_interfaces\nsystem_interfaces.solver/superlu: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i input.i --error --error-unused --error-override --no-gdb-backtrace\nsystem_interfaces.solver/superlu: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.solver/superlu: MPIR_Init_thread(586)..............:\nsystem_interfaces.solver/superlu: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.solver/superlu: MPIDI_CH3_Init(105)................:\nsystem_interfaces.solver/superlu: MPID_nem_init(324).................:\nsystem_interfaces.solver/superlu: MPID_nem_tcp_init(175).............:\nsystem_interfaces.solver/superlu: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.solver/superlu: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsystem_interfaces.solver/superlu: (unknown)(): Invalid group\nsystem_interfaces.solver/superlu: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.solver/superlu: MPIR_Init_thread(586)..............:\nsystem_interfaces.solver/superlu: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.solver/superlu: MPIDI_CH3_Init(105)................:\nsystem_interfaces.solver/superlu: MPID_nem_init(324).................:\nsystem_interfaces.solver/superlu: MPID_nem_tcp_init(175).............:\nsystem_interfaces.solver/superlu: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.solver/superlu: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsystem_interfaces.solver/superlu: (unknown)(): Invalid group\nsystem_interfaces.solver/superlu:\nsystem_interfaces.solver/superlu:\nsystem_interfaces.solver/superlu: Exit Code: 8\nsystem_interfaces.solver/superlu: ################################################################################\nsystem_interfaces.solver/superlu: Tester failed, reason: CRASH\nsystem_interfaces.solver/superlu:\nsystem_interfaces.solver/superlu ................................................. [min_cpus=2] FAILED (CRASH)\nmultiapps/picard.steady_with_custom_convergence_check ..................................................... OK\noutputs/console.norms ..................................................................................... OK\ninterfacekernels/1d_interface.reaction_1D_steady_CSVDiff .................................................. OK\nbcs/periodic.testperiodic_vector: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/bcs/periodic\nbcs/periodic.testperiodic_vector: Running command: /Users/almeag-mac/projects1/moose/test/moose_test-opt -i periodic_vector_bc_test.i --error --error-unused --error-override --no-gdb-backtrace\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector: ERROR: Bad FEType.family == LAGRANGE_VEC\nbcs/periodic.testperiodic_vector: Stack frames: 17\nbcs/periodic.testperiodic_vector: 0: 0   libmesh_opt.0.dylib                 0x000000010ffd53bb libMesh::print_trace(std::__1::basic_ostream<char, std::__1::char_traits >&) + 1067\nbcs/periodic.testperiodic_vector: 1: 1   libmesh_opt.0.dylib                 0x000000010ffd1e2d libMesh::MacroFunctions::report_error(char const*, int, char const*, char const*) + 269\nbcs/periodic.testperiodic_vector: 2: 2   libmesh_opt.0.dylib                 0x00000001100a6147 libMesh::FEGenericBase::build(unsigned int, libMesh::FEType const&) + 4471\nbcs/periodic.testperiodic_vector: 3: 3   libmesh_opt.0.dylib                 0x00000001100ae548 libMesh::FEGenericBase::compute_periodic_constraints(libMesh::DofConstraints&, libMesh::DofMap&, libMesh::PeriodicBoundaries const&, libMesh::MeshBase const&, libMesh::PointLocatorBase const*, unsigned int, libMesh::Elem const*) + 168\nbcs/periodic.testperiodic_vector: 4: 4   libmesh_opt.0.dylib                 0x000000010ff9d218 (anonymous namespace)::ComputeConstraints::operator()(libMesh::StoredRange<libMesh::MeshBase::const_element_iterator, libMesh::Elem const*> const&) const + 280\nbcs/periodic.testperiodic_vector: 5: 5   libmesh_opt.0.dylib                 0x000000010ff9c68c libMesh::DofMap::create_dof_constraints(libMesh::MeshBase const&, double) + 844\nbcs/periodic.testperiodic_vector: 6: 6   libmesh_opt.0.dylib                 0x0000000110686721 libMesh::System::reinit_constraints() + 33\nbcs/periodic.testperiodic_vector: 7: 7   libmesh_opt.0.dylib                 0x0000000110685c6a libMesh::System::init_data() + 202\nbcs/periodic.testperiodic_vector: 8: 8   libmesh_opt.0.dylib                 0x000000011068e7f8 libMesh::System::init() + 40\nbcs/periodic.testperiodic_vector: 9: 9   libmesh_opt.0.dylib                 0x0000000110657312 libMesh::EquationSystems::init() + 1234\nbcs/periodic.testperiodic_vector: 10: 10  libmoose-opt.0.dylib                0x000000010eeff433 FEProblemBase::init() + 1043\nbcs/periodic.testperiodic_vector: 11: 11  libmoose-opt.0.dylib                0x000000010f1f183c ActionWarehouse::executeActionsWithAction(std::__1::basic_string<char, std::__1::char_traits, std::__1::allocator > const&) + 940\nbcs/periodic.testperiodic_vector: 12: 12  libmoose-opt.0.dylib                0x000000010f228cf8 ActionWarehouse::executeAllActions() + 232\nbcs/periodic.testperiodic_vector: 13: 13  libmoose-opt.0.dylib                0x000000010f69c2e0 MooseApp::runInputFile() + 80\nbcs/periodic.testperiodic_vector: 14: 14  libmoose-opt.0.dylib                0x000000010f697a4c MooseApp::run() + 2684\nbcs/periodic.testperiodic_vector: 15: 15  moose_test-opt                      0x000000010e52ec34 main + 132\nbcs/periodic.testperiodic_vector: 16: 16  libdyld.dylib                       0x00007fff67d0bcc9 start + 1\nbcs/periodic.testperiodic_vector: [0] /Users/almeag-mac/projects/raccoon/moose/scripts/../libmesh/src/fe/fe_base.C, line 335, compiled Jun 13 2021 at 10:45:15\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector: *** ERROR ***\nbcs/periodic.testperiodic_vector: ERROR: Bad FEType.family == LAGRANGE_VEC\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector: Stack frames: 6\nbcs/periodic.testperiodic_vector: 0: 0   libmesh_opt.0.dylib                 0x000000010ffd53bb libMesh::print_trace(std::__1::basic_ostream<char, std::__1::char_traits >&) + 1067\nbcs/periodic.testperiodic_vector: 1: 1   libmoose-opt.0.dylib                0x000000010f6c9854 moose::internal::mooseErrorRaw(std::__1::basic_string<char, std::__1::char_traits, std::__1::allocator >, std::__1::basic_string<char, std::__1::char_traits, std::__1::allocator >) + 852\nbcs/periodic.testperiodic_vector: 2: 2   libmoose-opt.0.dylib                0x000000010f044a6e void mooseError<char const*>(char const*&&) + 270\nbcs/periodic.testperiodic_vector: 3: 3   libmoose-opt.0.dylib                0x000000010f698404 MooseApp::run() + 5172\nbcs/periodic.testperiodic_vector: 4: 4   moose_test-opt                      0x000000010e52ec34 main + 132\nbcs/periodic.testperiodic_vector: 5: 5   libdyld.dylib                       0x00007fff67d0bcc9 start + 1\nbcs/periodic.testperiodic_vector: application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\nbcs/periodic.testperiodic_vector: [unset]: write_line error; fd=-1 buf=:cmd=abort exitcode=1\nbcs/periodic.testperiodic_vector: :\nbcs/periodic.testperiodic_vector: system msg for write_line failure : Bad file descriptor\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector: ERROR: Bad FEType.family == LAGRANGE_VEC\nbcs/periodic.testperiodic_vector: Stack frames: 17\nbcs/periodic.testperiodic_vector: 0: 0   libmesh_opt.0.dylib                 0x000000010ffd53bb libMesh::print_trace(std::__1::basic_ostream<char, std::__1::char_traits >&) + 1067\nbcs/periodic.testperiodic_vector: 1: 1   libmesh_opt.0.dylib                 0x000000010ffd1e2d libMesh::MacroFunctions::report_error(char const*, int, char const*, char const*) + 269\nbcs/periodic.testperiodic_vector: 2: 2   libmesh_opt.0.dylib                 0x00000001100a6147 libMesh::FEGenericBase::build(unsigned int, libMesh::FEType const&) + 4471\nbcs/periodic.testperiodic_vector: 3: 3   libmesh_opt.0.dylib                 0x00000001100ae548 libMesh::FEGenericBase::compute_periodic_constraints(libMesh::DofConstraints&, libMesh::DofMap&, libMesh::PeriodicBoundaries const&, libMesh::MeshBase const&, libMesh::PointLocatorBase const*, unsigned int, libMesh::Elem const*) + 168\nbcs/periodic.testperiodic_vector: 4: 4   libmesh_opt.0.dylib                 0x000000010ff9d218 (anonymous namespace)::ComputeConstraints::operator()(libMesh::StoredRange<libMesh::MeshBase::const_element_iterator, libMesh::Elem const*> const&) const + 280\nbcs/periodic.testperiodic_vector: 5: 5   libmesh_opt.0.dylib                 0x000000010ff9c68c libMesh::DofMap::create_dof_constraints(libMesh::MeshBase const&, double) + 844\nbcs/periodic.testperiodic_vector: 6: 6   libmesh_opt.0.dylib                 0x0000000110686721 libMesh::System::reinit_constraints() + 33\nbcs/periodic.testperiodic_vector: 7: 7   libmesh_opt.0.dylib                 0x0000000110685c6a libMesh::System::init_data() + 202\nbcs/periodic.testperiodic_vector: 8: 8   libmesh_opt.0.dylib                 0x000000011068e7f8 libMesh::System::init() + 40\nbcs/periodic.testperiodic_vector: 9: 9   libmesh_opt.0.dylib                 0x0000000110657312 libMesh::EquationSystems::init() + 1234\nbcs/periodic.testperiodic_vector: 10: 10  libmoose-opt.0.dylib                0x000000010eeff433 FEProblemBase::init() + 1043\nbcs/periodic.testperiodic_vector: 11: 11  libmoose-opt.0.dylib                0x000000010f1f183c ActionWarehouse::executeActionsWithAction(std::__1::basic_string<char, std::__1::char_traits, std::__1::allocator > const&) + 940\nbcs/periodic.testperiodic_vector: 12: 12  libmoose-opt.0.dylib                0x000000010f228cf8 ActionWarehouse::executeAllActions() + 232\nbcs/periodic.testperiodic_vector: 13: 13  libmoose-opt.0.dylib                0x000000010f69c2e0 MooseApp::runInputFile() + 80\nbcs/periodic.testperiodic_vector: 14: 14  libmoose-opt.0.dylib                0x000000010f697a4c MooseApp::run() + 2684\nbcs/periodic.testperiodic_vector: 15: 15  moose_test-opt                      0x000000010e52ec34 main + 132\nbcs/periodic.testperiodic_vector: 16: 16  libdyld.dylib                       0x00007fff67d0bcc9 start + 1\nbcs/periodic.testperiodic_vector: [0] /Users/almeag-mac/projects/raccoon/moose/scripts/../libmesh/src/fe/fe_base.C, line 335, compiled Jun 13 2021 at 10:45:15\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector: *** ERROR ***\nbcs/periodic.testperiodic_vector: ERROR: Bad FEType.family == LAGRANGE_VEC\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector: Stack frames: 6\nbcs/periodic.testperiodic_vector: 0: 0   libmesh_opt.0.dylib                 0x000000010ffd53bb libMesh::print_trace(std::__1::basic_ostream<char, std::__1::char_traits >&) + 1067\nbcs/periodic.testperiodic_vector: 1: 1   libmoose-opt.0.dylib                0x000000010f6c9854 moose::internal::mooseErrorRaw(std::__1::basic_string<char, std::__1::char_traits, std::__1::allocator >, std::__1::basic_string<char, std::__1::char_traits, std::__1::allocator >) + 852\nbcs/periodic.testperiodic_vector: 2: 2   libmoose-opt.0.dylib                0x000000010f044a6e void mooseError<char const*>(char const*&&) + 270\nbcs/periodic.testperiodic_vector: 3: 3   libmoose-opt.0.dylib                0x000000010f698404 MooseApp::run() + 5172\nbcs/periodic.testperiodic_vector: 4: 4   moose_test-opt                      0x000000010e52ec34 main + 132\nbcs/periodic.testperiodic_vector: 5: 5   libdyld.dylib                       0x00007fff67d0bcc9 start + 1\nbcs/periodic.testperiodic_vector: application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\nbcs/periodic.testperiodic_vector: [unset]: write_line error; fd=-1 buf=:cmd=abort exitcode=1\nbcs/periodic.testperiodic_vector: :\nbcs/periodic.testperiodic_vector: system msg for write_line failure : Bad file descriptor\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector: Exit Code: 1\nbcs/periodic.testperiodic_vector: ################################################################################\nbcs/periodic.testperiodic_vector: Tester failed, reason: ERRMSG\nbcs/periodic.testperiodic_vector:\nbcs/periodic.testperiodic_vector ............................................................. FAILED (ERRMSG)\noutputs/csv.sort .......................................................................................... OK\nIssue #14\ninterfaces/postprocessorinterface.missing_errors/by_name .................................................. OK\ninterfaces/vectorpostprocessorinterface.missing_errors/by_name ............................................ OK\ninterfaces/userobjectinterface.has_uo/name_T .............................................................. OK\nmisc/exception.parallel_exception_jacobian_transient ...................................................... OK\nnodalkernels/constraint_enforcement.vi/rsls_amg ........................................................... OK\nauxkernels/solution_aux.exodus_direct ..................................................................... OK\ntag.systems/test_tag_nodal_kernels ........................................................................ OK\ngeomsearch/2d_moving_penetration.pl_test3q ................................................................ OK\nmesh_modifiers/block_deleter.delete/BlockDeleterTest12 .................................................... OK\nmaterials/material.exception/serial ....................................................................... OK\nkernels/hfem.variable_dirichlet ........................................................................... OK\nmesh/mesh_generation.annular/disc ......................................................................... OK\ngeomsearch/3d_moving_penetration.pl_test3q ................................................................ OK\nmultiapps/steffensen_postprocessor.pp_transient/app_begin_transfers_begin_steffensen_sub .................. OK\nkernels/vector_fe.jacobian ................................................................................ OK\nmultiapps/secant_postprocessor.pp_transient/app_begin_transfers_begin_secant_sub .......................... OK\ntransfers/multiapp_conservative_transfer.userobject_transfer_csv .......................................... OK\nmesh/custom_partitioner.group/custom_linear_partitioner: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/mesh/custom_partitioner\nmesh/custom_partitioner.group/custom_linear_partitioner: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i custom_linear_partitioner_test.i --error --error-unused --error-override --no-gdb-backtrace\nmesh/custom_partitioner.group/custom_linear_partitioner: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner:\nmesh/custom_partitioner.group/custom_linear_partitioner:\nmesh/custom_partitioner.group/custom_linear_partitioner: Exit Code: 8\nmesh/custom_partitioner.group/custom_linear_partitioner: ################################################################################\nmesh/custom_partitioner.group/custom_linear_partitioner: Tester failed, reason: CRASH\nmesh/custom_partitioner.group/custom_linear_partitioner:\nmesh/custom_partitioner.group/custom_linear_partitioner .......................... [min_cpus=2] FAILED (CRASH)\nmeshgenerators/break_mesh_by_block_generator.surrounding_block_restricted/split_all ....................... OK\ntime_steppers/iteration_adaptive.pps_lim .................................................................. OK\nuserobjects/setup_interface_count.setup_interface_count/NodalSideUserObject ............................... OK\nmesh/high_order_elems.test_prism15_refine ................................................................. OK\nmultiapps/secant.variables_transient/app_end_transfers_begin .............................................. OK\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/transfers/multiapp_vector_pp_transfer\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i master.i --error --error-unused --error-override --no-gdb-backtrace\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIR_Init_thread(586)..............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_init(324).................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: (unknown)(): Invalid group\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIR_Init_thread(586)..............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_init(324).................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: (unknown)(): Invalid group\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIR_Init_thread(586)..............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_init(324).................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: (unknown)(): Invalid group\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIR_Init_thread(586)..............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_init(324).................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: (unknown)(): Invalid group\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Exit Code: 8\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: ################################################################################\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Tester failed, reason: CRASH\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer ......................... [min_cpus=2] FAILED (CRASH)\npostprocessors/num_iterations.methods/implicit_euler ...................................................... OK\ninterfacekernels/2d_interface.vector_2d ................................................................... OK\nproblems/eigen_problem/eigensolvers.eigen_scalar_kernel ................................................... OK\nmaterials/derivative_material_interface.bad_evaluation/nan ................................................ OK\ngeomsearch/3d_moving_penetration_smoothing.overlapping/pl_test4ns ......................................... OK\nreporters/mesh_info.info/default: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/reporters/mesh_info\nreporters/mesh_info.info/default: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i mesh_info.i --error --error-unused --error-override --no-gdb-backtrace\nreporters/mesh_info.info/default: Fatal error in MPI_Init_thread: Invalid group, error stack:\nreporters/mesh_info.info/default: MPIR_Init_thread(586)..............:\nreporters/mesh_info.info/default: MPID_Init(224).....................: channel initialization failed\nreporters/mesh_info.info/default: MPIDI_CH3_Init(105)................:\nreporters/mesh_info.info/default: MPID_nem_init(324).................:\nreporters/mesh_info.info/default: MPID_nem_tcp_init(175).............:\nreporters/mesh_info.info/default: MPID_nem_tcp_get_business_card(401):\nreporters/mesh_info.info/default: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nreporters/mesh_info.info/default: (unknown)(): Invalid group\nreporters/mesh_info.info/default: Fatal error in MPI_Init_thread: Invalid group, error stack:\nreporters/mesh_info.info/default: MPIR_Init_thread(586)..............:\nreporters/mesh_info.info/default: MPID_Init(224).....................: channel initialization failed\nreporters/mesh_info.info/default: MPIDI_CH3_Init(105)................:\nreporters/mesh_info.info/default: MPID_nem_init(324).................:\nreporters/mesh_info.info/default: MPID_nem_tcp_init(175).............:\nreporters/mesh_info.info/default: MPID_nem_tcp_get_business_card(401):\nreporters/mesh_info.info/default: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nreporters/mesh_info.info/default: (unknown)(): Invalid group\nreporters/mesh_info.info/default:\nreporters/mesh_info.info/default: ################################################################################\nreporters/mesh_info.info/default: Tester failed, reason: CRASH\nreporters/mesh_info.info/default:\nreporters/mesh_info.info/default ................................................. [min_cpus=2] FAILED (CRASH)\nfunctions/solution_function.solution_function/grad_p1 ..................................................... OK\nproblems/eigen_problem/eigensolvers.dg_krylovschur ........................................................ OK\nreporters/mesh_info.info/limit ............................................................................ OK\nmultiapps/secant.variables_transient/app_end_transfers_end ................................................ OK\nsystem_interfaces.solver/mumps: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/system_interfaces\nsystem_interfaces.solver/mumps: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i input.i --error --error-unused --error-override --no-gdb-backtrace\nsystem_interfaces.solver/mumps: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.solver/mumps: MPIR_Init_thread(586)..............:\nsystem_interfaces.solver/mumps: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.solver/mumps: MPIDI_CH3_Init(105)................:\nsystem_interfaces.solver/mumps: MPID_nem_init(324).................:\nsystem_interfaces.solver/mumps: MPID_nem_tcp_init(175).............:\nsystem_interfaces.solver/mumps: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.solver/mumps: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsystem_interfaces.solver/mumps: (unknown)(): Invalid group\nsystem_interfaces.solver/mumps: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.solver/mumps: MPIR_Init_thread(586)..............:\nsystem_interfaces.solver/mumps: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.solver/mumps: MPIDI_CH3_Init(105)................:\nsystem_interfaces.solver/mumps: MPID_nem_init(324).................:\nsystem_interfaces.solver/mumps: MPID_nem_tcp_init(175).............:\nsystem_interfaces.solver/mumps: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.solver/mumps: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nsystem_interfaces.solver/mumps: (unknown)(): Invalid group\nsystem_interfaces.solver/mumps:\nsystem_interfaces.solver/mumps:\nsystem_interfaces.solver/mumps: Exit Code: 8\nsystem_interfaces.solver/mumps: ################################################################################\nsystem_interfaces.solver/mumps: Tester failed, reason: CRASH\nsystem_interfaces.solver/mumps:\nsystem_interfaces.solver/mumps ................................................... [min_cpus=2] FAILED (CRASH)\ngeomsearch/3d_moving_penetration_smoothing.overlapping/pl_test4nstt ....................................... OK\nfunctions/solution_function.solution_function/grad_p2 ..................................................... OK\noutputs/console.transient ................................................................................. OK\ninterfacekernels/1d_interface.reaction_1D_transient ....................................................... OK\nauxkernels/vector_postprocessor_visualization.test: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/auxkernels/vector_postprocessor_visualization\nauxkernels/vector_postprocessor_visualization.test: Running command: mpiexec -n 3 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i vector_postprocessor_visualization.i --error --error-unused --error-override --no-gdb-backtrace\nauxkernels/vector_postprocessor_visualization.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nauxkernels/vector_postprocessor_visualization.test: MPIR_Init_thread(586)..............:\nauxkernels/vector_postprocessor_visualization.test: MPID_Init(224).....................: channel initialization failed\nauxkernels/vector_postprocessor_visualization.test: MPIDI_CH3_Init(105)................:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_init(324).................:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_init(175).............:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_get_business_card(401):\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nauxkernels/vector_postprocessor_visualization.test: (unknown)(): Invalid group\nauxkernels/vector_postprocessor_visualization.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nauxkernels/vector_postprocessor_visualization.test: MPIR_Init_thread(586)..............:\nauxkernels/vector_postprocessor_visualization.test: MPID_Init(224).....................: channel initialization failed\nauxkernels/vector_postprocessor_visualization.test: MPIDI_CH3_Init(105)................:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_init(324).................:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_init(175).............:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_get_business_card(401):\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nauxkernels/vector_postprocessor_visualization.test: (unknown)(): Invalid group\nauxkernels/vector_postprocessor_visualization.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nauxkernels/vector_postprocessor_visualization.test: MPIR_Init_thread(586)..............:\nauxkernels/vector_postprocessor_visualization.test: MPID_Init(224).....................: channel initialization failed\nauxkernels/vector_postprocessor_visualization.test: MPIDI_CH3_Init(105)................:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_init(324).................:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_init(175).............:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_get_business_card(401):\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nauxkernels/vector_postprocessor_visualization.test: (unknown)(): Invalid group\nauxkernels/vector_postprocessor_visualization.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nauxkernels/vector_postprocessor_visualization.test: MPIR_Init_thread(586)..............:\nauxkernels/vector_postprocessor_visualization.test: MPID_Init(224).....................: channel initialization failed\nauxkernels/vector_postprocessor_visualization.test: MPIDI_CH3_Init(105)................:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_init(324).................:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_init(175).............:\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_get_business_card(401):\nauxkernels/vector_postprocessor_visualization.test: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nauxkernels/vector_postprocessor_visualization.test: (unknown)(): Invalid group\nauxkernels/vector_postprocessor_visualization.test:\nauxkernels/vector_postprocessor_visualization.test:\nauxkernels/vector_postprocessor_visualization.test: Exit Code: 8\nauxkernels/vector_postprocessor_visualization.test: ################################################################################\nauxkernels/vector_postprocessor_visualization.test: Tester failed, reason: CRASH\nuxkernels/vector_postprocessor_visualization.test:\nauxkernels/vector_postprocessor_visualization.test ............................... [min_cpus=3] FAILED (CRASH)\nbcs/dmg_periodic.check_one_step: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/bcs/dmg_periodic\nbcs/dmg_periodic.check_one_step: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i dmg_periodic_bc.i /UserObjects/uo/type=CheckGhostedBoundaries /UserObjects/uo/total_num_bdry_sides=160 Outputs/hide=\"pid\" Outputs/exodus=false Executioner/num_steps=1 --error --error-unused --error-override --no-gdb-backtrace\nbcs/dmg_periodic.check_one_step: Fatal error in MPI_Init_thread: Invalid group, error stack:\nbcs/dmg_periodic.check_one_step: MPIR_Init_thread(586)..............:\nbcs/dmg_periodic.check_one_step: MPID_Init(224).....................: channel initialization failed\nbcs/dmg_periodic.check_one_step: MPIDI_CH3_Init(105)................:\nbcs/dmg_periodic.check_one_step: MPID_nem_init(324).................:\nbcs/dmg_periodic.check_one_step: MPID_nem_tcp_init(175).............:\nbcs/dmg_periodic.check_one_step: MPID_nem_tcp_get_business_card(401):\nbcs/dmg_periodic.check_one_step: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nbcs/dmg_periodic.check_one_step: (unknown)(): Invalid group\nbcs/dmg_periodic.check_one_step: Fatal error in MPI_Init_thread: Invalid group, error stack:\nbcs/dmg_periodic.check_one_step: MPIR_Init_thread(586)..............:\nbcs/dmg_periodic.check_one_step: MPID_Init(224).....................: channel initialization failed\nbcs/dmg_periodic.check_one_step: MPIDI_CH3_Init(105)................:\nbcs/dmg_periodic.check_one_step: MPID_nem_init(324).................:\nbcs/dmg_periodic.check_one_step: MPID_nem_tcp_init(175).............:\nbcs/dmg_periodic.check_one_step: MPID_nem_tcp_get_business_card(401):\nbcs/dmg_periodic.check_one_step: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nbcs/dmg_periodic.check_one_step: (unknown)(): Invalid group\nbcs/dmg_periodic.check_one_step:\nbcs/dmg_periodic.check_one_step:\nbcs/dmg_periodic.check_one_step: Exit Code: 8\nbcs/dmg_periodic.check_one_step: ################################################################################\nbcs/dmg_periodic.check_one_step: Tester failed, reason: CRASH\nbcs/dmg_periodic.check_one_step:\nbcs/dmg_periodic.check_one_step .................................................. [min_cpus=2] FAILED (CRASH)\nmultiapps/secant_postprocessor.pp_transient/app_end_transfers_begin_secant_sub ............................ OK\noutputs/vtk.files/parallel: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/outputs/vtk\noutputs/vtk.files/parallel: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i vtk_parallel.i --error --error-unused --error-override --no-gdb-backtrace\noutputs/vtk.files/parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/vtk.files/parallel: MPIR_Init_thread(586)..............:\noutputs/vtk.files/parallel: MPID_Init(224).....................: channel initialization failed\noutputs/vtk.files/parallel: MPIDI_CH3_Init(105)................:\noutputs/vtk.files/parallel: MPID_nem_init(324).................:\noutputs/vtk.files/parallel: MPID_nem_tcp_init(175).............:\noutputs/vtk.files/parallel: MPID_nem_tcp_get_business_card(401):\noutputs/vtk.files/parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/vtk.files/parallel: (unknown)(): Invalid group\noutputs/vtk.files/parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/vtk.files/parallel: MPIR_Init_thread(586)..............:\noutputs/vtk.files/parallel: MPID_Init(224).....................: channel initialization failed\noutputs/vtk.files/parallel: MPIDI_CH3_Init(105)................:\noutputs/vtk.files/parallel: MPID_nem_init(324).................:\noutputs/vtk.files/parallel: MPID_nem_tcp_init(175).............:\noutputs/vtk.files/parallel: MPID_nem_tcp_get_business_card(401):\noutputs/vtk.files/parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/vtk.files/parallel: (unknown)(): Invalid group\noutputs/vtk.files/parallel:\noutputs/vtk.files/parallel:\noutputs/vtk.files/parallel: Exit Code: 8\noutputs/vtk.files/parallel: ################################################################################\noutputs/vtk.files/parallel: Tester failed, reason: CRASH\noutputs/vtk.files/parallel:\noutputs/vtk.files/parallel ....................................................... [min_cpus=2] FAILED (CRASH)\nproblems/eigen_problem/eigensolvers.eigen_as_master ....................................................... OK\nics/depend_on_uo.ic_depend_on_uo: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/ics/depend_on_uo\nics/depend_on_uo.ic_depend_on_uo: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i geometric_neighbors_ic.i --error --error-unused --error-override --no-gdb-backtrace\nics/depend_on_uo.ic_depend_on_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\nics/depend_on_uo.ic_depend_on_uo: MPIR_Init_thread(586)..............:\nics/depend_on_uo.ic_depend_on_uo: MPID_Init(224).....................: channel initialization failed\nics/depend_on_uo.ic_depend_on_uo: MPIDI_CH3_Init(105)................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_init(324).................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(175).............:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_get_business_card(401):\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nics/depend_on_uo.ic_depend_on_uo: (unknown)(): Invalid group\nics/depend_on_uo.ic_depend_on_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\nics/depend_on_uo.ic_depend_on_uo: MPIR_Init_thread(586)..............:\nics/depend_on_uo.ic_depend_on_uo: MPID_Init(224).....................: channel initialization failed\nics/depend_on_uo.ic_depend_on_uo: MPIDI_CH3_Init(105)................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_init(324).................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(175).............:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_get_business_card(401):\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nics/depend_on_uo.ic_depend_on_uo: (unknown)(): Invalid group\nics/depend_on_uo.ic_depend_on_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\nics/depend_on_uo.ic_depend_on_uo: MPIR_Init_thread(586)..............:\nics/depend_on_uo.ic_depend_on_uo: MPID_Init(224).....................: channel initialization failed\nics/depend_on_uo.ic_depend_on_uo: MPIDI_CH3_Init(105)................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_init(324).................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(175).............:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_get_business_card(401):\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nics/depend_on_uo.ic_depend_on_uo: (unknown)(): Invalid group\nics/depend_on_uo.ic_depend_on_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\nics/depend_on_uo.ic_depend_on_uo: MPIR_Init_thread(586)..............:\nics/depend_on_uo.ic_depend_on_uo: MPID_Init(224).....................: channel initialization failed\nics/depend_on_uo.ic_depend_on_uo: MPIDI_CH3_Init(105)................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_init(324).................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(175).............:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_get_business_card(401):\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nics/depend_on_uo.ic_depend_on_uo: (unknown)(): Invalid group\nics/depend_on_uo.ic_depend_on_uo:\nics/depend_on_uo.ic_depend_on_uo:\nics/depend_on_uo.ic_depend_on_uo: Exit Code: 8\nics/depend_on_uo.ic_depend_on_uo: ################################################################################\nics/depend_on_uo.ic_depend_on_uo: Tester failed, reason: CRASH\nics/depend_on_uo.ic_depend_on_uo:\nics/depend_on_uo.ic_depend_on_uo ................................................. [min_cpus=2] FAILED (CRASH)\nbcs/periodic.testperiodic_dp: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/bcs/periodic\nbcs/periodic.testperiodic_dp: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i periodic_bc_displaced_problem.i --error --error-unused --error-override --no-gdb-backtrace\nbcs/periodic.testperiodic_dp: Fatal error in MPI_Init_thread: Invalid group, error stack:\nbcs/periodic.testperiodic_dp: MPIR_Init_thread(586)..............:\nbcs/periodic.testperiodic_dp: MPID_Init(224).....................: channel initialization failed\nbcs/periodic.testperiodic_dp: MPIDI_CH3_Init(105)................:\nbcs/periodic.testperiodic_dp: MPID_nem_init(324).................:\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_init(175).............:\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_get_business_card(401):\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nbcs/periodic.testperiodic_dp: (unknown)(): Invalid group\nbcs/periodic.testperiodic_dp: Fatal error in MPI_Init_thread: Invalid group, error stack:\nbcs/periodic.testperiodic_dp: MPIR_Init_thread(586)..............:\nbcs/periodic.testperiodic_dp: MPID_Init(224).....................: channel initialization failed\nbcs/periodic.testperiodic_dp: MPIDI_CH3_Init(105)................:\nbcs/periodic.testperiodic_dp: MPID_nem_init(324).................:\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_init(175).............:\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_get_business_card(401):\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nbcs/periodic.testperiodic_dp: (unknown)(): Invalid group\nbcs/periodic.testperiodic_dp:\nbcs/periodic.testperiodic_dp:\nbcs/periodic.testperiodic_dp: Exit Code: 8\nbcs/periodic.testperiodic_dp: ################################################################################\nbcs/periodic.testperiodic_dp: Tester failed, reason: CRASH\nbcs/periodic.testperiodic_dp:\nbcs/periodic.testperiodic_dp ..................................................... [min_cpus=2] FAILED (CRASH)\ngeomsearch/3d_moving_penetration_smoothing.overlapping/pl_test4qns ........................................ OK\nmultiapps/picard.steady_with_postprocessor_convergence .................................................... OK\nmultiapps/steffensen.variables_transient/app_begin_transfers_end_steffensen_sub ........................... OK\ninterfacekernels/1d_interface.reaction_1D_transient_Jac ................................................... OK\ntag.controls-tagging ...................................................................................... OK\nauxkernels/solution_aux.exodus_interp_restart/part2 ....................................................... OK\nmultiapps/picard_postprocessor.pp_transient/app_begin_transfers_begin_picard_sub .......................... OK\nfvkernels/mms/non-orthogonal.compact: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/fvkernels/mms/non-orthogonal\nfvkernels/mms/non-orthogonal.compact: Running command: python -m unittest -v test.TestCompactADR\nfvkernels/mms/non-orthogonal.compact: The 'mms' package requires sympy for symbolic function evaluation, it can be installed by running pip install sympy --user.\nfvkernels/mms/non-orthogonal.compact: Running: /Users/almeag-mac/projects1/moose/test/moose_test-opt -i advection-diffusion-reaction.i Mesh/uniform_refine=0\nfvkernels/mms/non-orthogonal.compact: test (test.TestCompactADR) ... Fatal error in MPI_Init_thread: Invalid group, error stack:\nfvkernels/mms/non-orthogonal.compact: MPIR_Init_thread(586)..............:\nfvkernels/mms/non-orthogonal.compact: MPID_Init(224).....................: channel initialization failed\nfvkernels/mms/non-orthogonal.compact: MPIDI_CH3_Init(105)................:\nfvkernels/mms/non-orthogonal.compact: MPID_nem_init(324).................:\nfvkernels/mms/non-orthogonal.compact: MPID_nem_tcp_init(175).............:\nfvkernels/mms/non-orthogonal.compact: MPID_nem_tcp_get_business_card(401):\nfvkernels/mms/non-orthogonal.compact: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nfvkernels/mms/non-orthogonal.compact: (unknown)(): Invalid group\nfvkernels/mms/non-orthogonal.compact: ERROR\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact: ======================================================================\nfvkernels/mms/non-orthogonal.compact: ERROR: test (test.TestCompactADR)\nfvkernels/mms/non-orthogonal.compact: ----------------------------------------------------------------------\nfvkernels/mms/non-orthogonal.compact: Traceback (most recent call last):\nfvkernels/mms/non-orthogonal.compact:   File \"/Users/almeag-mac/projects1/moose/test/tests/fvkernels/mms/non-orthogonal/test.py\", line 7, in test\nfvkernels/mms/non-orthogonal.compact:     df1 = mms.run_spatial('advection-diffusion-reaction.i', 7, mpi=2)\nfvkernels/mms/non-orthogonal.compact:   File \"/Users/almeag-mac/projects1/moose/python/mms/runner.py\", line 129, in run_spatial\nfvkernels/mms/non-orthogonal.compact:     return _runner(*args, rtype=SPATIAL, **kwargs)\nfvkernels/mms/non-orthogonal.compact:   File \"/Users/almeag-mac/projects1/moose/python/mms/runner.py\", line 102, in _runner\nfvkernels/mms/non-orthogonal.compact:     raise IOError(\"The CSV output does not exist: {}\".format(csv))\nfvkernels/mms/non-orthogonal.compact: OSError: The CSV output does not exist: None\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact: ----------------------------------------------------------------------\nfvkernels/mms/non-orthogonal.compact: Ran 1 test in 0.311s\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact: FAILED (errors=1)\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact: Exit Code: 1\nfvkernels/mms/non-orthogonal.compact: ################################################################################\nfvkernels/mms/non-orthogonal.compact: Tester failed, reason: CRASH\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact ............................................. [min_cpus=2] FAILED (CRASH)\ntime_steppers/iteration_adaptive.multi_piecewise_linear_function_point .................................... OK\nrestart/restartable_types.parallel/first: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/restart/restartable_types\nrestart/restartable_types.parallel/first: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i restartable_types.i --error --error-unused --error-override --no-gdb-backtrace\nrestart/restartable_types.parallel/first: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrestart/restartable_types.parallel/first: MPIR_Init_thread(586)..............:\nrestart/restartable_types.parallel/first: MPID_Init(224).....................: channel initialization failed\nrestart/restartable_types.parallel/first: MPIDI_CH3_Init(105)................:\nrestart/restartable_types.parallel/first: MPID_nem_init(324).................:\nrestart/restartable_types.parallel/first: MPID_nem_tcp_init(175).............:\nrestart/restartable_types.parallel/first: MPID_nem_tcp_get_business_card(401):\nrestart/restartable_types.parallel/first: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrestart/restartable_types.parallel/first: (unknown)(): Invalid group\nrestart/restartable_types.parallel/first:\nrestart/restartable_types.parallel/first:\nrestart/restartable_types.parallel/first: Exit Code: 8\nrestart/restartable_types.parallel/first: ################################################################################\nrestart/restartable_types.parallel/first: Tester failed, reason: CRASH\nrestart/restartable_types.parallel/first:\nrestart/restartable_types.parallel/first ......................................... [min_cpus=2] FAILED (CRASH)\nrestart/restartable_types.parallel/second .......................................... [skipped dependency] SKIP\ngeomsearch/3d_moving_penetration.pl_test4q ................................................................ OK\npreconditioners/hmg.hmg_3D: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/preconditioners/hmg\npreconditioners/hmg.hmg_3D: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i diffusion_hmg.i Mesh/dmg/dim=3 Mesh/dmg/nz=10 Outputs/file_base=diffusion_hmg_3d_out -log_view --error --error-unused --error-override --no-gdb-backtrace\npreconditioners/hmg.hmg_3D: Fatal error in MPI_Init_thread: Invalid group, error stack:\npreconditioners/hmg.hmg_3D: MPIR_Init_thread(586)..............:\npreconditioners/hmg.hmg_3D: MPID_Init(224).....................: channel initialization failed\npreconditioners/hmg.hmg_3D: MPIDI_CH3_Init(105)................:\npreconditioners/hmg.hmg_3D: MPID_nem_init(324).................:\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_init(175).............:\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_get_business_card(401):\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\npreconditioners/hmg.hmg_3D: (unknown)(): Invalid group\npreconditioners/hmg.hmg_3D: Fatal error in MPI_Init_thread: Invalid group, error stack:\npreconditioners/hmg.hmg_3D: MPIR_Init_thread(586)..............:\npreconditioners/hmg.hmg_3D: MPID_Init(224).....................: channel initialization failed\npreconditioners/hmg.hmg_3D: MPIDI_CH3_Init(105)................:\npreconditioners/hmg.hmg_3D: MPID_nem_init(324).................:\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_init(175).............:\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_get_business_card(401):\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\npreconditioners/hmg.hmg_3D: (unknown)(): Invalid group\npreconditioners/hmg.hmg_3D: ################################################################################\npreconditioners/hmg.hmg_3D:\npreconditioners/hmg.hmg_3D: Unable to match the following pattern against the program's output:\npreconditioners/hmg.hmg_3D:\npreconditioners/hmg.hmg_3D: PETSc\\s+Preconditioner:\\s+hmg\\s+strong_threshold:\\s+0.7\npreconditioners/hmg.hmg_3D:\npreconditioners/hmg.hmg_3D: ################################################################################\npreconditioners/hmg.hmg_3D: Tester failed, reason: EXPECTED OUTPUT MISSING\npreconditioners/hmg.hmg_3D:\npreconditioners/hmg.hmg_3D ..................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\noutputs/iterative.start_stop/output_end_step .............................................................. OK\nmultiapps/steffensen_postprocessor.pp_transient/app_end_transfers_end_steffensen_sub ...................... OK\nics/depend_on_uo.scalar_ic_from_uo: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/ics/depend_on_uo\nics/depend_on_uo.scalar_ic_from_uo: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i scalar_ic_from_uo.i --error --error-unused --error-override --no-gdb-backtrace\nics/depend_on_uo.scalar_ic_from_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\nics/depend_on_uo.scalar_ic_from_uo: MPIR_Init_thread(586)..............:\nics/depend_on_uo.scalar_ic_from_uo: MPID_Init(224).....................: channel initialization failed\nics/depend_on_uo.scalar_ic_from_uo: MPIDI_CH3_Init(105)................:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_init(324).................:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_init(175).............:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_get_business_card(401):\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nics/depend_on_uo.scalar_ic_from_uo: (unknown)(): Invalid group\nics/depend_on_uo.scalar_ic_from_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\nics/depend_on_uo.scalar_ic_from_uo: MPIR_Init_thread(586)..............:\nics/depend_on_uo.scalar_ic_from_uo: MPID_Init(224).....................: channel initialization failed\nics/depend_on_uo.scalar_ic_from_uo: MPIDI_CH3_Init(105)................:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_init(324).................:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_init(175).............:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_get_business_card(401):\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nics/depend_on_uo.scalar_ic_from_uo: (unknown)(): Invalid group\nics/depend_on_uo.scalar_ic_from_uo:\nics/depend_on_uo.scalar_ic_from_uo:\nics/depend_on_uo.scalar_ic_from_uo: Exit Code: 8\nics/depend_on_uo.scalar_ic_from_uo: ################################################################################\nics/depend_on_uo.scalar_ic_from_uo: Tester failed, reason: CRASH\nics/depend_on_uo.scalar_ic_from_uo:\nics/depend_on_uo.scalar_ic_from_uo ............................................... [min_cpus=2] FAILED (CRASH)\noutputs/console.transient_perf_int ........................................................................ OK\nnodalkernels/constraint_enforcement.vi/ssls_amg ........................................................... OK\nmesh/mesh_only.mesh_only_checkpoint: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/mesh/mesh_only\nmesh/mesh_only.mesh_only_checkpoint: Running command: mpiexec -n 3 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i mesh_only.i Mesh/parallel_type=distributed --mesh-only 3d_chimney.cpr --error --error-unused --error-override --no-gdb-backtrace\nmesh/mesh_only.mesh_only_checkpoint: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/mesh_only.mesh_only_checkpoint: MPIR_Init_thread(586)..............:\nmesh/mesh_only.mesh_only_checkpoint: MPID_Init(224).....................: channel initialization failed\nmesh/mesh_only.mesh_only_checkpoint: MPIDI_CH3_Init(105)................:\nmesh/mesh_only.mesh_only_checkpoint: MPID_nem_init(324).................:\nmesh/mesh_only.mesh_only_checkpoint: MPID_nem_tcp_init(175).............:\nmesh/mesh_only.mesh_only_checkpoint: MPID_nem_tcp_get_business_card(401):\nmesh/mesh_only.mesh_only_checkpoint: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/mesh_only.mesh_only_checkpoint: (unknown)(): Invalid group\nmesh/mesh_only.mesh_only_checkpoint: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/mesh_only.mesh_only_checkpoint: MPIR_Init_thread(586)..............:\nmesh/mesh_only.mesh_only_checkpoint: MPID_Init(224).....................: channel initialization failed\nmesh/mesh_only.mesh_only_checkpoint: MPIDI_CH3_Init(105)................:\nmesh/mesh_only.mesh_only_checkpoint: MPID_nem_init(324).................:\nmesh/mesh_only.mesh_only_checkpoint: MPID_nem_tcp_init(175).............:\nmesh/mesh_only.mesh_only_checkpoint: MPID_nem_tcp_get_business_card(401):\nmesh/mesh_only.mesh_only_checkpoint: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/mesh_only.mesh_only_checkpoint: (unknown)(): Invalid group\nmesh/mesh_only.mesh_only_checkpoint:\nmesh/mesh_only.mesh_only_checkpoint:\nmesh/mesh_only.mesh_only_checkpoint: Exit Code: 8\nmesh/mesh_only.mesh_only_checkpoint: ################################################################################\nmesh/mesh_only.mesh_only_checkpoint: Tester failed, reason: CRASH\nmesh/mesh_only.mesh_only_checkpoint:\nmesh/mesh_only.mesh_only_checkpoint .............................................. [min_cpus=3] FAILED (CRASH)\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/mesh/custom_partitioner\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i custom_linear_partitioner_test_displacement.i --error --error-unused --error-override --no-gdb-backtrace\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: Exit Code: 8\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: ################################################################################\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement: Tester failed, reason: CRASH\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement:\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement ............. [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/csv_reader.tester_fail: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/vectorpostprocessors/csv_reader\nvectorpostprocessors/csv_reader.tester_fail: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i read.i UserObjects/tester/rank=1 UserObjects/tester/gold='1 2 3' Outputs/csv=false --error --error-unused --error-override --no-gdb-backtrace --keep-cout --redirect-output tester_fail\nvectorpostprocessors/csv_reader.tester_fail: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/csv_reader.tester_fail: MPIR_Init_thread(586)..............:\nvectorpostprocessors/csv_reader.tester_fail: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/csv_reader.tester_fail: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_init(324).................:\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/csv_reader.tester_fail: (unknown)(): Invalid group\nvectorpostprocessors/csv_reader.tester_fail: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/csv_reader.tester_fail: MPIR_Init_thread(586)..............:\nvectorpostprocessors/csv_reader.tester_fail: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/csv_reader.tester_fail: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_init(324).................:\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nvectorpostprocessors/csv_reader.tester_fail: (unknown)(): Invalid group\nvectorpostprocessors/csv_reader.tester_fail: ################################################################################\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: Unable to match the following pattern against the program's output:\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: The supplied gold data does not match the VPP data on the given rank.\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: ################################################################################\nvectorpostprocessors/csv_reader.tester_fail: Tester failed, reason: EXPECTED ERROR MISSING\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail ..................... [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\ngeomsearch/2d_moving_penetration.pl_test3ns ............................................................... OK\ngeomsearch/3d_moving_penetration_smoothing.overlapping/pl_test4qnstt ...................................... OK\ntransfers/multiapp_nearest_node_transfer.parallel: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/transfers/multiapp_nearest_node_transfer\ntransfers/multiapp_nearest_node_transfer.parallel: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i parallel_master.i --error --error-unused --error-override --no-gdb-backtrace\ntransfers/multiapp_nearest_node_transfer.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_nearest_node_transfer.parallel: MPIR_Init_thread(586)..............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_nearest_node_transfer.parallel: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_init(324).................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ntransfers/multiapp_nearest_node_transfer.parallel: (unknown)(): Invalid group\ntransfers/multiapp_nearest_node_transfer.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_nearest_node_transfer.parallel: MPIR_Init_thread(586)..............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_nearest_node_transfer.parallel: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_init(324).................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ntransfers/multiapp_nearest_node_transfer.parallel: (unknown)(): Invalid group\ntransfers/multiapp_nearest_node_transfer.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_nearest_node_transfer.parallel: MPIR_Init_thread(586)..............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_nearest_node_transfer.parallel: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_init(324).................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ntransfers/multiapp_nearest_node_transfer.parallel: (unknown)(): Invalid group\ntransfers/multiapp_nearest_node_transfer.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_nearest_node_transfer.parallel: MPIR_Init_thread(586)..............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_nearest_node_transfer.parallel: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_init(324).................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ntransfers/multiapp_nearest_node_transfer.parallel: (unknown)(): Invalid group\ntransfers/multiapp_nearest_node_transfer.parallel:\ntransfers/multiapp_nearest_node_transfer.parallel:\ntransfers/multiapp_nearest_node_transfer.parallel: Exit Code: 8\ntransfers/multiapp_nearest_node_transfer.parallel: ################################################################################\ntransfers/multiapp_nearest_node_transfer.parallel: Tester failed, reason: CRASH\ntransfers/multiapp_nearest_node_transfer.parallel:\ntransfers/multiapp_nearest_node_transfer.parallel ................................ [min_cpus=2] FAILED (CRASH)\nmaterials/stateful_prop.ad/reg ............................................................................ OK\nmaterials/derivative_material_interface.postprocessor_coupling/parsed_material ............................ OK\nfvkernels/mms/non-orthogonal.extended: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/fvkernels/mms/non-orthogonal\nfvkernels/mms/non-orthogonal.extended: Running command: python -m unittest -v test.TestExtendedADR\nfvkernels/mms/non-orthogonal.extended: The 'mms' package requires sympy for symbolic function evaluation, it can be installed by running pip install sympy --user.\nfvkernels/mms/non-orthogonal.extended: Running: /Users/almeag-mac/projects1/moose/test/moose_test-opt -i extended-adr.i Mesh/uniform_refine=0\nfvkernels/mms/non-orthogonal.extended: test (test.TestExtendedADR) ... Fatal error in MPI_Init_thread: Invalid group, error stack:\nfvkernels/mms/non-orthogonal.extended: MPIR_Init_thread(586)..............:\nfvkernels/mms/non-orthogonal.extended: MPID_Init(224).....................: channel initialization failed\nfvkernels/mms/non-orthogonal.extended: MPIDI_CH3_Init(105)................:\nfvkernels/mms/non-orthogonal.extended: MPID_nem_init(324).................:\nfvkernels/mms/non-orthogonal.extended: MPID_nem_tcp_init(175).............:\nfvkernels/mms/non-orthogonal.extended: MPID_nem_tcp_get_business_card(401):\nfvkernels/mms/non-orthogonal.extended: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nfvkernels/mms/non-orthogonal.extended: (unknown)(): Invalid group\nfvkernels/mms/non-orthogonal.extended: ERROR\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended: ======================================================================\nfvkernels/mms/non-orthogonal.extended: ERROR: test (test.TestExtendedADR)\nfvkernels/mms/non-orthogonal.extended: ----------------------------------------------------------------------\nfvkernels/mms/non-orthogonal.extended: Traceback (most recent call last):\nfvkernels/mms/non-orthogonal.extended:   File \"/Users/almeag-mac/projects1/moose/test/tests/fvkernels/mms/non-orthogonal/test.py\", line 23, in test\nfvkernels/mms/non-orthogonal.extended:     df1 = mms.run_spatial('extended-adr.i', 7, mpi=2)\nfvkernels/mms/non-orthogonal.extended:   File \"/Users/almeag-mac/projects1/moose/python/mms/runner.py\", line 129, in run_spatial\nfvkernels/mms/non-orthogonal.extended:     return _runner(*args, rtype=SPATIAL, **kwargs)\nfvkernels/mms/non-orthogonal.extended:   File \"/Users/almeag-mac/projects1/moose/python/mms/runner.py\", line 102, in _runner\nfvkernels/mms/non-orthogonal.extended:     raise IOError(\"The CSV output does not exist: {}\".format(csv))\nfvkernels/mms/non-orthogonal.extended: OSError: The CSV output does not exist: None\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended: ----------------------------------------------------------------------\nfvkernels/mms/non-orthogonal.extended: Ran 1 test in 0.261s\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended: FAILED (errors=1)\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended: Exit Code: 1\nfvkernels/mms/non-orthogonal.extended: ################################################################################\nfvkernels/mms/non-orthogonal.extended: Tester failed, reason: CRASH\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended ............................................ [min_cpus=2] FAILED (CRASH)\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/mesh/custom_partitioner\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i custom_linear_partitioner_restart_test.i --error --error-unused --error-override --no-gdb-backtrace\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner_restart:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: Exit Code: 8\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: ################################################################################\nmesh/custom_partitioner.group/custom_linear_partitioner_restart: Tester failed, reason: CRASH\nmesh/custom_partitioner.group/custom_linear_partitioner_restart:\nmesh/custom_partitioner.group/custom_linear_partitioner_restart .................. [min_cpus=2] FAILED (CRASH)\nkernels/vector_fe.coupled_vector_gradient ................................................................. OK\nvectorpostprocessors/csv_reader.read_preic ................................................................ OK\nmesh/high_order_elems.test_pyramid13 ...................................................................... OK\ntime_steppers/iteration_adaptive.multi_piecewise_linear_function_change ................................... OK\ngeomsearch/2d_moving_penetration.pl_test3qns .............................................................. OK\npostprocessors/num_iterations.methods/l_stable_dirk3 ...................................................... OK\nmultiapps/secant.variables_transient/app_begin_transfers_end_secant_sub ................................... OK\noutputs/vtk.solution/diff_serial_mesh_parallel: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/outputs/vtk\noutputs/vtk.solution/diff_serial_mesh_parallel: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i vtk_diff_serial_mesh_parallel.i --error --error-unused --error-override --no-gdb-backtrace\noutputs/vtk.solution/diff_serial_mesh_parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/vtk.solution/diff_serial_mesh_parallel: MPIR_Init_thread(586)..............:\noutputs/vtk.solution/diff_serial_mesh_parallel: MPID_Init(224).....................: channel initialization failed\noutputs/vtk.solution/diff_serial_mesh_parallel: MPIDI_CH3_Init(105)................:\noutputs/vtk.solution/diff_serial_mesh_parallel: MPID_nem_init(324).................:\noutputs/vtk.solution/diff_serial_mesh_parallel: MPID_nem_tcp_init(175).............:\noutputs/vtk.solution/diff_serial_mesh_parallel: MPID_nem_tcp_get_business_card(401):\noutputs/vtk.solution/diff_serial_mesh_parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/vtk.solution/diff_serial_mesh_parallel: (unknown)(): Invalid group\noutputs/vtk.solution/diff_serial_mesh_parallel:\noutputs/vtk.solution/diff_serial_mesh_parallel: ################################################################################\noutputs/vtk.solution/diff_serial_mesh_parallel: Tester failed, reason: CRASH\noutputs/vtk.solution/diff_serial_mesh_parallel:\noutputs/vtk.solution/diff_serial_mesh_parallel ................................... [min_cpus=2] FAILED (CRASH)\nmaterials/stateful_prop.ad/ad ............................................................................. OK\ngeomsearch/3d_moving_penetration.pl_test4tt ............................................................... OK\ninterfaces/random.parallel_verification: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/interfaces/random\ninterfaces/random.parallel_verification: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i random.i --error --error-unused --error-override --no-gdb-backtrace\ninterfaces/random.parallel_verification: Fatal error in MPI_Init_thread: Invalid group, error stack:\ninterfaces/random.parallel_verification: MPIR_Init_thread(586)..............:\ninterfaces/random.parallel_verification: MPID_Init(224).....................: channel initialization failed\ninterfaces/random.parallel_verification: MPIDI_CH3_Init(105)................:\ninterfaces/random.parallel_verification: MPID_nem_init(324).................:\ninterfaces/random.parallel_verification: MPID_nem_tcp_init(175).............:\ninterfaces/random.parallel_verification: MPID_nem_tcp_get_business_card(401):\ninterfaces/random.parallel_verification: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ninterfaces/random.parallel_verification: (unknown)(): Invalid group\ninterfaces/random.parallel_verification: Fatal error in MPI_Init_thread: Invalid group, error stack:\ninterfaces/random.parallel_verification: MPIR_Init_thread(586)..............:\ninterfaces/random.parallel_verification: MPID_Init(224).....................: channel initialization failed\ninterfaces/random.parallel_verification: MPIDI_CH3_Init(105)................:\ninterfaces/random.parallel_verification: MPID_nem_init(324).................:\ninterfaces/random.parallel_verification: MPID_nem_tcp_init(175).............:\ninterfaces/random.parallel_verification: MPID_nem_tcp_get_business_card(401):\ninterfaces/random.parallel_verification: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\ninterfaces/random.parallel_verification: (unknown)(): Invalid group\ninterfaces/random.parallel_verification:\ninterfaces/random.parallel_verification:\ninterfaces/random.parallel_verification: Exit Code: 8\ninterfaces/random.parallel_verification: ################################################################################\ninterfaces/random.parallel_verification: Tester failed, reason: CRASH\ninterfaces/random.parallel_verification:\ninterfaces/random.parallel_verification .......................................... [min_cpus=2] FAILED (CRASH)\ninterfaces/random.test_par_mesh .................................................... [skipped dependency] SKIP\ninterfaces/random.threads_verification ............................................. [skipped dependency] SKIP\nmaterials/derivative_material_interface.postprocessor_coupling/derivative_parsed_material ................. OK\noutputs/console._console_const ............................................................................ OK\nmesh/mesh_generation.annular_except1_deprecated ........................................................... OK\nbcs/periodic.testwedge .................................................................................... OK\nproblems/eigen_problem/eigensolvers.eigen_as_sub .......................................................... OK\nnodalkernels/constraint_enforcement.unbounded ............................................................. OK\nmeshgenerators/distributed_rectilinear/partition.2D_3: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/meshgenerators/distributed_rectilinear/partition\nmeshgenerators/distributed_rectilinear/partition.2D_3: Running command: mpiexec -n 3 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i squarish_partition.i --error --error-unused --error-override --no-gdb-backtrace\nmeshgenerators/distributed_rectilinear/partition.2D_3: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIR_Init_thread(586)..............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIDI_CH3_Init(105)................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_init(324).................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(175).............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmeshgenerators/distributed_rectilinear/partition.2D_3: (unknown)(): Invalid group\nmeshgenerators/distributed_rectilinear/partition.2D_3: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIR_Init_thread(586)..............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIDI_CH3_Init(105)................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_init(324).................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(175).............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmeshgenerators/distributed_rectilinear/partition.2D_3: (unknown)(): Invalid group\nmeshgenerators/distributed_rectilinear/partition.2D_3: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIR_Init_thread(586)..............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIDI_CH3_Init(105)................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_init(324).................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(175).............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmeshgenerators/distributed_rectilinear/partition.2D_3: (unknown)(): Invalid group\nmeshgenerators/distributed_rectilinear/partition.2D_3: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIR_Init_thread(586)..............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIDI_CH3_Init(105)................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_init(324).................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(175).............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmeshgenerators/distributed_rectilinear/partition.2D_3: (unknown)(): Invalid group\nmeshgenerators/distributed_rectilinear/partition.2D_3: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIR_Init_thread(586)..............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIDI_CH3_Init(105)................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_init(324).................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(175).............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmeshgenerators/distributed_rectilinear/partition.2D_3: (unknown)(): Invalid group\nmeshgenerators/distributed_rectilinear/partition.2D_3: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIR_Init_thread(586)..............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIDI_CH3_Init(105)................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_init(324).................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(175).............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nmeshgenerators/distributed_rectilinear/partition.2D_3: (unknown)(): Invalid group\nmeshgenerators/distributed_rectilinear/partition.2D_3:\nmeshgenerators/distributed_rectilinear/partition.2D_3:\nmeshgenerators/distributed_rectilinear/partition.2D_3: Exit Code: 8\nmeshgenerators/distributed_rectilinear/partition.2D_3: ################################################################################\nmeshgenerators/distributed_rectilinear/partition.2D_3: Tester failed, reason: CRASH\nmeshgenerators/distributed_rectilinear/partition.2D_3:\nmeshgenerators/distributed_rectilinear/partition.2D_3 ............................ [min_cpus=3] FAILED (CRASH)\nkernels/vector_fe.comp_error .............................................................................. OK\nrestart/kernel_restartable.parallel_error/error2 ................................... [skipped dependency] SKIP\nrestart/kernel_restartable.parallel_error/error1: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/restart/kernel_restartable\nrestart/kernel_restartable.parallel_error/error1: Running command: mpiexec -n 2 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i kernel_restartable.i --error --error-unused --error-override --no-gdb-backtrace\nrestart/kernel_restartable.parallel_error/error1: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrestart/kernel_restartable.parallel_error/error1: MPIR_Init_thread(586)..............:\nrestart/kernel_restartable.parallel_error/error1: MPID_Init(224).....................: channel initialization failed\nrestart/kernel_restartable.parallel_error/error1: MPIDI_CH3_Init(105)................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_init(324).................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(175).............:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_get_business_card(401):\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrestart/kernel_restartable.parallel_error/error1: (unknown)(): Invalid group\nrestart/kernel_restartable.parallel_error/error1: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrestart/kernel_restartable.parallel_error/error1: MPIR_Init_thread(586)..............:\nrestart/kernel_restartable.parallel_error/error1: MPID_Init(224).....................: channel initialization failed\nrestart/kernel_restartable.parallel_error/error1: MPIDI_CH3_Init(105)................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_init(324).................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(175).............:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_get_business_card(401):\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrestart/kernel_restartable.parallel_error/error1: (unknown)(): Invalid group\nrestart/kernel_restartable.parallel_error/error1: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrestart/kernel_restartable.parallel_error/error1: MPIR_Init_thread(586)..............:\nrestart/kernel_restartable.parallel_error/error1: MPID_Init(224).....................: channel initialization failed\nrestart/kernel_restartable.parallel_error/error1: MPIDI_CH3_Init(105)................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_init(324).................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(175).............:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_get_business_card(401):\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrestart/kernel_restartable.parallel_error/error1: (unknown)(): Invalid group\nrestart/kernel_restartable.parallel_error/error1: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrestart/kernel_restartable.parallel_error/error1: MPIR_Init_thread(586)..............:\nrestart/kernel_restartable.parallel_error/error1: MPID_Init(224).....................: channel initialization failed\nrestart/kernel_restartable.parallel_error/error1: MPIDI_CH3_Init(105)................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_init(324).................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(175).............:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_get_business_card(401):\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrestart/kernel_restartable.parallel_error/error1: (unknown)(): Invalid group\nrestart/kernel_restartable.parallel_error/error1:\nrestart/kernel_restartable.parallel_error/error1:\nrestart/kernel_restartable.parallel_error/error1: Exit Code: 8\nrestart/kernel_restartable.parallel_error/error1: ################################################################################\nrestart/kernel_restartable.parallel_error/error1: Tester failed, reason: CRASH\nrestart/kernel_restartable.parallel_error/error1:\nrestart/kernel_restartable.parallel_error/error1 ................................. [min_cpus=2] FAILED (CRASH)\nrestart/kernel_restartable.thread_error/threads_error .............................. [skipped dependency] SKIP\nrestart/kernel_restartable.thread_error/with_threads ............................... [skipped dependency] SKIP\nmesh/high_order_elems.test_pyramid14 ...................................................................... OK\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/relationship_managers/geometric_neighbors\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Running command: mpiexec -n 3 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i geometric_edge_neighbors.i Mesh/Partitioner/type=GridPartitioner Mesh/Partitioner/nx=1 Mesh/Partitioner/ny=3 --error --error-unused --error-override --no-gdb-backtrace\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIR_Init_thread(586)..............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_Init(224).....................: channel initialization failed\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIDI_CH3_Init(105)................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_init(324).................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(175).............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_get_business_card(401):\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: (unknown)(): Invalid group\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIR_Init_thread(586)..............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_Init(224).....................: channel initialization failed\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIDI_CH3_Init(105)................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_init(324).................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(175).............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_get_business_card(401):\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: (unknown)(): Invalid group\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIR_Init_thread(586)..............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_Init(224).....................: channel initialization failed\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIDI_CH3_Init(105)................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_init(324).................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(175).............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_get_business_card(401):\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: (unknown)(): Invalid group\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIR_Init_thread(586)..............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_Init(224).....................: channel initialization failed\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIDI_CH3_Init(105)................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_init(324).................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(175).............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_get_business_card(401):\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: (unknown)(): Invalid group\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIR_Init_thread(586)..............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_Init(224).....................: channel initialization failed\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIDI_CH3_Init(105)................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_init(324).................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(175).............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_get_business_card(401):\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: (unknown)(): Invalid group\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIR_Init_thread(586)..............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_Init(224).....................: channel initialization failed\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPIDI_CH3_Init(105)................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_init(324).................:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(175).............:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_get_business_card(401):\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: (unknown)(): Invalid group\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Exit Code: 8\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: ################################################################################\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor: Tester failed, reason: CRASH\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor:\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor ................ [min_cpus=3] FAILED (CRASH)\noutputs/xml.parallel/distributed: Working Directory: /Users/almeag-mac/projects1/moose/test/tests/outputs/xml\noutputs/xml.parallel/distributed: Running command: mpiexec -n 3 /Users/almeag-mac/projects1/moose/test/moose_test-opt -i xml.i VectorPostprocessors/distributed/parallel_type=DISTRIBUTED Outputs/file_base=xml_distributed_out --error --error-unused --error-override --no-gdb-backtrace\noutputs/xml.parallel/distributed: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/xml.parallel/distributed: MPIR_Init_thread(586)..............:\noutputs/xml.parallel/distributed: MPID_Init(224).....................: channel initialization failed\noutputs/xml.parallel/distributed: MPIDI_CH3_Init(105)................:\noutputs/xml.parallel/distributed: MPID_nem_init(324).................:\noutputs/xml.parallel/distributed: MPID_nem_tcp_init(175).............:\noutputs/xml.parallel/distributed: MPID_nem_tcp_get_business_card(401):\noutputs/xml.parallel/distributed: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/xml.parallel/distributed: (unknown)(): Invalid group\noutputs/xml.parallel/distributed: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/xml.parallel/distributed: MPIR_Init_thread(586)..............:\noutputs/xml.parallel/distributed: MPID_Init(224).....................: channel initialization failed\noutputs/xml.parallel/distributed: MPIDI_CH3_Init(105)................:\noutputs/xml.parallel/distributed: MPID_nem_init(324).................:\noutputs/xml.parallel/distributed: MPID_nem_tcp_init(175).............:\noutputs/xml.parallel/distributed: MPID_nem_tcp_get_business_card(401):\noutputs/xml.parallel/distributed: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/xml.parallel/distributed: (unknown)(): Invalid group\noutputs/xml.parallel/distributed: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/xml.parallel/distributed: MPIR_Init_thread(586)..............:\noutputs/xml.parallel/distributed: MPID_Init(224).....................: channel initialization failed\noutputs/xml.parallel/distributed: MPIDI_CH3_Init(105)................:\noutputs/xml.parallel/distributed: MPID_nem_init(324).................:\noutputs/xml.parallel/distributed: MPID_nem_tcp_init(175).............:\noutputs/xml.parallel/distributed: MPID_nem_tcp_get_business_card(401):\noutputs/xml.parallel/distributed: MPID_nem_tcp_init(373).............: gethostbyname failed, FN601235 (errno 0)\noutputs/xml.parallel/distributed: (unknown)(): Invalid group\noutputs/xml.parallel/distributed:\noutputs/xml.parallel/distributed: ################################################################################\noutputs/xml.parallel/distributed: Tester failed, reason: CRASH\noutputs/xml.parallel/distributed:\noutputs/xml.parallel/distributed ................................................. [min_cpus=3] FAILED (CRASH)\nrunWorker Exception: Traceback (most recent call last):\nFile \"/Users/almeag-mac/projects1/moose/python/TestHarness/schedulers/Scheduler.py\", line 445, in runJob\nself.queueJobs(Jobs, j_lock)\nFile \"/Users/almeag-mac/projects1/moose/python/TestHarness/schedulers/Scheduler.py\", line 266, in queueJobs\nself.__runner_pool_jobs.add(self.run_pool.apply_async(self.runJob, (job, Jobs, j_lock)))\nFile \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/multiprocessing/pool.py\", line 362, in apply_async\nraise ValueError(\"Pool not running\")\nValueError: Pool not running\nrunWorker Exception: Traceback (most recent call last):\nFile \"/Users/almeag-mac/projects1/moose/python/TestHarness/schedulers/Scheduler.py\", line 445, in runJob\nself.queueJobs(Jobs, j_lock)\nFile \"/Users/almeag-mac/projects1/moose/python/TestHarness/schedulers/Scheduler.py\", line 266, in queueJobs\nself.__runner_pool_jobs.add(self.run_pool.apply_async(self.runJob, (job, Jobs, j_lock)))\nFile \"/Users/almeag-mac/miniconda3/envs/moose/lib/python3.7/multiprocessing/pool.py\", line 362, in apply_async\nraise ValueError(\"Pool not running\")\nValueError: Pool not running\n\n\nSteps to Reproduce\nRunning Moose tests.\nImpact\nCould not build a Moose multiapp.",
          "url": "https://github.com/idaholab/moose/discussions/18081",
          "updatedAt": "2021-06-21T15:41:47Z",
          "publishedAt": "2021-06-14T13:56:08Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "Closed issue and moved to a discussion - this is a local environment issue and not a moose issue.\nPlease see: https://mooseframework.inl.gov/help/troubleshooting.html#failingtests, specifically the section titled \"gethostbyname failed\"",
                  "url": "https://github.com/idaholab/moose/discussions/18081#discussioncomment-868411",
                  "updatedAt": "2022-06-15T16:02:20Z",
                  "publishedAt": "2021-06-14T14:04:37Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Help on ContactSplit",
          "author": {
            "login": "matthiasneuner"
          },
          "bodyText": "Hi,\nI am currently trying to set up a simulation using the ContactSplit module for preconditioning a contact module.\nHowever, I end up with following PETSc message:\n[0]PETSC ERROR: --------------------- Error Message --------------------------------------------------------------\n[0]PETSC ERROR: Petsc has generated inconsistent data\n[0]PETSC ERROR: Unhandled case, must have at least two fields, not 0\n\nMy Contact and ContactSplit definition look like:\n\n[Preconditioning]\n  active='contact_fsp'\n\n  [contact_fsp]\n      type = FSP\n      topsplit = 'contact_interior' \n      [contact_interior]\n        splitting = 'contact interior'\n        splitting_type = multiplicative\n      []\n      [./interior]\n          type = ContactSplit\n          vars = 'disp_x disp_y disp_z'\n          uncontact_primary   = 'set_hammer_1'\n          uncontact_secondary    = 'set_channel_1'\n          uncontact_displaced = '1'\n          include_all_contact_nodes = 1\n          petsc_options_iname = '-ksp_type -ksp_max_it -ksp_rtol -ksp_gmres_restart -pc_type -pc_hypre_type -pc_hypre_boomeramg_max_iter -pc_hypre_strong_threshold'\n          petsc_options_value = ' preonly 10 1e-4 201                hypre    boomeramg      1                            0.25'\n      [../]\n      [./contact]\n          type = ContactSplit\n          vars = 'disp_x disp_y disp_z'\n          contact_primary   = 'set_hammer_1'\n          contact_secondary    = 'set_channel_1'\n          contact_displaced = '1'\n          include_all_contact_nodes = 1\n          petsc_options_iname = '-ksp_type -ksp_max_it -pc_type -pc_asm_overlap -sub_pc_type   -pc_factor_levels'\n          petsc_options_value = '  preonly 10 asm  1 lu 0'\n      [../]\n  []\n\n  [smp_strumpack]\n    type = SMP\n    full = true\n    petsc_options_iname = '-pc_type -pc_solver_type'\n    petsc_options_value = 'lu strumpack'\n  []\n\n[]\n\n[Contact]\n  [1]\n    formulation = penalty\n    primary = set_hammer_1\n    secondary = set_channel_1\n    model = frictionless\n    penalty = 1e+6\n  []\n[]\n\nwhich I actually implemented following the testcases from the contact module.\nI am not sure what the options contact_displaced = '1' and uncontact_displaced = '1' should do, but deactivating them doesn't resolve any issue.\nPlease, can somebody give a hint on the correct implementation of ContactSplit?\n\u20ac: Please find the files to reproduce the error here: https://fileshare.uibk.ac.at/d/ce29dee42ca04aa3a7e3/",
          "url": "https://github.com/idaholab/moose/discussions/17501",
          "updatedAt": "2022-11-03T15:00:53Z",
          "publishedAt": "2021-04-03T11:44:16Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "jiangwen84"
                  },
                  "bodyText": "@dewenyushu I remembered you used ContactSplit before, and could you help answer the question?",
                  "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-575273",
                  "updatedAt": "2022-11-03T15:01:40Z",
                  "publishedAt": "2021-04-06T14:37:47Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "dewenyushu"
                  },
                  "bodyText": "Hi @matthiasneuner\nI need to admit that this problem seems to be a more complicated case than what I've played with before. Let's see if I may be of some help to you:\nBy looking at your mesh, I realized that you actually have two sets of contact surfaces, i.e., set_hammer_1 in contact with set_channel_1, and set_hammer_2 in contact with set_channel_2. However, the sideset 'set_hammer_2' is missing in your mesh thus your FSP block.\nSince FSP uses the sideset info to extract the degree of freedoms and apply different preconditioners on the corresponding sub-problems, I would suggest making this correction in your mesh and add the second pair of contact surfaces in FSP.\nI am not sure about the  contact_displaced  and uncontact_displace  usage here though. @fdkong do you have some suggestions?",
                  "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-576272",
                  "updatedAt": "2022-11-03T15:01:53Z",
                  "publishedAt": "2021-04-06T18:06:58Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "matthiasneuner"
                          },
                          "bodyText": "Hello, thank you for the response.\nIndeed, I want to include later on a second contact pair (set_hammer_2/set_channel_2). However, currently it has no contact definition assigned, so only (set_hammer_1/set_channel_1) is active at the moment.\nBut this should not be the cause of the problem?",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-576590",
                          "updatedAt": "2022-11-03T15:01:53Z",
                          "publishedAt": "2021-04-06T19:26:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dewenyushu"
                          },
                          "bodyText": "Oh, yeah this should not be the cause of the problem then. Would you mind creating a new issue for this? I suspect this example contains some corner cases that are not properly taken care of in the FSP implementation.",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-576791",
                          "updatedAt": "2022-11-03T15:01:53Z",
                          "publishedAt": "2021-04-06T20:09:58Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "fdkong"
                  },
                  "bodyText": "I am not sure about the contact_displaced and uncontact_displace usage here though. @fdkong do you have some suggestions?\n\ncontact_displaced and uncontact_displace set a flag to check if we want to use a displaced mesh or not. If use the displaced mesh, geometric search information from the displaced mesh will be taken.\n            if (displaced)\n            {\n              std::shared_ptr<DisplacedProblem> displaced_problem =\n                  dmm->_nl->_fe_problem.getDisplacedProblem();\n              if (!displaced_problem)\n              {\n                std::ostringstream err;\n                err << \"Cannot use a displaced contact (\" << it.second.first << \",\"\n                    << it.second.second << \") with an undisplaced problem\";\n                mooseError(err.str());\n              }\n              locator = displaced_problem->geomSearchData()._penetration_locators[it.first];\n            }\n            else\n              locator = dmm->_nl->_fe_problem.geomSearchData()._penetration_locators[it.first];",
                  "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-576909",
                  "updatedAt": "2022-11-03T15:01:52Z",
                  "publishedAt": "2021-04-06T20:46:41Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "fdkong"
                  },
                  "bodyText": "[0]PETSC ERROR: --------------------- Error Message --------------------------------------------------------------\n[0]PETSC ERROR: Petsc has generated inconsistent data\n[0]PETSC ERROR: Unhandled case, must have at least two fields, not 0\n\nIt  seems all petsc options were  removed before calling PETSc, and PETSc got nothing from MOOSE, and could not set FSP",
                  "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-576915",
                  "updatedAt": "2022-11-03T15:01:51Z",
                  "publishedAt": "2021-04-06T20:48:20Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "fdkong"
                  },
                  "bodyText": "You could take out of these blocks, and try again. Especially I did not use FSP with a Predictor block.\n  [TimeStepper]\n    type = IterationAdaptiveDT\n    optimal_iterations = 8\n    iteration_window = 3\n    linear_iteration_ratio = 1000\n    growth_factor = 1.5\n    cutback_factor = 0.5\n    dt = 1.0e-4\n  []\n  [Predictor]\n    type = SimplePredictor\n    scale = 1.0\n    skip_after_failed_timestep = true\n  []",
                  "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-576935",
                  "updatedAt": "2022-11-03T15:01:51Z",
                  "publishedAt": "2021-04-06T20:52:41Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "matthiasneuner"
                          },
                          "bodyText": "Unfortunately, this does not resolve the issue.",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-579580",
                          "updatedAt": "2022-11-03T15:01:43Z",
                          "publishedAt": "2021-04-07T12:40:04Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Has the issue been created? If so please link it to this discussion",
                  "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-705445",
                  "updatedAt": "2022-11-03T15:01:43Z",
                  "publishedAt": "2021-05-06T17:02:28Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "matthiasneuner"
                          },
                          "bodyText": "Not yet; I had no time to investigate this further, and I am still not sure if it is not my fault. I will create an issue with a MWE in the next few days.",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-707418",
                          "updatedAt": "2022-11-03T15:04:54Z",
                          "publishedAt": "2021-05-07T05:02:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "Thanks. Right, it might not be your fault at all. However, a MWE will help us to get things fixed.\nThanks so much.",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-719151",
                          "updatedAt": "2022-11-03T15:01:42Z",
                          "publishedAt": "2021-05-10T15:36:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Any news on this?",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-805255",
                          "updatedAt": "2022-11-03T15:01:41Z",
                          "publishedAt": "2021-05-31T05:09:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "@matthiasneuner Do you have  a MWE yet?",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-811900",
                          "updatedAt": "2022-11-03T15:01:42Z",
                          "publishedAt": "2021-06-01T16:32:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "matthiasneuner"
                          },
                          "bodyText": "Hello, sorry for the long delay. I am trying to create a MWE, but somehow i fail to create a simple MWE which works even for a monolithic preconditioning. Please, can you tell me what is wrong with this simple MWE:\nhttps://fileshare.uibk.ac.at/d/8f9f69e228674162b027/\n? The contact never becomes active, and penetration occurs.\nIn any case, switching to  contact split basically invokes the same error initially discussed in this thread ..\nThank you in advance!",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-857745",
                          "updatedAt": "2022-11-03T15:01:42Z",
                          "publishedAt": "2021-06-11T11:37:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "Create an issue here #18072\nWill look into it when there is a chance",
                          "url": "https://github.com/idaholab/moose/discussions/17501#discussioncomment-859651",
                          "updatedAt": "2022-11-03T15:01:42Z",
                          "publishedAt": "2021-06-11T17:59:12Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "import chigger failed",
          "author": {
            "login": "hugary1995"
          },
          "bodyText": "Hi all,\nI wanted to use chigger to generate some animations, but I can't seem to load chigger even though I have moose environment activated. What am I missing?\nThanks,\nGary",
          "url": "https://github.com/idaholab/moose/discussions/18032",
          "updatedAt": "2022-11-03T09:28:57Z",
          "publishedAt": "2021-06-08T13:16:00Z",
          "category": {
            "name": "Q&A Tools"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "I forgot to add moose/python to PYTHONPATH...",
                  "url": "https://github.com/idaholab/moose/discussions/18032#discussioncomment-839936",
                  "updatedAt": "2022-11-03T09:29:05Z",
                  "publishedAt": "2021-06-08T13:22:28Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "I am glad I am not the only one, I do this all the time. \ud83e\udd23",
                          "url": "https://github.com/idaholab/moose/discussions/18032#discussioncomment-858555",
                          "updatedAt": "2022-11-03T09:29:06Z",
                          "publishedAt": "2021-06-11T14:47:15Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Phase field models of grain growth",
          "author": {
            "login": "alikhan-zada"
          },
          "bodyText": "Hello everyone,\nNovice question alert! \ud83d\ude43\nI started studying the phase field method and there are some things that I am confused about.\nSo, if I have a multicomponent-multiphase material, say something like steel, would the standard grain growth model implemented in MOOSE be enough to visualize grain growth under a heat source, like in welding?\nI am not interested in dendrite formation or the various phase transformations that might take place at different temperatures, but rather with the end microstructure of the steel.\nAlso, what other free energy components would I need to add in order to find out the material properties across the weld structure, for instance I would like to see how the stress and hardness varies across the weld as well as along it, in the various regions such as the HAZ, CGHAZ and FGHAZ?\nAnother thing that I would like to do is to know the different component segregation across the weld profile and if both, the component and the above part can be done together or not?\nThank you for reading my post, I would really appreciate your answers\nRegards,\nAli",
          "url": "https://github.com/idaholab/moose/discussions/17912",
          "updatedAt": "2022-06-09T08:20:09Z",
          "publishedAt": "2021-05-22T16:10:34Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "laagesen"
                  },
                  "bodyText": "Hi Ali- we have done something similar to what it sounds like you want to do. Please see the following paper:\nhttps://www.tandfonline.com/doi/full/10.1080/00295450.2020.1838877\nHere we used the standard grain growth model in MOOSE, although it is not really physically parameterized for a particular material system. We didn't really look at properties in this case- to do that, you would need to take the resulting microstructures and import them into a crystal plasticity model to see how they behave under deformation.",
                  "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-783471",
                  "updatedAt": "2022-06-09T08:24:56Z",
                  "publishedAt": "2021-05-25T21:46:49Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "alikhan-zada"
                          },
                          "bodyText": "Thank you @laagesen, for your answer, but what I want to do is something similar to\n\nIs it possible to have two decoupled domains where I can transfer the whole heat profile changing in time to another domain which solves only the phase field equation using MultiApps?\nOr the other option is to just output the heat data over the domain w.r.t. space and time and just use that as an input to the second domain.\nRegards,\nAli",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-793628",
                          "updatedAt": "2022-06-09T08:25:00Z",
                          "publishedAt": "2021-05-27T15:18:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "laagesen"
                          },
                          "bodyText": "The first option would be doable via MultiApps. There are many different ways to transfer data among MultiApps. The documentation page below has a comprehensive list:\nhttps://mooseframework.inl.gov/syntax/Transfers/\nHowever before delving into this too deeply, I would try to figure out how you are going to model the heat transfer problem. Do you have a model for this in mind already?",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-793936",
                          "updatedAt": "2022-06-09T08:25:01Z",
                          "publishedAt": "2021-05-27T16:19:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "alikhan-zada"
                          },
                          "bodyText": "The heat source model for a laser is already implemented in MOOSE:\nhttps://mooseframework.inl.gov/source/materials/FunctionPathEllipsoidHeatSource.html\nI am thinking of changing the source term to that of a double ellipsoid model which should be fairly simple. The only thing that would be left would be to match the domain sizes and non-dimensionalize the parameters accordingly.",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-793983",
                          "updatedAt": "2022-06-09T08:25:00Z",
                          "publishedAt": "2021-05-27T16:27:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "SudiptaBiswas"
                          },
                          "bodyText": "I did similar simulations by changing the GB Mobility as a function of time as the heat source moves. That is also another way to simplify the grain growth simulations.",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-799890",
                          "updatedAt": "2022-08-24T11:27:04Z",
                          "publishedAt": "2021-05-28T22:22:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "alikhan-zada"
                          },
                          "bodyText": "@SudiptaBiswas, is it possible for you to share the input files?\nHonestly, it would save me from doing a lot of things myself :)",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-800666",
                          "updatedAt": "2022-08-24T11:27:04Z",
                          "publishedAt": "2021-05-29T07:38:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "SudiptaBiswas"
                          },
                          "bodyText": "Sorry, it involves a small development that was not checked in to MOOSE. I don't have an input file that will come in handy for you. You can design the mobility material following the heat source material you mentioned and provide the material as mobility for the grain growth kernels.",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-811928",
                          "updatedAt": "2022-08-24T11:27:04Z",
                          "publishedAt": "2021-06-01T16:39:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "alikhan-zada"
                          },
                          "bodyText": "Hello again Larry and Sudipta,\nThank you for your help so far.\nAfter going through some literature, I have come across something that is very close to what I want to do (which was already in my initial question).\nSo, I have two equations:\n\nThe total free energy F is given as:\n\nThe local free energy density consists of two parts and is given as:\n\nFgrain is similar to the grain growth model in MOOSE, with an interaction term for phase and grain interaction, given as:\n\nFphase is differentiates between phases and is given by:\n\nWhere phi(tau) is a function dependent on the temporal changes in the system i.e. tau = T/Tl; Tl is the liquidus temperature and T is the temperature at a given position and time.\nMy question is how do I implement the first two equations in MOOSE? They should be coupled together? No?\nI understand that I can take in the temperature using a SolutionFunction or a SolutionUO or use a MultiApp (but the mesh will need to be the same, which I don't want).\nDo I need to code the equations myself or is there a MOOSE system that I can utilize?\nRegards,\nAli",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-843229",
                          "updatedAt": "2022-08-24T11:27:04Z",
                          "publishedAt": "2021-06-09T03:35:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "laagesen"
                          },
                          "bodyText": "The first two equations are Allen-Cahn equations and you should be able to implement them with existing phase-field kernels. Please have a look at the phase-field documentation page:\nhttps://mooseframework.inl.gov/modules/phase_field/index.html\nAnd in particular the page on basic phase-field equations will explain the basic idea of how to use existing kernels to implement the Allen-Cahn equation:\nhttps://mooseframework.inl.gov/modules/phase_field/Phase_Field_Equations.html\nIf you are wanting to simulate a large number of grains, it may be more convenient in the long run to write an action that will add all the necessary kernels. In this case, as in the current MOOSE grain growth model, you would not be using the free energy based approach but instead deriving the evolution equations explicitly and adding the corresponding kernels. More information about the grain growth model can be found here:\nhttps://mooseframework.inl.gov/modules/phase_field/Grain_Growth_Model.html",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-843300",
                          "updatedAt": "2022-08-24T11:27:06Z",
                          "publishedAt": "2021-06-09T04:21:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "alikhan-zada"
                          },
                          "bodyText": "Thank you Larry,\nSo I will have to write my own Kernel and implement an action system like in the Grain Growth Model?\nAlso the equations are coupled together as can be seen in the local free energy. They also have derivatives w.r.t. different variables.\nAli",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-843314",
                          "updatedAt": "2022-08-24T11:27:06Z",
                          "publishedAt": "2021-06-09T04:31:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "laagesen"
                          },
                          "bodyText": "To try to answer your question as directly as possible, you can probably avoid writing any new kernels or actions if you don't mind having a very complex/long input file. This is totally fine and will not cause any problems, it just may be harder to read. To do this, you would create a DerivativeParsedMaterial that includes f_phase and f_grain. Then you would need to add kernels to create an Allen-Cahn equation for each variable as described in\nhttps://mooseframework.inl.gov/modules/phase_field/Phase_Field_Equations.html\nThe AllenCahn kernel for each variable will take the DerivativeParsedMaterial as an input parameter and will automatically take the partial derivative of the free energy with respect to that variable. You do not need to do anything special because of the fact that the variables are coupled together in the free energy; taking the partial derivative wrt each variable is all you need to do and this is handled automatically. ACInterface kernels will also be needed for the terms resulting from f_grad.\nIf you want to have simpler input files, you would need to create a new action that would handle the coupling terms in f_grain. You could probably use an AllenCahn kernel for d(f_phase)/d(zeta).",
                          "url": "https://github.com/idaholab/moose/discussions/17912#discussioncomment-847537",
                          "updatedAt": "2022-08-24T11:27:06Z",
                          "publishedAt": "2021-06-09T19:52:24Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Groundwater models - boundary conditions",
          "author": {
            "login": "josebastiase"
          },
          "bodyText": "Hi,\nI'm working on a groundwater model (HM - coupled) to simulate land subsidence due to heavy groundwater extraction. My questions:\n\n\nFor the top layer of the model which is unsaturated and drained, I'm using PorousFlowPiecewiseLinearSink to fix the porepressure at the boundaries. I would like to match the boundary conditions with the water head level observed in the field. But I guess that is not really possible with PorousFlowPiecewiseLinearSink, but trying different values of the flux_function flag one can get really close. Is this approach correct/ideal?\n\n\nThe model is bounded in one side by a river (I'm using the water level of the river as BC). What kind of boundary condition would be more suitable in this situation? I would use PorousFlowPiecewiseLinearSink again but a line sink/source may also do the job?\n\n\nThanks in advance!\nJose",
          "url": "https://github.com/idaholab/moose/discussions/18031",
          "updatedAt": "2022-10-26T16:59:50Z",
          "publishedAt": "2021-06-08T12:21:21Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "WilkAndy"
                  },
                  "bodyText": "What you're doing is fine, but below I've written some additional thoughts.\n(1) You might like to fix the porepressure (or head) using a DirichletBC instead, or a combo of DirichletBC (on some bdys) and PorousFlowPiecewiseLinearSink (on the other bdys), depending on your situation.  DirichletBC has the advantage of blindly agreeing with head observations, while the Sink has the advantage of agreeing with known fluxes.  If you know fluxes and/or heads then you can calibrate your hydraulic conductivity (permeability) to achieve the flows or heads observed.  When using the sink make sure your flux_function is chosen sensibly by checking the porepressure or head hasn't gone too crazy, and checking the fluxes out of the model are reasonable.\n(2) Depending on your mesh and geometry, a PorousFlowPolyLineSink might be more appropriate, because the river could be thought of as a polyline.   Have a read about 1/3rd down this page for comments about perennial, ephemeral and rate-limited streams, and riverbed conductance: https://mooseframework.inl.gov/modules/porous_flow/sinks.html .  But it will accomplish essentially the same thing as your PiecewiseLinearSink (assuming your boundary for the PiecewiseLinearSink is set appropriately).  Also be aware that both these approaches can essentially fix porepressure or head at the nodal positions surrounding the sink if you choose a \"strong\" flux_function, but that could be exactly what you want, as the river can be thought of as an infinite supply or demand of water.\na",
                  "url": "https://github.com/idaholab/moose/discussions/18031#discussioncomment-855940",
                  "updatedAt": "2022-10-26T17:00:02Z",
                  "publishedAt": "2021-06-10T23:07:21Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "josebastiase"
                          },
                          "bodyText": "Hi Andy,\nThanks for your reply. I'm currently using a combination of  PorousFlowPiecewiseLinearSink and DirichletBC in the model:\nPorousFlowPiecewiseLinearSink  for the top layer\nDirichletBC for all confined layers\nThe problem with DirichletBC is that the first layer of the model is unsaturated and drained, so a DirichletBC will fill with water the unsaturated zone and eventually water will leave the model from the top. Whereas with PorousFlowPiecewiseLinearSink I got more \"control\" so the latter doesn't happen, but is really hard to achieve a decent calibration.",
                          "url": "https://github.com/idaholab/moose/discussions/18031#discussioncomment-857208",
                          "updatedAt": "2022-10-26T17:00:02Z",
                          "publishedAt": "2021-06-11T08:48:08Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Traiwit"
                  },
                  "bodyText": "Hi @josebastiase and @WilkAndy\nnot related to the q's, but on the same model\nJust wondering, how did you set the initial stress for this kind of problem?\nI always have my geometry shrink due to gravity during the equilibrium step (1st step) using a simple rhogh.\n[Kernels]\n  [flux]\n  type = PorousFlowAdvectiveFlux\n  use_displaced_mesh = true\n  variable = porepressure\n  gravity = '0 0 -9.81'\n  fluid_component = 0\n[]\n[]\n\n  [./ini_xx]\n    type = PiecewiseLinear\n    axis = z\n      x = '0 1000'\n      y = '-29430000 0'\n  [../]\n  [./ini_yy]\n    type = PiecewiseLinear\n    axis = z\n      x = '0 1000'\n      y = '-29430000 0'\n  [../]\n  [./ini_zz]\n    type = PiecewiseLinear\n    axis = z\n      x = '0 1000'\n      y = '-19620000 0'  #2000*9.81*1000\n  [../]\n\n[./ini_stress]\n    type = ComputeEigenstrainFromInitialStress\n    eigenstrain_name = ini_stress\n    initial_stress = 'ini_xx 0 0 0 ini_yy 0 0 0 ini_zz'\n  [../]\n\nKind regards,\nTraiwit",
                  "url": "https://github.com/idaholab/moose/discussions/18031#discussioncomment-856011",
                  "updatedAt": "2022-10-26T17:00:03Z",
                  "publishedAt": "2021-06-10T23:52:52Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "WilkAndy"
                          },
                          "bodyText": "Have a look at porous_flow/examples/coal_mining/coarse_with_fluid.i\na",
                          "url": "https://github.com/idaholab/moose/discussions/18031#discussioncomment-856079",
                          "updatedAt": "2022-10-26T17:00:03Z",
                          "publishedAt": "2021-06-11T00:36:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "Hi @WilkAndy,\nI did, not sure where are these equations from, but I will give it a go, thanks!\n  [./ini_pp]\n    type = ParsedFunction\n    vars = 'bulk p0 g    rho0'\n    vals = '2E3 0.0 1E-5 1E3'\n    value = '-bulk*log(exp(-p0/bulk)+g*rho0*z/bxculk)'\n  [../]\n  [./ini_xx]\n    type = ParsedFunction\n    vars = 'bulk p0 g    rho0 biot'\n    vals = '2E3 0.0 1E-5 1E3  0.7'\n    value = '0.8*(2500*10E-6*z+biot*(-bulk*log(exp(-p0/bulk)+g*rho0*z/bulk)))'\n  [../]\n  [./ini_zz]\n    type = ParsedFunction\n    vars = 'bulk p0 g    rho0 biot'\n    vals = '2E3 0.0 1E-5 1E3  0.7'\n    value = '2500*10E-6*z+biot*(-bulk*log(exp(-p0/bulk)+g*rho0*z/bulk))'\n  [../]",
                          "url": "https://github.com/idaholab/moose/discussions/18031#discussioncomment-856086",
                          "updatedAt": "2022-10-26T17:00:03Z",
                          "publishedAt": "2021-06-11T00:39:22Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "WilkAndy"
                          },
                          "bodyText": "250010E-6z is the gravitational load from a dry system.  biot is because you must prescribe effective stresses, not total stresses.  (-bulk * log ....) is a fancy way of doing hydrostatic: see https://mooseframework.inl.gov/modules/porous_flow/tests/gravity/gravity_tests.html",
                          "url": "https://github.com/idaholab/moose/discussions/18031#discussioncomment-856203",
                          "updatedAt": "2022-10-26T17:00:04Z",
                          "publishedAt": "2021-06-11T01:41:54Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}