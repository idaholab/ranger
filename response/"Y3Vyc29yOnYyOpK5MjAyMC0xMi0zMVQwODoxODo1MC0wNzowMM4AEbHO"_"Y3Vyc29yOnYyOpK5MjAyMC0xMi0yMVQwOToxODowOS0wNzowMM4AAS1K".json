{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMC0xMi0yMVQwOToxODowOS0wNzowMM4AAS1K"
    },
    "edges": [
      {
        "node": {
          "title": "First Time Conda Install - \"Always ok\" test failing",
          "author": {
            "login": "crswong888"
          },
          "bodyText": "Hello, I've never used the Conda MOOSE environment before. Surprisingly, I got all the way up to compiling the test app with no issues! Sadly, TestHarness is doing some weird things. For example:\n(moose) inl602177:~ christopherwong$ cd projects/moose/\n(moose) inl602177:moose christopherwong$ cd test/\n(moose) inl602177:test christopherwong$ make -j4\n(moose) inl602177:test christopherwong$ ./run_tests -i always_ok -p 2\ntests/test_harness.always_ok: Working Directory: /Users/christopherwong/projects/moose/test/tests/test_harness\ntests/test_harness.always_ok: Running command: mpiexec -n 2 /Users/christopherwong/projects/moose/test/moose_test-opt -i good.i --error --error-unused --error-override --no-gdb-backtrace\ntests/test_harness.always_ok: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntests/test_harness.always_ok: MPIR_Init_thread(586)..............: \ntests/test_harness.always_ok: MPID_Init(224).....................: channel initialization failed\ntests/test_harness.always_ok: MPIDI_CH3_Init(105)................: \ntests/test_harness.always_ok: MPID_nem_init(324).................: \ntests/test_harness.always_ok: MPID_nem_tcp_init(175).............: \ntests/test_harness.always_ok: MPID_nem_tcp_get_business_card(401): \ntests/test_harness.always_ok: MPID_nem_tcp_init(373).............: gethostbyname failed, inl602177 (errno 0)\ntests/test_harness.always_ok: (unknown)(): Invalid group\ntests/test_harness.always_ok: \ntests/test_harness.always_ok: \ntests/test_harness.always_ok: Exit Code: 8\ntests/test_harness.always_ok: ################################################################################\ntests/test_harness.always_ok: Tester failed, reason: CRASH\ntests/test_harness.always_ok: \ntests/test_harness.always_ok ...................................................... [OVERSIZED] FAILED (CRASH)\n\n\nFinal Test Results:\n--------------------------------------------------------------------------------------------------------------\ntests/test_harness.always_ok ...................................................... [OVERSIZED] FAILED (CRASH)\n--------------------------------------------------------------------------------------------------------------\nRan 1 tests in 1.3 seconds.\n0 passed, 0 skipped, 0 pending, 1 FAILED\n(moose) inl602177:test christopherwong$ \n\nAnyone got a clue what's going on?",
          "url": "https://github.com/idaholab/moose/discussions/16576",
          "updatedAt": "2023-08-03T20:32:28Z",
          "publishedAt": "2020-12-23T03:39:03Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "cticenhour"
                  },
                  "bodyText": "You need to add your hostname to your /etc/hosts file referencing127.0.0.1 - your localhost address.\nFirst run the command hostname in Terminal.\nLet's say that returns mycoolname. Then using sudo, edit /etc/hosts to add 127.0.0.1    mycoolname to the last line.\nA sample hosts file and a bit more information about the gethostbyname issue can be found on the MOOSE website here",
                  "url": "https://github.com/idaholab/moose/discussions/16576#discussioncomment-236177",
                  "updatedAt": "2023-08-03T20:32:33Z",
                  "publishedAt": "2020-12-23T04:26:25Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "cticenhour"
                          },
                          "bodyText": "I also see that you're on an INL machine - the error number 0 (as opposed to 3 in the linked FAQ) popped up recently on a new hire's Mac as well. If the following tip doesn't work and you are on a Mac, also try to turn off the Firewall (look in Security & Privacy in System Preferences). If that doesn't work, and Jamf management software is installed, you might need to get special permission to change some configuration settings.",
                          "url": "https://github.com/idaholab/moose/discussions/16576#discussioncomment-236181",
                          "updatedAt": "2023-08-03T20:32:33Z",
                          "publishedAt": "2020-12-23T04:32:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "cticenhour"
                          },
                          "bodyText": "Tagging @aeslaughter",
                          "url": "https://github.com/idaholab/moose/discussions/16576#discussioncomment-236188",
                          "updatedAt": "2023-08-03T20:32:43Z",
                          "publishedAt": "2020-12-23T04:48:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "crswong888"
                          },
                          "bodyText": "okay your answer here was 100% the solution to the \"always ok\" test failing. I also had another issue where my mac kept asking permission to run the test binary - for every single regression test - and what we talked about on slack solved that too. Seems like I got everything in working order now :)",
                          "url": "https://github.com/idaholab/moose/discussions/16576#discussioncomment-243010",
                          "updatedAt": "2023-08-03T20:32:49Z",
                          "publishedAt": "2020-12-26T20:53:43Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Installing MOOSE with GCC 7 and OpenMPI 4.0.1",
          "author": {
            "login": "jinca"
          },
          "bodyText": "Hello, I have used the following modules to compile MOOSE:\nCurrently Loaded Modulefiles:\n\nslurm                             3) turbovnc/2.0.1                    5) singularity/current               7) cmake-3.9.4-gcc-5.4.0-fldgles     9) openmpi-4.0.1-gcc-7.2.0-myumaue\ndot                               4) vgl/2.5.1/64                      6) rhel7/global                      8) python/3.6                       10) gcc/7\n\nI was able to compiled PETSC and LibMesh, but I got the following errors when I run the tests:\ngeomsearch/2d_moving_penetration.pl_test4ns ............................................................... OK\nsamplers/base.global_vs_local/base_2rank ..................................................... [min_cpus=2] OK\ntime_integrators/convergence.implicit_astabledirk4_bootstrap/level1 ....................................... OK\nmaterials/derivative_material_interface.ad_ordering/execution ............................................. OK\ngeomsearch/2d_moving_penetration.pl_test4qns .............................................................. OK\ntime_integrators/convergence.implicit_astabledirk4_bootstrap/level2 ....................................... OK\nmaterials/derivative_material_interface.ad_new_getproperty_semantics ...................................... OK\nmaterials/derivative_material_interface.ad_mutliblock ..................................................... OK\nsamplers/base.global_vs_local/base_3rank ..................................................... [min_cpus=3] OK\ngeomsearch/2d_moving_penetration.pl_test4nns .............................................................. OK\nsamplers/base.global_vs_local/rand_1rank .................................................................. OK\nmaterials/material.exception/rank0 ................................................. [skipped dependency] SKIP\nuserobjects/threaded_general_user_object.thread_copies_guo/th3 ............................ [min_threads=3] OK\nvectorpostprocessors/line_value_sampler.scaling ........................................................... OK\nvectorpostprocessors/csv_reader.tester_fail ..................... [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\npreconditioners/hmg.hmg ........................................ [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\noutputs/json/distributed.info/default .................................... [min_cpus=2] FAILED (MISSING FILES)\npreconditioners/hmg.hmg_3D ..................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel . [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nmisc/exception.parallel_error_residual_transient_non_zero_rank .. [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\npreconditioners/hmg.hmg_strumpack .............................. [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank .. [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\nreporters/mesh_info.info/files ........................................... [min_cpus=2] FAILED (MISSING FILES)\nmesh/mesh_only.mesh_only_checkpoint ...................................... [min_cpus=3] FAILED (MISSING FILES)\nmesh/splitting.check/pre_split ................................. [min_cpus=3] FAILED (EXPECTED OUTPUT MISSING)\nmesh/splitting.check/forced_pre_split .......................... [min_cpus=3] FAILED (EXPECTED OUTPUT MISSING)\nmesh/custom_partitioner.group/custom_linear_partitioner ........................ [min_cpus=2] FAILED (EXODIFF)\nics/depend_on_uo.ic_depend_on_uo ............................................... [min_cpus=2] FAILED (EXODIFF)\nvectorpostprocessors/work_balance.work_balance/replicated ...................... [min_cpus=2] FAILED (CSVDIFF)\npostprocessors/num_residual_eval.test .......................................... [min_cpus=2] FAILED (CSVDIFF)\nrestart/kernel_restartable.parallel_error/error1 ............................... [min_cpus=2] FAILED (EXODIFF)\nics/depend_on_uo.scalar_ic_from_uo ............................................. [min_cpus=2] FAILED (CSVDIFF)\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject ...... [min_cpus=2] FAILED (CSVDIFF)\noutputs/variables.nemesis_hide ................................................. [min_cpus=2] FAILED (EXODIFF)\nmisc/exception.parallel_exception_residual_transient_non_zero_rank ............. [min_cpus=2] FAILED (EXODIFF)\nvectorpostprocessors/parallel_consistency.test ................................. [min_cpus=2] FAILED (EXODIFF)\ntransfers/multiapp_nearest_node_transfer.parallel .............................. [min_cpus=2] FAILED (EXODIFF)\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank ............. [min_cpus=2] FAILED (EXODIFF)\nbcs/dmg_periodic.2d ............................................................ [min_cpus=2] FAILED (EXODIFF)\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer ....................... [min_cpus=2] FAILED (CSVDIFF)\nbcs/periodic.testperiodic_dp ................................................... [min_cpus=2] FAILED (EXODIFF)\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement ........... [min_cpus=2] FAILED (EXODIFF)\ninterfaces/random.parallel_verification ........................................ [min_cpus=2] FAILED (EXODIFF)\nvectorpostprocessors/parallel_consistency.broadcast ............................ [min_cpus=2] FAILED (EXODIFF)\nbcs/dmg_periodic.3d ............................................................ [min_cpus=2] FAILED (EXODIFF)\ninterfacekernels/2d_interface.parallel_fdp_test ................................ [min_cpus=2] FAILED (EXODIFF)\nmesh/checkpoint.test_2 ......................................................... [min_cpus=2] FAILED (EXODIFF)\nmeshgenerators/meta_data_store.test_meta_data_with_use_split ................... [min_cpus=2] FAILED (CSVDIFF)\nbcs/dmg_periodic.1d ............................................................ [min_cpus=2] FAILED (EXODIFF)\nmesh/checkpoint.test_2a ........................................................ [min_cpus=2] FAILED (EXODIFF)\ninterfaces/random.parallel_verification_uo ..................................... [min_cpus=2] FAILED (EXODIFF)\nmesh/custom_partitioner.group/custom_linear_partitioner_restart ................ [min_cpus=2] FAILED (EXODIFF)\nmesh/splitting.use_split ....................................................... [min_cpus=3] FAILED (EXODIFF)\ngeomsearch/penetration_locator.parallel_test ................................... [min_cpus=3] FAILED (EXODIFF)\nauxkernels/vector_postprocessor_visualization.test ............................. [min_cpus=3] FAILED (EXODIFF)\ninterfaces/random.material_parallel ............................................ [min_cpus=2] FAILED (EXODIFF)\nmultiapps/max_procs_per_app.test ............................................... [min_cpus=3] FAILED (EXODIFF)\nvectorpostprocessors/distributed.parallel_type/replicated ...................... [min_cpus=3] FAILED (CSVDIFF)\nrelationship_managers/evaluable.evaluable_neighbors_replicated ................. [min_cpus=3] FAILED (EXODIFF)\nrelationship_managers/evaluable.edge_neighbor .................................. [min_cpus=3] FAILED (EXODIFF)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor .............. [min_cpus=3] FAILED (EXODIFF)\noutputs/reporters.csv/distributed .............................................. [min_cpus=3] FAILED (CSVDIFF)\nvectorpostprocessors/line_value_sampler.parallel ............................... [min_cpus=3] FAILED (CSVDIFF)\nmaterials/material.exception/rank1 ............................................. [min_cpus=4] FAILED (EXODIFF)\nRan 2410 tests in 861.8 seconds.\n2360 passed, 97 skipped, 0 pending, 50 FAILED\nMAX FAILURES REACHED",
          "url": "https://github.com/idaholab/moose/discussions/16546",
          "updatedAt": "2022-08-02T05:52:28Z",
          "publishedAt": "2020-12-18T21:23:30Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "srinath-chakravarthy"
                  },
                  "bodyText": "./run_tests -j 4 should get rid of the failed tests",
                  "url": "https://github.com/idaholab/moose/discussions/16546#discussioncomment-224371",
                  "updatedAt": "2022-08-02T05:52:39Z",
                  "publishedAt": "2020-12-18T21:43:19Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jinca"
                          },
                          "bodyText": "Hi, thanks I did that after the installations, it is starting again:\n./run_tests -j 4\nreporters/mesh_info.info/default ..................................................... [PLATFORM!=DARWIN] SKIP\nfvkernels/block-restriction.overlapping-mats ....................................... [METHOD!=DEVEL, DBG] SKIP\nfvkernels/fv_simple_diffusion.unstructured-rz ................................ [AD_INDEXING_TYPE!=GLOBAL] SKIP\nfvkernels/fv_adapt.steady-mms ................................................ [no pandas, no matplotlib] SKIP\nfvkernels/fv_adapt.adapt ..................................................... [AD_INDEXING_TYPE!=GLOBAL] SKIP\nfvkernels/fv-to-fe-coupling.deps-not-satisfied ..................................... [METHOD!=DEVEL, DBG] SKIP\nfvkernels/mms/grad-reconstruction.extended-cartesian ......................... [AD_INDEXING_TYPE!=GLOBAL] SKIP\nfvkernels/mms/grad-reconstruction.rz ......................................... [AD_INDEXING_TYPE!=GLOBAL] SKIP\nfvkernels/mms/grad-reconstruction.cartesian .................................. [AD_INDEXING_TYPE!=GLOBAL] SKIP\nfvkernels/mms/non-orthogonal.extended .............. [AD_INDEXING_TYPE!=GLOBAL, no pandas, no matplotlib] SKIP\nfvkernels/mms/non-orthogonal.compact ............... [AD_INDEXING_TYPE!=GLOBAL, no pandas, no matplotlib] SKIP\noutputs/nemesis.nemesis_elemental_distributed .................................. [MESH_MODE!=DISTRIBUTED] SKIP\noutputs/nemesis.nemesis_scalar_distributed ..................................... [MESH_MODE!=DISTRIBUTED] SKIP\noutputs/png.image_tests/wedge ............................................................ [LIBPNG!=TRUE] SKIP\noutputs/png.image_tests/square_domain .................................................... [LIBPNG!=TRUE] SKIP\noutputs/png.image_tests/adv_diff_reaction ................................................ [LIBPNG!=TRUE] SKIP\noutputs/iterative.vtk ....................................................................... [VTK!=TRUE] SKIP\noutputs/format.tecplot_bin_test_override ............................................... [TECPLOT!=FALSE] SKIP\noutputs/vtk.solution/diff_serial_mesh_parallel .............................................. [VTK!=TRUE] SKIP\noutputs/vtk.files/parallel .................................................................. [VTK!=TRUE] SKIP\noutputs/vtk.solution/diff_serial_mesh ....................................................... [VTK!=TRUE] SKIP\noutputs/vtk.solution/diff_parallel_mesh ...................... [#6149, MESH_MODE!=DISTRIBUTED, VTK!=TRUE] SKIP\noutputs/vtk.files/serial .................................................................... [VTK!=TRUE] SKIP\nauxkernels/solution_aux.aux_nonlinear_solution/from_xdr ........................ [--with-dof-id-bytes!=4] SKIP\ntransfers/multiapp_interpolation_transfer.fromsub_restricted ......................... [PLATFORM!=DARWIN] SKIP\nreporters/constant_reporter.constant_reporter ............................................................. OK\nreporters/iteration_info.info/default ..................................................................... OK\n...",
                          "url": "https://github.com/idaholab/moose/discussions/16546#discussioncomment-224431",
                          "updatedAt": "2022-08-02T05:52:39Z",
                          "publishedAt": "2020-12-18T22:09:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "The skips listed here are normal, the failures you are seeing indicate that your MPI is not setup correctly. You should be able to get a more detailed error messages from one the failing tests, you can scroll up for the details or just run a single test, for example:\n./run_tests --re=materials/material.exception/rank1\nThe -j will help the tests run through faster, but will not change the result. the [min_cpus=4] is just indicating that test requires 4 processors. It will run with 4 automatically.",
                          "url": "https://github.com/idaholab/moose/discussions/16546#discussioncomment-230159",
                          "updatedAt": "2022-08-02T05:52:39Z",
                          "publishedAt": "2020-12-21T16:24:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Is this on INL HPC ?\nWhat does 'which gcc; which mpicc' report ?\nYou want to make sure to compile with the compiler associated with your MPI distribution",
                          "url": "https://github.com/idaholab/moose/discussions/16546#discussioncomment-230689",
                          "updatedAt": "2022-08-02T05:52:39Z",
                          "publishedAt": "2020-12-21T19:25:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jinca"
                          },
                          "bodyText": "I rerun again and I got this time 8 FAILS. This test is meant to have 0 FAILS?\noutputs/xml.parallel/distributed ............................................................. [min_cpus=3] OK\noutputs/xml.iterations .................................................................................... OK\nmeshgenerators/dmg_displaced_mesh.pbc/adaptivity_autoscaling .............................................. OK\nmeshgenerators/distributed_rectilinear_mesh_generator.mesh_adaptivity ..................................... OK\noutputs/nemesis.nemesis_elemental_replicated ................................................. [min_cpus=4] OK\nrestart/kernel_restartable.thread_error/with_threads ...................................... [min_threads=4] OK\noutputs/nemesis.nemesis_scalar_replicated .................................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.ptscotch ...................................................... [min_cpus=4] OK\nmesh/nemesis.nemesis_repartitioning_test ..................................................... [min_cpus=4] OK\nrestart/kernel_restartable.thread_error/threads_error ..................................... [min_threads=2] OK\nuserobjects/threaded_general_user_object.thread_copies_guo/th4 ............................ [min_threads=4] OK\nuserobjects/threaded_general_user_object.thread_copies_guo/th5 ....... [min_threads=5,insufficient slots] SKIP\nuserobjects/threaded_general_user_object.thread_copies_guo/th6 ....... [insufficient slots,min_threads=6] SKIP\nuserobjects/threaded_general_user_object.thread_copies_guo/th7 ....... [min_threads=7,insufficient slots] SKIP\nuserobjects/threaded_general_user_object.thread_copies_guo/th8 ....... [insufficient slots,min_threads=8] SKIP\npartitioners/petsc_partitioner.ptscotch_weight_elment ........................................ [min_cpus=4] OK\nauxkernels/ghosting_aux.ghosting/show_with_local ............................................. [min_cpus=4] OK\nauxkernels/ghosting_aux.no_algebraic_ghosting ................................................ [min_cpus=4] OK\npartitioners/petsc_partitioner.ptscotch_weight_side .......................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.ptscotch_weight_both .......................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.parmetis ...................................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.parmetis_weight_element ....................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.parmetis_weight_side .......................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.parmetis_weight_both .......................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.parmetis_presplit_mesh ........................................ [min_cpus=2] OK\nvectorpostprocessors/csv_reader.tester_fail ..................... [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\nbcs/dmg_periodic.1d ............................................................. [min_cpus=2] FAILED (ERRMSG)\nmisc/exception.parallel_error_residual_transient_non_zero_rank .. [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank ................ [min_cpus=2] FAILED (NO CRASH)\nvectorpostprocessors/work_balance.work_balance/replicated ...................... [min_cpus=2] FAILED (CSVDIFF)\noutputs/variables.nemesis_hide ................................................. [min_cpus=2] FAILED (EXODIFF)\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch .............. [min_cpus=4] FAILED (EXODIFF)\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out ............. [min_cpus=3] FAILED (EXODIFF)\nRan 2479 tests in 565.8 seconds.\n2471 passed, 93 skipped, 0 pending, 8 FAILED",
                          "url": "https://github.com/idaholab/moose/discussions/16546#discussioncomment-236097",
                          "updatedAt": "2022-08-02T05:53:27Z",
                          "publishedAt": "2020-12-23T02:32:20Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "You should indeed have 0 fails in the test suite.\nCould you please paste the error messages for each failed test?",
                  "url": "https://github.com/idaholab/moose/discussions/16546#discussioncomment-236111",
                  "updatedAt": "2022-08-02T05:52:39Z",
                  "publishedAt": "2020-12-23T02:44:29Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jinca"
                          },
                          "bodyText": "vectorpostprocessors/work_balance.work_balance/replicated ...................... [min_cpus=2] FAILED (CSVDIFF)\n\nvectorpostprocessors/work_balance.work_balance/replicated: Working Directory: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/vectorpostprocessors/work_balance\nvectorpostprocessors/work_balance.work_balance/replicated: Running command: mpiexec -n 2 /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/moose_test-opt -i work_balance.i --error --error-unused --error-override --no-gdb-backtrace\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated: Framework Information:\nvectorpostprocessors/work_balance.work_balance/replicated: MOOSE Version:           git commit 2f250bf011 on 2020-12-24\nvectorpostprocessors/work_balance.work_balance/replicated: LibMesh Version:         964118e30ec2da83ed6642171ccd23074e57c2ad\nvectorpostprocessors/work_balance.work_balance/replicated: PETSc Version:           3.14.2\nvectorpostprocessors/work_balance.work_balance/replicated: SLEPc Version:           3.14.0\nvectorpostprocessors/work_balance.work_balance/replicated: Current Time:            Fri Dec 25 22:20:17 2020\nvectorpostprocessors/work_balance.work_balance/replicated: Executable Timestamp:    Fri Dec 25 21:18:15 2020\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated: Parallelism:\nvectorpostprocessors/work_balance.work_balance/replicated:   Num Processors:          2\nvectorpostprocessors/work_balance.work_balance/replicated:   Num Threads:             1\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated: Mesh:\nvectorpostprocessors/work_balance.work_balance/replicated:   Parallel Type:           replicated\nvectorpostprocessors/work_balance.work_balance/replicated:   Mesh Dimension:          2\nvectorpostprocessors/work_balance.work_balance/replicated:   Spatial Dimension:       2\nvectorpostprocessors/work_balance.work_balance/replicated:   Nodes:\nvectorpostprocessors/work_balance.work_balance/replicated:     Total:                 121\nvectorpostprocessors/work_balance.work_balance/replicated:     Local:                 66\nvectorpostprocessors/work_balance.work_balance/replicated:   Elems:\nvectorpostprocessors/work_balance.work_balance/replicated:     Total:                 100\nvectorpostprocessors/work_balance.work_balance/replicated:     Local:                 50\nvectorpostprocessors/work_balance.work_balance/replicated:   Num Subdomains:          1\nvectorpostprocessors/work_balance.work_balance/replicated:   Num Partitions:          2\nvectorpostprocessors/work_balance.work_balance/replicated:   Partitioner:             linear\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated: Nonlinear System:\nvectorpostprocessors/work_balance.work_balance/replicated:   Num DOFs:                121\nvectorpostprocessors/work_balance.work_balance/replicated:   Num Local DOFs:          66\nvectorpostprocessors/work_balance.work_balance/replicated:   Variables:               \"u\"\nvectorpostprocessors/work_balance.work_balance/replicated:   Finite Element Types:    \"LAGRANGE\"\nvectorpostprocessors/work_balance.work_balance/replicated:   Approximation Orders:    \"FIRST\"\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated: Auxiliary System:\nvectorpostprocessors/work_balance.work_balance/replicated:   Num DOFs:                221\nvectorpostprocessors/work_balance.work_balance/replicated:   Num Local DOFs:          116\nvectorpostprocessors/work_balance.work_balance/replicated:   Variables:               \"someaux\" \"otheraux\"\nvectorpostprocessors/work_balance.work_balance/replicated:   Finite Element Types:    \"LAGRANGE\" \"MONOMIAL\"\nvectorpostprocessors/work_balance.work_balance/replicated:   Approximation Orders:    \"FIRST\" \"CONSTANT\"\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated: Execution Information:\nvectorpostprocessors/work_balance.work_balance/replicated:   Executioner:             Steady\nvectorpostprocessors/work_balance.work_balance/replicated:   Solver Mode:             Preconditioned JFNK\nvectorpostprocessors/work_balance.work_balance/replicated:   PETSc Preconditioner:    hypre boomeramg\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated:  0 Nonlinear |R| = 3.082207e+00\nvectorpostprocessors/work_balance.work_balance/replicated:       0 Linear |R| = 3.082207e+00\nvectorpostprocessors/work_balance.work_balance/replicated:       1 Linear |R| = 1.109669e-01\nvectorpostprocessors/work_balance.work_balance/replicated:       2 Linear |R| = 4.138566e-03\nvectorpostprocessors/work_balance.work_balance/replicated:       3 Linear |R| = 1.332120e-04\nvectorpostprocessors/work_balance.work_balance/replicated:       4 Linear |R| = 3.761998e-06\nvectorpostprocessors/work_balance.work_balance/replicated:   Linear solve converged due to CONVERGED_RTOL iterations 4\nvectorpostprocessors/work_balance.work_balance/replicated:  1 Nonlinear |R| = 3.747395e-06\nvectorpostprocessors/work_balance.work_balance/replicated:       0 Linear |R| = 3.747395e-06\nvectorpostprocessors/work_balance.work_balance/replicated:       1 Linear |R| = 1.565163e-07\nvectorpostprocessors/work_balance.work_balance/replicated:       2 Linear |R| = 3.823892e-09\nvectorpostprocessors/work_balance.work_balance/replicated:       3 Linear |R| = 1.288260e-10\nvectorpostprocessors/work_balance.work_balance/replicated:       4 Linear |R| = 2.866952e-12\nvectorpostprocessors/work_balance.work_balance/replicated:   Linear solve converged due to CONVERGED_RTOL iterations 4\nvectorpostprocessors/work_balance.work_balance/replicated:  2 Nonlinear |R| = 2.864804e-12\nvectorpostprocessors/work_balance.work_balance/replicated: Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 2\nvectorpostprocessors/work_balance.work_balance/replicated:  Solve Converged!\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated: Framework Information:\nvectorpostprocessors/work_balance.work_balance/replicated: MOOSE Version:           git commit 2f250bf011 on 2020-12-24\nvectorpostprocessors/work_balance.work_balance/replicated: LibMesh Version:         964118e30ec2da83ed6642171ccd23074e57c2ad\nvectorpostprocessors/work_balance.work_balance/replicated: PETSc Version:           3.14.2\nvectorpostprocessors/work_balance.work_balance/replicated: SLEPc Version:           3.14.0\nvectorpostprocessors/work_balance.work_balance/replicated: Current Time:            Fri Dec 25 22:20:17 2020\nvectorpostprocessors/work_balance.work_balance/replicated: Executable Timestamp:    Fri Dec 25 21:18:15 2020\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated: Parallelism:\nvectorpostprocessors/work_balance.work_balance/replicated:   Num Processors:          2\nvectorpostprocessors/work_balance.work_balance/replicated:   Num Threads:             1\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated: Mesh:\nvectorpostprocessors/work_balance.work_balance/replicated:   Parallel Type:           replicated\nvectorpostprocessors/work_balance.work_balance/replicated:   Mesh Dimension:          2\nvectorpostprocessors/work_balance.work_balance/replicated:   Spatial Dimension:       2\nvectorpostprocessors/work_balance.work_balance/replicated:   Nodes:\nvectorpostprocessors/work_balance.work_balance/replicated:     Total:                 121\nvectorpostprocessors/work_balance.work_balance/replicated:     Local:                 66\nvectorpostprocessors/work_balance.work_balance/replicated:   Elems:\nvectorpostprocessors/work_balance.work_balance/replicated:     Total:                 100\nvectorpostprocessors/work_balance.work_balance/replicated:     Local:                 50\nvectorpostprocessors/work_balance.work_balance/replicated:   Num Subdomains:          1\nvectorpostprocessors/work_balance.work_balance/replicated:   Num Partitions:          2\nvectorpostprocessors/work_balance.work_balance/replicated:   Partitioner:             linear\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated: Nonlinear System:\nvectorpostprocessors/work_balance.work_balance/replicated:   Num DOFs:                121\nvectorpostprocessors/work_balance.work_balance/replicated:   Num Local DOFs:          66\nvectorpostprocessors/work_balance.work_balance/replicated:   Variables:               \"u\"\nvectorpostprocessors/work_balance.work_balance/replicated:   Finite Element Types:    \"LAGRANGE\"\nvectorpostprocessors/work_balance.work_balance/replicated:   Approximation Orders:    \"FIRST\"\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated: Auxiliary System:\nvectorpostprocessors/work_balance.work_balance/replicated:   Num DOFs:                221\nvectorpostprocessors/work_balance.work_balance/replicated:   Num Local DOFs:          116\nvectorpostprocessors/work_balance.work_balance/replicated:   Variables:               \"someaux\" \"otheraux\"\nvectorpostprocessors/work_balance.work_balance/replicated:   Finite Element Types:    \"LAGRANGE\" \"MONOMIAL\"\nvectorpostprocessors/work_balance.work_balance/replicated:   Approximation Orders:    \"FIRST\" \"CONSTANT\"\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated: Execution Information:\nvectorpostprocessors/work_balance.work_balance/replicated:   Executioner:             Steady\nvectorpostprocessors/work_balance.work_balance/replicated:   Solver Mode:             Preconditioned JFNK\nvectorpostprocessors/work_balance.work_balance/replicated:   PETSc Preconditioner:    hypre boomeramg\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated:  0 Nonlinear |R| = 3.082207e+00\nvectorpostprocessors/work_balance.work_balance/replicated:       0 Linear |R| = 3.082207e+00\nvectorpostprocessors/work_balance.work_balance/replicated:       1 Linear |R| = 1.109669e-01\nvectorpostprocessors/work_balance.work_balance/replicated:       2 Linear |R| = 4.138566e-03\nvectorpostprocessors/work_balance.work_balance/replicated:       3 Linear |R| = 1.332120e-04\nvectorpostprocessors/work_balance.work_balance/replicated:       4 Linear |R| = 3.761998e-06\nvectorpostprocessors/work_balance.work_balance/replicated:   Linear solve converged due to CONVERGED_RTOL iterations 4\nvectorpostprocessors/work_balance.work_balance/replicated:  1 Nonlinear |R| = 3.747395e-06\nvectorpostprocessors/work_balance.work_balance/replicated:       0 Linear |R| = 3.747395e-06\nvectorpostprocessors/work_balance.work_balance/replicated:       1 Linear |R| = 1.565163e-07\nvectorpostprocessors/work_balance.work_balance/replicated:       2 Linear |R| = 3.823892e-09\nvectorpostprocessors/work_balance.work_balance/replicated:       3 Linear |R| = 1.288260e-10\nvectorpostprocessors/work_balance.work_balance/replicated:       4 Linear |R| = 2.866952e-12\nvectorpostprocessors/work_balance.work_balance/replicated:   Linear solve converged due to CONVERGED_RTOL iterations 4\nvectorpostprocessors/work_balance.work_balance/replicated:  2 Nonlinear |R| = 2.864804e-12\nvectorpostprocessors/work_balance.work_balance/replicated: Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 2\nvectorpostprocessors/work_balance.work_balance/replicated:  Solve Converged!\nvectorpostprocessors/work_balance.work_balance/replicated: Running csvdiff: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/python/mooseutils/csvdiff.py /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/vectorpostprocessors/work_balance/gold/work_balance_out_all_wb_0000.csv /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/vectorpostprocessors/work_balance/work_balance_out_all_wb_0000.csv --relative-tolerance 5.5e-06 --abs-zero 1e-10\nvectorpostprocessors/work_balance.work_balance/replicated: ERROR: In file work_balance_out_all_wb_0000.csv: The values in column \"num_partition_hardware_id_sides\" don't match @ t0\nvectorpostprocessors/work_balance.work_balance/replicated: \trelative diff:   0.000e+00 ~ 1.000e+01 = 1.000e+00 (1.000e+00)\nvectorpostprocessors/work_balance.work_balance/replicated: In file work_balance_out_all_wb_0000.csv: The values in column \"partition_hardware_id_surface_area\" don't match @ t0\nvectorpostprocessors/work_balance.work_balance/replicated: \trelative diff:   0.000e+00 ~ 1.000e+00 = 1.000e+00 (1.000e+00)\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated: ################################################################################\nvectorpostprocessors/work_balance.work_balance/replicated: Tester failed, reason: CSVDIFF\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated ...................... [min_cpus=2] FAILED (CSVDIFF)\n\n\nmisc/exception.parallel_error_residual_transient_non_zero_rank ................ [min_cpus=2] FAILED (NO CRASH)\n\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Working Directory: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/misc/exception\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Running command: mpiexec -n 2 /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/moose_test-opt -i parallel_exception_residual_transient.i Kernels/exception/rank=1 Kernels/exception/should_throw=false Outputs/exodus=false --error --error-unused --error-override --no-gdb-backtrace --keep-cout --redirect-output parallel_error_residual_transient_non_zero_rank\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Linear solve converged due to CONVERGED_RTOL iterations 2\nmisc/exception.parallel_error_residual_transient_non_zero_rank: --------------------------------------------------------------------------\nmisc/exception.parallel_error_residual_transient_non_zero_rank: MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD\nmisc/exception.parallel_error_residual_transient_non_zero_rank: with errorcode 1.\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\nmisc/exception.parallel_error_residual_transient_non_zero_rank: You may or may not see output from other processes, depending on\nmisc/exception.parallel_error_residual_transient_non_zero_rank: exactly when Open MPI kills them.\nmisc/exception.parallel_error_residual_transient_non_zero_rank: --------------------------------------------------------------------------\nmisc/exception.parallel_error_residual_transient_non_zero_rank: srun: Job step aborted: Waiting up to 122 seconds for job step to finish.\nmisc/exception.parallel_error_residual_transient_non_zero_rank: slurmstepd: error: *** STEP 32642259.136 ON cpu-p-628 CANCELLED AT 2020-12-25T22:22:09 ***\nmisc/exception.parallel_error_residual_transient_non_zero_rank: srun: error: cpu-p-629: task 1: Segmentation fault\nmisc/exception.parallel_error_residual_transient_non_zero_rank: srun: error: cpu-p-628: task 0: Killed\nmisc/exception.parallel_error_residual_transient_non_zero_rank: ################################################################################\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Output from /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/misc/exception/parallel_error_residual_transient_non_zero_rank.processor.0\nmisc/exception.parallel_error_residual_transient_non_zero_rank: ################################################################################\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Framework Information:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: MOOSE Version:           git commit 2f250bf011 on 2020-12-24\nmisc/exception.parallel_error_residual_transient_non_zero_rank: LibMesh Version:         964118e30ec2da83ed6642171ccd23074e57c2ad\nmisc/exception.parallel_error_residual_transient_non_zero_rank: PETSc Version:           3.14.2\nmisc/exception.parallel_error_residual_transient_non_zero_rank: SLEPc Version:           3.14.0\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Current Time:            Fri Dec 25 22:22:09 2020\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Executable Timestamp:    Fri Dec 25 21:18:15 2020\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Parallelism:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Num Processors:          2\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Num Threads:             1\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Mesh:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Parallel Type:           replicated\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Mesh Dimension:          2\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Spatial Dimension:       2\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Nodes:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:     Total:                 72\nmisc/exception.parallel_error_residual_transient_non_zero_rank:     Local:                 36\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Elems:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:     Total:                 50\nmisc/exception.parallel_error_residual_transient_non_zero_rank:     Local:                 25\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Num Subdomains:          2\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Num Partitions:          2\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Partitioner:             metis\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Nonlinear System:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Num DOFs:                72\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Num Local DOFs:          36\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Variables:               \"u\"\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Finite Element Types:    \"LAGRANGE\"\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Approximation Orders:    \"FIRST\"\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Execution Information:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Executioner:             Transient\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   TimeStepper:             ConstantDT\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Solver Mode:             Preconditioned JFNK\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Time Step 0, time = 0\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Time Step 1, time = 0.01, dt = 0.01\nmisc/exception.parallel_error_residual_transient_non_zero_rank:  0 Nonlinear |R| = 7.071068e-01\nmisc/exception.parallel_error_residual_transient_non_zero_rank:       0 Linear |R| = 7.071068e-01\nmisc/exception.parallel_error_residual_transient_non_zero_rank:       1 Linear |R| = 1.290385e-03\nmisc/exception.parallel_error_residual_transient_non_zero_rank:       2 Linear |R| = 4.866985e-06\nmisc/exception.parallel_error_residual_transient_non_zero_rank:  1 Nonlinear |R| = 4.866755e-06\nmisc/exception.parallel_error_residual_transient_non_zero_rank:       0 Linear |R| = 4.866755e-06\nmisc/exception.parallel_error_residual_transient_non_zero_rank: ################################################################################\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Output from /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/misc/exception/parallel_error_residual_transient_non_zero_rank.processor.1\nmisc/exception.parallel_error_residual_transient_non_zero_rank: ################################################################################\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Framework Information:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: MOOSE Version:           git commit 2f250bf011 on 2020-12-24\nmisc/exception.parallel_error_residual_transient_non_zero_rank: LibMesh Version:         964118e30ec2da83ed6642171ccd23074e57c2ad\nmisc/exception.parallel_error_residual_transient_non_zero_rank: PETSc Version:           3.14.2\nmisc/exception.parallel_error_residual_transient_non_zero_rank: SLEPc Version:           3.14.0\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Current Time:            Fri Dec 25 22:22:03 2020\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Executable Timestamp:    Fri Dec 25 21:18:15 2020\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Parallelism:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Num Processors:          2\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Num Threads:             1\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Mesh:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Parallel Type:           replicated\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Mesh Dimension:          2\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Spatial Dimension:       2\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Nodes:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:     Total:                 72\nmisc/exception.parallel_error_residual_transient_non_zero_rank:     Local:                 36\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Elems:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:     Total:                 50\nmisc/exception.parallel_error_residual_transient_non_zero_rank:     Local:                 25\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Num Subdomains:          2\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Num Partitions:          2\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Partitioner:             metis\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Nonlinear System:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Num DOFs:                72\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Num Local DOFs:          36\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Variables:               \"u\"\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Finite Element Types:    \"LAGRANGE\"\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Approximation Orders:    \"FIRST\"\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Execution Information:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Executioner:             Transient\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   TimeStepper:             ConstantDT\nmisc/exception.parallel_error_residual_transient_non_zero_rank:   Solver Mode:             Preconditioned JFNK\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Time Step 0, time = 0\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Time Step 1, time = 0.01, dt = 0.01\nmisc/exception.parallel_error_residual_transient_non_zero_rank:  0 Nonlinear |R| = 7.071068e-01\nmisc/exception.parallel_error_residual_transient_non_zero_rank:       0 Linear |R| = 7.071068e-01\nmisc/exception.parallel_error_residual_transient_non_zero_rank:       1 Linear |R| = 1.290385e-03\nmisc/exception.parallel_error_residual_transient_non_zero_rank:       2 Linear |R| = 4.866985e-06\nmisc/exception.parallel_error_residual_transient_non_zero_rank:  1 Nonlinear |R| = 4.866755e-06\nmisc/exception.parallel_error_residual_transient_non_zero_rank:       0 Linear |R| = 4.866755e-06\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: *** ERROR ***\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Intentional error triggered during residual calculation\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank: ################################################################################\nmisc/exception.parallel_error_residual_transient_non_zero_rank: Tester failed, reason: NO CRASH\nmisc/exception.parallel_error_residual_transient_non_zero_rank:\nmisc/exception.parallel_error_residual_transient_non_zero_rank ................ [min_cpus=2] FAILED (NO CRASH)\n\n\nbcs/dmg_periodic.1d ............................................................. [min_cpus=2] FAILED (ERRMSG)\n\nbcs/dmg_periodic.1d: Working Directory: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/bcs/dmg_periodic\nbcs/dmg_periodic.1d: Running command: mpiexec -n 2 /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/moose_test-opt -i dmg_periodic_bc.i Outputs/hide=\"pid\" Outputs/file_base=dmg_periodic_bc_out_1d Mesh/dmg/dim=1 Mesh/dmg/nx=100 Mesh/dmg/ny=0 Mesh/dmg/ymax=0 BCs/Periodic/all/auto_direction=\"x\" AuxKernels/periodic_dist/point=\"1 0 0\" Kernels/forcing/y_center=0 Kernels/forcing/amplitude=0.1 Kernels/forcing/x_center=0 --error --error-unused --error-override --no-gdb-backtrace\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d: *** ERROR ***\nbcs/dmg_periodic.1d: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/bcs/dmg_periodic/dmg_periodic_bc.i:62: wrong number of values in scalar component parameter AuxKernels/periodic_dist/point: size 1 is not a multiple of 3\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d: --------------------------------------------------------------------------\nbcs/dmg_periodic.1d: MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD\nbcs/dmg_periodic.1d: with errorcode 1.\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d: NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\nbcs/dmg_periodic.1d: You may or may not see output from other processes, depending on\nbcs/dmg_periodic.1d: exactly when Open MPI kills them.\nbcs/dmg_periodic.1d: --------------------------------------------------------------------------\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d: *** ERROR ***\nbcs/dmg_periodic.1d: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/bcs/dmg_periodic/dmg_periodic_bc.i:62: wrong number of values in scalar component parameter AuxKernels/periodic_dist/point: size 1 is not a multiple of 3\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d: --------------------------------------------------------------------------\nbcs/dmg_periodic.1d: MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD\nbcs/dmg_periodic.1d: with errorcode 1.\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d: NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\nbcs/dmg_periodic.1d: You may or may not see output from other processes, depending on\nbcs/dmg_periodic.1d: exactly when Open MPI kills them.\nbcs/dmg_periodic.1d: --------------------------------------------------------------------------\nbcs/dmg_periodic.1d: srun: Job step aborted: Waiting up to 122 seconds for job step to finish.\nbcs/dmg_periodic.1d: slurmstepd: error: *** STEP 32642259.156 ON cpu-p-628 CANCELLED AT 2020-12-25T22:22:20 ***\nbcs/dmg_periodic.1d: srun: error: cpu-p-629: task 1: Exited with exit code 1\nbcs/dmg_periodic.1d: srun: error: cpu-p-628: task 0: Exited with exit code 1\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d: *** ERROR ***\nbcs/dmg_periodic.1d: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/bcs/dmg_periodic/dmg_periodic_bc.i:62: wrong number of values in scalar component parameter AuxKernels/periodic_dist/point: size 1 is not a multiple of 3\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d: --------------------------------------------------------------------------\nbcs/dmg_periodic.1d: MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD\nbcs/dmg_periodic.1d: with errorcode 1.\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d: NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\nbcs/dmg_periodic.1d: You may or may not see output from other processes, depending on\nbcs/dmg_periodic.1d: exactly when Open MPI kills them.\nbcs/dmg_periodic.1d: --------------------------------------------------------------------------\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d: *** ERROR ***\nbcs/dmg_periodic.1d: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/bcs/dmg_periodic/dmg_periodic_bc.i:62: wrong number of values in scalar component parameter AuxKernels/periodic_dist/point: size 1 is not a multiple of 3\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d: --------------------------------------------------------------------------\nbcs/dmg_periodic.1d: MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD\nbcs/dmg_periodic.1d: with errorcode 1.\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d: NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\nbcs/dmg_periodic.1d: You may or may not see output from other processes, depending on\nbcs/dmg_periodic.1d: exactly when Open MPI kills them.\nbcs/dmg_periodic.1d: --------------------------------------------------------------------------\nbcs/dmg_periodic.1d: srun: Job step aborted: Waiting up to 122 seconds for job step to finish.\nbcs/dmg_periodic.1d: slurmstepd: error: *** STEP 32642259.156 ON cpu-p-628 CANCELLED AT 2020-12-25T22:22:20 ***\nbcs/dmg_periodic.1d: srun: error: cpu-p-629: task 1: Exited with exit code 1\nbcs/dmg_periodic.1d: srun: error: cpu-p-628: task 0: Exited with exit code 1\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d: Exit Code: 0\nbcs/dmg_periodic.1d: ################################################################################\nbcs/dmg_periodic.1d: Tester failed, reason: ERRMSG\nbcs/dmg_periodic.1d:\nbcs/dmg_periodic.1d ............................................................. [min_cpus=2] FAILED (ERRMSG)\n\n\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank ................ [min_cpus=2] FAILED (NO CRASH)\n\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Working Directory: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/misc/exception\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Running command: mpiexec -n 2 /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/moose_test-opt -i parallel_exception_residual_transient.i Kernels/exception/rank=1 Kernels/exception/should_throw=false Outputs/exodus=false --error --error-unused --error-override --no-gdb-backtrace --keep-cout --redirect-output parallel_error_jacobian_transient_non_zero_rank\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Linear solve converged due to CONVERGED_RTOL iterations 2\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: --------------------------------------------------------------------------\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: with errorcode 1.\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: You may or may not see output from other processes, depending on\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: exactly when Open MPI kills them.\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: --------------------------------------------------------------------------\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: srun: Job step aborted: Waiting up to 122 seconds for job step to finish.\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: slurmstepd: error: *** STEP 32642259.158 ON cpu-p-628 CANCELLED AT 2020-12-25T22:22:21 ***\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: srun: error: cpu-p-629: task 1: Segmentation fault\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: srun: error: cpu-p-628: task 0: Killed\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: ################################################################################\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Output from /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/misc/exception/parallel_error_jacobian_transient_non_zero_rank.processor.0\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: ################################################################################\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Framework Information:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: MOOSE Version:           git commit 2f250bf011 on 2020-12-24\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: LibMesh Version:         964118e30ec2da83ed6642171ccd23074e57c2ad\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: PETSc Version:           3.14.2\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: SLEPc Version:           3.14.0\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Current Time:            Fri Dec 25 22:22:21 2020\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Executable Timestamp:    Fri Dec 25 21:18:15 2020\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Parallelism:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Num Processors:          2\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Num Threads:             1\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Mesh:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Parallel Type:           replicated\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Mesh Dimension:          2\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Spatial Dimension:       2\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Nodes:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:     Total:                 72\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:     Local:                 36\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Elems:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:     Total:                 50\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:     Local:                 25\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Num Subdomains:          2\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Num Partitions:          2\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Partitioner:             metis\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Nonlinear System:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Num DOFs:                72\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Num Local DOFs:          36\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Variables:               \"u\"\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Finite Element Types:    \"LAGRANGE\"\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Approximation Orders:    \"FIRST\"\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Execution Information:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Executioner:             Transient\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   TimeStepper:             ConstantDT\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Solver Mode:             Preconditioned JFNK\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Time Step 0, time = 0\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Time Step 1, time = 0.01, dt = 0.01\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:  0 Nonlinear |R| = 7.071068e-01\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:       0 Linear |R| = 7.071068e-01\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:       1 Linear |R| = 1.290385e-03\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:       2 Linear |R| = 4.866985e-06\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:  1 Nonlinear |R| = 4.866755e-06\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:       0 Linear |R| = 4.866755e-06\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: ################################################################################\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Output from /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/misc/exception/parallel_error_jacobian_transient_non_zero_rank.processor.1\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: ################################################################################\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Framework Information:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: MOOSE Version:           git commit 2f250bf011 on 2020-12-24\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: LibMesh Version:         964118e30ec2da83ed6642171ccd23074e57c2ad\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: PETSc Version:           3.14.2\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: SLEPc Version:           3.14.0\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Current Time:            Fri Dec 25 22:22:15 2020\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Executable Timestamp:    Fri Dec 25 21:18:15 2020\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Parallelism:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Num Processors:          2\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Num Threads:             1\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Mesh:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Parallel Type:           replicated\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Mesh Dimension:          2\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Spatial Dimension:       2\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Nodes:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:     Total:                 72\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:     Local:                 36\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Elems:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:     Total:                 50\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:     Local:                 25\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Num Subdomains:          2\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Num Partitions:          2\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Partitioner:             metis\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Nonlinear System:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Num DOFs:                72\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Num Local DOFs:          36\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Variables:               \"u\"\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Finite Element Types:    \"LAGRANGE\"\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Approximation Orders:    \"FIRST\"\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Execution Information:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Executioner:             Transient\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   TimeStepper:             ConstantDT\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:   Solver Mode:             Preconditioned JFNK\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Time Step 0, time = 0\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Time Step 1, time = 0.01, dt = 0.01\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:  0 Nonlinear |R| = 7.071068e-01\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:       0 Linear |R| = 7.071068e-01\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:       1 Linear |R| = 1.290385e-03\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:       2 Linear |R| = 4.866985e-06\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:  1 Nonlinear |R| = 4.866755e-06\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:       0 Linear |R| = 4.866755e-06\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: *** ERROR ***\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Intentional error triggered during residual calculation\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: ################################################################################\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank: Tester failed, reason: NO CRASH\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank ................ [min_cpus=2] FAILED (NO CRASH)\n\n\nvectorpostprocessors/csv_reader.tester_fail ................................... [min_cpus=2] FAILED (NO CRASH)\n\nvectorpostprocessors/csv_reader.tester_fail: Working Directory: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/vectorpostprocessors/csv_reader\nvectorpostprocessors/csv_reader.tester_fail: Running command: mpiexec -n 2 /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/moose_test-opt -i read.i UserObjects/tester/rank=1 UserObjects/tester/gold='1 2 3' Outputs/csv=false --error --error-unused --error-override --no-gdb-backtrace --keep-cout --redirect-output tester_fail\nvectorpostprocessors/csv_reader.tester_fail: --------------------------------------------------------------------------\nvectorpostprocessors/csv_reader.tester_fail: MPI_ABORT was invoked on rank 1 in communicator MPI_COMM_WORLD\nvectorpostprocessors/csv_reader.tester_fail: with errorcode 1.\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\nvectorpostprocessors/csv_reader.tester_fail: You may or may not see output from other processes, depending on\nvectorpostprocessors/csv_reader.tester_fail: exactly when Open MPI kills them.\nvectorpostprocessors/csv_reader.tester_fail: --------------------------------------------------------------------------\nvectorpostprocessors/csv_reader.tester_fail: srun: Job step aborted: Waiting up to 122 seconds for job step to finish.\nvectorpostprocessors/csv_reader.tester_fail: slurmstepd: error: *** STEP 32642259.162 ON cpu-p-628 CANCELLED AT 2020-12-25T22:22:23 ***\nvectorpostprocessors/csv_reader.tester_fail: srun: error: cpu-p-629: task 1: Killed\nvectorpostprocessors/csv_reader.tester_fail: srun: error: cpu-p-628: task 0: Killed\nvectorpostprocessors/csv_reader.tester_fail: ################################################################################\nvectorpostprocessors/csv_reader.tester_fail: Output from /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/vectorpostprocessors/csv_reader/tester_fail.processor.0\nvectorpostprocessors/csv_reader.tester_fail: ################################################################################\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: Framework Information:\nvectorpostprocessors/csv_reader.tester_fail: MOOSE Version:           git commit 2f250bf011 on 2020-12-24\nvectorpostprocessors/csv_reader.tester_fail: LibMesh Version:         964118e30ec2da83ed6642171ccd23074e57c2ad\nvectorpostprocessors/csv_reader.tester_fail: PETSc Version:           3.14.2\nvectorpostprocessors/csv_reader.tester_fail: SLEPc Version:           3.14.0\nvectorpostprocessors/csv_reader.tester_fail: Current Time:            Fri Dec 25 22:22:23 2020\nvectorpostprocessors/csv_reader.tester_fail: Executable Timestamp:    Fri Dec 25 21:18:15 2020\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: Parallelism:\nvectorpostprocessors/csv_reader.tester_fail:   Num Processors:          2\nvectorpostprocessors/csv_reader.tester_fail:   Num Threads:             1\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: Mesh:\nvectorpostprocessors/csv_reader.tester_fail:   Parallel Type:           replicated\nvectorpostprocessors/csv_reader.tester_fail:   Mesh Dimension:          1\nvectorpostprocessors/csv_reader.tester_fail:   Spatial Dimension:       1\nvectorpostprocessors/csv_reader.tester_fail:   Nodes:\nvectorpostprocessors/csv_reader.tester_fail:     Total:                 2\nvectorpostprocessors/csv_reader.tester_fail:     Local:                 2\nvectorpostprocessors/csv_reader.tester_fail:   Elems:\nvectorpostprocessors/csv_reader.tester_fail:     Total:                 1\nvectorpostprocessors/csv_reader.tester_fail:     Local:                 1\nvectorpostprocessors/csv_reader.tester_fail:   Num Subdomains:          1\nvectorpostprocessors/csv_reader.tester_fail:   Num Partitions:          1\nvectorpostprocessors/csv_reader.tester_fail:   Partitioner:             metis\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: Execution Information:\nvectorpostprocessors/csv_reader.tester_fail:   Executioner:             Steady\nvectorpostprocessors/csv_reader.tester_fail:   Solver Mode:             Preconditioned JFNK\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail:  Solve Skipped!\nvectorpostprocessors/csv_reader.tester_fail: ################################################################################\nvectorpostprocessors/csv_reader.tester_fail: Output from /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/vectorpostprocessors/csv_reader/tester_fail.processor.1\nvectorpostprocessors/csv_reader.tester_fail: ################################################################################\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: Framework Information:\nvectorpostprocessors/csv_reader.tester_fail: MOOSE Version:           git commit 2f250bf011 on 2020-12-24\nvectorpostprocessors/csv_reader.tester_fail: LibMesh Version:         964118e30ec2da83ed6642171ccd23074e57c2ad\nvectorpostprocessors/csv_reader.tester_fail: PETSc Version:           3.14.2\nvectorpostprocessors/csv_reader.tester_fail: SLEPc Version:           3.14.0\nvectorpostprocessors/csv_reader.tester_fail: Current Time:            Fri Dec 25 22:22:17 2020\nvectorpostprocessors/csv_reader.tester_fail: Executable Timestamp:    Fri Dec 25 21:18:15 2020\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: Parallelism:\nvectorpostprocessors/csv_reader.tester_fail:   Num Processors:          2\nvectorpostprocessors/csv_reader.tester_fail:   Num Threads:             1\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: Mesh:\nvectorpostprocessors/csv_reader.tester_fail:   Parallel Type:           replicated\nvectorpostprocessors/csv_reader.tester_fail:   Mesh Dimension:          1\nvectorpostprocessors/csv_reader.tester_fail:   Spatial Dimension:       1\nvectorpostprocessors/csv_reader.tester_fail:   Nodes:\nvectorpostprocessors/csv_reader.tester_fail:     Total:                 2\nvectorpostprocessors/csv_reader.tester_fail:     Local:                 0\nvectorpostprocessors/csv_reader.tester_fail:   Elems:\nvectorpostprocessors/csv_reader.tester_fail:     Total:                 1\nvectorpostprocessors/csv_reader.tester_fail:     Local:                 0\nvectorpostprocessors/csv_reader.tester_fail:   Num Subdomains:          1\nvectorpostprocessors/csv_reader.tester_fail:   Num Partitions:          1\nvectorpostprocessors/csv_reader.tester_fail:   Partitioner:             metis\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: Execution Information:\nvectorpostprocessors/csv_reader.tester_fail:   Executioner:             Steady\nvectorpostprocessors/csv_reader.tester_fail:   Solver Mode:             Preconditioned JFNK\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail:  Solve Skipped!\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: *** ERROR ***\nvectorpostprocessors/csv_reader.tester_fail: The supplied gold data does not match the VPP data on the given rank.\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: ################################################################################\nvectorpostprocessors/csv_reader.tester_fail: Tester failed, reason: NO CRASH\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail ................................... [min_cpus=2] FAILED (NO CRASH)\n\n\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch .............. [min_cpus=4] FAILED (EXODIFF)\n\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Working Directory: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/meshgenerators/distributed_rectilinear_mesh_generator\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Running command: mpiexec -n 4 /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/moose_test-opt -i distributed_rectilinear_mesh_generator.i Mesh/gmg/dim=3 Mesh/gmg/nx=20 Mesh/gmg/ny=20 Mesh/gmg/nz=20 Mesh/gmg/part_package=hierarch Mesh/gmg/num_cores_per_compute_node=2 Outputs/file_base=distributed_rectilinear_mesh_generator_out_3d_hierarch Outputs/hide=\"pid npid\"  --error --error-unused --error-override --no-gdb-backtrace\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Framework Information:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: MOOSE Version:           git commit 2f250bf011 on 2020-12-24\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: LibMesh Version:         964118e30ec2da83ed6642171ccd23074e57c2ad\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: PETSc Version:           3.14.2\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: SLEPc Version:           3.14.0\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Current Time:            Fri Dec 25 22:22:39 2020\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Executable Timestamp:    Fri Dec 25 21:18:15 2020\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Parallelism:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Num Processors:          4\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Num Threads:             1\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Mesh:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Parallel Type:           distributed\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Mesh Dimension:          3\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Spatial Dimension:       3\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Nodes:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:     Total:                 9261\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:     Local:                 2556\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Elems:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:     Total:                 8000\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:     Local:                 1998\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Num Subdomains:          1\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Num Partitions:          1\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Partitioner:             parmetis\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Nonlinear System:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Num DOFs:                9261\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Num Local DOFs:          2556\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Variables:               \"u\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Finite Element Types:    \"LAGRANGE\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Approximation Orders:    \"FIRST\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Auxiliary System:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Num DOFs:                17261\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Num Local DOFs:          4554\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Variables:               \"pid\" \"npid\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Finite Element Types:    \"MONOMIAL\" \"LAGRANGE\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Approximation Orders:    \"CONSTANT\" \"FIRST\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Execution Information:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Executioner:             Steady\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Solver Mode:             NEWTON\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   MOOSE Preconditioner:    SMP (auto)\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Warning:  This MeshOutput subclass only supports meshes which have been serialized!\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Warning:  This MeshOutput subclass only supports meshes which have been serialized!\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:  0 Nonlinear |R| = 2.100000e+01\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       0 Linear |R| = 2.100000e+01\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       1 Linear |R| = 1.301958e-02\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       2 Linear |R| = 4.944610e-04\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       3 Linear |R| = 1.000931e-05\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Linear solve converged due to CONVERGED_RTOL iterations 3\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:  1 Nonlinear |R| = 1.000931e-05\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       0 Linear |R| = 1.000931e-05\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       1 Linear |R| = 2.739268e-07\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       2 Linear |R| = 7.530361e-09\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       3 Linear |R| = 1.373364e-10\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       4 Linear |R| = 3.146130e-12\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Linear solve converged due to CONVERGED_RTOL iterations 4\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:  2 Nonlinear |R| = 3.146131e-12\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 2\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:  Solve Converged!\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Framework Information:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: MOOSE Version:           git commit 2f250bf011 on 2020-12-24\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: LibMesh Version:         964118e30ec2da83ed6642171ccd23074e57c2ad\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: PETSc Version:           3.14.2\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: SLEPc Version:           3.14.0\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Current Time:            Fri Dec 25 22:22:39 2020\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Executable Timestamp:    Fri Dec 25 21:18:15 2020\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Parallelism:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Num Processors:          4\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Num Threads:             1\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Mesh:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Parallel Type:           distributed\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Mesh Dimension:          3\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Spatial Dimension:       3\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Nodes:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:     Total:                 9261\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:     Local:                 2556\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Elems:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:     Total:                 8000\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:     Local:                 1998\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Num Subdomains:          1\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Num Partitions:          1\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Partitioner:             parmetis\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Nonlinear System:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Num DOFs:                9261\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Num Local DOFs:          2556\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Variables:               \"u\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Finite Element Types:    \"LAGRANGE\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Approximation Orders:    \"FIRST\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Auxiliary System:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Num DOFs:                17261\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Num Local DOFs:          4554\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Variables:               \"pid\" \"npid\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Finite Element Types:    \"MONOMIAL\" \"LAGRANGE\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Approximation Orders:    \"CONSTANT\" \"FIRST\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Execution Information:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Executioner:             Steady\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Solver Mode:             NEWTON\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   MOOSE Preconditioner:    SMP (auto)\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Warning:  This MeshOutput subclass only supports meshes which have been serialized!\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Warning:  This MeshOutput subclass only supports meshes which have been serialized!\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:  0 Nonlinear |R| = 2.100000e+01\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       0 Linear |R| = 2.100000e+01\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       1 Linear |R| = 1.301958e-02\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       2 Linear |R| = 4.944610e-04\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       3 Linear |R| = 1.000931e-05\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Linear solve converged due to CONVERGED_RTOL iterations 3\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:  1 Nonlinear |R| = 1.000931e-05\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       0 Linear |R| = 1.000931e-05\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       1 Linear |R| = 2.739268e-07\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       2 Linear |R| = 7.530361e-09\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       3 Linear |R| = 1.373364e-10\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:       4 Linear |R| = 3.146130e-12\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   Linear solve converged due to CONVERGED_RTOL iterations 4\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:  2 Nonlinear |R| = 3.146131e-12\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 2\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:  Solve Converged!\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Running exodiff: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/framework/contrib/exodiff/exodiff -m  -F 1e-10 -t 5.5e-06  /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/meshgenerators/distributed_rectilinear_mesh_generator/gold/distributed_rectilinear_mesh_generator_out_3d_hierarch.e /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/meshgenerators/distributed_rectilinear_mesh_generator/distributed_rectilinear_mesh_generator_out_3d_hierarch.e\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: ERROR:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:    *****************************************************************\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:               EXODIFF\t(Version: 2.90) Modified: 2018-02-15\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:               Authors:  Richard Drake, rrdrake@sandia.gov\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:                         Greg Sjaardema, gdsjaar@sandia.gov\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:               Run on    2020/12/25   22:22:41 GMT\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:    *****************************************************************\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Reading first file ...\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Reading second file ...\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   FILE 1: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/meshgenerators/distributed_rectilinear_mesh_generator/gold/distributed_rectilinear_mesh_generator_out_3d_hierarch.e\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:    Title: distributed_rectilinear_mesh_generator_out_3d_hierarch.e\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:           Dim = 3, Blocks = 1, Nodes = 9261, Elements = 8000, Nodesets = 6, Sidesets = 6\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:           Vars: Global = 0, Nodal = 1, Element = 0, Nodeset = 0, Sideset = 0, Times = 2\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   FILE 2: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/meshgenerators/distributed_rectilinear_mesh_generator/distributed_rectilinear_mesh_generator_out_3d_hierarch.e\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:    Title: distributed_rectilinear_mesh_generator_out_3d_hierarch.e\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:           Dim = 3, Blocks = 1, Nodes = 9261, Elements = 8000, Nodesets = 6, Sidesets = 6\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:           Vars: Global = 0, Nodal = 2, Element = 0, Nodeset = 0, Sideset = 0, Times = 2\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: DIFFERENCE .. The nodal variable \"npid\" is in the second file but not the first.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 1 with global id 1 in file1 has the global id 2557 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 2 with global id 2 in file1 has the global id 2558 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 3 with global id 3 in file1 has the global id 2559 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 4 with global id 4 in file1 has the global id 2560 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 5 with global id 5 in file1 has the global id 2561 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 6 with global id 6 in file1 has the global id 2562 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 7 with global id 7 in file1 has the global id 2563 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 8 with global id 8 in file1 has the global id 2564 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 9 with global id 9 in file1 has the global id 2565 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 10 with global id 10 in file1 has the global id 1 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 11 with global id 11 in file1 has the global id 2 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 12 with global id 12 in file1 has the global id 3 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 13 with global id 13 in file1 has the global id 4 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 14 with global id 14 in file1 has the global id 5 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 15 with global id 15 in file1 has the global id 6 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 16 with global id 16 in file1 has the global id 7 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 17 with global id 17 in file1 has the global id 8 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 18 with global id 18 in file1 has the global id 9 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 19 with global id 19 in file1 has the global id 10 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 20 with global id 20 in file1 has the global id 11 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 21 with global id 21 in file1 has the global id 12 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 22 with global id 22 in file1 has the global id 2566 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 23 with global id 23 in file1 has the global id 2567 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 24 with global id 24 in file1 has the global id 2568 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 25 with global id 25 in file1 has the global id 2569 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 26 with global id 26 in file1 has the global id 2570 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 27 with global id 27 in file1 has the global id 2571 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 28 with global id 28 in file1 has the global id 2572 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 29 with global id 29 in file1 has the global id 2573 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 30 with global id 30 in file1 has the global id 2574 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 31 with global id 31 in file1 has the global id 13 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 32 with global id 32 in file1 has the global id 14 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 33 with global id 33 in file1 has the global id 15 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 34 with global id 34 in file1 has the global id 16 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 35 with global id 35 in file1 has the global id 17 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 36 with global id 36 in file1 has the global id 18 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 37 with global id 37 in file1 has the global id 19 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 38 with global id 38 in file1 has the global id 20 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 39 with global id 39 in file1 has the global id 21 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 40 with global id 40 in file1 has the global id 22 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 41 with global id 41 in file1 has the global id 23 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 42 with global id 42 in file1 has the global id 24 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 43 with global id 43 in file1 has the global id 2575 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 44 with global id 44 in file1 has the global id 2576 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 45 with global id 45 in file1 has the global id 2577 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 46 with global id 46 in file1 has the global id 2578 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 47 with global id 47 in file1 has the global id 2579 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 48 with global id 48 in file1 has the global id 2580 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 49 with global id 49 in file1 has the global id 2581 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 50 with global id 50 in file1 has the global id 2582 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 51 with global id 51 in file1 has the global id 2583 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 52 with global id 52 in file1 has the global id 25 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 53 with global id 53 in file1 has the global id 26 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 54 with global id 54 in file1 has the global id 27 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 55 with global id 55 in file1 has the global id 28 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 56 with global id 56 in file1 has the global id 29 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 57 with global id 57 in file1 has the global id 30 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 58 with global id 58 in file1 has the global id 31 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 59 with global id 59 in file1 has the global id 32 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 60 with global id 60 in file1 has the global id 33 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 61 with global id 61 in file1 has the global id 34 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 62 with global id 62 in file1 has the global id 35 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 63 with global id 63 in file1 has the global id 36 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 64 with global id 64 in file1 has the global id 2584 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 65 with global id 65 in file1 has the global id 2585 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 66 with global id 66 in file1 has the global id 2586 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 67 with global id 67 in file1 has the global id 2587 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 68 with global id 68 in file1 has the global id 2588 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 69 with global id 69 in file1 has the global id 2589 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 70 with global id 70 in file1 has the global id 2590 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 71 with global id 71 in file1 has the global id 2591 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 72 with global id 72 in file1 has the global id 2592 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 73 with global id 73 in file1 has the global id 37 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 74 with global id 74 in file1 has the global id 38 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 75 with global id 75 in file1 has the global id 39 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 76 with global id 76 in file1 has the global id 40 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 77 with global id 77 in file1 has the global id 41 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 78 with global id 78 in file1 has the global id 42 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 79 with global id 79 in file1 has the global id 43 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 80 with global id 80 in file1 has the global id 44 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 81 with global id 81 in file1 has the global id 45 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 82 with global id 82 in file1 has the global id 46 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 83 with global id 83 in file1 has the global id 47 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 84 with global id 84 in file1 has the global id 48 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 85 with global id 85 in file1 has the global id 2593 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 86 with global id 86 in file1 has the global id 2594 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 87 with global id 87 in file1 has the global id 2595 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 88 with global id 88 in file1 has the global id 2596 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 89 with global id 89 in file1 has the global id 2597 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 90 with global id 90 in file1 has the global id 2598 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 91 with global id 91 in file1 has the global id 2599 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 92 with global id 92 in file1 has the global id 2600 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 93 with global id 93 in file1 has the global id 2601 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 94 with global id 94 in file1 has the global id 49 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 95 with global id 95 in file1 has the global id 50 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 96 with global id 96 in file1 has the global id 51 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 97 with global id 97 in file1 has the global id 52 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 98 with global id 98 in file1 has the global id 53 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 99 with global id 99 in file1 has the global id 54 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 100 with global id 100 in file1 has the global id 55 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local node 101 with global id 101 in file1 has the global id 56 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. Too many warnings, skipping remainder...\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 1 with global id 1 in file1 has the global id 1999 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 2 with global id 2 in file1 has the global id 2000 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 3 with global id 3 in file1 has the global id 2001 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 4 with global id 4 in file1 has the global id 2002 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 5 with global id 5 in file1 has the global id 2003 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 6 with global id 6 in file1 has the global id 2004 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 7 with global id 7 in file1 has the global id 2005 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 8 with global id 8 in file1 has the global id 2006 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 9 with global id 9 in file1 has the global id 2007 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 10 with global id 10 in file1 has the global id 1 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 11 with global id 11 in file1 has the global id 2 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 12 with global id 12 in file1 has the global id 3 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 13 with global id 13 in file1 has the global id 4 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 14 with global id 14 in file1 has the global id 5 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 15 with global id 15 in file1 has the global id 6 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 16 with global id 16 in file1 has the global id 7 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 17 with global id 17 in file1 has the global id 8 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 18 with global id 18 in file1 has the global id 9 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 19 with global id 19 in file1 has the global id 10 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 20 with global id 20 in file1 has the global id 11 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 21 with global id 21 in file1 has the global id 2008 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 22 with global id 22 in file1 has the global id 2009 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 23 with global id 23 in file1 has the global id 2010 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 24 with global id 24 in file1 has the global id 2011 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 25 with global id 25 in file1 has the global id 2012 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 26 with global id 26 in file1 has the global id 2013 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 27 with global id 27 in file1 has the global id 2014 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 28 with global id 28 in file1 has the global id 2015 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 29 with global id 29 in file1 has the global id 2016 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 30 with global id 30 in file1 has the global id 12 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 31 with global id 31 in file1 has the global id 13 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 32 with global id 32 in file1 has the global id 14 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 33 with global id 33 in file1 has the global id 15 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 34 with global id 34 in file1 has the global id 16 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 35 with global id 35 in file1 has the global id 17 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 36 with global id 36 in file1 has the global id 18 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 37 with global id 37 in file1 has the global id 19 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 38 with global id 38 in file1 has the global id 20 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 39 with global id 39 in file1 has the global id 21 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 40 with global id 40 in file1 has the global id 22 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 41 with global id 41 in file1 has the global id 2017 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 42 with global id 42 in file1 has the global id 2018 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 43 with global id 43 in file1 has the global id 2019 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 44 with global id 44 in file1 has the global id 2020 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 45 with global id 45 in file1 has the global id 2021 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 46 with global id 46 in file1 has the global id 2022 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 47 with global id 47 in file1 has the global id 2023 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 48 with global id 48 in file1 has the global id 2024 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 49 with global id 49 in file1 has the global id 2025 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 50 with global id 50 in file1 has the global id 23 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 51 with global id 51 in file1 has the global id 24 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 52 with global id 52 in file1 has the global id 25 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 53 with global id 53 in file1 has the global id 26 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 54 with global id 54 in file1 has the global id 27 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 55 with global id 55 in file1 has the global id 28 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 56 with global id 56 in file1 has the global id 29 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 57 with global id 57 in file1 has the global id 30 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 58 with global id 58 in file1 has the global id 31 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 59 with global id 59 in file1 has the global id 32 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 60 with global id 60 in file1 has the global id 33 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 61 with global id 61 in file1 has the global id 2026 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 62 with global id 62 in file1 has the global id 2027 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 63 with global id 63 in file1 has the global id 2028 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 64 with global id 64 in file1 has the global id 2029 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 65 with global id 65 in file1 has the global id 2030 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 66 with global id 66 in file1 has the global id 2031 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 67 with global id 67 in file1 has the global id 2032 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 68 with global id 68 in file1 has the global id 2033 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 69 with global id 69 in file1 has the global id 2034 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 70 with global id 70 in file1 has the global id 34 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 71 with global id 71 in file1 has the global id 35 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 72 with global id 72 in file1 has the global id 36 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 73 with global id 73 in file1 has the global id 37 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 74 with global id 74 in file1 has the global id 38 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 75 with global id 75 in file1 has the global id 39 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 76 with global id 76 in file1 has the global id 40 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 77 with global id 77 in file1 has the global id 41 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 78 with global id 78 in file1 has the global id 42 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 79 with global id 79 in file1 has the global id 43 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 80 with global id 80 in file1 has the global id 44 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 81 with global id 81 in file1 has the global id 2035 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 82 with global id 82 in file1 has the global id 2036 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 83 with global id 83 in file1 has the global id 2037 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 84 with global id 84 in file1 has the global id 2038 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 85 with global id 85 in file1 has the global id 2039 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 86 with global id 86 in file1 has the global id 2040 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 87 with global id 87 in file1 has the global id 2041 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 88 with global id 88 in file1 has the global id 2042 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 89 with global id 89 in file1 has the global id 2043 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 90 with global id 90 in file1 has the global id 45 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 91 with global id 91 in file1 has the global id 46 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 92 with global id 92 in file1 has the global id 47 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 93 with global id 93 in file1 has the global id 48 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 94 with global id 94 in file1 has the global id 49 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 95 with global id 95 in file1 has the global id 50 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 96 with global id 96 in file1 has the global id 51 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 97 with global id 97 in file1 has the global id 52 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 98 with global id 98 in file1 has the global id 53 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 99 with global id 99 in file1 has the global id 54 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 100 with global id 100 in file1 has the global id 55 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. The local element 101 with global id 101 in file1 has the global id 2044 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: WARNING .. Too many warnings, skipping remainder...\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Nodal coordinates will be compared .. tol:    1e-06 (absolute), floor:        0\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Time step values will be compared .. tol:  5.5e-06 (relative), floor:    1e-10\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: No Global variables on either file.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Nodal variables to be compared:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: \tu                                tol:  5.5e-06 (relative), floor:    1e-10\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: No Element variables on either file.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: No Element Attribute variables on either file.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: No Nodeset variables on either file.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: No Sideset variables on either file.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Sideset Distribution Factors will be compared .. tol:    1e-06 (relative), floor:        0\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   ==============================================================\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   NOTE: All node and element ids are reported as global ids.\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Sideset Distribution Factors:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   --------- Time step 1, 0.0000000e+00 ~ 0.0000000e+00, rel diff:  0.00000e+00 ---------\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Nodal variables:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:   --------- Time step 2, 1.0000000e+00 ~ 1.0000000e+00, rel diff:  0.00000e+00 ---------\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Nodal variables:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: exodiff: Files are different\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: ################################################################################\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch: Tester failed, reason: EXODIFF\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch:\nmeshgenerators/distributed_rectilinear_mesh_generator.3D_hierarch .............. [min_cpus=4] FAILED (EXODIFF)\n\n\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out ............. [min_cpus=3] FAILED (EXODIFF)\n\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Working Directory: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/meshgenerators/distributed_rectilinear_mesh_generator\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Running command: mpiexec -n 3 /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/moose_test-opt -i distributed_rectilinear_mesh_generator.i Mesh/gmg/num_cores_for_partition=2 Mesh/gmg/dim=3 Mesh/gmg/nx=20 Mesh/gmg/ny=20 Mesh/gmg/nz=20 Outputs/file_base=drmg_3d_scomm_out Outputs/hide=\"pid npid\"  --error --error-unused --error-override --no-gdb-backtrace\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Framework Information:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: MOOSE Version:           git commit 2f250bf011 on 2020-12-24\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: LibMesh Version:         964118e30ec2da83ed6642171ccd23074e57c2ad\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: PETSc Version:           3.14.2\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: SLEPc Version:           3.14.0\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Current Time:            Fri Dec 25 22:22:47 2020\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Executable Timestamp:    Fri Dec 25 21:18:15 2020\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Parallelism:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Num Processors:          3\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Num Threads:             1\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Mesh:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Parallel Type:           distributed\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Mesh Dimension:          3\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Spatial Dimension:       3\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Nodes:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:     Total:                 9261\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:     Local:                 3406\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Elems:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:     Total:                 8000\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:     Local:                 2677\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Num Subdomains:          1\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Num Partitions:          1\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Partitioner:             parmetis\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Nonlinear System:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Num DOFs:                9261\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Num Local DOFs:          3406\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Variables:               \"u\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Finite Element Types:    \"LAGRANGE\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Approximation Orders:    \"FIRST\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Auxiliary System:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Num DOFs:                17261\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Num Local DOFs:          6083\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Variables:               \"pid\" \"npid\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Finite Element Types:    \"MONOMIAL\" \"LAGRANGE\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Approximation Orders:    \"CONSTANT\" \"FIRST\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Execution Information:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Executioner:             Steady\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Solver Mode:             NEWTON\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   MOOSE Preconditioner:    SMP (auto)\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Warning:  This MeshOutput subclass only supports meshes which have been serialized!\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Warning:  This MeshOutput subclass only supports meshes which have been serialized!\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:  0 Nonlinear |R| = 2.100000e+01\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       0 Linear |R| = 2.100000e+01\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       1 Linear |R| = 1.256983e-02\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       2 Linear |R| = 3.233320e-04\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       3 Linear |R| = 1.207826e-05\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Linear solve converged due to CONVERGED_RTOL iterations 3\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:  1 Nonlinear |R| = 1.207826e-05\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       0 Linear |R| = 1.207826e-05\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       1 Linear |R| = 4.345058e-07\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       2 Linear |R| = 9.621480e-09\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       3 Linear |R| = 1.565968e-10\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       4 Linear |R| = 4.325451e-12\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Linear solve converged due to CONVERGED_RTOL iterations 4\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:  2 Nonlinear |R| = 4.325443e-12\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 2\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:  Solve Converged!\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Framework Information:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: MOOSE Version:           git commit 2f250bf011 on 2020-12-24\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: LibMesh Version:         964118e30ec2da83ed6642171ccd23074e57c2ad\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: PETSc Version:           3.14.2\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: SLEPc Version:           3.14.0\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Current Time:            Fri Dec 25 22:22:47 2020\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Executable Timestamp:    Fri Dec 25 21:18:15 2020\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Parallelism:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Num Processors:          3\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Num Threads:             1\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Mesh:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Parallel Type:           distributed\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Mesh Dimension:          3\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Spatial Dimension:       3\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Nodes:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:     Total:                 9261\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:     Local:                 3406\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Elems:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:     Total:                 8000\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:     Local:                 2677\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Num Subdomains:          1\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Num Partitions:          1\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Partitioner:             parmetis\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Nonlinear System:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Num DOFs:                9261\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Num Local DOFs:          3406\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Variables:               \"u\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Finite Element Types:    \"LAGRANGE\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Approximation Orders:    \"FIRST\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Auxiliary System:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Num DOFs:                17261\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Num Local DOFs:          6083\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Variables:               \"pid\" \"npid\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Finite Element Types:    \"MONOMIAL\" \"LAGRANGE\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Approximation Orders:    \"CONSTANT\" \"FIRST\"\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Execution Information:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Executioner:             Steady\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Solver Mode:             NEWTON\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   MOOSE Preconditioner:    SMP (auto)\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Warning:  This MeshOutput subclass only supports meshes which have been serialized!\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Warning:  This MeshOutput subclass only supports meshes which have been serialized!\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:  0 Nonlinear |R| = 2.100000e+01\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       0 Linear |R| = 2.100000e+01\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       1 Linear |R| = 1.256983e-02\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       2 Linear |R| = 3.233320e-04\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       3 Linear |R| = 1.207826e-05\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Linear solve converged due to CONVERGED_RTOL iterations 3\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:  1 Nonlinear |R| = 1.207826e-05\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       0 Linear |R| = 1.207826e-05\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       1 Linear |R| = 4.345058e-07\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       2 Linear |R| = 9.621480e-09\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       3 Linear |R| = 1.565968e-10\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:       4 Linear |R| = 4.325451e-12\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   Linear solve converged due to CONVERGED_RTOL iterations 4\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:  2 Nonlinear |R| = 4.325443e-12\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Nonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 2\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:  Solve Converged!\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Running exodiff: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/framework/contrib/exodiff/exodiff -m  -F 1e-10 -t 5.5e-06  /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/meshgenerators/distributed_rectilinear_mesh_generator/gold/drmg_3d_scomm_out.e /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/meshgenerators/distributed_rectilinear_mesh_generator/drmg_3d_scomm_out.e\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: ERROR:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:    *****************************************************************\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:               EXODIFF\t(Version: 2.90) Modified: 2018-02-15\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:               Authors:  Richard Drake, rrdrake@sandia.gov\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:                         Greg Sjaardema, gdsjaar@sandia.gov\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:               Run on    2020/12/25   22:22:47 GMT\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:    *****************************************************************\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Reading first file ...\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Reading second file ...\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   FILE 1: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/meshgenerators/distributed_rectilinear_mesh_generator/gold/drmg_3d_scomm_out.e\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:    Title: drmg_3d_scomm_out.e\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:           Dim = 3, Blocks = 1, Nodes = 9261, Elements = 8000, Nodesets = 6, Sidesets = 6\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:           Vars: Global = 0, Nodal = 1, Element = 0, Nodeset = 0, Sideset = 0, Times = 2\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   FILE 2: /home/ir-inca1/projects/building_moose_with_dinosaurs/moose/test/tests/meshgenerators/distributed_rectilinear_mesh_generator/drmg_3d_scomm_out.e\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:    Title: drmg_3d_scomm_out.e\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:           Dim = 3, Blocks = 1, Nodes = 9261, Elements = 8000, Nodesets = 6, Sidesets = 6\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:           Vars: Global = 0, Nodal = 2, Element = 0, Nodeset = 0, Sideset = 0, Times = 2\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: DIFFERENCE .. The nodal variable \"npid\" is in the second file but not the first.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 1 with global id 1 in file1 has the global id 6517 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 2 with global id 2 in file1 has the global id 6518 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 3 with global id 3 in file1 has the global id 6519 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 4 with global id 4 in file1 has the global id 6520 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 5 with global id 5 in file1 has the global id 6521 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 6 with global id 6 in file1 has the global id 6522 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 7 with global id 7 in file1 has the global id 6523 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 8 with global id 8 in file1 has the global id 6524 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 9 with global id 9 in file1 has the global id 6525 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 10 with global id 10 in file1 has the global id 6526 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 11 with global id 11 in file1 has the global id 6527 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 12 with global id 12 in file1 has the global id 6528 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 13 with global id 13 in file1 has the global id 6529 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 14 with global id 14 in file1 has the global id 1 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 15 with global id 15 in file1 has the global id 2 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 16 with global id 16 in file1 has the global id 3 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 17 with global id 17 in file1 has the global id 4 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 18 with global id 18 in file1 has the global id 5 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 19 with global id 19 in file1 has the global id 6 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 20 with global id 20 in file1 has the global id 7 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 21 with global id 21 in file1 has the global id 8 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 22 with global id 22 in file1 has the global id 6530 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 23 with global id 23 in file1 has the global id 6531 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 24 with global id 24 in file1 has the global id 6532 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 25 with global id 25 in file1 has the global id 6533 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 26 with global id 26 in file1 has the global id 6534 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 27 with global id 27 in file1 has the global id 6535 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 28 with global id 28 in file1 has the global id 6536 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 29 with global id 29 in file1 has the global id 6537 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 30 with global id 30 in file1 has the global id 6538 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 31 with global id 31 in file1 has the global id 6539 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 32 with global id 32 in file1 has the global id 6540 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 33 with global id 33 in file1 has the global id 6541 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 34 with global id 34 in file1 has the global id 6542 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 35 with global id 35 in file1 has the global id 9 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 36 with global id 36 in file1 has the global id 10 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 37 with global id 37 in file1 has the global id 11 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 38 with global id 38 in file1 has the global id 12 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 39 with global id 39 in file1 has the global id 13 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 40 with global id 40 in file1 has the global id 14 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 41 with global id 41 in file1 has the global id 15 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 42 with global id 42 in file1 has the global id 16 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 43 with global id 43 in file1 has the global id 6543 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 44 with global id 44 in file1 has the global id 6544 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 45 with global id 45 in file1 has the global id 6545 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 46 with global id 46 in file1 has the global id 6546 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 47 with global id 47 in file1 has the global id 6547 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 48 with global id 48 in file1 has the global id 6548 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 49 with global id 49 in file1 has the global id 6549 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 50 with global id 50 in file1 has the global id 6550 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 51 with global id 51 in file1 has the global id 6551 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 52 with global id 52 in file1 has the global id 6552 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 53 with global id 53 in file1 has the global id 6553 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 54 with global id 54 in file1 has the global id 6554 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 55 with global id 55 in file1 has the global id 6555 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 56 with global id 56 in file1 has the global id 17 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 57 with global id 57 in file1 has the global id 18 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 58 with global id 58 in file1 has the global id 19 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 59 with global id 59 in file1 has the global id 20 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 60 with global id 60 in file1 has the global id 21 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 61 with global id 61 in file1 has the global id 22 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 62 with global id 62 in file1 has the global id 23 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 63 with global id 63 in file1 has the global id 24 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 64 with global id 64 in file1 has the global id 6556 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 65 with global id 65 in file1 has the global id 6557 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 66 with global id 66 in file1 has the global id 6558 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 67 with global id 67 in file1 has the global id 6559 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 68 with global id 68 in file1 has the global id 6560 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 69 with global id 69 in file1 has the global id 6561 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 70 with global id 70 in file1 has the global id 6562 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 71 with global id 71 in file1 has the global id 6563 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 72 with global id 72 in file1 has the global id 6564 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 73 with global id 73 in file1 has the global id 6565 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 74 with global id 74 in file1 has the global id 6566 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 75 with global id 75 in file1 has the global id 6567 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 76 with global id 76 in file1 has the global id 6568 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 77 with global id 77 in file1 has the global id 25 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 78 with global id 78 in file1 has the global id 26 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 79 with global id 79 in file1 has the global id 27 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 80 with global id 80 in file1 has the global id 28 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 81 with global id 81 in file1 has the global id 29 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 82 with global id 82 in file1 has the global id 30 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 83 with global id 83 in file1 has the global id 31 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 84 with global id 84 in file1 has the global id 32 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 85 with global id 85 in file1 has the global id 6569 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 86 with global id 86 in file1 has the global id 6570 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 87 with global id 87 in file1 has the global id 6571 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 88 with global id 88 in file1 has the global id 6572 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 89 with global id 89 in file1 has the global id 6573 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 90 with global id 90 in file1 has the global id 6574 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 91 with global id 91 in file1 has the global id 6575 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 92 with global id 92 in file1 has the global id 6576 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 93 with global id 93 in file1 has the global id 6577 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 94 with global id 94 in file1 has the global id 6578 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 95 with global id 95 in file1 has the global id 6579 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 96 with global id 96 in file1 has the global id 6580 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 97 with global id 97 in file1 has the global id 6581 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 98 with global id 98 in file1 has the global id 33 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 99 with global id 99 in file1 has the global id 34 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 100 with global id 100 in file1 has the global id 35 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local node 101 with global id 101 in file1 has the global id 36 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. Too many warnings, skipping remainder...\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 1 with global id 1 in file1 has the global id 5361 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 2 with global id 2 in file1 has the global id 5362 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 3 with global id 3 in file1 has the global id 5363 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 4 with global id 4 in file1 has the global id 5364 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 5 with global id 5 in file1 has the global id 5365 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 6 with global id 6 in file1 has the global id 5366 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 7 with global id 7 in file1 has the global id 5367 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 8 with global id 8 in file1 has the global id 5368 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 9 with global id 9 in file1 has the global id 5369 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 10 with global id 10 in file1 has the global id 5370 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 11 with global id 11 in file1 has the global id 5371 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 12 with global id 12 in file1 has the global id 5372 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 13 with global id 13 in file1 has the global id 5373 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 14 with global id 14 in file1 has the global id 1 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 15 with global id 15 in file1 has the global id 2 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 16 with global id 16 in file1 has the global id 3 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 17 with global id 17 in file1 has the global id 4 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 18 with global id 18 in file1 has the global id 5 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 19 with global id 19 in file1 has the global id 6 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 20 with global id 20 in file1 has the global id 7 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 21 with global id 21 in file1 has the global id 5374 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 22 with global id 22 in file1 has the global id 5375 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 23 with global id 23 in file1 has the global id 5376 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 24 with global id 24 in file1 has the global id 5377 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 25 with global id 25 in file1 has the global id 5378 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 26 with global id 26 in file1 has the global id 5379 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 27 with global id 27 in file1 has the global id 5380 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 28 with global id 28 in file1 has the global id 5381 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 29 with global id 29 in file1 has the global id 5382 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 30 with global id 30 in file1 has the global id 5383 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 31 with global id 31 in file1 has the global id 5384 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 32 with global id 32 in file1 has the global id 5385 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 33 with global id 33 in file1 has the global id 5386 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 34 with global id 34 in file1 has the global id 8 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 35 with global id 35 in file1 has the global id 9 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 36 with global id 36 in file1 has the global id 10 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 37 with global id 37 in file1 has the global id 11 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 38 with global id 38 in file1 has the global id 12 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 39 with global id 39 in file1 has the global id 13 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 40 with global id 40 in file1 has the global id 14 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 41 with global id 41 in file1 has the global id 5387 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 42 with global id 42 in file1 has the global id 5388 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 43 with global id 43 in file1 has the global id 5389 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 44 with global id 44 in file1 has the global id 5390 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 45 with global id 45 in file1 has the global id 5391 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 46 with global id 46 in file1 has the global id 5392 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 47 with global id 47 in file1 has the global id 5393 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 48 with global id 48 in file1 has the global id 5394 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 49 with global id 49 in file1 has the global id 5395 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 50 with global id 50 in file1 has the global id 5396 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 51 with global id 51 in file1 has the global id 5397 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 52 with global id 52 in file1 has the global id 5398 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 53 with global id 53 in file1 has the global id 5399 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 54 with global id 54 in file1 has the global id 15 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 55 with global id 55 in file1 has the global id 16 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 56 with global id 56 in file1 has the global id 17 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 57 with global id 57 in file1 has the global id 18 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 58 with global id 58 in file1 has the global id 19 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 59 with global id 59 in file1 has the global id 20 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 60 with global id 60 in file1 has the global id 21 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 61 with global id 61 in file1 has the global id 5400 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 62 with global id 62 in file1 has the global id 5401 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 63 with global id 63 in file1 has the global id 5402 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 64 with global id 64 in file1 has the global id 5403 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 65 with global id 65 in file1 has the global id 5404 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 66 with global id 66 in file1 has the global id 5405 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 67 with global id 67 in file1 has the global id 5406 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 68 with global id 68 in file1 has the global id 5407 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 69 with global id 69 in file1 has the global id 5408 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 70 with global id 70 in file1 has the global id 5409 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 71 with global id 71 in file1 has the global id 5410 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 72 with global id 72 in file1 has the global id 5411 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 73 with global id 73 in file1 has the global id 5412 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 74 with global id 74 in file1 has the global id 22 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 75 with global id 75 in file1 has the global id 23 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 76 with global id 76 in file1 has the global id 24 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 77 with global id 77 in file1 has the global id 25 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 78 with global id 78 in file1 has the global id 26 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 79 with global id 79 in file1 has the global id 27 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 80 with global id 80 in file1 has the global id 28 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 81 with global id 81 in file1 has the global id 5413 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 82 with global id 82 in file1 has the global id 5414 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 83 with global id 83 in file1 has the global id 5415 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 84 with global id 84 in file1 has the global id 5416 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 85 with global id 85 in file1 has the global id 5417 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 86 with global id 86 in file1 has the global id 5418 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 87 with global id 87 in file1 has the global id 5419 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 88 with global id 88 in file1 has the global id 5420 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 89 with global id 89 in file1 has the global id 5421 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 90 with global id 90 in file1 has the global id 5422 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 91 with global id 91 in file1 has the global id 5423 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 92 with global id 92 in file1 has the global id 5424 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 93 with global id 93 in file1 has the global id 5425 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 94 with global id 94 in file1 has the global id 29 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 95 with global id 95 in file1 has the global id 30 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 96 with global id 96 in file1 has the global id 31 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 97 with global id 97 in file1 has the global id 32 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 98 with global id 98 in file1 has the global id 33 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 99 with global id 99 in file1 has the global id 34 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 100 with global id 100 in file1 has the global id 35 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. The local element 101 with global id 101 in file1 has the global id 5426 in file2.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: WARNING .. Too many warnings, skipping remainder...\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Nodal coordinates will be compared .. tol:    1e-06 (absolute), floor:        0\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Time step values will be compared .. tol:  5.5e-06 (relative), floor:    1e-10\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: No Global variables on either file.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Nodal variables to be compared:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: \tu                                tol:  5.5e-06 (relative), floor:    1e-10\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: No Element variables on either file.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: No Element Attribute variables on either file.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: No Nodeset variables on either file.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: No Sideset variables on either file.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Sideset Distribution Factors will be compared .. tol:    1e-06 (relative), floor:        0\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   ==============================================================\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   NOTE: All node and element ids are reported as global ids.\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Sideset Distribution Factors:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   --------- Time step 1, 0.0000000e+00 ~ 0.0000000e+00, rel diff:  0.00000e+00 ---------\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Nodal variables:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:   --------- Time step 2, 1.0000000e+00 ~ 1.0000000e+00, rel diff:  0.00000e+00 ---------\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Nodal variables:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: exodiff: Files are different\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: ################################################################################\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out: Tester failed, reason: EXODIFF\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out:\nmeshgenerators/distributed_rectilinear_mesh_generator.3d_scomm_out ............. [min_cpus=3] FAILED (EXODIFF)",
                          "url": "https://github.com/idaholab/moose/discussions/16546#discussioncomment-241537",
                          "updatedAt": "2020-12-25T23:07:54Z",
                          "publishedAt": "2020-12-25T17:53:05Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "MOOSE Team on Vacation!",
          "author": {
            "login": "aeslaughter"
          },
          "bodyText": "The MOOSE team and other developers from INL will be on vacation until Jan. 4, 2021. Consequently, our support will be limited during this time. Thanks for all the great questions and contributions in 2020, we look forward to another productive year in 2021. Please take some time to unplug and re-charge your brain meat.\nPlease feel free to share your success from 2020 with MOOSE here.",
          "url": "https://github.com/idaholab/moose/discussions/16587",
          "updatedAt": "2021-02-12T03:30:43Z",
          "publishedAt": "2020-12-24T19:24:19Z",
          "category": {
            "name": "News"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "@lindsayad Is the winner, most commits in the past year!",
                  "url": "https://github.com/idaholab/moose/discussions/16587#discussioncomment-240560",
                  "updatedAt": "2020-12-24T19:49:26Z",
                  "publishedAt": "2020-12-24T19:49:15Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "You win for LOC :-)",
                          "url": "https://github.com/idaholab/moose/discussions/16587#discussioncomment-240598",
                          "updatedAt": "2020-12-24T20:49:01Z",
                          "publishedAt": "2020-12-24T20:49:01Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Usage of lagrange finite elements in parallel execution",
          "author": {
            "login": "abarun22"
          },
          "bodyText": "Dear all,\nI wanted to use Lagrange family of finite elements for one my MOOSE parallel computation which requires certain nodal variables to be calculated along a specified boundary. I see that currently parallel computation is not possible with Lagrange family of elements and that the nodal based computation does strictly requires its presence. Here comes the error i received earlier.\n*** ERROR ***\nThe nodal patch recovery option, which calculates the Zienkiewicz-Zhu patch recovery for nodal variables (family = LAGRANGE), is not currently implemented for parallel runs. Run in serial if you must use the nodal patch capability\nMy future work on MOOSE is performance based and so i cannot afford to run in a serial mode. For your reference i am attaching here the input file i am using to run the problem. I would welcome any suggestions/idea on how to accomplish a parallel computation with Lagrange elements and nodal patch capabilities.\nKind regards,\nArun\nPieceWise_test.txt",
          "url": "https://github.com/idaholab/moose/discussions/16486",
          "updatedAt": "2022-06-28T11:36:33Z",
          "publishedAt": "2020-12-14T14:15:24Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "jiangwen84"
                  },
                  "bodyText": "First, nodal patch recovery has not been implemented for parallel yetl, and I am not sure whether it will be done very soon.\nSecondly, do you really need or want to use nodal patch recovery? Looks like you want to evaluate stress on a specific boundary. For this purpose, I suggest you consider evaluate and/or integrate stress using the quadratures on the boundary. That would be most accurate way.",
                  "url": "https://github.com/idaholab/moose/discussions/16486#discussioncomment-205356",
                  "updatedAt": "2022-06-28T11:36:36Z",
                  "publishedAt": "2020-12-14T16:00:51Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "abarun22"
                  },
                  "bodyText": "Hi Wen,\nThanks for the response. Well, my intention is to extract the mean\nstresses/strains from a selected boundary, irrespective of how it is\ncomputed i.e., through nodal or element based. I see that\n'ElementAverageValue' does not include a boundary identifier to facilitate\nthis sort of selective calculation. Broadly speaking i am interested in\ncreating a so called 'path' engulfing the boundary which is subjected to\nnecking and measure the stresses and strains (as the loading progresses)\nalong this reducing area. I would be glad to hear from you as how i can\noutput these data, possibly with the quadrature rules you mentioned, so\nthat i can plot the true stress/strain results .\nKind regards,\nArun\n\u2026\nOn Mon, Dec 14, 2020 at 4:01 PM Wen Jiang ***@***.***> wrote:\n First, nodal patch recovery has not been implemented for parallel yetl,\n and I am not sure whether it will be done very soon.\n\n Secondly, do you really need or want to use nodal patch recovery? Looks\n like you want to evaluate stress on a specific boundary. For this purpose,\n I suggest you consider evaluate and/or integrate stress using the\n quadratures on the boundary. That would be most accurate way.\n\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n <#16486 (comment)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/AJSA25ZUTJ34WR73IZRRH3TSUYZEHANCNFSM4U2ZZTBA>\n .",
                  "url": "https://github.com/idaholab/moose/discussions/16486#discussioncomment-205530",
                  "updatedAt": "2022-06-28T11:36:36Z",
                  "publishedAt": "2020-12-14T16:43:30Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "I would compute those materials on the boundary, and you can find some boundary material tests under moose/test/tests/materials/boundary_material\nBut you need to implement something like ElementIntegralMaterialProperty, but for side (boundary), not block. You can follow    SideIntegralVariablePostprocessor to implement that.",
                          "url": "https://github.com/idaholab/moose/discussions/16486#discussioncomment-206052",
                          "updatedAt": "2022-06-28T11:36:36Z",
                          "publishedAt": "2020-12-14T19:18:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Hi Wen,\nThanks for the response. Indeed my problem of stress dis-agreement at large strains has been solved by the inclusion of 'volumetric locking correction' in to the numerical scheme of things and so i may not be in an urgent need to implement  'ElementIntegralMaterialProperty' to be able to apply along the boundaries for stress calculation. However another advantage of using a nodal patch recovery is to avoid the rendering problems associated with the element based calculations. The attached file shows the difference between the images rendered by peacock for a strain contour done with 'ElementAverageValue' and 'AverageNodalVariableValue'. The later one provide dual benefits of selective calculation of field variables and perfect rendering of pretty pictures, but with a penalty in the form of sequential run and the associated slowness. The problem which takes just over half an hour might take 10-15 days to compute the whole solution, which is never going to be a preferred option. I would like to know if there are any other methods to obtain a smooth contour without having to use the time consuming nodal methods?\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/16486#discussioncomment-235116",
                          "updatedAt": "2022-06-28T11:36:36Z",
                          "publishedAt": "2020-12-22T17:45:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GregVernon"
                          },
                          "bodyText": "If you're just concerned with rendering pretty pictures, you could use ParaView's filter: Cell Data to Point Data :",
                          "url": "https://github.com/idaholab/moose/discussions/16486#discussioncomment-238166",
                          "updatedAt": "2022-06-28T11:36:37Z",
                          "publishedAt": "2020-12-23T19:26:05Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "jiangwen84"
                  },
                  "bodyText": "In addition to @GregVernon 's solution, you can also use FIRST order monomial for element variable. MOOSE/libmesh uses least square fitting to get the value at the nodes.\n[stress_xx]\norder = FIRST\ntype = MONOMIAL\n[]",
                  "url": "https://github.com/idaholab/moose/discussions/16486#discussioncomment-238174",
                  "updatedAt": "2022-06-28T11:36:38Z",
                  "publishedAt": "2020-12-23T19:29:01Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "abarun22"
                  },
                  "bodyText": "Thanks for the suggestions. I dont really get how we go from element\nvariables (as Wen described) to the nodal results. Are there any options to\nbe included in the input file to enforce this conversion?\n\u2026\nOn Wed, Dec 23, 2020 at 7:29 PM Wen Jiang ***@***.***> wrote:\n In addition to @GregVernon <https://github.com/GregVernon> 's solution,\n you can also use FIRST order monomial for element variable. MOOSE/libmesh\n uses least square fitting to get the value at the nodes.\n\n [stress_xx]\n order = FIRST\n type = MONOMIAL\n []\n\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n <#16486 (comment)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/AJSA252PYNTH47JWICIRSO3SWJAIVANCNFSM4U2ZZTBA>\n .",
                  "url": "https://github.com/idaholab/moose/discussions/16486#discussioncomment-238243",
                  "updatedAt": "2022-06-28T11:36:39Z",
                  "publishedAt": "2020-12-23T19:40:11Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "Let us use  modules/tensor_mechanics/test/tests/ad_linear_elasticity/applied_strain.i  as example\nAdd following blocks to the input file. This will output stress_xx as a nodal variable.\n[AuxVariables]\n[stress_xx]\norder = FIRST\nfamily = MONOMIAL\n[]\n[]\n[AuxKernels]\n[stress_xx]\ntype = ADRankTwoAux\nvariable = stress_xx\nrank_two_tensor = stress\nindex_i = 0\nindex_j = 0\n[]\n[]\nTensorMechancsMasterAction generate_output option will automatically use Elemental variable. So if you want to output any strain or stress as a nodal variable, you have to manually add the AuxVariable and AuxKernels like what is shown above.",
                          "url": "https://github.com/idaholab/moose/discussions/16486#discussioncomment-238403",
                          "updatedAt": "2022-06-28T11:36:39Z",
                          "publishedAt": "2020-12-23T20:12:41Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "I think what you really want is something similar to the SidesetReaction postprocessor available in the tensor_mechanics module.",
                  "url": "https://github.com/idaholab/moose/discussions/16486#discussioncomment-238846",
                  "updatedAt": "2022-06-28T11:36:40Z",
                  "publishedAt": "2020-12-24T01:38:20Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "abarun22"
                  },
                  "bodyText": "Thanks for your suggestion wen, i could now be able to get smooth\nstress/strain contours after conversion to nodal variable output. Thinking\non the lines of Gary, could i ask if it is possible to extend\n'ElementAverageValue' to accept domain boundaries, so that just by\nspecifying this identifier, we could calculate the results specifically to\na part of the model. I am not pretty sure if this can be done practically,\nbut just a thought to ponder upon.\n\nI am happy to work on the SidesetReaction postprocessor if that's something\nwe could develop quickly.\n\u2026\nOn Thu, Dec 24, 2020 at 1:38 AM Gary (Tianchen) Hu ***@***.***> wrote:\n I think what you really want is something similar to the SidesetReaction\n postprocessor available in the tensor_mechanics module.\n\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n <#16486 (comment)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/AJSA25YHOR6NDT73EVTUU43SWKLRRANCNFSM4U2ZZTBA>\n .",
                  "url": "https://github.com/idaholab/moose/discussions/16486#discussioncomment-239733",
                  "updatedAt": "2022-06-28T11:36:40Z",
                  "publishedAt": "2020-12-24T14:30:12Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "The ElementAverageValue iderives from ElementIntegralPostprocessor which loops all the elements. So you need to implement a new one that derives from SideIntegralPostprocessor which loops over a side/boundary.",
                          "url": "https://github.com/idaholab/moose/discussions/16486#discussioncomment-240051",
                          "updatedAt": "2022-12-15T04:40:54Z",
                          "publishedAt": "2020-12-24T15:34:06Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "The modeling problem of phase field and j2 coupling to simulate grain growth",
          "author": {
            "login": "PengWei97"
          },
          "bodyText": "Dear MOOSE experts,\nRecently I replaced the linear elasticity with a j2 plastic model based on poly_grain_growth_2D_eldrforce.i, and to embed elastic energy into the phase field model. But some errors occurred and the simulation could not be performed,\nThe input I wrote is as follows: poly_grain_growth_2D_eldrforce_j2.i\npoly_grain_growth_2D_eldrforce_j2.txt\n, and then run  on the terminal\nmpirun -np 5 ~/project/panda/panda-opt -i poly_grain_growth_2D_eldrforce_j2.i > poly_grain_growth_2D_eldrforce_j2.i\non the port and report an error after the simulation.\ncalled \u201cWORLD\u201d, \u201cWORLD_ORM_ORM_OR\u201d , 1)-process 2\npoly_grain_growth_2D_eldrforce_j2_01.log\nBased on the appeal error, I retyped the command  on the terminal\n~/project/panda/panda-opt -i poly_grain_growth_2D_eldrforce_j2.i> poly_grain_growth_2D_eldrforce_j2.log but the actual error is as follows:\napplication called MPI_Abort(MPI_COMM_WORLD, 1)-process 0\n[unset]: write_line error; fd=-1 buf=:cmd=abort exitcode=1\n:\nsystem msg for write_line failure: Bad file descriptor\n\npoly_grain_growth_2D_eldrforce_j2_02.log\nI think the input file I wrote has some problems.As a moose beginner, I hope the experts can provide me with guidance.\nAny suggestions or recommendations to fix these problems would be greatly appreciated.\nThank you\nWei Peng",
          "url": "https://github.com/idaholab/moose/discussions/16577",
          "updatedAt": "2022-06-29T20:08:28Z",
          "publishedAt": "2020-12-23T07:00:51Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": []
          }
        }
      },
      {
        "node": {
          "title": "Terminating the simualtion depending on my microstructure",
          "author": {
            "login": "KamalnathOSU"
          },
          "bodyText": "Hi all,\nI am using KKS model ( phase-field) module to simulate precipitate growth. I want to terminate my simulation when the volume fraction of precipitate reaches a target value (instead of using \"end_time\" flag in my executioner.) I am measuring the volume fraction using a postprocessor.\nCurrently, I am thinking of write a error message using \"mooseError()\" when my target volume fraction is reached. But in this way, my microstructure at the end of the simulation won't be saved. Is there a better way to do this ?\nRegards,\nKamal",
          "url": "https://github.com/idaholab/moose/discussions/16569",
          "updatedAt": "2022-07-10T05:25:11Z",
          "publishedAt": "2020-12-22T19:12:06Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "jessecarterMOOSE"
                  },
                  "bodyText": "There's a Terminator user object that you can add to your input file that will stop the simulation when a postprocessor value is reached.\nHere is an example:\nhttps://github.com/idaholab/moose/blob/next/test/tests/userobjects/Terminator/terminator.i",
                  "url": "https://github.com/idaholab/moose/discussions/16569#discussioncomment-235291",
                  "updatedAt": "2022-07-10T05:25:02Z",
                  "publishedAt": "2020-12-22T19:20:57Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "KamalnathOSU"
                          },
                          "bodyText": "Thank you @jessecarterMOOSE .  Does it save the last microstructure in the output file ?",
                          "url": "https://github.com/idaholab/moose/discussions/16569#discussioncomment-235324",
                          "updatedAt": "2022-07-10T05:25:02Z",
                          "publishedAt": "2020-12-22T19:33:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "You can use the Outputs block to save your solution at the last step using execute_on=FINAL.",
                          "url": "https://github.com/idaholab/moose/discussions/16569#discussioncomment-235335",
                          "updatedAt": "2022-07-10T05:25:02Z",
                          "publishedAt": "2020-12-22T19:39:49Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Integrate an AuxVariable along a plane?",
          "author": {
            "login": "mangerij"
          },
          "bodyText": "Hi,\nI looked around and found ElementsAlongPlane and it seems you can get information about elements given some plane coordinate and plane normal. Is there a way to integrate an AuxVariable given the normal and coordinate? Also can this be block restricted?\nthanks",
          "url": "https://github.com/idaholab/moose/discussions/16481",
          "updatedAt": "2020-12-22T00:35:39Z",
          "publishedAt": "2020-12-14T10:28:48Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "I don't think that capability exists, but can always be added.",
                  "url": "https://github.com/idaholab/moose/discussions/16481#discussioncomment-205395",
                  "updatedAt": "2020-12-14T16:09:52Z",
                  "publishedAt": "2020-12-14T16:09:35Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hi,\nYou could add the Plane as a sideset in your mesh then use one of the integral postprocessors.",
                  "url": "https://github.com/idaholab/moose/discussions/16481#discussioncomment-232478",
                  "updatedAt": "2020-12-21T23:10:24Z",
                  "publishedAt": "2020-12-21T23:10:11Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "mangerij"
                  },
                  "bodyText": "We have tried that. It is a bit tricky when using tets and your plane cuts at weird angles in the mesh. And then you end up getting extra nodes near that desired sideset which give different solutions.\n\nOn Dec 22, 2020 1:10 AM, Guillaume Giudicelli <notifications@github.com> wrote:\n\n*Message sent from a system outside of UConn.*\n\n\nHi,\nYou could add the Plane as a sideset in your mesh then use one of the integral postprocessors.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fidaholab%2Fmoose%2Fdiscussions%2F16481%23discussioncomment-232478&data=04%7C01%7Cjohn.mangeri%40uconn.edu%7Ca1c3e03004a045f5572208d8a6059985%7C17f1a87e2a254eaab9df9d439034b080%7C0%7C0%7C637441890352215909%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=SRdxkBuWFY1Qb88PWUhgYBH1BQ%2F5g6ZlnJnuWWyV16E%3D&reserved=0>, or unsubscribe<https://nam10.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FABZ65FAHZ26JQBUO6W5SQ4TSV7IWBANCNFSM4U2QYSLA&data=04%7C01%7Cjohn.mangeri%40uconn.edu%7Ca1c3e03004a045f5572208d8a6059985%7C17f1a87e2a254eaab9df9d439034b080%7C0%7C0%7C637441890352215909%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C1000&sdata=ueDvTW7ASTK6kDq4Ws%2BVWA%2BW6dCGAlnDrouVNo8D5B8%3D&reserved=0>.",
                  "url": "https://github.com/idaholab/moose/discussions/16481#discussioncomment-232655",
                  "updatedAt": "2020-12-21T23:58:12Z",
                  "publishedAt": "2020-12-21T23:58:00Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Ok if you dont want it to influence your solution but are open to adding the plane in a mesh, then I suggest you use a Multiapp and transfer the solution fields to this other app. You can still use the original mesh in the initial app.\nYou can use the Transfers to interpolate the solution on the new mesh, and you can use the postprocessor on that mesh to obtain the desired integral.",
                          "url": "https://github.com/idaholab/moose/discussions/16481#discussioncomment-232743",
                          "updatedAt": "2020-12-22T00:06:29Z",
                          "publishedAt": "2020-12-22T00:06:29Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Running a Python Script in MOOSE (via MultiApp?)",
          "author": {
            "login": "wrkendrick"
          },
          "bodyText": "Hi all,\nI'm currently running MOOSE as a heat transfer solver and I'm interested in adding in an existing script that solves a heat pipe environment. The issue is that the heat pipe script was originally FORTRAN and was recently, painstakingly, written into Python, so I have no interest in doing the same for C++.\nI've seen some evidence of people utilizing Python-based scripts in MOOSE, but I can't find any clear examples or actual code of how you go about tying them together. I understand the basics of embedding Python into C++, but I'm not sure that's the right method, or if there's just some easy implementation via the MultiApp functionality.\nI wanted to ask y'all for any advice or experiences you may have had on this sort of topic. Any help is greatly appreciated!",
          "url": "https://github.com/idaholab/moose/discussions/16497",
          "updatedAt": "2022-09-13T22:47:02Z",
          "publishedAt": "2020-12-15T15:53:37Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "You should probably create a MOOSE Wrapped App that is capable of running the python via C++.\nWe do have a heat pipe application at INL, but I am not sure it is being released outside of the lab. @joshuahansel can you comment on that?",
                  "url": "https://github.com/idaholab/moose/discussions/16497#discussioncomment-210317",
                  "updatedAt": "2022-09-13T22:47:03Z",
                  "publishedAt": "2020-12-15T16:08:42Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "joshuahansel"
                          },
                          "bodyText": "Sockeye (the MOOSE-based heat pipe application) does have users external to INL, but it's not open-source or anything - you'd need to apply for access. I'm not sure on what the criteria are (but probably include organization, project, and nationality). Let me you know if you'd like to try or if you'd like to learn more about Sockeye.",
                          "url": "https://github.com/idaholab/moose/discussions/16497#discussioncomment-210467",
                          "updatedAt": "2022-09-13T22:47:03Z",
                          "publishedAt": "2020-12-15T16:28:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "permcody"
                          },
                          "bodyText": "Sockeye is licensable: You can apply here: www.inl.gov/ncrc\nCalling Python from C++ is possible, but you'll likely find yourself spending a lot of time working out the technical details. This gets further complicated when you consider that MOOSE is parallel. How exactly do you plan to perform parallel calculations in your Python code or do you just want to limit it to a single processor?",
                          "url": "https://github.com/idaholab/moose/discussions/16497#discussioncomment-210661",
                          "updatedAt": "2022-09-13T22:47:04Z",
                          "publishedAt": "2020-12-15T17:46:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "wrkendrick"
                          },
                          "bodyText": "You should probably create a MOOSE Wrapped App that is capable of running the python via C++.\nWe do have a heat pipe application at INL, but I am not sure it is being released outside of the lab. @joshuahansel can you comment on that?\n\nIs there any more info / examples of using a Wrapped App in the MOOSE framework? I'm looking but having a hard time telling what specifically I need to create/pass. The wiki may be a little bugged, too, because when I look to see what variables I can use with ExternalObject I can't really tell: https://mooseframework.inl.gov/source/problems/ExternalProblem.html. It seems like a wrapped app is what I need, though.\n\nSockeye is licensable: You can apply here: www.inl.gov/ncrc\nCalling Python from C++ is possible, but you'll likely find yourself spending a lot of time working out the technical details. This gets further complicated when you consider that MOOSE is parallel. How exactly do you plan to perform parallel calculations in your Python code or do you just want to limit it to a single processor?\n\nThat's a good point I hadn't really thought about. I think for the short term I can live on a single processor, I can sacrifice some runtime if it means that I can actually run.",
                          "url": "https://github.com/idaholab/moose/discussions/16497#discussioncomment-210847",
                          "updatedAt": "2022-09-13T22:47:05Z",
                          "publishedAt": "2020-12-15T18:48:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hi\nI'm sure there are openly accessible examples of external apps wrapped using ExternalProblem. I don't know them but you could ask for access to:\n\nCardinal at ANL, with NEK5000 wrapped this way\nWarthog at ORNL / ANL, with Proteus link\nfor an example.\n\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/16497#discussioncomment-232206",
                          "updatedAt": "2022-09-13T22:47:05Z",
                          "publishedAt": "2020-12-21T22:14:22Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Derived classes of template classes",
          "author": {
            "login": "srinath-chakravarthy"
          },
          "bodyText": "Hi all,\nI am attempting to create a derived template class of ADMatDiffusion with added parameters. For some reason i am unable to use getParam within this. I always get an error \"expected primary-expression before > token\". The code is listed below.\nMy workaround was to duplicate getParam in the specialization, but this involves code duplication for every specialization\n#pragma once\n\n#include \"ADMatDiffusionBase.h\"\n\ntemplate<typename T> \nclass ADChemoMechanoDiffusionTempl : public ADMatDiffusionBase<T>\n{\npublic: \n    static InputParameters validParams();\n\n    ADChemoMechanoDiffusionTempl(const InputParameters & parameters);\n\nprotected:\n    virtual ADRealVectorValue precomputeQpResidual() override;\n    const bool _mu_coupled; \n    const unsigned int _mu_var;\n    Real _temperature; \n    Real _gas_constant;\n\n    const ADVariableGradient * _grad_mu;\n    using ADMatDiffusionBase<T> :: _qp;\n    using ADMatDiffusionBase<T> :: _diffusivity;\n    using ADMatDiffusionBase<T> :: _u;\n    using ADMatDiffusionBase<T> :: adCoupledGradient;\n    using ADMatDiffusionBase<T> :: coupled;\n    using ADMatDiffusionBase<T> :: isCoupled;\n    using ADMatDiffusionBase<T> :: getParam;\nprivate:\n\n};\n\ntemplate<typename T> \nInputParameters\nADChemoMechanoDiffusionTempl<T>::validParams()\n{\n  InputParameters params = ADMatDiffusionBase<T>::validParams();\n  params.addClassDescription(\"Diffusion equation kernel that takes an anisotropic diffusivity \"\n                             \"from a material property and \"\n                             \"computes the gradient of the chemical potential from \"\n                             \"elastic and growth contributions\");\n  params.addCoupledVar(\"stress_based_chemical_potential\", \n                          \"Name of the variable for the stress_based_chemical_potential\");\n  params.addParam<Real>(\"gas_constant\", 8.314426, \"Universal Gas Constant\");\n  params.addParam<Real>(\"temperature\", 298, \"temperature\");\n\n  params.set<bool>(\"use_displaced_mesh\") = false;\n  return params;\n}\n\ntemplate <typename T>\nADChemoMechanoDiffusionTempl<T>::ADChemoMechanoDiffusionTempl(const InputParameters & parameters)\n  : ADMatDiffusionBase<T>(parameters),\n    _mu_coupled(isCoupled(\"stress_based_chemical_potential\")),\n    _mu_var(_mu_coupled ? coupled(\"stress_based_chemical_potential\") : 0), \n    _gas_constant(getParam<Real>(\"gas_constant\")), \n    _temperature(getParam<Real>(\"temperature\"))\n{\n    if (_mu_coupled)\n        _grad_mu = &adCoupledGradient(\"stress_based_chemical_potential\");\n\n}\n\ntemplate<typename T> \nADRealVectorValue\nADChemoMechanoDiffusionTempl<T>::precomputeQpResidual()\n{\n    auto residual = ADMatDiffusionBase<T>::precomputeQpResidual();\n    if (_mu_coupled)\n        residual += _diffusivity[_qp] * _u[_qp] * (*_grad_mu)[_qp] / (_gas_constant * _temperature) ;\n\n    return residual;\n}",
          "url": "https://github.com/idaholab/moose/discussions/16540",
          "updatedAt": "2022-11-08T08:34:44Z",
          "publishedAt": "2020-12-18T16:29:27Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "addParam and getParam are template functions. You need to supply the type in the call: addParam<Real>(...) and getParam<Real>(...)\nThis might help you:\nhttps://mooseframework.inl.gov/getting_started/examples_and_tutorials/tutorial01_app_development/step06_input_params.html#step-6-define-a-set-of-input-parameters",
                  "url": "https://github.com/idaholab/moose/discussions/16540#discussioncomment-230172",
                  "updatedAt": "2022-12-28T03:37:55Z",
                  "publishedAt": "2020-12-21T16:28:43Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "dschwen"
                          },
                          "bodyText": "He is",
                          "url": "https://github.com/idaholab/moose/discussions/16540#discussioncomment-230293",
                          "updatedAt": "2022-12-28T03:37:55Z",
                          "publishedAt": "2020-12-21T17:05:54Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "Those weren't there in the original post.",
                          "url": "https://github.com/idaholab/moose/discussions/16540#discussioncomment-230496",
                          "updatedAt": "2022-12-28T03:37:55Z",
                          "publishedAt": "2020-12-21T18:09:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "srinath-chakravarthy"
                          },
                          "bodyText": "Strange, I did not alter my post.",
                          "url": "https://github.com/idaholab/moose/discussions/16540#discussioncomment-230508",
                          "updatedAt": "2022-12-28T03:37:55Z",
                          "publishedAt": "2020-12-21T18:12:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "@dschwen Made an edit, I think he is messing with me.",
                          "url": "https://github.com/idaholab/moose/discussions/16540#discussioncomment-230551",
                          "updatedAt": "2022-12-28T03:38:24Z",
                          "publishedAt": "2020-12-21T18:36:49Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "dschwen"
                  },
                  "bodyText": "try this-> template getParam<Real>(...",
                  "url": "https://github.com/idaholab/moose/discussions/16540#discussioncomment-230295",
                  "updatedAt": "2022-12-28T03:38:24Z",
                  "publishedAt": "2020-12-21T17:06:12Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "srinath-chakravarthy"
                          },
                          "bodyText": "Thanks. Will try it now and report back shortly.",
                          "url": "https://github.com/idaholab/moose/discussions/16540#discussioncomment-230509",
                          "updatedAt": "2022-12-28T03:54:40Z",
                          "publishedAt": "2020-12-21T18:12:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "srinath-chakravarthy"
                          },
                          "bodyText": "That worked. Thanks so much.",
                          "url": "https://github.com/idaholab/moose/discussions/16540#discussioncomment-230650",
                          "updatedAt": "2022-12-28T03:54:45Z",
                          "publishedAt": "2020-12-21T19:20:25Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "sets of blocks?",
          "author": {
            "login": "crswong888"
          },
          "bodyText": "Is there a way to create a simple alias for a set of blocks? Most of the time I reference any of the individual blocks, its always the same ones grouped together, and they only need to be unique because of material properties. I'm dealing with a 3D model of a Westinghouse fuel assembly here, and so there's a lot going on.\nAs a simple example, I'm looking for some way to reference blocks 1 and 2 by a single name instead of both explicitly, while still being able to reference 1 and 2 individually:\n[Variables]\n  [disp_x]\n    block = '1 2'\n  []\n[]\n\n[Kernels]\n  [some_kernel]\n    block = '1 2'\n  []\n[]\n\n[Materials]\n  [mat1]\n    block = 1\n  []\n  [mat2]\n    block = 2\n  []\n[]\n\nThe model I'm working with has 18 blocks, not just 2, so trust me, it's a lot more painful than the above example.",
          "url": "https://github.com/idaholab/moose/discussions/16548",
          "updatedAt": "2023-04-25T19:41:09Z",
          "publishedAt": "2020-12-19T03:39:37Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "How about defining\nmy_awesome_block_name = '1 2'\n\non the top of your input file?\nLater on you can use\nblock = ${my_awesome_block_name}",
                  "url": "https://github.com/idaholab/moose/discussions/16548#discussioncomment-224871",
                  "updatedAt": "2023-04-25T19:41:31Z",
                  "publishedAt": "2020-12-19T05:50:04Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "crswong888"
                          },
                          "bodyText": "Wow I did not know this kind of syntax was available for MOOSE HIT files. This is very valuable. Thanks!",
                          "url": "https://github.com/idaholab/moose/discussions/16548#discussioncomment-226015",
                          "updatedAt": "2023-04-25T19:41:46Z",
                          "publishedAt": "2020-12-19T21:24:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "https://mooseframework.inl.gov/application_usage/input_syntax.html",
                          "url": "https://github.com/idaholab/moose/discussions/16548#discussioncomment-230143",
                          "updatedAt": "2023-04-25T19:41:46Z",
                          "publishedAt": "2020-12-21T16:18:09Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}