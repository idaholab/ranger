{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMy0wOC0xNVQwNToyNzoyMC0wNjowMM4AUyy8"
    },
    "edges": [
      {
        "node": {
          "title": "Simple questions about Materials and visualization",
          "author": {
            "login": "bosxered"
          },
          "bodyText": "Hello Moose experts,\nI want to visualize Material properties with higher resolution.\nI mean not like constant-order variables, but like first or second-order variables.\nIs it possible?\n\nActually, for higher resolution, I tried to follow https://mooseframework.inl.gov/syntax/Outputs/index.html but for me, material properties are gone in the ParaView. I posted some parts of my input file. Is it the wrong way to use refinements and displaced mesh?\n[Materials]\n  [./current_density_y]\n    type = ParsedMaterial\n    property_name = 'current_density'\n    coupled_variables = 'Ey'\n    material_property_names = 'electrical_conductivity'\n    expression = 'electrical_conductivity*Ey'\n    outputs = exodus\n    use_displaced_mesh = true\n  [../]\n[]\n\n[Outputs]\n  exodus = true\n  csv = true\n  file_base = output\n\n  [myexo]\n    type = Exodus\n    file_base = half\n    interval = 2\n#    start_step = 4340\n#    refinements = 3\n#    use_displaced = true\n  []\n\n  [myexo2]\n    type = Exodus\n    file_base = half-dis\n    interval = 2\n#    start_step = 4340\n    refinements = 2\n#    use_displaced = true\n    show_material_properties = 'current_density'\n  []\n\n  [mymycp]\n    type = Checkpoint\n    num_files = 10\n    interval = 50\n    file_base = mycp\n  []\n[]\n\n\nThere is Current_density (Material properties), but a cell label (not point label) in the Paraview.\n\nNo Material properties in the half-dis.e. I can't find them.\nI think the refinement is working because...\nhalf-dis.e: 6.4MB\nhalf.e: 3.4MB\noutput.e: 6.2MB.\nBest Wishes,\nJeonghwan",
          "url": "https://github.com/idaholab/moose/discussions/25222",
          "updatedAt": "2023-08-17T05:53:18Z",
          "publishedAt": "2023-08-16T13:56:07Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nIt's not impossible, it might not work without code changes. Let's try\nYou can try to use MaterialRealAux with a higher order monomial auxiliary variable.\nYou cannot use the short syntax in the Materials and Outputs blocks for higher order, you ll have to use the auxkernels system\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/25222#discussioncomment-6741196",
                  "updatedAt": "2023-08-16T14:00:15Z",
                  "publishedAt": "2023-08-16T14:00:14Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "sourabhkadambi"
                  },
                  "bodyText": "Hello. There's a filter called Cell Data to Point Data in ParaView which might be what you're looking to do. See https://vtk.org/doc/nightly/html/classvtkCellDataToPointData.html",
                  "url": "https://github.com/idaholab/moose/discussions/25222#discussioncomment-6746001",
                  "updatedAt": "2023-08-17T00:33:30Z",
                  "publishedAt": "2023-08-17T00:33:29Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "bosxered"
                          },
                          "bodyText": "It's exactly what I wanted. Thanks a lot!",
                          "url": "https://github.com/idaholab/moose/discussions/25222#discussioncomment-6747476",
                          "updatedAt": "2023-08-17T05:53:19Z",
                          "publishedAt": "2023-08-17T05:53:18Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Matrix on-diagonal entry 6 was exactly zero during LU factorization in MatrixTools::inverse (PC failed due to FACTOR_NUMERIC_ZEROPIVOT)",
          "author": null,
          "bodyText": "Any other option to correct this error?\nMatrix on-diagonal entry 6 was exactly zero during LU factorization in MatrixTools::inverse.\n  Linear solve did not converge due to DIVERGED_PC_FAILED iterations 0\n                 PC failed due to FACTOR_NUMERIC_ZEROPIVOT\n\nI tried -pc_factor_shift of NONZERO and -sub_pc_factor_shift_type NONZERO. They didn't work. Here is my jacobian output:\nKernel for variable 'pwater':\n  (0,1) Off-diagonal Jacobian for variable 'disp_x' needs to be implemented\n  (0,2) Off-diagonal Jacobian for variable 'disp_y' needs to be implemented\n\nKernel for variable 'disp_x':\n  (1,1) On-diagonal Jacobian is wrong (off by 108.1 %)\n  (1,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 14.93 %)\n  (1,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 13.17 %)\n\nKernel for variable 'disp_y':\n  (2,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 14.93 %)\n  (2,2) On-diagonal Jacobian is wrong (off by 103.8 %)\n  (2,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 12.99 %)\n\nKernel for variable 'disp_z':\n  (3,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 13.17 %)\n  (3,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 12.99 %)\n  (3,3) On-diagonal Jacobian is wrong (off by 780.4 %)\n\nKernel for variable 'pwater':\n  (0,1) Off-diagonal Jacobian for variable 'disp_x' needs to be implemented\n  (0,2) Off-diagonal Jacobian for variable 'disp_y' needs to be implemented\n\nKernel for variable 'disp_x':\n  (1,1) On-diagonal Jacobian is wrong (off by 108.1 %)\n  (1,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 14.93 %)\n  (1,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 13.17 %)\n\nKernel for variable 'disp_y':\n  (2,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 14.93 %)\n  (2,2) On-diagonal Jacobian is wrong (off by 103.8 %)\n  (2,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 12.99 %)\n\nKernel for variable 'disp_z':\n  (3,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 13.17 %)\n  (3,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 12.99 %)\n  (3,3) On-diagonal Jacobian is wrong (off by 780.5 %)\n\nKernel for variable 'pwater':\n  (0,1) Off-diagonal Jacobian for variable 'disp_x' needs to be implemented\n  (0,2) Off-diagonal Jacobian for variable 'disp_y' needs to be implemented\n\nKernel for variable 'disp_x':\n  (1,1) On-diagonal Jacobian is wrong (off by 108.1 %)\n  (1,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 14.93 %)\n  (1,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 13.17 %)\n\nKernel for variable 'disp_y':\n  (2,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 14.93 %)\n  (2,2) On-diagonal Jacobian is wrong (off by 103.8 %)\n  (2,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 12.99 %)\n\nKernel for variable 'disp_z':\n  (3,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 13.17 %)\n  (3,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 12.99 %)\n  (3,3) On-diagonal Jacobian is wrong (off by 780.5 %)\n\nKernel for variable 'pwater':\n  (0,1) Off-diagonal Jacobian for variable 'disp_x' needs to be implemented\n  (0,2) Off-diagonal Jacobian for variable 'disp_y' needs to be implemented\n\nKernel for variable 'disp_x':\n  (1,1) On-diagonal Jacobian is wrong (off by 108.1 %)\n  (1,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 14.93 %)\n  (1,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 13.18 %)\n\nKernel for variable 'disp_y':\n  (2,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 14.93 %)\n  (2,2) On-diagonal Jacobian is wrong (off by 103.8 %)\n  (2,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 12.99 %)\n\nKernel for variable 'disp_z':\n  (3,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 13.18 %)\n  (3,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 12.99 %)\n  (3,3) On-diagonal Jacobian is wrong (off by 780.5 %)\n\nKernel for variable 'pwater':\n  (0,1) Off-diagonal Jacobian for variable 'disp_x' needs to be implemented\n  (0,2) Off-diagonal Jacobian for variable 'disp_y' needs to be implemented\n\nKernel for variable 'disp_x':\n  (1,1) On-diagonal Jacobian is wrong (off by 108.1 %)\n  (1,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 14.93 %)\n  (1,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 13.18 %)\n\nKernel for variable 'disp_y':\n  (2,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 14.93 %)\n  (2,2) On-diagonal Jacobian is wrong (off by 103.8 %)\n  (2,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 12.99 %)\n\nKernel for variable 'disp_z':\n  (3,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 13.18 %)\n  (3,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 12.99 %)\n  (3,3) On-diagonal Jacobian is wrong (off by 780.5 %)\n\nKernel for variable 'pwater':\n  (0,1) Off-diagonal Jacobian for variable 'disp_x' needs to be implemented\n  (0,2) Off-diagonal Jacobian for variable 'disp_y' needs to be implemented\n\nKernel for variable 'disp_x':\n  (1,1) On-diagonal Jacobian is wrong (off by 108.1 %)\n  (1,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 14.93 %)\n  (1,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 13.18 %)\n\nKernel for variable 'disp_y':\n  (2,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 14.93 %)\n  (2,2) On-diagonal Jacobian is wrong (off by 103.8 %)\n  (2,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 12.99 %)\n\nKernel for variable 'disp_z':\n  (3,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 13.18 %)\n  (3,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 12.99 %)\n  (3,3) On-diagonal Jacobian is wrong (off by 780.5 %)\n\nKernel for variable 'pwater':\n  (0,1) Off-diagonal Jacobian for variable 'disp_x' needs to be implemented\n  (0,2) Off-diagonal Jacobian for variable 'disp_y' needs to be implemented\n\nKernel for variable 'disp_x':\n  (1,1) On-diagonal Jacobian is wrong (off by 108.1 %)\n  (1,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 14.93 %)\n  (1,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 13.18 %)\n\nKernel for variable 'disp_y':\n  (2,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 14.93 %)\n  (2,2) On-diagonal Jacobian is wrong (off by 103.8 %)\n  (2,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 12.99 %)\n\nKernel for variable 'disp_z':\n  (3,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 13.18 %)\n  (3,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 12.99 %)\n  (3,3) On-diagonal Jacobian is wrong (off by 780.5 %)\n\nKernel for variable 'pwater':\n  (0,1) Off-diagonal Jacobian for variable 'disp_x' needs to be implemented\n  (0,2) Off-diagonal Jacobian for variable 'disp_y' needs to be implemented\n\nKernel for variable 'disp_x':\n  (1,1) On-diagonal Jacobian is wrong (off by 108.1 %)\n  (1,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 14.93 %)\n  (1,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 13.18 %)\n\nKernel for variable 'disp_y':\n  (2,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 14.93 %)\n  (2,2) On-diagonal Jacobian is wrong (off by 103.8 %)\n  (2,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 12.99 %)\n\nKernel for variable 'disp_z':\n  (3,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 13.18 %)\n  (3,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 12.99 %)\n  (3,3) On-diagonal Jacobian is wrong (off by 780.5 %)\n\nKernel for variable 'pwater':\n  (0,1) Off-diagonal Jacobian for variable 'disp_x' needs to be implemented\n  (0,2) Off-diagonal Jacobian for variable 'disp_y' needs to be implemented\n\nKernel for variable 'disp_x':\n  (1,1) On-diagonal Jacobian is wrong (off by 108.1 %)\n  (1,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 14.93 %)\n  (1,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 13.18 %)\n\nKernel for variable 'disp_y':\n  (2,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 14.93 %)\n  (2,2) On-diagonal Jacobian is wrong (off by 103.8 %)\n  (2,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 12.99 %)\n\nKernel for variable 'disp_z':\n  (3,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 13.18 %)\n  (3,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 12.99 %)\n  (3,3) On-diagonal Jacobian is wrong (off by 780.5 %)\n\nKernel for variable 'pwater':\n  (0,1) Off-diagonal Jacobian for variable 'disp_x' needs to be implemented\n  (0,2) Off-diagonal Jacobian for variable 'disp_y' needs to be implemented\n\nKernel for variable 'disp_x':\n  (1,1) On-diagonal Jacobian is wrong (off by 108.1 %)\n  (1,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 14.93 %)\n  (1,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 13.18 %)\n\nKernel for variable 'disp_y':\n  (2,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 14.93 %)\n  (2,2) On-diagonal Jacobian is wrong (off by 103.8 %)\n  (2,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 12.99 %)\n\nKernel for variable 'disp_z':\n  (3,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 13.18 %)\n  (3,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 12.99 %)\n  (3,3) On-diagonal Jacobian is wrong (off by 780.5 %)\n\nKernel for variable 'pwater':\n  (0,1) Off-diagonal Jacobian for variable 'disp_x' needs to be implemented\n  (0,2) Off-diagonal Jacobian for variable 'disp_y' needs to be implemented\n\nKernel for variable 'disp_x':\n  (1,1) On-diagonal Jacobian is wrong (off by 108.1 %)\n  (1,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 14.93 %)\n  (1,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 13.18 %)\n\nKernel for variable 'disp_y':\n  (2,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 14.93 %)\n  (2,2) On-diagonal Jacobian is wrong (off by 103.8 %)\n  (2,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 12.99 %)\n\nKernel for variable 'disp_z':\n  (3,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 13.18 %)\n  (3,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 12.99 %)\n  (3,3) On-diagonal Jacobian is wrong (off by 780.5 %)\n\nKernel for variable 'pwater':\n  (0,1) Off-diagonal Jacobian for variable 'disp_x' needs to be implemented\n  (0,2) Off-diagonal Jacobian for variable 'disp_y' needs to be implemented\n\nKernel for variable 'disp_x':\n  (1,1) On-diagonal Jacobian is wrong (off by 108.1 %)\n  (1,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 14.93 %)\n  (1,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 13.18 %)\n\nKernel for variable 'disp_y':\n  (2,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 14.93 %)\n  (2,2) On-diagonal Jacobian is wrong (off by 103.8 %)\n  (2,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 12.99 %)\n\nKernel for variable 'disp_z':\n  (3,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 13.18 %)\n  (3,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 12.99 %)\n  (3,3) On-diagonal Jacobian is wrong (off by 780.5 %)\n\nKernel for variable 'pwater':\n  (0,1) Off-diagonal Jacobian for variable 'disp_x' needs to be implemented\n  (0,2) Off-diagonal Jacobian for variable 'disp_y' needs to be implemented\n\nKernel for variable 'disp_x':\n  (1,1) On-diagonal Jacobian is wrong (off by 108.1 %)\n  (1,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 14.93 %)\n  (1,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 13.18 %)\n\nKernel for variable 'disp_y':\n  (2,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 14.93 %)\n  (2,2) On-diagonal Jacobian is wrong (off by 103.8 %)\n  (2,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 12.99 %)\n\nKernel for variable 'disp_z':\n  (3,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 13.18 %)\n  (3,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 12.99 %)\n  (3,3) On-diagonal Jacobian is wrong (off by 780.5 %)\n\nKernel for variable 'pwater':\n  (0,1) Off-diagonal Jacobian for variable 'disp_x' needs to be implemented\n  (0,2) Off-diagonal Jacobian for variable 'disp_y' needs to be implemented\n\nKernel for variable 'disp_x':\n  (1,1) On-diagonal Jacobian is wrong (off by 108.1 %)\n  (1,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 14.93 %)\n  (1,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 13.18 %)\n\nKernel for variable 'disp_y':\n  (2,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 14.93 %)\n  (2,2) On-diagonal Jacobian is wrong (off by 103.8 %)\n  (2,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 12.99 %)\n\nKernel for variable 'disp_z':\n  (3,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 13.18 %)\n  (3,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 12.99 %)\n  (3,3) On-diagonal Jacobian is wrong (off by 780.5 %)\n\nKernel for variable 'pwater':\n  (0,1) Off-diagonal Jacobian for variable 'disp_x' needs to be implemented\n  (0,2) Off-diagonal Jacobian for variable 'disp_y' needs to be implemented\n\nKernel for variable 'disp_x':\n  (1,1) On-diagonal Jacobian is wrong (off by 108.1 %)\n  (1,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 14.93 %)\n  (1,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 13.18 %)\n\nKernel for variable 'disp_y':\n  (2,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 14.93 %)\n  (2,2) On-diagonal Jacobian is wrong (off by 103.8 %)\n  (2,3) Off-diagonal Jacobian for variable 'disp_z' is questionable (off by 12.99 %)\n\nKernel for variable 'disp_z':\n  (3,1) Off-diagonal Jacobian for variable 'disp_x' is questionable (off by 13.18 %)\n  (3,2) Off-diagonal Jacobian for variable 'disp_y' is questionable (off by 12.99 %)\n  (3,3) On-diagonal Jacobian is wrong (off by 780.5 %)",
          "url": "https://github.com/idaholab/moose/discussions/25209",
          "updatedAt": "2023-08-17T01:01:10Z",
          "publishedAt": "2023-08-14T22:36:12Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "lindsayad"
                  },
                  "bodyText": "Can you share your Executioner and Preconditioning blocks? The option is -pc_factor_shift_type NONZERO or -sub_pc_factor_shift_type NONZERO (the latter of which you tried). Which one it is depends on your -pc_type or -sub_pc_type. You can also add -options_left on the command line to see what options PETSc does and doesn't take",
                  "url": "https://github.com/idaholab/moose/discussions/25209#discussioncomment-6725704",
                  "updatedAt": "2023-08-14T22:42:19Z",
                  "publishedAt": "2023-08-14T22:41:06Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": null,
                          "bodyText": "sure. I used the former. Here are my blocks:\n[Preconditioning]\n  active = preferred_but_might_not_be_installed                \n  [basic]\n    type = SMP\n    full = true\n    petsc_options = '-ksp_diagonal_scale -ksp_diagonal_scale_fix'\n    petsc_options_iname = '-pc_type -sub_pc_type -sub_pc_factor_shift_type -pc_asm_overlap'\n    petsc_options_value = ' asm      lu           NONZERO                   2'\n  []\n  [preferred_but_might_not_be_installed]\n    type = SMP\n    full = true\n    petsc_options_iname = '-pc_type -pc_factor_mat_solver_package -pc_factor_shift_type'\n    petsc_options_value = ' lu       mumps           NONZERO '\n  []\n[]\n\n[Executioner]\n  type = Transient\n  solve_type = Newton\n  end_time = 2e3 \n  [TimeStepper]\n    type = IterationAdaptiveDT\n    dt = 1e-1           \n    optimal_iterations = 10\n  []\n   nl_abs_tol = 1e-10\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/25209#discussioncomment-6725747",
                          "updatedAt": "2023-08-14T22:51:29Z",
                          "publishedAt": "2023-08-14T22:51:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Matrix on-diagonal entry 6 was exactly zero during LU factorization in MatrixTools::inverse.\n\nThis actually looks like an error out of MOOSE, not PETSc. Are you using a physics module? It seems possible that you are doing something with tensor mechanics based off use of MatrixTools::inverse in the MOOSE repository. Do you have a MOOSE-only input that we could run to reproduce?",
                          "url": "https://github.com/idaholab/moose/discussions/25209#discussioncomment-6726080",
                          "updatedAt": "2023-08-15T00:03:40Z",
                          "publishedAt": "2023-08-15T00:03:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": null,
                          "bodyText": "Hey @lindsayad. yeah you're right. I'm combining tensor mechanics and P.Flow. I attached a txt file containing the MOOSE input.",
                          "url": "https://github.com/idaholab/moose/discussions/25209#discussioncomment-6726606",
                          "updatedAt": "2023-08-15T14:25:41Z",
                          "publishedAt": "2023-08-15T02:15:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": null,
                          "bodyText": "same error found in MOOSE-only input file:\n# This input combines PorousFlow 'tutorial 04' with MOOSE 'combined_creep_plasticity'\n[Mesh]\n  [annular]\n    type = AnnularMeshGenerator\n    nr = 10\n    rmin = 1.0\n    rmax = 10\n    growth_r = 1.4\n    nt = 4\n    dmin = 0\n    dmax = 90\n  []\n  [make3D]\n    type = MeshExtruderGenerator\n    extrusion_vector = '0 0 12'\n    num_layers = 3\n    bottom_sideset = 'bottom'\n    top_sideset = 'top'\n    input = annular\n  []\n  [shift_down]\n    type = TransformGenerator\n    transform = TRANSLATE\n    vector_value = '0 0 -6'\n    input = make3D\n  []\n  [aquifer]\n    type = SubdomainBoundingBoxGenerator\n    block_id = 1\n    bottom_left = '0 0 -2'\n    top_right = '10 10 2'\n    input = shift_down\n  []\n  [injection_area]\n    type = ParsedGenerateSideset\n    combinatorial_geometry = 'x*x+y*y<1.01'\n    included_subdomain_ids = 1\n    new_sideset_name = 'injection_area'\n    input = 'aquifer'\n  []\n  [rename]\n    type = RenameBlockGenerator\n    old_block = '0 1'\n    new_block = 'caps aquifer'\n    input = 'injection_area'\n  []\n[]\n\n[GlobalParams]\n  displacements = 'disp_x disp_y disp_z'\n  PorousFlowDictator = dictator\n  biot_coefficient = 1.0\n[]\n\n[Variables]\n  [porepressure]\n  []\n[]\n\n[PorousFlowBasicTHM]\n  porepressure = porepressure\n  coupling_type = HydroMechanical\n  gravity = '0 0 0'\n  fp = the_simple_fluid\n  use_displaced_mesh = false\n[]\n\n[BCs]\n  [constant_injection_porepressure]\n    type = DirichletBC\n    variable = porepressure\n    value = 1E6\n    boundary = injection_area\n  []\n\n  [roller_tmax]\n    type = DirichletBC\n    variable = disp_x\n    value = 0\n    boundary = dmax\n  []\n  [roller_tmin]\n    type = DirichletBC\n    variable = disp_y\n    value = 0\n    boundary = dmin\n  []\n  [roller_top_bottom]\n    type = DirichletBC\n    variable = disp_z\n    value = 0\n    boundary = 'top bottom'\n  []\n  [cavity_pressure_x]\n    type = Pressure\n    boundary = injection_area\n    variable = disp_x\n    component = 0\n    factor = 1E6\n    use_displaced_mesh = false\n  []\n  [cavity_pressure_y]\n    type = Pressure\n    boundary = injection_area\n    variable = disp_y\n    component = 1\n    factor = 1E6\n    use_displaced_mesh = false\n  []\n[]\n\n[FluidProperties]\n  [the_simple_fluid]\n    type = SimpleFluidProperties\n    bulk_modulus = 2E9\n    viscosity = 1.0E-3\n    density0 = 1000.0\n    thermal_expansion = 0.0002\n    porepressure_coefficient = 0\n  []\n[]\n\n[Materials]\n  [porosity]\n    type = PorousFlowPorosity\n    porosity_zero = 0.1\n  []\n  [biot_modulus]\n    type = PorousFlowConstantBiotModulus\n    solid_bulk_compliance = 2E-7\n    fluid_bulk_modulus = 1E7\n  []\n  [permeability_aquifer]\n    type = PorousFlowPermeabilityConst\n    block = aquifer\n    permeability = '1E-14 0 0   0 1E-14 0   0 0 1E-14'\n  []\n  [permeability_caps]\n    type = PorousFlowPermeabilityConst\n    block = caps\n    permeability = '1E-15 0 0   0 1E-15 0   0 0 1E-16'\n  []\n\n[elasticity_tensor_aquifer]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 1e3\n    poissons_ratio = 0.3\n    block = 'aquifer'\n []\n  [elasticity_tensor_caps]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 1e3\n    poissons_ratio = 0.3\n    block = 'caps'\n  []\n  \n  [creep_plas]\n    type = ComputeMultipleInelasticStress\n    tangent_operator = elastic\n    inelastic_models = 'creep plas'\n    max_iterations = 50\n    absolute_tolerance = 1e-05\n    combined_inelastic_strain_weights = '0.0 1.0'\n    max_iterations = 60\n    relative_tolerance = 1e-5\n    block = 'aquifer caps'\n  [] \n  [creep]\n    type = PowerLawCreepStressUpdate\n    coefficient = 0.5e-7\n    n_exponent = 5\n    m_exponent = -0.5\n    activation_energy = 0\n    block = 'aquifer caps'\n  [] \n  [plas]\n    type = IsotropicPlasticityStressUpdate\n    hardening_constant = 100\n    yield_stress = 20\n    block = 'aquifer caps'\n  []\n[]\n\n[Modules/TensorMechanics/Master]\n    strain = SMALL\n    incremental = true\n    add_variables = true\n    generate_output = 'stress_yy stress_xx stress_zz creep_strain_xx creep_strain_yy creep_strain_zz elastic_strain_yy'  \n    use_automatic_differentiation = false\n  [aquifer]\n  block = 'aquifer'\n  use_automatic_differentiation = false\n  scaling = 1E-5\n  []\n  [caps]\n   block = 'caps'\n   use_automatic_differentiation = false\n  scaling = 1E-5\n  []\n[]\n\n[Functions]\n  [./top_pull]\n    type = PiecewiseLinear\n    x = '  0   1   1.5'\n    y = '-20 -40   -20'\n  [../]\n\n  [./dts]\n    type = PiecewiseLinear\n    x = '0        0.5    1.0    1.5'\n    y = '0.015  0.015  0.005  0.005'\n  [../]\n[]\n\n[Preconditioning]\n  active = basic\n  [basic]\n    type = SMP\n    full = true\n    petsc_options = '-ksp_diagonal_scale -ksp_diagonal_scale_fix'\n    petsc_options_iname = '-pc_type -sub_pc_type -sub_pc_factor_shift_type -pc_asm_overlap'\n    petsc_options_value = ' asm      lu           NONZERO                   2'\n  []\n  [preferred_but_might_not_be_installed]\n    type = SMP\n    full = true\n    petsc_options_iname = '-pc_type -pc_factor_mat_solver_package'\n    petsc_options_value = ' lu       mumps'\n  []\n[]\n\n[Executioner]\n  type = Transient\n  solve_type = Newton\n  end_time = 1e6 \n  [TimeStepper]\n    type = IterationAdaptiveDT\n    dt = 1e-1           \n    optimal_iterations = 40\n  []\n   nl_abs_tol = 1e-11\n[]\n\n[Outputs]\n  exodus = true\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/25209#discussioncomment-6733141",
                          "updatedAt": "2023-08-15T18:20:40Z",
                          "publishedAt": "2023-08-15T18:20:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Well the exception for the above input file comes out of ComputeMultipleInelasticStress.\n#0  0x00007fffec8b71c1 in __cxa_throw () from /lib/x86_64-linux-gnu/libstdc++.so.6\n#1  0x00007ffff6ba05ab in ComputeMultipleInelasticStress::updateQpState (this=<optimized out>, \n    elastic_strain_increment=..., combined_inelastic_strain_increment=...)\n    at /home/lindad/projects/moose/modules/tensor_mechanics/src/materials/ComputeMultipleInelasticStress.C:381\n#2  0x00007ffff6f7ae2d in ComputeMultipleInelasticStress::computeQpStressIntermediateConfiguration (\n    this=0x555556303bc0)\n    at /home/lindad/projects/moose/modules/tensor_mechanics/src/materials/ComputeMultipleInelasticStress.C:260\n#3  0x00007ffff6f77091 in ComputeMultipleInelasticStress::computeQpStress (this=0x555556303bc0)\n    at /home/lindad/projects/moose/modules/tensor_mechanics/src/materials/ComputeMultipleInelasticStress.C:208\n#4  0x00007ffff6f66647 in ComputeGeneralStressBase::computeQpProperties (this=0x555556303bc0)\n    at /home/lindad/projects/moose/modules/tensor_mechanics/src/materials/ComputeGeneralStressBase.C:46\n#5  0x00007ffff55a1461 in Material::computeProperties (this=0x555556303bc0)\n    at /home/lindad/projects/moose/framework/src/materials/Material.C:132\n\nSince you are using porous flow, and @WilkAndy seems to be the original author of the ComputeMultipleInelasticStress class, perhaps he or @cpgr might have some ideas about why your simulation is going astray",
                          "url": "https://github.com/idaholab/moose/discussions/25209#discussioncomment-6733400",
                          "updatedAt": "2023-08-15T18:55:39Z",
                          "publishedAt": "2023-08-15T18:55:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": null,
                          "bodyText": "Update: my model converges seamlessly after modifying the BCs, although I still receive a similar message after running the Jacobian debugger. It will be good to know how the MOOSE-only input file works out. Thanks.",
                          "url": "https://github.com/idaholab/moose/discussions/25209#discussioncomment-6746111",
                          "updatedAt": "2023-08-17T01:01:11Z",
                          "publishedAt": "2023-08-17T01:01:10Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "My some question about different method for INS flow in Finite Element Methods",
          "author": {
            "login": "suqingji"
          },
          "bodyText": "Dear all developers,\nThere are some different seperate method to solve INS problem with finite element method in MOOSE, such as being used in lid_driven_split and lid_driven_chorin. So Do they have any advatages and disadvatages. Could you please introduce about them and show some articles about those alogrithm.\nBest wishes.",
          "url": "https://github.com/idaholab/moose/discussions/25212",
          "updatedAt": "2023-08-16T14:54:28Z",
          "publishedAt": "2023-08-15T10:18:28Z",
          "category": {
            "name": "Q&A Modules: Navier-Stokes"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\n@freiler can comment on the Chorin projection\nthere should be a bibliography file in the navier stokes module doc / bib folder if you want to check the publications we have / we reference in the docs\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/25212#discussioncomment-6729702",
                  "updatedAt": "2023-08-15T11:23:05Z",
                  "publishedAt": "2023-08-15T11:23:05Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "suqingji"
                          },
                          "bodyText": "Thanks very much.",
                          "url": "https://github.com/idaholab/moose/discussions/25212#discussioncomment-6736407",
                          "updatedAt": "2023-08-16T03:46:32Z",
                          "publishedAt": "2023-08-16T03:46:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "freiler"
                          },
                          "bodyText": "The Chorin method involves splitting the Navier-Stokes equations into two separate steps: a prediction step for the velocity field and a correction step for the pressure field. Here's how it works in a nutshell:\n\n\nPrediction Step: In this step, a tentative velocity field is computed by solving the advection and diffusion terms of the Navier-Stokes equations without considering the current pressure gradient term and linearizing the advective term. This step essentially predicts the velocity field at the next time step.\n\n\nPressure Correction Step: The predicted velocity field is then used to compute a provisional pressure field using a Poisson equation derived from the incompressibility constraint. The pressure correction is applied to remove any divergence from the predicted velocity field.\n\n\nCorrection Step: The predicted velocity field is corrected by subtracting the gradient of the pressure field computed in the previous step. This ensures that the final velocity field is divergence-free.\n\n\nSome Advantages:\n-Simplicity: Chorin's method is one of the easiest concepts of segregated solvers to implement and understand.\n-Speed: Compared to fully coupled solvers.\nDisadvantages:\n-Accuracy: The accuracy can be limited compared to more advanced methods like fractional step methods or implicit methods.\n-Pressure Oscillations: Chorin's method can sometimes lead to spurious pressure oscillations, especially in situations with strong gradients in the flow field.\nReference about Chorin\n1 Chorin, A.J.: Numerical solution of the Navier-Stokes equations. Math.Comp. 22, 745\u2013762 (1968).",
                          "url": "https://github.com/idaholab/moose/discussions/25212#discussioncomment-6741790",
                          "updatedAt": "2023-08-16T14:54:29Z",
                          "publishedAt": "2023-08-16T14:54:28Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "update_and_rebuild_petsc.sh, unable to clone parmetis, could not excute git clone",
          "author": {
            "login": "bosxered"
          },
          "bodyText": "Hi Moose experts\nI tried to update the Moose as shown in #25127\nbut I failed so I wanna download it again.\nUnfortunately, there are some issues with downloading the Moose (for HPC).\n\n\n\n\nthis is the first warning. so that I commanded git clone https://bitbucket.org/petsc/pkg-metis.git /home2/users/jjh2021/lib/Moose4/moose/petsc/arch-moose/externalpackages/git.metis\nand I can go to the next step. It passed cloning the METIS.\n\n\n\n\nThis is the second problem. PARMETIS also was not downloaded. What's worse is the command git clone https://bitbucket.org/petsc/pkg-parmetis.git /home2/users/jjh2021/lib/Moose4/moose/petsc/linux-impi-mkl/externalpackages/git.parmetis was not executed, too. So, I downloaded it manually from the homepage and moved it from my Window PC to the linux machine.\n\n\n\n\nI downloaded METIS and PARMETIS manually, and then I rerun the update_and_rebuild_petsc.sh\n\nBut I think when I rerun the .sh file, pre-downloaded externalpackages are removed.\nCan you guys try git clone https://bitbucket.org/petsc/pkg-parmetis.git and tell me if there is the problem (and METIS, too)?\nAbsolutely, I confirmed that git clone another thing (for example git clone https://github.com/idaholab/moose.git) works.",
          "url": "https://github.com/idaholab/moose/discussions/25196",
          "updatedAt": "2023-08-16T12:40:48Z",
          "publishedAt": "2023-08-14T09:50:09Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "bosxered"
                  },
                  "bodyText": "For your information, I also upload the results of diagnostics.sh\nMon Aug 14 18:00:50 KST 2023\n\nSystem Arch: LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch Distributor ID: CentOS Description: CentOS Linux release 7.3.1611 (Core) Release: 7.3.1611 Codename: Core\n\nMOOSE Package Version: Custom Build\n\nCPU Count: 4\n\nMemory Free: 1442.520 MB\n\nVariable `which $CC` check:\n/home/users/jjh2021/mambaforge3/envs/moose/bin/mpicc\n\n$CC --version:\nx86_64-conda-linux-gnu-cc (conda-forge gcc 10.4.0-19) 10.4.0\nCopyright (C) 2020 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nMPICC:\nwhich mpicc:\n\t/home/users/jjh2021/mambaforge3/envs/moose/bin/mpicc\nmpicc -show:\n\tx86_64-conda-linux-gnu-cc -I/home/users/jjh2021/mambaforge3/envs/moose/include -I/home/users/jjh2021/mambaforge3/envs/moose/include -L/home/users/jjh2021/mambaforge3/envs/moose/lib -Wl,-rpath,/home/users/jjh2021/mambaforge3/envs/moose/lib -I/home/users/jjh2021/mambaforge3/envs/moose/include -L/home/users/jjh2021/mambaforge3/envs/moose/lib -Wl,-rpath -Wl,/home/users/jjh2021/mambaforge3/envs/moose/lib -Wl,--enable-new-dtags -lmpi\n\nCOMPILER x86_64-conda-linux-gnu-cc:\nx86_64-conda-linux-gnu-cc (conda-forge gcc 10.4.0-19) 10.4.0\nCopyright (C) 2020 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nPython:\n\t/home/users/jjh2021/mambaforge3/envs/moose/bin/python\n\tPython 3.10.8\n\nMODULES:\nNo Modulefiles Currently Loaded.\n\nENVIRONMENT:\nMANPATH=/usr/share/man:/usr/local/share/man\nMPI_INCLUDE=/home/users/jjh2021/mambaforge3/envs/moose/include\nHOSTNAME=gold\nGUESTFISH_INIT=\ufffd[1;34m\nDTK=/usr/local/hpc\nMPICH2=/usr/local/mpich2\nSHELL=/bin/bash\nTERM=xterm-256color\n_X11_NO_MITSHM=1\nXLIB_NO_SHM=1\nHISTSIZE=220000\nSSH_CLIENT=143.248.147.80 51865 22\nPETSC_ARCH=linux-impi-mkl\nPERL5LIB=/home/users/jjh2021/perl5/lib/perl5:\nCONDA_SHLVL=1\nCONDA_PROMPT_MODIFIER=(base) \nORG_INCLUDE=\nQTDIR=/usr/lib64/qt-3.3\nPERL_MB_OPT=--install_base /home/users/jjh2021/perl5\nQTINC=/usr/lib64/qt-3.3/include\nMPICH=/usr/local/mpich\nSSH_TTY=/dev/pts/52\nQT_GRAPHICSSYSTEM_CHECKED=1\nUSER=jjh2021\nHISTFILESIZE=20000\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:\nLD_LIBRARY_PATH=/opt/intel/mkl/8.1.1/lib/em64t::/usr/local/lib/liblbfgs-1.10/lib/.libs:/home/common/anaconda3/lib/:/home/common/lib_common/gcc-9.3.0_server/lib:/home/common/lib_common/gcc-9.3.0_server/lib64:/home/common/lib_common/gcc-9.3.0_server/lib/gcc/x86_64-pc-linux-gnu/9.3.0:/home/common/lib_common/gcc-9.3.0_server/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/home/users/jjh2021/mambaforge3/lib:/home/users/jjh2021/mambaforge3/envs/moose/bin:/usr/local/lib/liblbfgs-1.10/lib/.libs:/home/common/anaconda3/lib/:/home/common/lib_common/gcc-9.3.0_server/lib:/home/common/lib_common/gcc-9.3.0_server/lib64:/home/common/lib_common/gcc-9.3.0_server/lib/gcc/x86_64-pc-linux-gnu/9.3.0:/home/common/lib_common/gcc-9.3.0_server/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/home/users/jjh2021/mambaforge3/lib:/home/users/jjh2021/mambaforge3/envs/moose/bin\nORG_LIBRARY_PATH=/opt/intel/oneapi/mpi/2021.1.1//libfabric/lib:/opt/intel/oneapi/mpi/2021.1.1//lib/release:/opt/intel/oneapi/mpi/2021.1.1//lib:/opt/intel/oneapi/clck/2021.1.1/lib/intel64:/opt/intel/oneapi/mkl/2021.1.1/env/../lib/intel64:/opt/intel/oneapi/compiler/2021.1.1/linux/compiler/lib/intel64_lin:/opt/intel/oneapi/compiler/2021.1.1/linux/lib\nCONDA_EXE=/home/users/jjh2021/mambaforge3/bin/conda\nPVM_ROOT=/usr/share/pvm3\nMPI_LIB=/home/users/jjh2021/mambaforge3/envs/moose/lib\nMSM_PRODUCT=RWC2\nORG_MANPATH=/opt/intel/oneapi/mpi/2021.1.1/man:/opt/intel/oneapi/clck/2021.1.1/man:/usr/share/man:/usr/local/share/man:/opt/intel/oneapi/compiler/2021.1.1/documentation/en/man/common:\nGUESTFISH_PS1=\\[\ufffd[1;32m\\]><fs>\\[\ufffd[0;31m\\] \n_MITSHM=0\n_CE_CONDA=\nPATH=/home/users/jjh2021/mambaforge3/envs/moose/bin:/home/users/jjh2021/mambaforge3/lib:/home/users/jjh2021/mambaforge3/bin:/home/common/lib_common/gcc-9.3.0_server/bin:/home/users/jjh2021/mambaforge3/bin:/home/users/jjh2021/mambaforge3/condabin:/home/users/jjh2021/mambaforge3/envs/moose/bin:/home/users/jjh2021/mambaforge3/lib:/home/users/jjh2021/mambaforge3/bin:/home/common/lib_common/gcc-9.3.0_server/bin:/usr/local/drive-master/bin:/usr/local/go/bin:/opt/intel/oneapi/mpi/2021.1.1/libfabric/bin:/opt/intel/oneapi/mpi/2021.1.1/bin:/opt/intel/oneapi/mkl/2021.1.1/bin/intel64:/usr/local/mpich/bin:/usr/local/mpich2/bin:/usr/local/pbs/sbin:/usr/local/pbs/bin:/usr/local/maui/sbin:/usr/local/maui/bin:/usr/local/bwatch:/usr/local/hpc/bin:/opt/absoft/bin:/usr/local/ldap/bin:/usr/local/ldap/sbin:/usr/lib64/qt-3.3/bin:/home/users/jjh2021/perl5/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/local/ganglia/bin:/home/users/jjh2021/.local/bin:/home/users/jjh2021/bin\nMAIL=/var/spool/mail/jjh2021\nQT_X11_NO_MITSHM=1\nTETGEN_DIR=/home/common/tmp/hklee/bin_pf/sample_input/src/TETGEN\nXML_CATALOG_FILES=file:///home/users/jjh2021/mambaforge3/etc/xml/catalog file:///etc/xml/catalog\nCONDA_PREFIX=/home/users/jjh2021/mambaforge3\nPWD=/home2/users/jjh2021/lib/Moose4/moose/scripts\nF90=mpif90\nLDAP=/usr/local/ldap\nLANG=en_US.UTF-8\nPRODUCTNAME=RAID Web Console 2\nMODULEPATH=/usr/share/Modules/modulefiles:/etc/modulefiles\nABSOFT=/opt/absoft\nPBS=/usr/local/pbs\nGUESTFISH_OUTPUT=\ufffd[0m\nKDEDIRS=/usr\nP4_GLOBMEMSIZE=2147483648\nORG_FPATH=\nF77=mpif77\nHISTCONTROL=ignoredups\nSSH_ASKPASS=/usr/libexec/openssh/gnome-ssh-askpass\nCXX=mpicxx\n_CE_M=\nORG_PATH=/usr/local/drive-master/bin:/usr/local/go/bin:/opt/cuda/11.2/bin:/opt/cuda/11.2/samples/bin/x86_64/linux/release:/opt/intel/oneapi/mpi/2021.1.1//libfabric/bin:/opt/intel/oneapi/mpi/2021.1.1//bin:/opt/intel/oneapi/clck/2021.1.1/bin/intel64:/opt/intel/oneapi/mkl/2021.1.1/env/../bin/intel64:/opt/intel/oneapi/compiler/2021.1.1/linux/lib/oclfpga/llvm/aocl-bin:/opt/intel/oneapi/compiler/2021.1.1/linux/lib/oclfpga/bin:/opt/intel/oneapi/compiler/2021.1.1/linux/bin/intel64:/opt/intel/oneapi/compiler/2021.1.1/linux/bin:/opt/intel/oneapi/compiler/2021.1.1/linux/ioc/bin:/usr/local/mpich/bin:/usr/local/mpich2/bin:/usr/local/pbs/sbin:/usr/local/pbs/bin:/usr/local/maui/sbin:/usr/local/maui/bin:/usr/local/bwatch:/usr/local/hpc/bin:/opt/absoft/bin:/usr/local/ldap/bin:/usr/local/ldap/sbin:/usr/lib64/qt-3.3/bin:/home/users/jjh2021/perl5/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/local/ganglia/bin\nHOME=/home/users/jjh2021\nSHLVL=2\nGANGLIA=/usr/local/ganglia\nGOROOT=/usr/local/go\nORG_NLSPATH=/opt/intel/oneapi/mkl/2021.1.1/env/../lib/intel64/locale/%l_%t/%N\nBW=/usr/local/bwatch\nMSM_HOME=/usr/local/RAID Web Console 2\nFC=mpif90\nPERL_LOCAL_LIB_ROOT=:/home/users/jjh2021/perl5\nLOGNAME=jjh2021\nCONDA_PYTHON_EXE=/home/users/jjh2021/mambaforge3/bin/python\nQTLIB=/usr/lib64/qt-3.3/lib\nSSH_CONNECTION=143.248.147.80 51865 143.248.153.36 22\nORG_LD_LIBRARY_PATH=/opt/cuda/11.2/lib64:/opt/cuda/11.2/lib:/opt/cuda/11.2/extras/CUPTI/lib64:/opt/cuda/11.2/targets/x86_64-linux/lib/stubs:/opt/cuda/11.2/targets/x86_64-linux/lib:/opt/intel/oneapi/mpi/2021.1.1//libfabric/lib:/opt/intel/oneapi/mpi/2021.1.1//lib/release:/opt/intel/oneapi/mpi/2021.1.1//lib:/opt/intel/oneapi/mkl/2021.1.1/env/../lib/intel64:/opt/intel/oneapi/compiler/2021.1.1/linux/lib:/opt/intel/oneapi/compiler/2021.1.1/linux/lib/x64:/opt/intel/oneapi/compiler/2021.1.1/linux/lib/emu:/opt/intel/oneapi/compiler/2021.1.1/linux/lib/oclfpga/host/linux64/lib:/opt/intel/oneapi/compiler/2021.1.1/linux/lib/oclfpga/linux64/lib:/opt/intel/oneapi/compiler/2021.1.1/linux/compiler/lib/intel64_lin:/opt/intel/oneapi/compiler/2021.1.1/linux/compiler/lib:/opt/intel/mkl/8.1.1/lib/em64t:\nORG_MIC_LD_LIBRARY_PATH=\nMODULESHOME=/usr/share/Modules\nLESSOPEN=||/usr/bin/lesspipe.sh %s\nGOPATH=/usr/local/go/gopath\nCONDA_DEFAULT_ENV=base\nMKL=/opt/intel/mkl/8.1.1/lib/em64t\nDISPLAY=localhost:47.0\nCC=mpicc\nQT_PLUGIN_PATH=/usr/lib64/kde4/plugins:/usr/lib/kde4/plugins\nMPI_IEXEC=/home/users/jjh2021/mambaforge3/envs/moose/bin/mpiexec\nMAUI=/usr/local/maui\nGUESTFISH_RESTORE=\ufffd[0m\nPETSC_DIR=/usr/local/petsc\nORG_CPATH=/opt/intel/oneapi/mpi/2021.1.1//include:/opt/intel/oneapi/mkl/2021.1.1/env/../include:/opt/intel/oneapi/compiler/2021.1.1/linux/include\nPERL_MM_OPT=INSTALL_BASE=/home/users/jjh2021/perl5\nHISTTIMEFORMAT=%Y-%m-%d %H:%M:%S \nBASH_FUNC_module()=() {  eval `/usr/bin/modulecmd bash $*`\n}\n_=/usr/bin/env",
                  "url": "https://github.com/idaholab/moose/discussions/25196#discussioncomment-6719661",
                  "updatedAt": "2023-08-14T10:51:51Z",
                  "publishedAt": "2023-08-14T10:51:51Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Please try not to paste pictures, it sometimes forces us to do the same. But most importantly, searching this forum does not work when all the information about the error is contained within a picture.\n\nPETSC_ARCH is detecting that you are using Intel MKL compilers (linux-impi-mkl). Meanwhile, you are trying to use GCC via Mamba. There will be plenty of conflicts between the two. You need to somehow prevent your Intel Compiler from being available during build time of any MOOSE related event.",
                          "url": "https://github.com/idaholab/moose/discussions/25196#discussioncomment-6720536",
                          "updatedAt": "2023-08-14T12:52:45Z",
                          "publishedAt": "2023-08-14T12:52:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Also, it looks like you have PETSC_DIR, and PETSC_ARCH overrided somewhere already:\n(libmesh diagnostics)\nPETSC_ARCH=linux-impi-mkl\nPETSC_DIR=/usr/local/petsc\nI am not sure where/how these are being set on your machine. But these need to be unset, if you plan on building your own PETSc. Which... you will have to do since it seems the one being supplied by the system will require that you use the Intel Compiler (which MOOSE does not support).",
                          "url": "https://github.com/idaholab/moose/discussions/25196#discussioncomment-6720583",
                          "updatedAt": "2023-08-14T12:57:42Z",
                          "publishedAt": "2023-08-14T12:57:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "bosxered"
                          },
                          "bodyText": "Thanks for your kind answer and so sorry for bothering you by uploading pictures.\nI might have some mistakes when I opened a new terminal so that unset command didn't work.\nUnfortunately, I unset Intel compilers and tried again. But it failed.\nCould you please try git clone https://bitbucket.org/petsc/pkg-parmetis.git and tell me if there is the problem (and METIS, too)?\nAbsolutely, I confirmed that git clone another thing (for example, git clone https://github.com/idaholab/moose.git) works.\nHere are the log of update_and_rebuild_petsc.sh, results of diagnostics.sh, and env\nlog of update_and_rebuild_petsc.sh,\n/home2/users/jjh2021/lib/Moose5/moose/scripts\nSubmodule 'petsc' (https://gitlab.com/petsc/petsc.git) registered for path 'petsc'\nCloning into 'petsc'...\nremote: Enumerating objects: 1115624, done.\nremote: Counting objects: 100% (770/770), done.\nremote: Compressing objects: 100% (206/206), done.\nremote: Total 1115624 (delta 612), reused 609 (delta 563), pack-reused 1114854\nReceiving objects: 100% (1115624/1115624), 354.06 MiB | 44.75 MiB/s, done.\nResolving deltas: 100% (854569/854569), done.\nSubmodule path 'petsc': checked out '477e44bbb558b1357d86363677accbb4bcdfaabc'\nINFO: Checking for HDF5...\nINFO: HDF5 library not detected, opting to download via PETSc...\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nThe version of PETSc you are using is out-of-date, we recommend updating to the new release\n Available Version: 3.19.4   Installed Version: 3.16.6\nhttps://petsc.org/release/download/\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n=============================================================================================\n                      Configuring PETSc to compile on your system\n=============================================================================================\n=============================================================================================\n      ***** WARNING: CC (set to mpicc) found in environment variables - ignoring\n       use ./configure CC=$CC if you really want to use that value ******\n=============================================================================================\n=============================================================================================\n      ***** WARNING: CXX (set to mpicxx) found in environment variables - ignoring\n       use ./configure CXX=$CXX if you really want to use that value ******\n=============================================================================================\n=============================================================================================\n      ***** WARNING: FC (set to mpif90) found in environment variables - ignoring\n       use ./configure FC=$FC if you really want to use that value ******\n=============================================================================================\n=============================================================================================\n      ***** WARNING: F77 (set to mpif77) found in environment variables - ignoring\n       use ./configure F77=$F77 if you really want to use that value ******\n=============================================================================================\n=============================================================================================\n      ***** WARNING: F90 (set to mpif90) found in environment variables - ignoring\n       use ./configure F90=$F90 if you really want to use that value ******\n=============================================================================================\n=============================================================================================\n      ***** WARNING: Using default optimization C flags -g -O\n      You might consider manually setting optimal optimization flags for your system with\n       COPTFLAGS=\"optimization flags\" see config/examples/arch-*-opt.py for examples\n=============================================================================================\n=============================================================================================\n      ***** WARNING: Using default Cxx optimization flags -g -O\n      You might consider manually setting optimal optimization flags for your system with        CXXOPTFLAGS=\"optimization flags\" see config/examples/arch-*-opt.py for examples\n=============================================================================================\n=============================================================================================\n      ***** WARNING: Using default FORTRAN optimization flags -g -O\n      You might consider manually setting optimal optimization flags for your system with\n       FOPTFLAGS=\"optimization flags\" see config/examples/arch-*-opt.py for examples\n=============================================================================================\n=============================================================================================\n      ***** WARNING: Using default optimization CUDA flags -O3\n      You might consider manually setting optimal optimization flags for your system with\n       CUDAOPTFLAGS=\"optimization flags\" see config/examples/arch-*-opt.py for examples\n=============================================================================================\n=============================================================================================\n      ***** WARNING: You have a version of GNU make older than 4.0. It will work,\n      but may not support all the parallel testing options. You can install the\n      latest GNU make with your package manager, such as brew or macports, or use\n      the --download-make option to get the latest GNU make *****\n=============================================================================================\n=============================================================================================\n      Trying to download git://https://bitbucket.org/petsc/pkg-metis.git for METIS\n=============================================================================================\n=============================================================================================\n      Trying to download https://bitbucket.org/petsc/pkg-metis/get/v5.1.0-p10.tar.gz for METIS\n=============================================================================================\n      Trying to download git://https://bitbucket.org/petsc/pkg-metis.git for METIS\n=============================================================================================\n=============================================================================================\n      Trying to download https://bitbucket.org/petsc/pkg-metis/get/v5.1.0-p10.tar.gz for METIS\n=============================================================================================\n                      *******************************************************************************\n         UNABLE to CONFIGURE with GIVEN OPTIONS    (see configure.log for details):\n-------------------------------------------------------------------------------\nError during download/extract/detection of METIS:\nUnable to clone metis\nCould not execute \"['git clone https://bitbucket.org/petsc/pkg-metis.git /home2/users/jjh2021/lib/Moose5/moose/petsc/arch-moose/externalpackages/git.metis']\":\nCloning into '/home2/users/jjh2021/lib/Moose5/moose/petsc/arch-moose/externalpackages/git.metis'...fatal: unable to access 'https://bitbucket.org/petsc/pkg-metis.git/': Failed to connect to bitbucket.org port 443 after 254525 ms: Couldn't connect to server\nUnable to download package METIS from: git://https://bitbucket.org/petsc/pkg-metis.git\n* If URL specified manually - perhaps there is a typo?\n* If your network is disconnected - please reconnect and rerun ./configure\n* Or perhaps you have a firewall blocking the download\n* You can run with --with-packages-download-dir=/adirectory and ./configure will instruct you what packages to download manually\n* or you can download the above URL manually, to /yourselectedlocation\n  and use the configure option:\n  --download-metis=/yourselectedlocation\n    Unable to clone metis\nCould not execute \"['git clone https://bitbucket.org/petsc/pkg-metis.git /home2/users/jjh2021/lib/Moose5/moose/petsc/arch-moose/externalpackages/git.metis']\":\nCloning into '/home2/users/jjh2021/lib/Moose5/moose/petsc/arch-moose/externalpackages/git.metis'...fatal: unable to access 'https://bitbucket.org/petsc/pkg-metis.git/': Failed to connect to bitbucket.org port 443 after 254572 ms: Couldn't connect to server\nUnable to download package METIS from: git://https://bitbucket.org/petsc/pkg-metis.git\n* If URL specified manually - perhaps there is a typo?\n* If your network is disconnected - please reconnect and rerun ./configure\n* Or perhaps you have a firewall blocking the download\n* You can run with --with-packages-download-dir=/adirectory and ./configure will instruct you what packages to download manually\n* or you can download the above URL manually, to /yourselectedlocation\n  and use the configure option:\n  --download-metis=/yourselectedlocation\n    Unable to download package METIS from: https://bitbucket.org/petsc/pkg-metis/get/v5.1.0-p10.tar.gz\n* If URL specified manually - perhaps there is a typo?\n* If your network is disconnected - please reconnect and rerun ./configure\n* Or perhaps you have a firewall blocking the download\n* You can run with --with-packages-download-dir=/adirectory and ./configure will instruct you what packages to download manually\n* or you can download the above URL manually, to /yourselectedlocation/v5.1.0-p10.tar.gz\n  and use the configure option:\n  --download-metis=/yourselectedlocation/v5.1.0-p10.tar.gz\n\n*******************************************************************************\n\n\nresults of diagnostics.sh\nTue Aug 15 05:07:12 KST 2023\n\nSystem Arch: LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch Distributor ID: CentOS Description: CentOS Linux release 7.3.1611 (Core) Release: 7.3.1611 Codename: Core\n\nMOOSE Package Version: Custom Build\n\nCPU Count: 4\n\nMemory Free: 1982.402 MB\n\nVariable `which $CC` check:\n/home/users/jjh2021/mambaforge3/envs/moose/bin/mpicc\n\n$CC --version:\nx86_64-conda-linux-gnu-cc (conda-forge gcc 10.4.0-19) 10.4.0\nCopyright (C) 2020 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nMPICC:\nwhich mpicc:\n\t/home/users/jjh2021/mambaforge3/envs/moose/bin/mpicc\nmpicc -show:\n\tx86_64-conda-linux-gnu-cc -I/home/users/jjh2021/mambaforge3/envs/moose/include -I/home/users/jjh2021/mambaforge3/envs/moose/include -L/home/users/jjh2021/mambaforge3/envs/moose/lib -Wl,-rpath,/home/users/jjh2021/mambaforge3/envs/moose/lib -I/home/users/jjh2021/mambaforge3/envs/moose/include -L/home/users/jjh2021/mambaforge3/envs/moose/lib -Wl,-rpath -Wl,/home/users/jjh2021/mambaforge3/envs/moose/lib -Wl,--enable-new-dtags -lmpi\n\nCOMPILER x86_64-conda-linux-gnu-cc:\nx86_64-conda-linux-gnu-cc (conda-forge gcc 10.4.0-19) 10.4.0\nCopyright (C) 2020 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nPython:\n\t/home/users/jjh2021/mambaforge3/envs/moose/bin/python\n\tPython 3.10.8\n\nMODULES:\nNo Modulefiles Currently Loaded.\n\nPETSC_DIR not set\n\nENVIRONMENT:\nMPI_INCLUDE=/home/users/jjh2021/mambaforge3/envs/moose/include\nHOSTNAME=gold\nGUESTFISH_INIT=\ufffd[1;34m\nDTK=/usr/local/hpc\nMPICH2=/usr/local/mpich2\nINTEL_LICENSE_FILE=/opt/intel/oneapi/clck/2021.1.1/licensing:/opt/intel/licenses:/Users/Shared/Library/Application Support/Intel/Licenses\nSHELL=/bin/bash\nTERM=xterm-256color\n_X11_NO_MITSHM=1\nXLIB_NO_SHM=1\nHISTSIZE=220000\nSSH_CLIENT=143.248.147.80 49582 22\nPERL5LIB=/home/users/jjh2021/perl5/lib/perl5:\nCONDA_SHLVL=1\nCONDA_PROMPT_MODIFIER=(base) \nORG_INCLUDE=\nQTDIR=/usr/lib64/qt-3.3\nPERL_MB_OPT=--install_base /home/users/jjh2021/perl5\nQTINC=/usr/lib64/qt-3.3/include\nMPICH=/usr/local/mpich\nSSH_TTY=/dev/pts/43\nQT_GRAPHICSSYSTEM_CHECKED=1\nUSER=jjh2021\nCUDA_HOME=/opt/cuda/11.2\nHISTFILESIZE=20000\nLS_COLORS=rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.Z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.war=38;5;9:*.ear=38;5;9:*.sar=38;5;9:*.rar=38;5;9:*.alz=38;5;9:*.ace=38;5;9:*.zoo=38;5;9:*.cpio=38;5;9:*.7z=38;5;9:*.rz=38;5;9:*.cab=38;5;9:*.jpg=38;5;13:*.jpeg=38;5;13:*.gif=38;5;13:*.bmp=38;5;13:*.pbm=38;5;13:*.pgm=38;5;13:*.ppm=38;5;13:*.tga=38;5;13:*.xbm=38;5;13:*.xpm=38;5;13:*.tif=38;5;13:*.tiff=38;5;13:*.png=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.webm=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.axv=38;5;13:*.anx=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.axa=38;5;45:*.oga=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:\nLD_LIBRARY_PATH=:/usr/local/lib/liblbfgs-1.10/lib/.libs:/home/common/anaconda3/lib/:/home/common/lib_common/gcc-9.3.0_server/lib:/home/common/lib_common/gcc-9.3.0_server/lib64:/home/common/lib_common/gcc-9.3.0_server/lib/gcc/x86_64-pc-linux-gnu/9.3.0:/home/common/lib_common/gcc-9.3.0_server/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/home/users/jjh2021/mambaforge3/lib:/home/users/jjh2021/mambaforge3/envs/moose/bin:/usr/local/lib/liblbfgs-1.10/lib/.libs:/home/common/anaconda3/lib/:/home/common/lib_common/gcc-9.3.0_server/lib:/home/common/lib_common/gcc-9.3.0_server/lib64:/home/common/lib_common/gcc-9.3.0_server/lib/gcc/x86_64-pc-linux-gnu/9.3.0:/home/common/lib_common/gcc-9.3.0_server/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/home/users/jjh2021/mambaforge3/lib:/home/users/jjh2021/mambaforge3/envs/moose/bin\nCONDA_EXE=/home/users/jjh2021/mambaforge3/bin/conda\nPVM_ROOT=/usr/share/pvm3\nMPI_LIB=/home/users/jjh2021/mambaforge3/envs/moose/lib\nMSM_PRODUCT=RWC2\nGUESTFISH_PS1=\\[\ufffd[1;32m\\]><fs>\\[\ufffd[0;31m\\] \n_MITSHM=0\n_CE_CONDA=\nPATH=/home/users/jjh2021/mambaforge3/envs/moose/bin:/home/users/jjh2021/mambaforge3/lib:/home/users/jjh2021/mambaforge3/bin:/home/common/lib_common/gcc-9.3.0_server/bin:/home/users/jjh2021/mambaforge3/envs/moose/bin:/home/users/jjh2021/mambaforge3/lib:/home/users/jjh2021/mambaforge3/bin:/home/common/lib_common/gcc-9.3.0_server/bin:/usr/bin\nMAIL=/var/spool/mail/jjh2021\nQT_X11_NO_MITSHM=1\nTETGEN_DIR=/home/common/tmp/hklee/bin_pf/sample_input/src/TETGEN\nXML_CATALOG_FILES=file:///home/users/jjh2021/mambaforge3/etc/xml/catalog file:///etc/xml/catalog\nCONDA_PREFIX=/home/users/jjh2021/mambaforge3\nPWD=/home2/users/jjh2021/lib/Moose5/moose/scripts\nF90=mpif90\nLDAP=/usr/local/ldap\nLANG=en_US.UTF-8\nPRODUCTNAME=RAID Web Console 2\nMODULEPATH=/usr/share/Modules/modulefiles:/etc/modulefiles\nABSOFT=/opt/absoft\nPBS=/usr/local/pbs\nGUESTFISH_OUTPUT=\ufffd[0m\nKDEDIRS=/usr\nP4_GLOBMEMSIZE=2147483648\nORG_FPATH=\nF77=mpif77\nHISTCONTROL=ignoredups\nSSH_ASKPASS=/usr/libexec/openssh/gnome-ssh-askpass\nCXX=mpicxx\n_CE_M=\nHOME=/home/users/jjh2021\nSHLVL=2\nGANGLIA=/usr/local/ganglia\nGOROOT=/usr/local/go\nBW=/usr/local/bwatch\nMSM_HOME=/usr/local/RAID Web Console 2\nFC=mpif90\nPERL_LOCAL_LIB_ROOT=:/home/users/jjh2021/perl5\nLOGNAME=jjh2021\nCONDA_PYTHON_EXE=/home/users/jjh2021/mambaforge3/bin/python\nQTLIB=/usr/lib64/qt-3.3/lib\nACL_BOARD_VENDOR_PATH=/opt/Intel/OpenCLFPGA/oneAPI/Boards\nSSH_CONNECTION=143.248.147.80 49582 143.248.153.36 22\nORG_MIC_LD_LIBRARY_PATH=\nMODULESHOME=/usr/share/Modules\nLESSOPEN=||/usr/bin/lesspipe.sh %s\nGOPATH=/usr/local/go/gopath\nCONDA_DEFAULT_ENV=base\nDISPLAY=localhost:33.0\nCC=mpicc\nQT_PLUGIN_PATH=/usr/lib64/kde4/plugins:/usr/lib/kde4/plugins\nMPI_IEXEC=/home/users/jjh2021/mambaforge3/envs/moose/bin/mpiexec\nMAUI=/usr/local/maui\nGUESTFISH_RESTORE=\ufffd[0m\nPERL_MM_OPT=INSTALL_BASE=/home/users/jjh2021/perl5\nHISTTIMEFORMAT=%Y-%m-%d %H:%M:%S \nBASH_FUNC_module()=() {  eval `/usr/bin/modulecmd bash $*`\n}\n_=/usr/bin/env\n\nenv\nMPI_INCLUDE=/home/users/jjh2021/mambaforge3/envs/moose/include\nDTK=/usr/local/hpc\nGUESTFISH_INIT=\\e[1;34m\nHOSTNAME=gold\nINTEL_LICENSE_FILE=/opt/intel/oneapi/clck/2021.1.1/licensing:/opt/intel/licenses:/Users/Shared/Library/Application Support/Intel/Licenses\nMPICH2=/usr/local/mpich2\nXLIB_NO_SHM=1\n_X11_NO_MITSHM=1\nTERM=xterm-256color\nSHELL=/bin/bash\nHISTSIZE=220000\nSSH_CLIENT=143.248.147.80 49582 22\nCONDA_SHLVL=1\nPERL5LIB=/home/users/jjh2021/perl5/lib/perl5:\nCONDA_PROMPT_MODIFIER=(base) \nORG_INCLUDE=\nQTDIR=/usr/lib64/qt-3.3\nOLDPWD=/home2/users/jjh2021/lib/Moose5/moose\nMPICH=/usr/local/mpich\nQTINC=/usr/lib64/qt-3.3/include\nPERL_MB_OPT=--install_base /home/users/jjh2021/perl5\nSSH_TTY=/dev/pts/43\nQT_GRAPHICSSYSTEM_CHECKED=1\nHISTFILESIZE=20000\nCUDA_HOME=/opt/cuda/11.2\nUSER=jjh2021\nLD_LIBRARY_PATH=:/usr/local/lib/liblbfgs-1.10/lib/.libs:/home/common/anaconda3/lib/:/home/common/lib_common/gcc-9.3.0_server/lib:/home/common/lib_common/gcc-9.3.0_server/lib64:/home/common/lib_common/gcc-9.3.0_server/lib/gcc/x86_64-pc-linux-gnu/9.3.0:/home/common/lib_common/gcc-9.3.0_server/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/home/users/jjh2021/mambaforge3/lib:/home/users/jjh2021/mambaforge3/envs/moose/bin:/usr/local/lib/liblbfgs-1.10/lib/.libs:/home/common/anaconda3/lib/:/home/common/lib_common/gcc-9.3.0_server/lib:/home/common/lib_common/gcc-9.3.0_server/lib64:/home/common/lib_common/gcc-9.3.0_server/lib/gcc/x86_64-pc-linux-gnu/9.3.0:/home/common/lib_common/gcc-9.3.0_server/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/home/users/jjh2021/mambaforge3/lib:/home/users/jjh2021/mambaforge3/envs/moose/bin\nLS_COLORS=rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.Z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.war=38;5;9:*.ear=38;5;9:*.sar=38;5;9:*.rar=38;5;9:*.alz=38;5;9:*.ace=38;5;9:*.zoo=38;5;9:*.cpio=38;5;9:*.7z=38;5;9:*.rz=38;5;9:*.cab=38;5;9:*.jpg=38;5;13:*.jpeg=38;5;13:*.gif=38;5;13:*.bmp=38;5;13:*.pbm=38;5;13:*.pgm=38;5;13:*.ppm=38;5;13:*.tga=38;5;13:*.xbm=38;5;13:*.xpm=38;5;13:*.tif=38;5;13:*.tiff=38;5;13:*.png=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.webm=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.axv=38;5;13:*.anx=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.axa=38;5;45:*.oga=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:\nCONDA_EXE=/home/users/jjh2021/mambaforge3/bin/conda\nPVM_ROOT=/usr/share/pvm3\nMPI_LIB=/home/users/jjh2021/mambaforge3/envs/moose/lib\nMSM_PRODUCT=RWC2\n_CE_CONDA=\n_MITSHM=0\nGUESTFISH_PS1=\\[\\e[1;32m\\]><fs>\\[\\e[0;31m\\] \nQT_X11_NO_MITSHM=1\nMAIL=/var/spool/mail/jjh2021\nPATH=/home/users/jjh2021/mambaforge3/envs/moose/bin:/home/users/jjh2021/mambaforge3/lib:/home/users/jjh2021/mambaforge3/bin:/home/common/lib_common/gcc-9.3.0_server/bin:/home/users/jjh2021/mambaforge3/envs/moose/bin:/home/users/jjh2021/mambaforge3/lib:/home/users/jjh2021/mambaforge3/bin:/home/common/lib_common/gcc-9.3.0_server/bin:/usr/bin\nXML_CATALOG_FILES=file:///home/users/jjh2021/mambaforge3/etc/xml/catalog file:///etc/xml/catalog\nTETGEN_DIR=/home/common/tmp/hklee/bin_pf/sample_input/src/TETGEN\nCONDA_PREFIX=/home/users/jjh2021/mambaforge3\nF90=mpif90\nPWD=/home2/users/jjh2021/lib/Moose5/moose/scripts\nLDAP=/usr/local/ldap\nPRODUCTNAME=RAID Web Console 2\nLANG=en_US.UTF-8\nPBS=/usr/local/pbs\nABSOFT=/opt/absoft\nMODULEPATH=/usr/share/Modules/modulefiles:/etc/modulefiles\nKDEDIRS=/usr\nGUESTFISH_OUTPUT=\\e[0m\nP4_GLOBMEMSIZE=2147483648\nF77=mpif77\nORG_FPATH=\n_CE_M=\nCXX=mpicxx\nSSH_ASKPASS=/usr/libexec/openssh/gnome-ssh-askpass\nHISTCONTROL=ignoredups\nGANGLIA=/usr/local/ganglia\nSHLVL=1\nHOME=/home/users/jjh2021\nGOROOT=/usr/local/go\nFC=mpif90\nMSM_HOME=/usr/local/RAID Web Console 2\nBW=/usr/local/bwatch\nPERL_LOCAL_LIB_ROOT=:/home/users/jjh2021/perl5\nCONDA_PYTHON_EXE=/home/users/jjh2021/mambaforge3/bin/python\nLOGNAME=jjh2021\nACL_BOARD_VENDOR_PATH=/opt/Intel/OpenCLFPGA/oneAPI/Boards\nQTLIB=/usr/lib64/qt-3.3/lib\nORG_MIC_LD_LIBRARY_PATH=\nSSH_CONNECTION=143.248.147.80 49582 143.248.153.36 22\nMODULESHOME=/usr/share/Modules\nCONDA_DEFAULT_ENV=base\nGOPATH=/usr/local/go/gopath\nLESSOPEN=||/usr/bin/lesspipe.sh %s\nCC=mpicc\nDISPLAY=localhost:33.0\nMPI_IEXEC=/home/users/jjh2021/mambaforge3/envs/moose/bin/mpiexec\nQT_PLUGIN_PATH=/usr/lib64/kde4/plugins:/usr/lib/kde4/plugins\nMAUI=/usr/local/maui\nGUESTFISH_RESTORE=\\e[0m\nHISTTIMEFORMAT=%Y-%m-%d %H:%M:%S \nPERL_MM_OPT=INSTALL_BASE=/home/users/jjh2021/perl5\nBASH_FUNC_module()=() {  eval `/usr/bin/modulecmd bash $*`\n}\n_=/usr/bin/env",
                          "url": "https://github.com/idaholab/moose/discussions/25196#discussioncomment-6724518",
                          "updatedAt": "2023-08-14T20:23:43Z",
                          "publishedAt": "2023-08-14T20:23:42Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "bosxered"
                  },
                  "bodyText": "Hello guys.\nThanks for your attention and sorry again for bothering you.\nGood news: I solved the problem.\nHow? I downloaded it successfully on my PC by following the guide https://mooseframework.inl.gov/getting_started/installation/conda.html.\nIt's super easy when using Conda.\nAnd then I moved the moose with all environments, from my PC to the HPC cluster server.\nI just want to tell you and those who have the same problem that this way can be a solution.\nAbout this,\n\nCould you please try git clone https://bitbucket.org/petsc/pkg-parmetis.git and tell me if there is a problem (and METIS, too)?\nAbsolutely, I confirmed that git clone another thing (for example, git clone https://github.com/idaholab/moose.git) works.\n\nDownloading the metis and the parmetis is still problem even if I unset all the intel compilers.\nI tried just git clone not update_rebuild_petsc.sh but it failed.\nI personally think it could be not only a problem for me but also a problem for someone who have similar environments to mine.\nI hope you, the Moose developers can solve this somewhen if it is needed.\nThank you again, sincerely.",
                  "url": "https://github.com/idaholab/moose/discussions/25196#discussioncomment-6729818",
                  "updatedAt": "2023-08-15T11:39:26Z",
                  "publishedAt": "2023-08-15T11:38:50Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I m glad it worked, but it s pretty unconventional. The performance should take a hit and mpi (parallelism) might be buggy",
                          "url": "https://github.com/idaholab/moose/discussions/25196#discussioncomment-6730005",
                          "updatedAt": "2023-08-15T12:04:47Z",
                          "publishedAt": "2023-08-15T12:04:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I don't think we will be able to fix this, as this all seems to be related to the environment you are finding yourself in when logging into this HPC machine. Somehow, this machine seems to force a lot of \"special\" non-system paths into your environment.\nWith no modules loaded:\nMODULES:\nNo Modulefiles Currently Loaded.\nI see a whole bunch of non-system paths. The few that bother me the most:\nLD_LIBRARY_PATH=:/usr/local/lib/liblbfgs-1.10/lib/.libs:/home/common/anaconda3/lib/:/home/common/lib_common/gcc-9.3.0_server/lib:/home/common/lib_common/gcc-9.3.0_server/lib64:/home/common/lib_common/gcc-9.3.0_server/lib/gcc/x86_64-pc-linux-gnu/9.3.0:/home/common/lib_common/gcc-9.3.0_server/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/home/users/jjh2021/mambaforge3/lib:/home/users/jjh2021/mambaforge3/envs/moose/bin:/usr/local/lib/liblbfgs-1.10/lib/.libs:/home/common/anaconda3/lib/:/home/common/lib_common/gcc-9.3.0_server/lib:/home/common/lib_common/gcc-9.3.0_server/lib64:/home/common/lib_common/gcc-9.3.0_server/lib/gcc/x86_64-pc-linux-gnu/9.3.0:/home/common/lib_common/gcc-9.3.0_server/libexec/gcc/x86_64-pc-linux-gnu/9.3.0:/home/users/jjh2021/mambaforge3/lib:/home/users/jjh2021/mambaforge3/envs/moose/bin\n\nand\n\nPATH=/home/users/jjh2021/mambaforge3/envs/moose/bin:/home/users/jjh2021/mambaforge3/lib:/home/users/jjh2021/mambaforge3/bin:/home/common/lib_common/gcc-9.3.0_server/bin:/home/users/jjh2021/mambaforge3/envs/moose/bin:/home/users/jjh2021/mambaforge3/lib:/home/users/jjh2021/mambaforge3/bin:/home/common/lib_common/gcc-9.3.0_server/bin:/usr/bin\n\nThese two environment variable paths should line up appropriately. Basically, LD_LIBRARY_PATH is explaining to compilers to link to things found in the following paths from left to right, while PATH is using compilers found first from left to right.\nThere also appears to be Ananconda in here (Anaconda == Mambaforge). Having two \"Condas\" is a recipe for issues. But it appears to be working so far for you.\nWhile the new environment certainly looks cleaner, and free of Intel influence (except the license file but that is harmless), I still have concerns with it. And as @GiudGiud mentioned, using our Conda packages may work, but in a degraded state. Conda is simply not going to be optimized for any particular HPC cluster. Think of Conda as a one-size-fits-all solution. But if it working enough for your needs, then that certainly works for the rest of us :)\nAs for not being able to connect to https://bitbucket.org... that I can actually see being an issue in some locations. For a time, INL was blocking that service as well. I suspect that is what is happening here. If you continue to use our Conda packages, know that you do not need to build PETSc or libMesh.",
                          "url": "https://github.com/idaholab/moose/discussions/25196#discussioncomment-6730682",
                          "updatedAt": "2023-08-15T13:33:05Z",
                          "publishedAt": "2023-08-15T13:33:04Z",
                          "isAnswer": true
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "bosxered"
                          },
                          "bodyText": "I learned a lot thanks to you guys.\nSo far it is fine.",
                          "url": "https://github.com/idaholab/moose/discussions/25196#discussioncomment-6740456",
                          "updatedAt": "2023-08-16T12:40:31Z",
                          "publishedAt": "2023-08-16T12:40:30Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Extra Element ID but for nodes?",
          "author": {
            "login": "jessecarterMOOSE"
          },
          "bodyText": "Is this possible? Would be really convenient for defining field data for nodal auxkernels.",
          "url": "https://github.com/idaholab/moose/discussions/25219",
          "updatedAt": "2023-08-17T00:06:54Z",
          "publishedAt": "2023-08-15T21:51:43Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@roystgnr",
                  "url": "https://github.com/idaholab/moose/discussions/25219#discussioncomment-6735637",
                  "updatedAt": "2023-08-16T01:08:40Z",
                  "publishedAt": "2023-08-16T01:08:39Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "roystgnr"
                  },
                  "bodyText": "It's there in the libMesh internals; just MeshBase::add_node_integer instead of add_elem_integer (or add_node_datum to add something other than dof_id_type, or add_node_integers to add a bunch efficiently, or add_node_data).\nI don't think we have Exodus output support for it, though.  That was a hack Yaqi put in, I think he might only have put in the very specific case (element, integer, restricted range) he needed, and I've barely touched it since other than to make a couple use cases a little less precarious.",
                  "url": "https://github.com/idaholab/moose/discussions/25219#discussioncomment-6735690",
                  "updatedAt": "2023-08-16T01:23:33Z",
                  "publishedAt": "2023-08-16T01:23:27Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Quadrature point output",
          "author": {
            "login": "jessecarterMOOSE"
          },
          "bodyText": "I'm interested in saving material (quadrature point) data for postprocessing. Is there a supported mesh output format capable of storing quadrature point data? If not, is there a way to modify a VPP (like MaterialVectorPostprocessor) to apply to all elements?",
          "url": "https://github.com/idaholab/moose/discussions/25091",
          "updatedAt": "2023-08-15T21:54:16Z",
          "publishedAt": "2023-08-03T00:27:28Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "No luck on Qp output on meshes.\nModifying MaterialVPP to have the option to be either:\n\nblock restricted\non a list of elements (only thing we have now)\non all elements\n\nseems like a great thing for MOOSE",
                  "url": "https://github.com/idaholab/moose/discussions/25091#discussioncomment-6622170",
                  "updatedAt": "2023-08-03T00:34:54Z",
                  "publishedAt": "2023-08-03T00:34:53Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "Some day moose should support point cloud output format, and then we can write all material properties easily :)",
                          "url": "https://github.com/idaholab/moose/discussions/25091#discussioncomment-6622841",
                          "updatedAt": "2023-08-03T02:40:18Z",
                          "publishedAt": "2023-08-03T02:40:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "I opened an issue (#25093) but might try to do this myself. Couple of questions along those lines:\n\nCurrently the list of element ids comes from the user and gets saved as a set and looped over. How can I default that to a list of all element ids?\nHow can I also save the qp coordinates along with the material property data?",
                          "url": "https://github.com/idaholab/moose/discussions/25091#discussioncomment-6627171",
                          "updatedAt": "2023-08-03T12:13:41Z",
                          "publishedAt": "2023-08-03T12:13:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@hugary1995 we have the Positions system now so that could be the way to save the point cloud coordinates.\nWe dont have the output yet but I agree it would be good. Something like \"FunctionAtPositionsOutput\" would cover so many needs\nin fact this is where I m headed there:\n#24412\nbut I m not quite there\n@jessecarterMOOSE\nYou can make this non-const and build it in the constructor (from the mesh element list)\nFor 2, follow the same patter, first index by element id, then index by quadrature point index",
                          "url": "https://github.com/idaholab/moose/discussions/25091#discussioncomment-6627641",
                          "updatedAt": "2023-08-03T13:43:09Z",
                          "publishedAt": "2023-08-03T13:01:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "@jessecarterMOOSE You can make this non-const and build it in the constructor (from the mesh element list) For 2, follow the same patter, first index by element id, then index by quadrature point index\n\nYou make it sound so easy, but you may be overestimating my ability. Are there any examples of getting a list of element ids in the framework? And how do I get at qp coordinates from inside that VPP? I haven't developed VPP's before.",
                          "url": "https://github.com/idaholab/moose/discussions/25091#discussioncomment-6627931",
                          "updatedAt": "2023-08-03T13:25:16Z",
                          "publishedAt": "2023-08-03T13:25:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "If you use an ElementVPP (or a class deriving from ElementUO) you have the _q_point array that can be accessed at _qp to get quadrature points.\nDoing this may be easier actually than trying to modify the MaterialVPP...\nYou ll get block restriction etc already done well.\nIt's not clear to me actually what s best. I have not edited VPPs for that purpose before.\nto get a list of ids use this:\ngetMesh() on the MooseMesh (_mesh in the class probably) to get the libmesh meshbase\nthen\nthis routine to get a range of element on the blocks desired\nhttps://mooseframework.inl.gov/docs/doxygen/libmesh/classlibMesh_1_1MeshBase.html#ad3694fca532d45f4ad343ef66dfe2190",
                          "url": "https://github.com/idaholab/moose/discussions/25091#discussioncomment-6628174",
                          "updatedAt": "2023-08-03T13:48:27Z",
                          "publishedAt": "2023-08-03T13:44:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "Doing this may be easier actually than trying to modify the MaterialVPP...\n\nAh, so make a new VPP that contains all qp coords and merge that data with the Material VPP after-the-fact. Element and qp ids should match (right?) so that sounds reasonable.\n\nto get a list of ids use this: getMesh() on the MooseMesh (_mesh in the class probably) to get the libmesh meshbase then this routine to get a range of element on the blocks desired\n\nBut if I still want all elements, then I'd need to do an outer loop over all blocks? If so how do I get at all block ids? I'm assuming there's a method in that class for that...",
                          "url": "https://github.com/idaholab/moose/discussions/25091#discussioncomment-6628361",
                          "updatedAt": "2023-08-03T13:59:40Z",
                          "publishedAt": "2023-08-03T13:59:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "But if I still want all elements, then I'd need to do an outer loop over all blocks? If so how do I get at all block ids? I'm assuming there's a method in that class for that...\n\nthere is also a method that does all elements. There's both! MeshBase has all the methods.\nBut yeah, if you do a new class you could avoid these considerations by inheriting the right VPP class, I think",
                          "url": "https://github.com/idaholab/moose/discussions/25091#discussioncomment-6628415",
                          "updatedAt": "2023-08-03T14:04:54Z",
                          "publishedAt": "2023-08-03T14:04:54Z",
                          "isAnswer": true
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "jessecarterMOOSE"
                  },
                  "bodyText": "Added #25130 based on this discussion.",
                  "url": "https://github.com/idaholab/moose/discussions/25091#discussioncomment-6734654",
                  "updatedAt": "2023-08-15T21:48:28Z",
                  "publishedAt": "2023-08-15T21:48:27Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "FV convergence",
          "author": {
            "login": "heinono1"
          },
          "bodyText": "Hi (again). I am checking my FV version of Ferret. I think everything checks out insofar as the output seems to be correct, but the convergence seems to be much poorer compared to the FE version for the same test problem. In particular, the FV version takes many more nonlinear iterations (at smaller timesteps) than the FE version. I wonder if it is generally the case that FV converges more slowly than FE. If that should not be expected, I know I have more work to do ;-). I am using the NewmarkBeta TimeIntegrator both for FE and FV (by far the \"best\" one for this problem), and asm preconditioner.",
          "url": "https://github.com/idaholab/moose/discussions/25207",
          "updatedAt": "2023-08-15T19:05:03Z",
          "publishedAt": "2023-08-14T19:48:32Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "lindsayad"
                  },
                  "bodyText": "This is not expected. The first thing I would do is check your Jacobian. If those are fine, then we can consider other things. For one, I don't believe the community nor developers have spent a lot of time using time integration different than the default implicit Euler with FV. So I would not be 100% confident there aren't any issues there (good double negative)",
                  "url": "https://github.com/idaholab/moose/discussions/25207#discussioncomment-6724696",
                  "updatedAt": "2023-08-14T20:47:48Z",
                  "publishedAt": "2023-08-14T20:47:33Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Agree on both fronts.\n\nuse the Jacobian tester: https://mooseframework.inl.gov/help/development/analyze_jacobian.html\ntry implicit euler then bdf2 then Neumark. Do all your equations have a time derivative term?",
                          "url": "https://github.com/idaholab/moose/discussions/25207#discussioncomment-6724899",
                          "updatedAt": "2023-08-14T21:15:41Z",
                          "publishedAt": "2023-08-14T21:15:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "heinono1"
                          },
                          "bodyText": "Hi Alex and Guillaume. Jacobian was the first thing I was thinking of, too, but the FV system uses all AD, so I though I was off the hook on Jacobians....I can of course still use the Jacobian tester to see what comes out of that.\nFor the NewmarkBeta time integrator, we (John Mangeri and I) checked different time integrator for the FE version of ferret, and NewmarkBeta was hands down the best one. I have played with some other time integrators for the FV version and NewmarkBeta seems to perform best. But that may be somewhat irrelevant until I have fixed the convergence problem. Not all equations have time derivatives - there are three equations with time derivatives (for the different components of the vector magnetization director) but then also a Poisson equation that is solved simultaneously at each time step with the divergence of the magnetization as a source.",
                          "url": "https://github.com/idaholab/moose/discussions/25207#discussioncomment-6725607",
                          "updatedAt": "2023-08-14T22:23:56Z",
                          "publishedAt": "2023-08-14T22:23:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "AD should let you off the hook, but it's always good to have the testing confirmation.\nHave you checked that results are the same or are (mesh) converging to the same result between FE and FV? Even if the Jacobian is correct for a given set of residuals, if there are \"bugs\" in the residuals, then nonlinear convergence may be poor",
                          "url": "https://github.com/idaholab/moose/discussions/25207#discussioncomment-6725651",
                          "updatedAt": "2023-08-14T22:32:24Z",
                          "publishedAt": "2023-08-14T22:32:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You can accidentally loose the derivative still with AD if you relied on auxkernels or regular non-AD material properties somewhere. It should be fairly rare though, it s definitely easier to get it right with AD.\nSo you have a Poisson equation that is FV, and has NO time derivative?\n@lindsayad we had an issue for some schemes for the mass equation in NS that had no time derivative. Could be related?",
                          "url": "https://github.com/idaholab/moose/discussions/25207#discussioncomment-6727094",
                          "updatedAt": "2023-08-15T04:28:01Z",
                          "publishedAt": "2023-08-15T04:28:00Z",
                          "isAnswer": true
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "heinono1"
                          },
                          "bodyText": "Hi Alex and Guillaume. I started the jacobiandebugger last night. It is still running.\nThe Poisson Equation has no time derivatives. The nonlinear variables are the three Cartesian components of the magnetization director, and two scalar magnetostatic potentials, one for inside (phi1) the magnetic material, and one (phi2) for outside in the vacuum box. There are Dirichlet BC on phi2 on the boundaries of the vacuum box and at the boundary magnetic-vacuum I enforce continuity (phi1=phi2). I tried to use  FVTwoVarContinuityConstraint to enforce continuity but that did not work for reasons I don't understand: the code would execute but would end up with lambda~0 so and phi2=0 while phi1 was nonzero. So I wrote my own FVInterfaceKernel that does enforce continuity. I have checked that the solution to the Poisson equation for phi1 and phi2 is the same as for the FE version, and the jump discontinuity in derivatives of the magnetostatic potential(s) at the magnetic-vacuum interface is correct. The interface kernel is really simple, just\n#include \"FVMagneticInterfaceContinuity.h\"\n#include \"MathFVUtils.h\"\nregisterMooseObject(\"MooseApp\", FVMagneticInterfaceContinuity);\nInputParameters\nFVMagneticInterfaceContinuity::validParams()\n{\n  InputParameters params = FVInterfaceKernel::validParams();\n  params.addClassDescription(\"Computes the residual of magnetic potential across an interface for \"\n                             \"the finite volume method.\");\n  return params;\n}\nFVMagneticInterfaceContinuity::FVMagneticInterfaceContinuity(const InputParameters & params)\n  : FVInterfaceKernel(params)\n{\n}\nADReal\nFVMagneticInterfaceContinuity::computeQpResidual()\n{\n const auto prefactor = elemIsOne() ? 1 : -1.;\n return  prefactor *( var1().getBoundaryFaceValue(*_face_info, determineState()) -\n\t\t      var2().getBoundaryFaceValue(*_face_info, determineState()));\n}\n\nwhere var2 and var1 are phi2 and phi1, respectively. It seems to work but maybe it's causing some problems I don't know.\nI am running with FDP and lu pc_type. That does change some aspects of the convergence, but not much. The Poisson equation converges more rapidly (residuals of phi1 and phi2 go down to 10^-15 in one iteration) but the residuals of the three magnetic variables do not converge any more rapidly. Pending the completion of the jacobiandebugger it nevertheless seems like the Jacobians are OK as the FDP performs pretty much the same as SMP preconditioning. So this leads me to conclude there is not something not quite right with how I coded up the kernels. I'll keep on digging.\nI really appreciate your rapid responses and your help!\n-Olle",
                          "url": "https://github.com/idaholab/moose/discussions/25207#discussioncomment-6731204",
                          "updatedAt": "2023-08-16T01:41:16Z",
                          "publishedAt": "2023-08-15T14:33:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "heinono1"
                          },
                          "bodyText": "It appears that (at least one) problem was how I tried to calculate gradients of the components of the magnetization directors on element faces in an FVFluxKernel. I was using pattern recognition from some NS kernel and had\n  const auto & mag_x_interface = (has_elem && has_neighbor)\n    ? MetaPhysicL::raw_value(_mag_x.gradient(makeCDFace(*_face_info),state))\n    : MetaPhysicL::raw_value(_mag_x.gradient(\n    makeElemArg(has_elem ? &_face_info->elem() : _face_info->neighborPtr()),state));\n  const auto & mag_y_interface = (has_elem && has_neighbor)\n      ? MetaPhysicL::raw_value(_mag_y.gradient(makeCDFace(*_face_info),state))\n      : MetaPhysicL::raw_value(_mag_y.gradient(\n      makeElemArg(has_elem ? &_face_info->elem() : _face_info->neighborPtr()),state));\n  const auto & mag_z_interface = (has_elem && has_neighbor)\n      ? MetaPhysicL::raw_value(_mag_z.gradient(makeCDFace(*_face_info),state))\n      : MetaPhysicL::raw_value(_mag_z.gradient(\n      makeElemArg(has_elem ? &_face_info->elem() : _face_info->neighborPtr()),state));\n\nWhen I replace those lines with\n const auto & mag_x_interface = _mag_x.gradient(makeCDFace(*_face_info),state);\n const auto & mag_y_interface = _mag_y.gradient(makeCDFace(*_face_info),state);\n const auto & mag_z_interface = _mag_z.gradient(makeCDFace(*_face_info),state);\n\nthe nonlinear iterations converge as expected. I then tried to add a better interpolation of the gradients:\n const auto & mag_x_interface = (has_elem && has_neighbor)\n    ?_mag_x.gradient(makeCDFace(*_face_info),state)\n    : _mag_x.gradient(\n    makeElemArg(has_elem ? &_face_info->elem() : _face_info->neighborPtr()),state);\n  const auto & mag_y_interface = (has_elem && has_neighbor)\n      ? _mag_y.gradient(makeCDFace(*_face_info),state)\n      :_mag_y.gradient(\n      makeElemArg(has_elem ? &_face_info->elem() : _face_info->neighborPtr()),state);\n  const auto & mag_z_interface = (has_elem && has_neighbor)\n      ? _mag_z.gradient(makeCDFace(*_face_info),state)\n      : _mag_z.gradient(\n      makeElemArg(has_elem ? &_face_info->elem() : _face_info->neighborPtr()),state);\n\nwhich also converges with 2 or 3 nonlinear iterations.\nI am running a check now to see if the solutions converge to the same as the FE system that I use as benchmark.",
                          "url": "https://github.com/idaholab/moose/discussions/25207#discussioncomment-6731964",
                          "updatedAt": "2023-08-16T01:40:29Z",
                          "publishedAt": "2023-08-15T15:53:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Oh great news!",
                          "url": "https://github.com/idaholab/moose/discussions/25207#discussioncomment-6733040",
                          "updatedAt": "2023-08-15T18:06:41Z",
                          "publishedAt": "2023-08-15T18:06:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "heinono1"
                          },
                          "bodyText": "Looks like it checks out against the FE version :-)\nThanks for your help!",
                          "url": "https://github.com/idaholab/moose/discussions/25207#discussioncomment-6733435",
                          "updatedAt": "2023-08-15T19:01:25Z",
                          "publishedAt": "2023-08-15T19:01:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "\ud83c\udf89",
                          "url": "https://github.com/idaholab/moose/discussions/25207#discussioncomment-6733489",
                          "updatedAt": "2023-08-15T19:05:04Z",
                          "publishedAt": "2023-08-15T19:05:03Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Defining a Post Processor value that depends on a variable AND time/space?",
          "author": {
            "login": "richmondodufisan"
          },
          "bodyText": "I understand that the Function system is used for functions depending on time and space i.e f(x,y,z,t). In my post processor, I want to define a postprocessor value that is the integral of a variable (at the end of the timestep) multiplied by some function f(x,y,z,t) over a specific area/surface defined by a physical group/sideset. How would I do this? Since the function system can't take in variables. Would I need to create a whole new material source and header file and create an Aux variable? Or is there a way to do this in the input file?",
          "url": "https://github.com/idaholab/moose/discussions/25205",
          "updatedAt": "2023-08-15T15:33:01Z",
          "publishedAt": "2023-08-14T17:22:52Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "lindsayad"
                  },
                  "bodyText": "I think you can accomplish what you want with SideIntegralFunctorPostprocessor",
                  "url": "https://github.com/idaholab/moose/discussions/25205#discussioncomment-6723647",
                  "updatedAt": "2023-08-14T18:19:07Z",
                  "publishedAt": "2023-08-14T18:19:06Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nAlex is suggesting creating a Functor then using this postprocessor to integrate it.\ni would suggest you use either:\na ParsedMaterial then a postprocessor that integrates materials\na ParsedAux to fill an auxiliary variable then a postprocessor that integrates variables\nfor both you will need the expression that you wish to integrate, that can include variables or functions to be sufficiently simple that you can write them as a parsed expression\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/25205#discussioncomment-6724567",
                  "updatedAt": "2023-08-14T20:30:34Z",
                  "publishedAt": "2023-08-14T20:30:33Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "Thanks! That worked. I also discovered while using it that for some reason, it couldn't recognize \"pi\", which I found strange because typing pi worked fine in the function block. But I defined it as a constant in ParsedAux and that worked.",
                          "url": "https://github.com/idaholab/moose/discussions/25205#discussioncomment-6731774",
                          "updatedAt": "2023-08-15T15:33:01Z",
                          "publishedAt": "2023-08-15T15:33:00Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Question about ThinLayerHeatTransfer in Heat Conduction Module",
          "author": {
            "login": "richmondodufisan"
          },
          "bodyText": "\"The purpose of this kernel is to model heat transfer across a thin domain\". But I have a question on how it works.\nThe optional term \"heat_source\" is supposed to define a body heat source if it exists, correct? Just as ADMatHeatSource would do.\nHowever, if the heat source is on a surface, you would typically apply it as a boundary condition on the surface of the mesh defined by your nodeset/sideset.\nHow would you do so for the surface of this thin layer? To describe the system, I have a rectangular block which has a thin layer on top of it. The heat is applied to the top of the thin layer, and a boundary conductance is defined for the conductance in between the thin layer and the rectangular block.\nIf I was doing this with my own mesh (say the layer wasn't so thin), I would have defined a surface for the top of the first block, and define another surface for the interface between both blocks. I could then apply my heat source to the first layer and then define a conductance at the interface.\nWell, here, since my mesh does not include the thin layer, I can't define a surface for top of the thin layer. Instead, the top of my mesh is the lower rectangular block, and I already need to use that surface to specify the conductance.\nHow then, can I define the heat source applied at the surface of the thin layer?",
          "url": "https://github.com/idaholab/moose/discussions/25200",
          "updatedAt": "2023-08-15T13:45:21Z",
          "publishedAt": "2023-08-14T15:57:31Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "lindsayad"
                  },
                  "bodyText": "I believe @jiangwen84 created this object",
                  "url": "https://github.com/idaholab/moose/discussions/25200#discussioncomment-6723535",
                  "updatedAt": "2023-08-14T18:04:25Z",
                  "publishedAt": "2023-08-14T18:04:24Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "jiangwen84"
                  },
                  "bodyText": "\"The purpose of this kernel is to model heat transfer across a thin domain\". But I have a question on how it works.\nThe optional term \"heat_source\" is supposed to define a body heat source if it exists, correct? Just as ADMatHeatSource would do.\n\nYes, the heat_source term is for the volumetric heat source.\n\nHowever, if the heat source is on a surface, you would typically apply it as a boundary condition on the surface of the mesh defined by your nodeset/sideset.\n\nthrough the heat flux?\n\nHow would you do so for the surface of this thin layer? To describe the system, I have a rectangular block which has a thin layer on top of it. The heat is applied to the top of the thin layer, and a boundary conductance is defined for the conductance in between the thin layer and the rectangular block.\nIf I was doing this with my own mesh (say the layer wasn't so thin), I would have defined a surface for the top of the first block, and define another surface for the interface between both blocks. I could then apply my heat source to the first layer and then define a conductance at the interface.\n\nIf you explicitly mesh the thin layer, I do not think you need to use ThinLayerHeatTransfer. You can just assign the properties to the thin layer block.\n\nWell, here, since my mesh does not include the thin layer, I can't define a surface for top of the thin layer. Instead, the top of my mesh is the lower rectangular block, and I already need to use that surface to specify the conductance.\n\nIf you do not want to mesh the thin layer (no block defined for the thin layer), you probably can follow ThinLayerHeatTransfer to implement a thin layer heat transfer BC which only models the one side the the domain (v.s two blocks with interface inside ThinLayerHeatTransfer)\n\nHow then, can I define the heat source applied at the surface of the thin layer?",
                  "url": "https://github.com/idaholab/moose/discussions/25200#discussioncomment-6724792",
                  "updatedAt": "2023-08-14T20:59:59Z",
                  "publishedAt": "2023-08-14T20:59:58Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "through the heat flux?\n\nYes, the external heat source would be applied as a heat flux BC.\nYour suggestion sounds right, but I am not sure how to implement it given the syntax in the documentation. Where would I define the BC, i.e, the applied heat source? For example, see the interface kernel block below:\n[InterfaceKernels]\n  [thin_layer]\n    type = ThinLayerHeatTransfer\n    thermal_conductivity = \\#Thermal Conductivity of the Thin Layer\n    specific_heat = \\#Specific Heat Capacity of the Thin Layer\n    density = \\#Density of the Thin Layer\n    heat_source = \\#Volumetric Heat Source- in my case, leave this option out\n    thickness = \\#Thickness of the Thin Layer\n    variable = \"temp\" \\#Temperature, i.e the same variable used for the rest of the bulk\n    neighbor_var = \"temp\" \\#Temperature, i.e the variable used for the rest of the bulk\n    boundary = \"top_surface\" \\#The Top surface of my mesh\n  []\n[]\n\nIn the above example, I have used the \"top_surface\" physical group to define where to put the thin layer. Now, if I wanted to apply my heat flux BC (i.e, the applied heat source), I cannot use \"top_surface\" again. i.e:\n  [heat_source_term]\n    type = FunctionNeumannBC\n\tvariable = temp\n\tboundary = \\#Top of the Thin Layer. \n\tfunction = heat_source_function\n  []\n\nI tried setting boundary to \"thin_layer\" since that was the name of my interface kernel block, but that didn't work. How do I tell MOOSE to apply the Heat Source to the top of the thin layer?",
                          "url": "https://github.com/idaholab/moose/discussions/25200#discussioncomment-6730805",
                          "updatedAt": "2023-08-15T13:45:21Z",
                          "publishedAt": "2023-08-15T13:45:21Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Large mesh best practices and memory management",
          "author": {
            "login": "Eilloo"
          },
          "bodyText": "Hi all,\nI have a few questions about how best to deal with large meshes, as I have been running into errors which I think are to do with processes running out of memory.\nFor an idea of size, my mesh is around 5 million elements, and I am attempting to run my simulations across multiple nodes on an HPC.\n1. Mesh splitting:\nAs I understand it, this pre-splits the mesh into a fixed number of chunks for later use with distributed mesh. The number of chunks must match the number of processors the simulation will then be run on, and the idea is that you only need to store a small part of the mesh for the process to run. Ideal when the mesh is too large to fit into the memory.\nHowever, it also seems that you need to fit the whole mesh into memory in order to perform the splitting operations in the first place? If the mesh doesn't fit, you can't split it to alleviate that problem?\nIs the idea that you can usually fit the whole mesh into memory when it is just the mesh (ie, when performing the splitting), but during a simulation you need to allocate some of that memory to the nonlinear and aux systems, for instance, so you now can't afford to store the whole mesh?\nA further question is whether the mesh splitting operation somehow assumes that you will have the same amount of memory when you run the simulation as you did during splitting?\nThis is a tricky question to phrase, but the scenario is that I can fit a mesh one HPC node with a large amount of memory, and perform splitting operations there. However, if I try and use the resulting .cpr files on different HPC nodes with less memory, I run into mpi errors even if the number of processors is correct.\nI wondered if this is related to how the mesh is split in the first place.\n2. What is the difference between pre-splitting the mesh and setting --distributed-mesh, and setting parallel_type = distributed in the input file?\nUp to now, I had thought that setting parallel_type = distributed would automatically distribute the mesh across the number of processors you are running on.\nIs this the case, and if so, is the process behind the scenes any different than doing the splitting as a separate step?\n3. What about storing the nonlinear and auxiliary system variables?\nThe last thing to ask is whether the mesh is usually the memory bottleneck? I have a fairly large number of variables, and an especially large number of aux variables since I am extracting all my vector variable components due to #24193.\nWill these be distributed, or could this also be a problem? I wondered if this was the reason why I could successfully split a mesh, but mpi errors suggesting memory issues occur when I try and run the actual simulation.\nIn case it's useful, I usually get exit code 7 (bus error) when things don't work, and occasionally exit code 9 (killed) from mpirun.\n5 million elements doesn't seem completely unreasonable, so any insight into where I might be going wrong is greatly appreciated.\nThanks!",
          "url": "https://github.com/idaholab/moose/discussions/25053",
          "updatedAt": "2023-08-15T11:27:20Z",
          "publishedAt": "2023-07-28T14:08:33Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "For an idea of size, my mesh is around 5 million elements, and I am attempting to run my simulations across multiple nodes on an HPC.\n\nThis is a very reasonable size. How many degrees of freedom in nonlinear and aux systems (you can see this in the header printout)?\n\nAs I understand it, this pre-splits the mesh into a fixed number of chunks for later use with distributed mesh. The number of chunks must match the number of processors the simulation will then be run on, and the idea is that you only need to store a small part of the mesh for the process to run. Ideal when the mesh is too large to fit into the memory.\n\nThat's correct. You gain two things here by each processor only needing to load its small part of mesh (keep in mind that when loading from splits, you're forcing distributed mesh):\n\nEach processor loads a smaller file from disk, which is faster for startup than every process loading the same, huge, full mesh file\nYou save the memory of each process needing to know the full mesh at the beginning in order to do partitioning. If you don't use split mesh, each process will load the full mesh, do the same partitioning, and then delete elems that are not needed locally\n\n\nHowever, it also seems that you need to fit the whole mesh into memory in order to perform the splitting operations in the first place? If the mesh doesn't fit, you can't split it to alleviate that problem?\n\nYou should split the mesh only using a few processes. That is:\nmpiexec -n 4 /path/to/app.opt -i input.it --split-mesh 600\n\nEach process will still load the full mesh (so 4 copies of the mesh), and each will be responsible for saving 150 partitions in this case. As your mesh gets bigger, you can actually just run this in serial:\n/path/to/app.opt -i input.it --split-mesh 600\n\nand it'll take longer, but that's fine - it's only done once. As you get bigger and bigger meshes, most HPCs usually have high memory or visualization nodes. These are perfect for splitting meshes.\n\nIs the idea that you can usually fit the whole mesh into memory when it is just the mesh (ie, when performing the splitting), but during a simulation you need to allocate some of that memory to the nonlinear and aux systems, for instance, so you now can't afford to store the whole mesh?\n\nKind of. But, when you get into meshes that are millions of elements... in general we recommend pre splitting always so you don't have to even think about this problem.\n\nWhat is the difference between pre-splitting the mesh and setting --distributed-mesh, and setting parallel_type = distributed in the input file?\n\nLike I said above. When you don't pre split but use distributed mesh, the mesh is serialized (loaded fully) on all processes, partitioned on all processes, and then deleted. Thus, when you don't do pre split, at one point each process has the full mesh in memory. You can sometimes get away with this because this memory jump is done before adding systems/vectors/matrices/etc.\n\nThe last thing to ask is whether the mesh is usually the memory bottleneck? I have a fairly large number of variables, and an especially large number of aux variables since I am extracting all my vector variable components due to #24193. Will these be distributed, or could this also be a problem? I wondered if this was the reason why I could successfully split a mesh, but mpi errors suggesting memory issues occur when I try and run the actual simulation\n\nWhether or not you are replicated or distributed, vectors and matrices are always distributed. So - the overhead for this should not be different.\n\n5 million elements doesn't seem completely unreasonable, so any insight into where I might be going wrong is greatly appreciated.\n\nAs I asked above - can you share how many degrees of freedom you have in your problem? That would give us an idea on usage.",
                  "url": "https://github.com/idaholab/moose/discussions/25053#discussioncomment-6578013",
                  "updatedAt": "2023-07-28T16:50:00Z",
                  "publishedAt": "2023-07-28T16:49:19Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Eilloo"
                          },
                          "bodyText": "Thanks for the thorough explanation - that clears a lot up!\nI can indeed split the mesh on a high memory node in this case\nIn terms of dofs, we don't get as far as printing that header (this is using the split mesh in 1200 parts, although I have also tried using fewer splits in case too many boundaries is somehow a problem). The last reported step is 'Ghosting Ghosted Boundaries\".\nWe have 12 variables in the nonlinear system (counting vectors as 3), so around 60 million dofs(?). The auxiliary system is a similar order, 15 variables.",
                          "url": "https://github.com/idaholab/moose/discussions/25053#discussioncomment-6592297",
                          "updatedAt": "2023-07-31T09:14:37Z",
                          "publishedAt": "2023-07-31T08:11:39Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Eilloo"
                  },
                  "bodyText": "As a followup question to this, could someone confirm exactly what the reported memory use in the output file is telling us?\nI have been assuming that it is the memory used per process for each step, so we should multiply by the number of processors per node to get the total memory used per node (on a cluster). Is this correct?\nBelow is an example of the reporting I am referring to:\n  Initializing\n    Ghosting Ghosted Boundaries.                                                         [\ufffd[33m 18.59 s\ufffd[39m] [\ufffd[33m 2036 MB\ufffd[39m]\n    Updating Because Mesh Changed\n      Updating Mesh\n        Finished Building Boundary Elements List                                         [\ufffd[33m  2.67 s\ufffd[39m] [\ufffd[33m 2455 MB\ufffd[39m]\n      Finished Updating Mesh                                                             [\ufffd[33m 16.13 s\ufffd[39m] [\ufffd[33m 2490 MB\ufffd[39m]\n    Finished Updating Because Mesh Changed                                               [\ufffd[33m 16.37 s\ufffd[39m] [\ufffd[33m 2573 MB\ufffd[39m]\n    Updating Because Mesh Changed\n      Updating Mesh\n        Finished Building Boundary Elements List                                         [\ufffd[33m  2.59 s\ufffd[39m] [\ufffd[33m 2973 MB\ufffd[39m]\n      Finished Updating Mesh                                                             [\ufffd[33m 15.52 s\ufffd[39m] [\ufffd[33m 3008 MB\ufffd[39m]\n    Finished Updating Because Mesh Changed                                               [\ufffd[33m 15.74 s\ufffd[39m] [\ufffd[33m 3106 MB\ufffd[39m]\n    Initializing Equation Systems..                                                      [\ufffd[33m 21.55 s\ufffd[39m] [\ufffd[33m 3387 MB\ufffd[39m]\n    Initializing Displaced Equation System.                                              [\ufffd[33m 18.36 s\ufffd[39m] [\ufffd[33m 3622 MB\ufffd[39m]\n    Updating Because Mesh Changed\n      Updating Mesh                                                                      [\ufffd[33m 17.39 s\ufffd[39m] [\ufffd[33m 3664 MB\ufffd[39m]\n    Finished Updating Because Mesh Changed                                               [\ufffd[33m 17.60 s\ufffd[39m] [\ufffd[33m 3664 MB\ufffd[39m]\n  Finished Initializing                                                                  [\ufffd[33m108.22 s\ufffd[39m] [\ufffd[33m 3664 MB\ufffd[39m]\n\nAnd:\nCurrently Executing\n  Performing Initial Setup\n    Updating Geometric Search\n      Finding Nearest Nodes........                                                      [\ufffd[33m 49.86 s\ufffd[39m] [\ufffd[33m 3666 MB\ufffd[39m]\n      Updating Displaced GeometricSearch\n        Finding Nearest Nodes........                                                    [\ufffd[33m 48.65 s\ufffd[39m] [\ufffd[33m 3702 MB\ufffd[39m]\n      Finished Updating Displaced GeometricSearch                                        [\ufffd[33m 48.65 s\ufffd[39m] [\ufffd[33m 3702 MB\ufffd[39m]\n    Finished Updating Geometric Search                                                   [\ufffd[33m 98.51 s\ufffd[39m] [\ufffd[33m 3702 MB\ufffd[39m]\n    Reinitializing Because of Geometric Search Objects...............                    [\ufffd[33m 88.36 s\ufffd[39m] [\ufffd[33m 4368 MB\ufffd[39m]\n    Building SemiLocalElemMap.....                                                       [\ufffd[33m 36.10 s\ufffd[39m] [\ufffd[33m 4335 MB\ufffd[39m]\nlibMesh terminatinglibMesh terminating:",
                  "url": "https://github.com/idaholab/moose/discussions/25053#discussioncomment-6627981",
                  "updatedAt": "2023-08-03T13:29:43Z",
                  "publishedAt": "2023-08-03T13:29:42Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "It's roughly correct. It s the instantaneous current use, not the specific \"step use\"\nIt s not the most accurate thing because it only reports process 0, not the average use over all processes",
                          "url": "https://github.com/idaholab/moose/discussions/25053#discussioncomment-6628140",
                          "updatedAt": "2023-08-03T13:41:56Z",
                          "publishedAt": "2023-08-03T13:41:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Eilloo"
                          },
                          "bodyText": "Ok, thanks for clarifying.\nI am suspicious that something is being replicated when it shouldn't be to be reaching such large memory usage numbers... using the approximation of a minimum from 20022 we get around 21Gb. I gather the actual requirements can still be much higher depending on preconditioner choice and materials, but this is many orders of magnitude below the total memory available in a parallel run (ie, summing the memory across all nodes).\nDo you know of any way in MOOSE to effectively debug this kind of thing? Since the simulation is being killed during setup, I'm not sure I can make use of the memory usage and performance post-processors?",
                          "url": "https://github.com/idaholab/moose/discussions/25053#discussioncomment-6678794",
                          "updatedAt": "2023-08-09T10:20:37Z",
                          "publishedAt": "2023-08-09T10:20:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "you can use memory profiling to keep track of memory requirements\nlook at heap profiling here\nhttps://mooseframework.inl.gov/application_development/profiling.html\nThis will point you to the code that allocates large amounts of memory",
                          "url": "https://github.com/idaholab/moose/discussions/25053#discussioncomment-6682495",
                          "updatedAt": "2023-08-09T16:30:29Z",
                          "publishedAt": "2023-08-09T16:30:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Eilloo"
                          },
                          "bodyText": "Perfect, thanks - I'll look into this",
                          "url": "https://github.com/idaholab/moose/discussions/25053#discussioncomment-6688614",
                          "updatedAt": "2023-08-10T08:32:35Z",
                          "publishedAt": "2023-08-10T08:32:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Eilloo"
                          },
                          "bodyText": "Sorry to reopen an answered thread - I have one more question related to this:\n@loganharbour, when you say the vectors and matrices are always distributed, do you know what the process for splitting is?\nThrough some playing around, it seems as though the process is analogous to using distributed mesh without pre-splitting. In other words, is memory allocated on every processor for the whole system, and then parts which are not needed by each process are deleted?\nThe reason it seems this way is that I stop running into memory issues if I run across fewer processes, so that each one has more memory available to it. Apart from ghosting, if everything was pre-split, this wouldn't make a difference - for instance, halving the number of processors would double the memory available to each, but also double the amount stored on each.\nIt therefore seems like the matrices are all replicated at first, or perhaps something in between. If this is true, is there any pre-splitting type operation that can be done?",
                          "url": "https://github.com/idaholab/moose/discussions/25053#discussioncomment-6699983",
                          "updatedAt": "2023-08-11T09:44:53Z",
                          "publishedAt": "2023-08-11T09:44:52Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "@loganharbour, when you say the vectors and matrices are always distributed, do you know what the process for splitting is?\n\nThe way this is done is by partitioning, for which how this process done is equivalent for a given processor configuration (# procs), regardless of distributed or replicated mesh. This partitionining assigns nodal and elemental values in the mesh to a particular processor. Once this process is done, this denotes who owns what part of the vectors and matrices in the problem.\nWhen the vectors and matrices are allocated (regardless of distributed or replicated mesh), they are still distributed. If you do not need ghosting for a vector or matrix, each processor will only allocate the degrees of freedom that it owns. If you need some ghosting, you'll have a bit more allocation here in order to have space for the entries that you need that other processors own.\n\nThe reason it seems this way is that I stop running into memory issues if I run across fewer processes, so that each one has more memory available to it.\n\nThis is likely because you have more available memory on a compute node, no? This is a common process. Say you have a compute node that has 256GB of memory a piece and 48 procs. If you request this whole node, you get 256 / 48 = 5.3 GB / proc. If you request the whole node but only use half the procs, you get 256 / 24 = 10.6GB / proc. That is - when you have a memory constrained problem, we often request fewer processors but request a whole node so that we have more memory available.\nAre you sure that you're not just resolving your problems because you have more memory available?\n\nIt therefore seems like the matrices are all replicated at first, or perhaps something in between. If this is true, is there any pre-splitting type operation that can be done?\n\nThis isn't the case. The matricies and vectors (aside from ghosting) really are allocated in a distributed sense. With pre-split mesh, for a standard problem, there should be very little replicated data.\nHowever, there is a chance that you could be using an algorithm or some capability that doesn't support distribution and does some replication. Can you share what you're trying to run?",
                  "url": "https://github.com/idaholab/moose/discussions/25053#discussioncomment-6702977",
                  "updatedAt": "2023-08-11T15:26:50Z",
                  "publishedAt": "2023-08-11T15:26:49Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "There's also some overhead to ghosting variables & the mesh (one layer on the edge of every domain by default) that does go down as the number of processes is reduced (and removed entirely in serial)",
                          "url": "https://github.com/idaholab/moose/discussions/25053#discussioncomment-6703636",
                          "updatedAt": "2023-08-11T16:44:41Z",
                          "publishedAt": "2023-08-11T16:44:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Eilloo"
                          },
                          "bodyText": "This is interesting - it does indeed sound like most of these issues are resolved due to having more memory available. However, I am not clear about why, say, half the number of processors with double the memory is any better overall from a memory point of view?\nTo use an over-simplified example, suppose the matrices in your problem take up 8Gb of memory; one compute node has 4Gb of memory in total, and each compute node has 2 processors. If you request two nodes with two processors per node, the matrices would be split into four 2Gb parts and each processor would allocate only values on its part.\nHowever, if we request two nodes but only one processor per node, the matrices are split into two 4Gb parts instead, so even though there is more memory per processor, the total is always the same.\nI am sure I am missing something key here - apologies if this is a silly question, but hopefully the example highlights where I am going wrong!",
                          "url": "https://github.com/idaholab/moose/discussions/25053#discussioncomment-6728135",
                          "updatedAt": "2023-08-15T07:56:53Z",
                          "publishedAt": "2023-08-15T07:56:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "The total isn\u2019t quite the same exactly because of the cost of ghosting\n(And the distribution of the mesh)\nThere s also some marginal cost from creating all the objects in each process.\nthe key here is to measure. And probably make a table for the cost of each part of the simulation for 1,2,3\u2026 processes",
                          "url": "https://github.com/idaholab/moose/discussions/25053#discussioncomment-6729730",
                          "updatedAt": "2023-08-15T11:27:20Z",
                          "publishedAt": "2023-08-15T11:27:20Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}