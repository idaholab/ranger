{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMi0xMi0wN1QxNDoxNzo0OC0wNzowMM4ARsHn"
    },
    "edges": [
      {
        "node": {
          "title": "How to reduce the memory required to run\uff1f",
          "author": {
            "login": "DYLDYLDYL"
          },
          "bodyText": "Hi\uff01\nMy laptop's memory is 16G, and the program to run needs 17G.  I want to test it on the laptop first, and then run it on the server.  Is there any way to reduce the memory required to run\uff1f\nThanks in advance for your help!",
          "url": "https://github.com/idaholab/moose/discussions/22618",
          "updatedAt": "2023-01-03T23:21:02Z",
          "publishedAt": "2022-11-08T07:28:25Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nIf you are running in parallel, switch to serial to reduce memory.\nYou should remove any auxiliary variable that you dont really need as they cost memory.\nIf you can, the easiest solution is to run with a coarser mesh.\nAnother thing to improve is the preconditioning. What are you using right now?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/22618#discussioncomment-4086693",
                  "updatedAt": "2022-11-08T13:18:57Z",
                  "publishedAt": "2022-11-08T13:18:57Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "DYLDYLDYL"
                          },
                          "bodyText": "Thanks\uff01\nThe distribution of memory is as follows\n  Finished Reading Mesh                                                       [  0.34 s] [  132 MB]\n  Finished Setting Mesh                                                         [  0.61 s] [  244 MB]\n  Setting Up Displaced Mesh\n  Finished Preparing Mesh                                                    [  0.64 s] [  108 MB]\n  Finished Setting Up Displaced Mesh                                  [  0.64 s] [  108 MB]\n  Finished Initializing Equation Systems                                [  2.02 s] [  751 MB]\n  Finished Initializing Displaced Equation System                 [  0.49 s] [  215 MB]\n  Finished Initializing                                                              [  4.16 s] [  995 MB]\n  Finished Setting Up                                                             [  6.33 s] [ 1511 MB]\n  Finished Projecting Initial Solutions                                     [  6.18 s] [    0 MB]\n  Finished Computing Initial Material Values                          [  8.42 s] [ 4204 MB]\n  Finished Setting Up Materials                                              [  8.42 s] [ 4204 MB]\n  Updating Geometric Search\n  Updating Displaced GeometricSearch\n  Finding Nearest Nodes                                                                [  6.81 s] [  277 MB]\n  Finished Updating Displaced GeometricSearch                          [  6.81 s] [  277 MB]\n  Finished Updating Geometric Search                                          [  6.81 s] [  277 MB]\n  Reinitializing Because of Geometric Search Objects..                 [ 16.63 s] [  361 MB]\n\nI have only two auxiliary variables coupled time.\nMy mesh has only 27w elems.\nPrecondition:\n  petsc_options_iname = '-pc_type -pc_factor_shift_type -pc_factor_mat_solver_type'\n  petsc_options_value = 'lu NONZERO superlu_dist'\n  line_search = none\n\nAnd I wonder whether the simulation results and convergence ability are affected by not using preprocessing.\nThanks in advance for your help!",
                          "url": "https://github.com/idaholab/moose/discussions/22618#discussioncomment-4092380",
                          "updatedAt": "2022-11-09T01:52:47Z",
                          "publishedAt": "2022-11-09T01:51:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Preconditioning is usually necessary.\nYou can try some less-memory consuming forms, like Jacobi or block jacobi.\nYou can try to use non-full Jacobian preconditioning with PJFNK that will also consume less.\nI see 4GB in materials. I imagine they are stateful materials because we save the initial value.\nAre you outputting the material properties as well? That costs memory",
                          "url": "https://github.com/idaholab/moose/discussions/22618#discussioncomment-4101292",
                          "updatedAt": "2022-11-09T21:01:09Z",
                          "publishedAt": "2022-11-09T21:01:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "DYLDYLDYL"
                          },
                          "bodyText": "Thanks!\nI don't need to output the material properties,  and I wonder how to set it.\nMy [output] is only exodus = true.",
                          "url": "https://github.com/idaholab/moose/discussions/22618#discussioncomment-4104687",
                          "updatedAt": "2022-11-10T07:10:25Z",
                          "publishedAt": "2022-11-10T07:10:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "to output material properties you add output = in the Materials block.\nIf all the other options dont work, I would reduce the mesh for this local computation",
                          "url": "https://github.com/idaholab/moose/discussions/22618#discussioncomment-4107429",
                          "updatedAt": "2022-11-10T13:18:29Z",
                          "publishedAt": "2022-11-10T13:18:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "DYLDYLDYL"
                          },
                          "bodyText": "Thanks! I will try it.",
                          "url": "https://github.com/idaholab/moose/discussions/22618#discussioncomment-4122409",
                          "updatedAt": "2022-11-12T07:32:02Z",
                          "publishedAt": "2022-11-12T07:32:01Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "friedmud"
                  },
                  "bodyText": "Did you get this going?  Most-likely you were running out of memory due to using LU.  As @GiudGiud mentioned you might do better using asm or bjacobi.\nAnother thing to look into is running with distributed mesh: https://mooseframework.inl.gov/syntax/Mesh/splitting.html",
                  "url": "https://github.com/idaholab/moose/discussions/22618#discussioncomment-4361883",
                  "updatedAt": "2022-12-10T07:37:35Z",
                  "publishedAt": "2022-12-10T07:37:34Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "DYLDYLDYL"
                          },
                          "bodyText": "Thanks\uff01I found that the excessive memory usage was caused by my unreasonable mesh, and it has been resolved now\uff01",
                          "url": "https://github.com/idaholab/moose/discussions/22618#discussioncomment-4362229",
                          "updatedAt": "2022-12-10T08:38:30Z",
                          "publishedAt": "2022-12-10T08:38:29Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Tracking motion of mass particles suspended in a fluid",
          "author": {
            "login": "sb50nemo"
          },
          "bodyText": "Dear MOOSE team,\nI have an array of mass particles suspended in a fluid. Their position is changing with every time step owing to the force field. The transient positions of those particles are stored in a vector format, something like the following:\n$$\n\\begin{align}\n\\mathrm{X}[i, t] \\\\\n\\mathrm{Y}[i, t] \\\\\n\\mathrm{Z}[i, t] \\\\\n\\end{align}\n$$\nwhere $i$ is the index of the particle and $t$ stands for the timestep. Such a system with 4 particles can be seen in the screenshot below, where the 4 particles change their position with every timestep. The transient positions of these particles will be stored as explained above, $\\mathrm{X}[{i =1..4}, t={1..3} ], \\mathrm{Y}[{i =1..4}, t={1..3} ], \\mathrm{Z}[{i =1..4}, t={1..3} ]$\n\nI need your kind help in visualising the trajectory of these particles as shown in the following screenshot. Could you please help me by suggesting a tool or method to visualise these transient trajectories in MOOSE?",
          "url": "https://github.com/idaholab/moose/discussions/22504",
          "updatedAt": "2023-01-03T23:52:28Z",
          "publishedAt": "2022-10-26T13:23:35Z",
          "category": {
            "name": "Q&A Tools"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nAnd you are using the ray tracing module to do that?\nHave you tried these outputs if you are:\nhttps://mooseframework.inl.gov/source/outputs/RayTracingExodus.html ?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/22504#discussioncomment-3970079",
                  "updatedAt": "2022-10-26T13:28:11Z",
                  "publishedAt": "2022-10-26T13:28:11Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "sb50nemo"
                          },
                          "bodyText": "Hello Guillaume,\nThank you for your response. Actually, RayTracingExodus was my preferred choice to output the trajectories.\nBut as I have been advised by you and Derek, I need to write a RayKernel for changing the ray trajectory in discussion #21283, I am unable to figure out how to implement that.\nI was wondering if you could help me or guide me on how to change the trajectory using a RayKernel.\nI checked this resource for RayKernels (https://mooseframework.inl.gov/syntax/RayKernels/index.html).\nconst Point some_point(1, 2, 3); // must be within _current_elem!\nconst Point some_direction(1, 0, 0);\nchangeRayStartDirection(some_point, some_direction);\n\nI am puzzled as to how this can be implemented for my current scheme where the transient positions are stored in form of arrays.\nI would appreciate your help on this. Thank you.",
                          "url": "https://github.com/idaholab/moose/discussions/22504#discussioncomment-3970285",
                          "updatedAt": "2022-10-26T13:46:57Z",
                          "publishedAt": "2022-10-26T13:46:56Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You would have to treat the particles one by one, the ray kernels act on a single ray not multiple.\nThese 3 lines would be fine to initialize the trajectory of one particle.",
                          "url": "https://github.com/idaholab/moose/discussions/22504#discussioncomment-3982107",
                          "updatedAt": "2022-10-27T13:50:42Z",
                          "publishedAt": "2022-10-27T13:50:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "@oanaoana this made me think of you",
                          "url": "https://github.com/idaholab/moose/discussions/22504#discussioncomment-3986952",
                          "updatedAt": "2022-10-27T23:05:36Z",
                          "publishedAt": "2022-10-27T23:05:35Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "any update on this?",
                          "url": "https://github.com/idaholab/moose/discussions/22504#discussioncomment-4149431",
                          "updatedAt": "2022-11-15T18:20:45Z",
                          "publishedAt": "2022-11-15T18:20:45Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "friedmud"
                          },
                          "bodyText": "Pinging again to see if you have an update on this.",
                          "url": "https://github.com/idaholab/moose/discussions/22504#discussioncomment-4361861",
                          "updatedAt": "2022-12-10T07:34:30Z",
                          "publishedAt": "2022-12-10T07:34:30Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to couple 2D heat conduction equations with 1D fluid conservation equations?",
          "author": {
            "login": "Raven-pro"
          },
          "bodyText": "Hello everyone!\nI have developed a single-phase flow application based on three conservation equations. And I want to couple solid heat conduction equations with it to achieve conjugate heat transfer.\nAs the flow model is 1D, for each node of the channel in, like z direction, there should be a solid heat conduction equation to transfer heat in the x direction.\nSo this is a problem of coupling a 2D heat conduction equation and 1D fluid conservation equations, as I should use the temperature of the boundary of the heat structure to calculate heat transfer from solid to fluid.\nHow can I achieve it? THX!",
          "url": "https://github.com/idaholab/moose/discussions/22905",
          "updatedAt": "2023-01-03T23:56:58Z",
          "publishedAt": "2022-12-07T14:58:46Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou will want to use heat structures for now, see the tutorial\nhttps://mooseframework.inl.gov/docs/site/modules/thermal_hydraulics/tutorials/single_phase_flow/step02.html#step-2-conjugate-heat-transfer\nIn a few months we ll be able to run other modules (such a multi-D heat conduction using the heat conduction module) in combination with THM, we ll keep you posted.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/22905#discussioncomment-4334976",
                  "updatedAt": "2022-12-07T16:39:53Z",
                  "publishedAt": "2022-12-07T16:39:10Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "friedmud"
                  },
                  "bodyText": "You can put something together yourself pretty easily.\nUse GapValueAux in each direction to get the temperature on the other side of the gap in each direction... then simply apply CoupledConvectiveHeatFlux on both sides.\nNow - this won't be perfectly conservative... but with a reasonable mesh it will work well enough (it's what we used for many years before we got the more sophisticated schemes in GapHeatTransfer",
                  "url": "https://github.com/idaholab/moose/discussions/22905#discussioncomment-4361566",
                  "updatedAt": "2022-12-10T06:56:42Z",
                  "publishedAt": "2022-12-10T06:56:42Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Interface setting on CUBIT mesh",
          "author": {
            "login": "styyokuda"
          },
          "bodyText": "All,\nI have a mesh generated by CUBIT (please see the attached sketch).\nI would like to add interfaces on the left, right, top, and bottom sides of Block 2.\nAll I know is to use \"SideSetsBetweenSubdomainsGenerator\" which generates one interface around Block 2.\nCUBIT generated SideSets for these left, right, top and bottom boundaries.\nPlease help me to generate separately the interfaces on the left, right, top, and bottom sides of Block 2.\nSincerely,\nS. Thomas\nInterfaces_two_variables_two_blocks.pdf",
          "url": "https://github.com/idaholab/moose/discussions/22757",
          "updatedAt": "2022-12-10T06:27:59Z",
          "publishedAt": "2022-11-19T01:17:51Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou may use the SidesetsAroundSubdomainGenerator with a normal parameter since the sidesets are all with a different normal.\nhttps://mooseframework.inl.gov/moose/source/meshgenerators/SideSetsAroundSubdomainGenerator.html\nYou will need to use as many of these objects, chained using the input parameter, as there are sidesets to add.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/22757#discussioncomment-4191821",
                  "updatedAt": "2022-11-21T04:01:00Z",
                  "publishedAt": "2022-11-21T04:01:00Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "styyokuda"
                          },
                          "bodyText": "Hi Guillaume,\nI am using \"InterfaceDiffusion\" of \"InterfaceKernels\" whose interface is generated by \"SideSetsBetweenSubdomainsGenerator\" (test/tests/interfacekernels/hybrid/interface.i).\n\"SidesetsAroundSubdomainGenerator\" seems to generate the same interface as \"SideSetsBetweenSubdomainsGenerator\" does.\nPlease confirm that is true.\nSincerely,\nS. Thomas",
                          "url": "https://github.com/idaholab/moose/discussions/22757#discussioncomment-4326811",
                          "updatedAt": "2022-12-06T20:35:56Z",
                          "publishedAt": "2022-12-06T20:35:56Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "you ll have to use the normal parameter to distinguish each surface",
                          "url": "https://github.com/idaholab/moose/discussions/22757#discussioncomment-4326936",
                          "updatedAt": "2022-12-06T20:56:12Z",
                          "publishedAt": "2022-12-06T20:56:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "styyokuda"
                          },
                          "bodyText": "Thank you, Guillaume.\nBoth \"SidesetsAroundSubdomainGenerator\" and \"SideSetsBetweenSubdomainsGenerator\" produce the same results for the interface model.\nS. Thomas",
                          "url": "https://github.com/idaholab/moose/discussions/22757#discussioncomment-4358735",
                          "updatedAt": "2022-12-10T00:27:52Z",
                          "publishedAt": "2022-12-10T00:27:52Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Error after updating Moose",
          "author": {
            "login": "joe61vette"
          },
          "bodyText": "Hello:\nDue to a \"missing symbol\" problem with my app, I decided to eliminate some possibilities and so updated my Moose environment and Moose itself using the instructions from Getting Started.  Namely:\nconda activate moose\nconda update --all\n\ncd ~/projects/moose\ngit fetch origin\ngit rebase origin/master\n\nThen, I tried to run the tests.  I got more than 50 failures, and so ran\n\ngit clean -xfd\n\nUpon trying to run the tests, I then got this error:\n\n(moose) moose % cd test \n(moose) test % ./run_tests -j12\nBuilding and linking hit...\nmake[1]: Entering directory '/Users/joe/projects/moose/framework/contrib/hit'\nBuilding hit for python with python3-config\nmpicxx -std=c++17 -g -march=core2 -mtune=haswell -mssse3 -ftree-vectorize -fPIC -fPIE -fstack-protector-strong -O2 -pipe -stdlib=libc++ -fvisibility-inlines-hidden -fmessage-length=0 -isystem /Users/joe/mambaforge3/envs/moose/include -std=c++17 main.cc parse.cc lex.cc braceexpr.cc -o hit\nmake[1]: Leaving directory '/Users/joe/projects/moose/framework/contrib/hit'\nError! Could not find libmesh_config.h in any of the usual locations!\n\nThis is on an intel Mac, running 12.6.1.\nHelp please!  Thanks,\nJoe Kelly",
          "url": "https://github.com/idaholab/moose/discussions/22920",
          "updatedAt": "2022-12-09T18:33:57Z",
          "publishedAt": "2022-12-08T19:37:57Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hi Joe\nI'd be worried that git clean -xfd removed the moose install. Did you remake moose after updating libmesh?\nThe missing libmesh_config.h is pointing to a missing libmesh but that could be a red herring.\nI would also check the version of libmesh with\n mamba list | grep moose\nthen I would rebuild moose in moose/test\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4346390",
                  "updatedAt": "2022-12-08T19:42:47Z",
                  "publishedAt": "2022-12-08T19:42:46Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "joe61vette"
                  },
                  "bodyText": "Hi Guillaume:\nAfter running 'git clean -xfd' (as suggested in another discussion), I then went to /test and entered ./run_tests.  So I did not explicitly enter the make command.\nAlso:\n(moose) test % mamba list | grep moose\n# packages in environment at /Users/joe/mambaforge3/envs/moose:\nmoose-libmesh             2022.10.05              build_0    https://conda.software.inl.gov/public\nmoose-libmesh-vtk         9.1.0               he71d177_12    https://conda.software.inl.gov/public\nmoose-mpich               4.0.2                   build_2    https://conda.software.inl.gov/public\nmoose-petsc               3.16.6                  build_1    https://conda.software.inl.gov/public\nmoose-tools               2022.07.18      py310hcffbc79_0    https://conda.software.inl.gov/public\nThanks for the help,\nJoe",
                  "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4346435",
                  "updatedAt": "2022-12-08T21:32:37Z",
                  "publishedAt": "2022-12-08T19:48:47Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "The moose-libmesh and moose-tools are a little on the older side. I have:\nmoose-libmesh             2022.11.07              build_0    https://conda.software.inl.gov/public\nmoose-libmesh-vtk         9.1.0               h829a923_14    https://conda.software.inl.gov/public\nmoose-mpich               4.0.2                   build_4    https://conda.software.inl.gov/public\nmoose-petsc               3.16.6                  build_3    https://conda.software.inl.gov/public\nmoose-tools               2022.11.07      py310ha2ead4d_0    https://conda.software.inl.gov/public\n\nit could be that the update is failing, in which case you will need to rmeove the mamba environment and try again\nFor now, can you please try to make in moose/test and try the tests again",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4346459",
                          "updatedAt": "2022-12-08T19:51:39Z",
                          "publishedAt": "2022-12-08T19:51:38Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "joe61vette"
                  },
                  "bodyText": "Thanks Guillaume.  That fixed the problem with not being able to run the Moose tests.\nI then went back and ran 'mamba update --all' and now have:\nmoose-libmesh             2022.11.07              build_0    https://conda.software.inl.gov/public\nmoose-libmesh-vtk         9.1.0               hf2274b2_14    https://conda.software.inl.gov/public\nmoose-mpich               4.0.2                   build_4    https://conda.software.inl.gov/public\nmoose-petsc               3.16.6                  build_3    https://conda.software.inl.gov/public\nmoose-tools               2022.11.07      py310hcffbc79_0    https://conda.software.inl.gov/public\nSo, should I rerun 'git clean -xfd' in my moose directory now that I have successfully updated the environment?\nJoe",
                  "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4346541",
                  "updatedAt": "2022-12-08T21:32:57Z",
                  "publishedAt": "2022-12-08T20:04:05Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "not every time really, only when you run into compilation issues.",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4346551",
                          "updatedAt": "2022-12-08T20:04:44Z",
                          "publishedAt": "2022-12-08T20:04:43Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "joe61vette"
                  },
                  "bodyText": "So, now back to my original problem.  Just let me know if this should be a new issue.\nIn my app, I used 'git clean -xfd' and rebuilt.  I still get the dreaded \"dyld[75408]: missing symbol called\" when I try to run a test problem.\nIs there any way to find what symbol?\nThanks,\nJoe",
                  "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4346616",
                  "updatedAt": "2022-12-08T20:13:53Z",
                  "publishedAt": "2022-12-08T20:13:52Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "is this all you get?\nIs this in opt mode or debug mode?\nSince you git cleaned the rational explanation for that is now that there s a missing implementation in a class",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4346646",
                          "updatedAt": "2022-12-08T20:18:34Z",
                          "publishedAt": "2022-12-08T20:18:34Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "joe61vette"
                  },
                  "bodyText": "It was in opt mode.  And sadly, that is all I get.  I ran in dbg mode earlier and didn't see anything useful either.\nI'll go back and check every class for a missing function.  Likewise, I did an 'otool -L' and saw nothing meaningful.  Should I try it with dbg again?\nThanks for the help,\nJoe",
                  "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4346667",
                  "updatedAt": "2022-12-08T20:21:41Z",
                  "publishedAt": "2022-12-08T20:21:40Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@lindsayad thoughts about this?\nCan you please paste the otool -L output?",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4346677",
                          "updatedAt": "2022-12-08T20:23:57Z",
                          "publishedAt": "2022-12-08T20:23:56Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@milljm",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4346679",
                          "updatedAt": "2022-12-08T20:24:14Z",
                          "publishedAt": "2022-12-08T20:24:14Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "joe61vette"
                  },
                  "bodyText": "Here is the output:\n/Users/joe/projects/ASP/asp-opt:\n\t/Users/joe/projects/ASP/test/lib/libasp_test-opt.0.dylib (compatibility version 1.0.0, current version 1.0.0)\n\t/Users/joe/projects/ASP/lib/libasp-opt.0.dylib (compatibility version 1.0.0, current version 1.0.0)\n\t/Users/joe/projects/moose/modules/module_loader/lib/libmodule_loader_with_fp_ray_rdg_hc-opt.0.dylib (compatibility version 1.0.0, current version 1.0.0)\n\t/Users/joe/projects/moose/modules/heat_conduction/lib/libheat_conduction-opt.0.dylib (compatibility version 1.0.0, current version 1.0.0)\n\t/Users/joe/projects/moose/modules/rdg/lib/librdg-opt.0.dylib (compatibility version 1.0.0, current version 1.0.0)\n\t/Users/joe/projects/moose/modules/ray_tracing/lib/libray_tracing-opt.0.dylib (compatibility version 1.0.0, current version 1.0.0)\n\t/Users/joe/projects/moose/modules/fluid_properties/lib/libfluid_properties-opt.0.dylib (compatibility version 1.0.0, current version 1.0.0)\n\t/Users/joe/projects/moose/framework/libmoose-opt.0.dylib (compatibility version 1.0.0, current version 1.0.0)\n\t/Users/joe/projects/moose/framework/contrib/pcre/libpcre-opt.0.dylib (compatibility version 1.0.0, current version 1.0.0)\n\t/Users/joe/projects/moose/framework/contrib/hit/libhit-opt.0.dylib (compatibility version 1.0.0, current version 1.0.0)\n\t@rpath/../libmesh/lib/libmesh_opt.0.dylib (compatibility version 1.0.0, current version 1.0.0)\n\t@rpath/../libmesh/lib/libtimpi_opt.11.dylib (compatibility version 12.0.0, current version 12.0.0)\n\t@rpath/libhdf5_cpp.200.dylib (compatibility version 202.0.0, current version 202.0.0)\n\t@rpath/libvtkIOCore-9.1.1.dylib (compatibility version 1.0.0, current version 9.1.0)\n\t@rpath/libvtkCommonCore-9.1.1.dylib (compatibility version 1.0.0, current version 9.1.0)\n\t@rpath/libvtkCommonDataModel-9.1.1.dylib (compatibility version 1.0.0, current version 9.1.0)\n\t@rpath/libvtkFiltersCore-9.1.1.dylib (compatibility version 1.0.0, current version 9.1.0)\n\t@rpath/libvtkIOXML-9.1.1.dylib (compatibility version 1.0.0, current version 9.1.0)\n\t@rpath/libvtkImagingCore-9.1.1.dylib (compatibility version 1.0.0, current version 9.1.0)\n\t@rpath/libvtkIOImage-9.1.1.dylib (compatibility version 1.0.0, current version 9.1.0)\n\t@rpath/libvtkImagingMath-9.1.1.dylib (compatibility version 1.0.0, current version 9.1.0)\n\t@rpath/libvtkIOParallelXML-9.1.1.dylib (compatibility version 1.0.0, current version 9.1.0)\n\t@rpath/libvtkParallelMPI-9.1.1.dylib (compatibility version 1.0.0, current version 9.1.0)\n\t@rpath/libvtkParallelCore-9.1.1.dylib (compatibility version 1.0.0, current version 9.1.0)\n\t@rpath/libvtkCommonExecutionModel-9.1.1.dylib (compatibility version 1.0.0, current version 9.1.0)\n\t@rpath/libvtksys-9.1.1.dylib (compatibility version 1.0.0, current version 9.1.0)\n\t@rpath/libz.1.dylib (compatibility version 1.0.0, current version 1.2.13)\n\t@rpath/libslepc.3.16.dylib (compatibility version 3.16.0, current version 3.16.2)\n\t@rpath/libpetsc.3.16.dylib (compatibility version 3.16.0, current version 3.16.6)\n\t@rpath/libHYPRE-2.23.0.dylib (compatibility version 0.0.0, current version 0.0.0)\n\t@rpath/libstrumpack.dylib (compatibility version 0.0.0, current version 0.0.0)\n\t@rpath/libsuperlu_dist.7.dylib (compatibility version 7.0.0, current version 7.1.1)\n\t@rpath/libhdf5_hl.200.dylib (compatibility version 201.0.0, current version 201.1.0)\n\t@rpath/libhdf5.200.dylib (compatibility version 202.0.0, current version 202.0.0)\n\t@rpath/libparmetis.dylib (compatibility version 0.0.0, current version 0.0.0)\n\t@rpath/libmetis.dylib (compatibility version 0.0.0, current version 0.0.0)\n\t@rpath/libmpifort.12.dylib (compatibility version 15.0.0, current version 15.2.0)\n\t@rpath/libpmpi.12.dylib (compatibility version 15.0.0, current version 15.2.0)\n\t@rpath/libgfortran.5.dylib (compatibility version 6.0.0, current version 6.0.0)\n\t@rpath/libquadmath.0.dylib (compatibility version 1.0.0, current version 1.0.0)\n\t/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1319.0.0)\n\t@rpath/libc++.1.dylib (compatibility version 1.0.0, current version 1.0.0)\n\t@rpath/libgcc_s.1.dylib (compatibility version 1.0.0, current version 1.0.0)\n\t@rpath/libmpi.12.dylib (compatibility version 15.0.0, current version 15.2.0)\n\t@rpath/libmpicxx.12.dylib (compatibility version 15.0.0, current version 15.2.0)\n\t@rpath/libomp.dylib (compatibility version 5.0.0, current version 5.0.0)",
                  "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4346690",
                  "updatedAt": "2022-12-08T20:56:22Z",
                  "publishedAt": "2022-12-08T20:25:17Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "It looks like moose exists up one directory from your application. I see that git clean -xfd was run in previous posts. But I am not sure where it was performed. That said, when obtaining new versions of moose-libmesh (which means anything downstream of that; moose-petsc, moose-mpich), it is necessary to clean your build, rebuild, and then you can test. For you that means:\ncd ~/projects/moose\ngit clean -xfd\ncd ~/projects/ASP\ngit clean -xfd\nmake\nOf course, be careful using git clean -xfd. This will delete any uncommitted, non-tracked files.\nThat said, did you perform a git clean in both the moose directory and your application directory? The links you provide all look good. But then again, the files won't change names, but the symbols in those file might (moose-libmesh's @rpath/../libmesh/lib/libmesh_opt.0.dylib library will have been updated if moose-libmesh was updated.)",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4346913",
                          "updatedAt": "2022-12-08T21:03:15Z",
                          "publishedAt": "2022-12-08T21:03:14Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "joe61vette"
                  },
                  "bodyText": "I will redo my \"cleaning\" just to make sure it was done in the right order.\nI tried doing a non_unity build just in case that might turn up something.  Ironically, the error it found was in the framework:\nIn file included from /Users/joe/projects/moose/framework/src/postprocessors/CumulativeValuePostprocessor.C:10:\nIn file included from /Users/joe/projects/moose/framework/build/header_symlinks/CumulativeValuePostprocessor.h:12:\nIn file included from /Users/joe/projects/moose/framework/build/header_symlinks/GeneralPostprocessor.h:13:\nIn file included from /Users/joe/projects/moose/framework/build/header_symlinks/Postprocessor.h:12:\nIn file included from /Users/joe/projects/moose/framework/build/header_symlinks/OutputInterface.h:13:\nIn file included from /Users/joe/projects/moose/framework/build/header_symlinks/MooseTypes.h:13:\nIn file included from /Users/joe/projects/moose/framework/build/header_symlinks/ADReal.h:13:\nIn file included from /Users/joe/projects/moose/framework/build/header_symlinks/DualRealOps.h:16:\nIn file included from /Users/joe/projects/moose/framework/build/header_symlinks/SparseOps.h:13:\nIn file included from /Users/joe/mambaforge3/envs/moose/libmesh/include/metaphysicl/dualsemidynamicsparsenumberarray.h:31:\nIn file included from /Users/joe/mambaforge3/envs/moose/libmesh/include/metaphysicl/dualsemidynamicsparsenumberarray_decl.h:27:\n/Users/joe/mambaforge3/envs/moose/libmesh/include/metaphysicl/dualnumber_decl.h:35:10: fatal error: 'metaphysicl/ct_types.h' file not found\n#include \"metaphysicl/ct_types.h\"\n         ^~~~~~~~~~~~~~~~~~~~~~~~\n\nIs that anything I should worry about?\nThanks.",
                  "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4346935",
                  "updatedAt": "2022-12-08T21:34:00Z",
                  "publishedAt": "2022-12-08T21:09:29Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Any order works, so if you performed a git clean in both, at any time after a mamba update --all should suffice. However:\nThis actually looks like what you have installed via Conda, is out-of-sync with what version of moose you have cloned (this version of moose might need an older version of moose-libmesh).\nI suppose our instructions should say something like:\n\nIf you update moose then\n\nyou should also update Conda (mamba update --all)\n\n\nIf you update Conda then\n\nyou should also update moose (git fetch; git pull)",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4346970",
                          "updatedAt": "2022-12-08T21:16:25Z",
                          "publishedAt": "2022-12-08T21:16:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "joe61vette"
                          },
                          "bodyText": "Did the cleaning in the order suggested, just to make sure.  Rebuilt and same problem with 'missing symbol called'.",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4346974",
                          "updatedAt": "2022-12-08T21:17:11Z",
                          "publishedAt": "2022-12-08T21:17:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I see you've done a fetch/rebase... You should have the latest available everything then... what is your origin?\ncd ~/projects/moose\ngit remote -v",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4346986",
                          "updatedAt": "2022-12-08T21:19:17Z",
                          "publishedAt": "2022-12-08T21:19:16Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "joe61vette"
                  },
                  "bodyText": "Here it is:\n(moose) moose % git remote -v\norigin\thttps://github.com/idaholab/moose.git (fetch)\norigin\thttps://github.com/idaholab/moose.git (push)",
                  "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4347000",
                  "updatedAt": "2022-12-08T21:20:42Z",
                  "publishedAt": "2022-12-08T21:20:42Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "joe61vette"
                          },
                          "bodyText": "Just rebuilt Moose & reran all tests.  No failures.",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4347055",
                          "updatedAt": "2022-12-08T21:29:02Z",
                          "publishedAt": "2022-12-08T21:29:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I am not seeing an issue in how/what you've done thus far:\ncd ~/projects/moose\ngit rebase origin/master\nmamba activate moose\nmamba update --all\ngit clean -xfd\ncd ../ASP\ngit clean -xfd\nmake\nThis appears to have been done throughout the course of this thread. But the error:\nIn file included from /Users/joe/mambaforge3/envs/moose/libmesh/include/metaphysicl/dualsemidynamicsparsenumberarray_decl.h:27:\n/Users/joe/mambaforge3/envs/moose/libmesh/include/metaphysicl/dualnumber_decl.h:35:10: fatal error: 'metaphysicl/ct_types.h' file not found\n#include \"metaphysicl/ct_types.h\"\n         ^~~~~~~~~~~~~~~~~~~~~~~~\n\ndoes indicate something out of sync. Either the version of moose at ~/projects/moose, or what version of moose-libmesh is installed in your Conda environment (but you have the latest version, so... there is no disputing that).\nYou are also using the 'right' moose according to your otool -L asp-opt readout. I am a bit stumped at the moment...",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4347148",
                          "updatedAt": "2022-12-08T21:42:11Z",
                          "publishedAt": "2022-12-08T21:42:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "@joe61vette, if possible, can you try to respond within a thread? It's a little more difficult to follow when the discussion is carried across Answers. What is the output of echo $LIBMESH_DIR?",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4347872",
                          "updatedAt": "2022-12-09T00:22:39Z",
                          "publishedAt": "2022-12-09T00:22:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "I'm also curious what the undefined symbol was",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4347875",
                          "updatedAt": "2022-12-09T00:23:22Z",
                          "publishedAt": "2022-12-09T00:23:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "joe61vette"
                          },
                          "bodyText": "Here you go:\nASP % echo $LIBMESH_DIR\n/Users/joe/mambaforge3/envs/moose/libmesh\nThe major problem is that I don't know what symbol is missing.  Here is the error message:\n(moose) Tests % asp 1d-gravity.i\ndyld[2210]: missing symbol called\nzsh: abort      /Users/joe/projects/ASP/asp-opt -i 1d-gravity.i\nEarlier I tried a dbg version but no info as to the missing symbol there either.\nTomorrow, I will systematically remove routines from my app until I find the one that causes the dyld problem.  I can't think of anything else.",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4347909",
                          "updatedAt": "2022-12-09T00:36:01Z",
                          "publishedAt": "2022-12-09T00:36:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "joe61vette"
                          },
                          "bodyText": "Good Morning All:  I removed all of the header and source files from my app except those generated automatically when you run stork.sh.  Recompiled and the simple_diffusion.i test problem ran.  So, I'll begin adding back my routines to find the culprit.\nThanks again for the help,\nJoe",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4352732",
                          "updatedAt": "2022-12-09T14:43:14Z",
                          "publishedAt": "2022-12-09T14:43:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "joe61vette"
                          },
                          "bodyText": "It appears that I was missing a small utility function.  Something that got missed when switching branches. Thanks everyone for the help.  At least I did need it to get my Moose environment straightened out.  I absolutely have to remember to use \"mamba update --all\" instead of \"conda update --all\" .",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4353978",
                          "updatedAt": "2022-12-09T16:56:02Z",
                          "publishedAt": "2022-12-09T16:56:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Sorry @joe61vette that sounds like a painful process. It's really obnoxious that your system didn't emit information about the missing symbol. My only suggestion in the future if you run into that same problem would be to try to get access to a Linux system because they list undefined symbols in detail (albeit mangled, but it can be demangled easily using web tools like demangler.com) at link time. Mac lazily checks for undefined symbols during run-time and apparently sometimes doesn't give detailed info about what's missing.\nI'm very glad you were able to get it worked out though!",
                          "url": "https://github.com/idaholab/moose/discussions/22920#discussioncomment-4355159",
                          "updatedAt": "2022-12-09T18:33:58Z",
                          "publishedAt": "2022-12-09T18:33:57Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How does the IntegratedBC work?",
          "author": {
            "login": "SomnusYu"
          },
          "bodyText": "Hi all,\nI find that the return value of NeumanBC is negative in NeumannBC.C. Could anyone kindly explain it to me? Why it is negative?\nBest regards,\nSomnusYu",
          "url": "https://github.com/idaholab/moose/discussions/22931",
          "updatedAt": "2023-01-03T23:22:15Z",
          "publishedAt": "2022-12-09T15:24:16Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "jiangwen84"
                  },
                  "bodyText": "You can read some basics of finite element method, e.g., weak form, etc. After applying integration by parts and divergence theory, there will be a negative sign for the Neumann boundary conditions.",
                  "url": "https://github.com/idaholab/moose/discussions/22931#discussioncomment-4353097",
                  "updatedAt": "2022-12-09T15:31:41Z",
                  "publishedAt": "2022-12-09T15:31:40Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "SomnusYu"
                          },
                          "bodyText": "Thank you!",
                          "url": "https://github.com/idaholab/moose/discussions/22931#discussioncomment-4353214",
                          "updatedAt": "2022-12-09T15:49:00Z",
                          "publishedAt": "2022-12-09T15:49:00Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "friedmud"
                  },
                  "bodyText": "It's due to the fact that we always write a Poisson/Laplacian/Diffusion problem with a negative on the divergence term.  This is a convention because when you integrate by parts to form the weak form you end up with a symmetric positive definite (SPD) matrix.\nYou can see the whole process here... including the integration by parts that generates the boundary integral (which still has a negative on it).",
                  "url": "https://github.com/idaholab/moose/discussions/22931#discussioncomment-4353117",
                  "updatedAt": "2022-12-09T15:35:08Z",
                  "publishedAt": "2022-12-09T15:35:07Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "friedmud"
                          },
                          "bodyText": "https://mooseframework.inl.gov/getting_started/examples_and_tutorials/tutorial01_app_development/step04_weak_form.html#step-4-generate-a-weak-form",
                          "url": "https://github.com/idaholab/moose/discussions/22931#discussioncomment-4353159",
                          "updatedAt": "2022-12-09T15:40:22Z",
                          "publishedAt": "2022-12-09T15:40:22Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "SomnusYu"
                          },
                          "bodyText": "OK, thank you very much!",
                          "url": "https://github.com/idaholab/moose/discussions/22931#discussioncomment-4353212",
                          "updatedAt": "2022-12-09T15:48:48Z",
                          "publishedAt": "2022-12-09T15:48:47Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "errors when update and rebuild libmesh",
          "author": {
            "login": "Joseph-0123"
          },
          "bodyText": "Dear all,\nOur uni cluster finished maintenance last week. But I find I cannot use MOOSE. So I update and rebuild MOOSE following the HPC Cluster.\nBut when I run ./scripts/update_and_rebuild_libmesh.sh, I have the following errors. Could you please help me fix it? Thanks a lot.\nJoseph\n(base) cd ~/projects/moose\n(base) ./scripts/update_and_rebuild_libmesh.sh\nmpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\nmpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\nmpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\nmpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\n/home/projects/moose/scripts/diagnostics.sh: line 40: --version: command not found\nPETSc submodule will be used. PETSc submodule is our default solver.\nIMPORTANT: If you did not run the update_and_rebuild_petsc.sh script yet, please run it before building libMesh\n---------------------------------------------\n----------- Configuring libMesh -------------\n---------------------------------------------\nchecking build system type... x86_64-pc-linux-gnu\nchecking host system type... x86_64-pc-linux-gnu\nchecking target system type... x86_64-pc-linux-gnu\nchecking for a BSD-compatible install... /homeprojects/moose/scripts/../libmesh/build-aux/install-sh -C\nchecking whether build environment is sane... yes\nchecking for a thread-safe mkdir -p... /usr/bin/mkdir -p\nchecking for gawk... gawk\nchecking whether make sets $(MAKE)... yes\nchecking whether make supports nested variables... yes\nchecking whether UID '231419' is supported by ustar format... yes\nchecking whether GID '44347' is supported by ustar format... yes\nchecking how to create a ustar tar archive... gnutar\nchecking whether make supports nested variables... (cached) yes\nchecking whether to enable maintainer-specific portions of Makefiles... no\nchecking for src/base/libmesh.C... no\n<<< Configuring build directory for VPATH build >>>\nchecking for perl... /usr/bin/perl\nnote: MPI library path not given...\ntrying prefix=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2\nnote: MPI library path not given...\ntrying prefix=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2\nchecking whether make supports the include directive... yes (GNU style)\nchecking whether the C compiler works... no\nconfigure: error: in `/home/projects/moose/libmesh/build':\nconfigure: error: C compiler cannot create executables\nSee `config.log' for more details\nRunning make -j 1...\nmake: *** No targets specified and no makefile found.  Stop.",
          "url": "https://github.com/idaholab/moose/discussions/22720",
          "updatedAt": "2022-12-08T21:10:33Z",
          "publishedAt": "2022-11-16T10:20:49Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "Joseph-0123"
                  },
                  "bodyText": "The update_and_rebuild_petsc.sh has no problem. But the update_and_rebuild_libmesh.sh has errors.",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4156724",
                  "updatedAt": "2022-11-16T12:55:43Z",
                  "publishedAt": "2022-11-16T12:55:42Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "hello\nIt seems there are issues with the compiler.\nCould you please report the output of the diagnostics script in moose/scripts",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4158901",
                          "updatedAt": "2022-11-16T16:27:23Z",
                          "publishedAt": "2022-11-16T16:27:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "hello\nIt seems there are issues with the compiler.\nCould you please report the output of the diagnostics script in moose/scripts\n\nThanks for your help. I get the following information.\n(base) ./scripts/diagnostics.sh\nWed Nov 16 17:37:48 CET 2022\nSystem Arch: LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch Distributor ID: RedHatEnterprise Description: Red Hat Enterprise Linux release 8.4 (Ootpa) Release: 8.4 Codename: Ootpa\nMOOSE Package Version: Custom Build\nCPU Count: 80\nMemory Free: 320052.148 MB\nmpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\nVariable which $CC check:\n/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin/mpicc\n$CC --version:\nmpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\nmpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\nmpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\n./scripts/diagnostics.sh: line 40: --version: command not found\nMPICC:\nwhich mpicc:\n/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin/mpicc\nmpicc -show:\nCOMPILER :\nPython:\n/home/miniconda3/bin/python\nPython 3.7.13\nMODULES:\nCurrently Loaded Modules:\n\ncompiler/gnu/10.2   2) mpi/openmpi/4.0\n\nPETSC_DIR not set\nENVIRONMENT:\nCONDA_SHLVL=1\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:.tar=01;31:.tgz=01;31:.arc=01;31:.arj=01;31:.taz=01;31:.lha=01;31:.lz4=01;31:.lzh=01;31:.lzma=01;31:.tlz=01;31:.txz=01;31:.tzo=01;31:.t7z=01;31:.zip=01;31:.z=01;31:.dz=01;31:.gz=01;31:.lrz=01;31:.lz=01;31:.lzo=01;31:.xz=01;31:.zst=01;31:.tzst=01;31:.bz2=01;31:.bz=01;31:.tbz=01;31:.tbz2=01;31:.tz=01;31:.deb=01;31:.rpm=01;31:.jar=01;31:.war=01;31:.ear=01;31:.sar=01;31:.rar=01;31:.alz=01;31:.ace=01;31:.zoo=01;31:.cpio=01;31:.7z=01;31:.rz=01;31:.cab=01;31:.wim=01;31:.swm=01;31:.dwm=01;31:.esd=01;31:.jpg=01;35:.jpeg=01;35:.mjpg=01;35:.mjpeg=01;35:.gif=01;35:.bmp=01;35:.pbm=01;35:.pgm=01;35:.ppm=01;35:.tga=01;35:.xbm=01;35:.xpm=01;35:.tif=01;35:.tiff=01;35:.png=01;35:.svg=01;35:.svgz=01;35:.mng=01;35:.pcx=01;35:.mov=01;35:.mpg=01;35:.mpeg=01;35:.m2v=01;35:.mkv=01;35:.webm=01;35:.ogm=01;35:.mp4=01;35:.m4v=01;35:.mp4v=01;35:.vob=01;35:.qt=01;35:.nuv=01;35:.wmv=01;35:.asf=01;35:.rm=01;35:.rmvb=01;35:.flc=01;35:.avi=01;35:.fli=01;35:.flv=01;35:.gl=01;35:.dl=01;35:.xcf=01;35:.xwd=01;35:.yuv=01;35:.cgm=01;35:.emf=01;35:.ogv=01;35:.ogx=01;35:.aac=01;36:.au=01;36:.flac=01;36:.m4a=01;36:.mid=01;36:.midi=01;36:.mka=01;36:.mp3=01;36:.mpc=01;36:.ogg=01;36:.ra=01;36:.wav=01;36:.oga=01;36:.opus=01;36:.spx=01;36:.xspf=01;36:\nLD_LIBRARY_PATH=/home/moose-compilers/gcc-10.2.0/lib64:/home/moose-compilers/gcc-10.2.0/lib:/home/moose-compilers/gcc-10.2.0/lib/gcc/x86_64-pc-linux-gnu/10.2.0:/home/moose-compilers/gcc-10.2.0/libexec/gcc/x86_64-pc-linux-gnu/10.2.0:/home/moose-compilers/mpich-3.3/lib:/opt/bwhpc/common/compiler/gnu/10.2.0/lib:/opt/bwhpc/common/compiler/gnu/10.2.0/lib64:/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib:/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64\nCONDA_EXE=/home/miniconda3/bin/conda\n__LMOD_REF_COUNT_PATH=/opt/bwhpc/common/compiler/gnu/10.2.0/bin:1;/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin:1;/home/.local/bin:1;/home/bin:1;/software/all/bin:1;/usr/share/Modules/bin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1\nModuleTable002=LAp9LApbIm1waS9vcGVubXBpIl0gPSB7CmZuID0gIi9zb2Z0d2FyZS9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29tcGlsZXIvZ251LzEwLjIvbXBpL29wZW5tcGkvNC4wLmx1YSIsCmZ1bGxOYW1lID0gIm1waS9vcGVubXBpLzQuMCIsCmxvYWRPcmRlciA9IDIsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAibXBpL29wZW5tcGkvNC4wIiwKd1YgPSAiMDAwMDAwMDA0Lip6ZmluYWwiLAp9LAp9LAptcGF0aEEgPSB7CiIvc29mdHdhcmUvYndocGMvY29tbW9uL21vZHVsZWZpbGVzL0NvbXBpbGVyL2dudS8xMC4yIiwgIi9vcHQvYndocGMva2l0L21vZHVsZWZpbGVzIiwgIi9vcHQvYndocGMvY29tbW9uL21vZHVsZWZp\nSSH_CONNECTION=2a00:1398:4:7e02:e5a8:991c:1523:d3e 49843 2a00:1398:4:1805::810d:3814 22\nMODULES_RUN_QUARANTINE=LD_LIBRARY_PATH LD_PRELOAD\nLANG=en_US.UTF-8\nLMOD_SYSTEM_NAME=uc2\nHISTTIMEFORMAT=%F %T\nGNU_HOME=/opt/bwhpc/common/compiler/gnu/10.2.0\nLMOD_FAMILY_COMPILER_VERSION=10.2\nHOSTNAME=uc2n995.localdomain\nOLDPWD=/home/projects/moose/scripts\nGNU_MAN_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/share/man\nC_INCLUDE_PATH=/home//moose-compilers/mpich-3.3/include:\nMPI_INC_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/include\nMPI_MAN_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/share/man\nFPATH=/home//moose-compilers/mpich-3.3/include:\nGNU_BIN_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/bin\nCONDA_PREFIX=/home/miniconda3\n__LMOD_REF_COUNT_LD_LIBRARY_PATH=/opt/bwhpc/common/compiler/gnu/10.2.0/lib:1;/opt/bwhpc/common/compiler/gnu/10.2.0/lib64:1;/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib:1;/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64:1\nMPIDIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2\nGNU_VERSION=10.2.0\nMPI_LIB_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib\n_CE_M=\nCC=mpicc\nPROJ_LIB=/home/miniconda3/share/proj\nXDG_SESSION_ID=11076\nMODULES_CMD=/usr/share/Modules/libexec/modulecmd.tcl\nUSER=qz9211\nGNU_LIB_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/lib\n__LMOD_REF_COUNT_MODULEPATH=/software/bwhpc/common/modulefiles/Compiler/gnu/10.2:1;/opt/bwhpc/modulefiles:1;/opt/bwhpc/common/modulefiles/Core:1\nPWD=/home/projects/moose\nHOME=/home/\nTMP=/scratch\nCONDA_PYTHON_EXE=/home/miniconda3/bin/python\nSSH_CLIENT=2a00:1398:4:7e02:e5a8:991c:1523:d3e 49843 22\nLMOD_VERSION=8.6.16\nLMOD_PAGER=less\nFAMILY_COMPILER_VERSION=10.2\nF77=mpif77\nXDG_DATA_DIRS=/home/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share\nFAMILY_COMPILER=compiler/gnu\n_CE_CONDA=\nMKL_NUM_THREADS=1\nTMPDIR=/scratch\nLMOD_sys=Linux\nModuleTable001=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpjb21waWxlciA9ICJjb21waWxlci9nbnUiLAp9LAptVCA9IHsKWyJjb21waWxlci9nbnUiXSA9IHsKZm4gPSAiL29wdC9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29yZS9jb21waWxlci9nbnUvMTAuMi5sdWEiLApmdWxsTmFtZSA9ICJjb21waWxlci9nbnUvMTAuMiIsCmxvYWRPcmRlciA9IDEsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAxLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY29tcGlsZXIvZ251LzEwLjIiLAp3ViA9ICJeMDAwMDAwMTAuMDAwMDAwMDAyLip6ZmluYWwi\nLOADEDMODULES=compiler/gnu/10.2:mpi/openmpi/4.0\nFC=mpif90\n__LMOD_REF_COUNT_MANPATH=/opt/bwhpc/common/compiler/gnu/10.2.0/share/man:1;/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/share/man:1;/opt/lmod/lmod/share/man:1\nSCRATCH=/scratch\nModuleTable003=bGVzL0NvcmUiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL29wdC9id2hwYy9raXQvbW9kdWxlZmlsZXM6L29wdC9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29yZSIsCn0K\nLMOD_ROOT=/opt/lmod\nCONDA_PROMPT_MODIFIER=(base)\nSSH_TTY=/dev/pts/33\nMAIL=/var/spool/mail/qz9211\nMPI_VERSION=4.0.5-gnu-10.2\nCXX=mpicxx\nMPI_HOME=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2\nMPI_LIB64_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64\nSHELL=/bin/bash\nTERM=xterm\nLMOD_SITE_NAME=TIK\nModuleTable_Sz=3\nLMOD_FAMILY_COMPILER=compiler/gnu\nLSDF=/lsdf\nTMOUT=36000\nGNU_LIB64_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/lib64\nMPI_BIN_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin\nSHLVL=2\nMANPATH=/home/moose-compilers/mpich-3.3/share/man:/opt/bwhpc/common/compiler/gnu/10.2.0/share/man:/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/share/man:/opt/lmod/lmod/share/man::\nTEMP=/scratch\nF90=mpif90\nMODULEPATH=/software/bwhpc/common/modulefiles/Compiler/gnu/10.2:/opt/modulefiles:/opt/bwhpc/common/modulefiles/Core\nLOGNAME=qz9211\nDBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/231419/bus\nCLUSTER=uc2\nXDG_RUNTIME_DIR=/run/user/231419\nCPLUS_INCLUDE_PATH=/home/moose-compilers/mpich-3.3/include:\nMODULEPATH_modshare=/usr/share/modulefiles:1:/usr/share/Modules/modulefiles:1:/etc/modulefiles:1\nMODULEPATH_ROOT=/opt/modulefiles\nLMOD_PACKAGE_PATH=/etc/lmod/?.lua;;\nPATH=/home/miniconda3/bin:/home/miniconda3/condabin:/home/moose-compilers/gcc-10.2.0/bin:/home/moose-compilers/mpich-3.3/bin:/home/moose-compilers/miniconda/bin:/opt/bwhpc/common/compiler/gnu/10.2.0/bin:/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin:/home/.local/bin:/home/bin:/software/all/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin\nGNU_INC_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/include\nLMFILES=/opt/bwhpc/common/modulefiles/Core/compiler/gnu/10.2.lua:/software/bwhpc/common/modulefiles/Compiler/gnu/10.2/mpi/openmpi/4.0.lua\nMODULESHOME=/opt/lmod/lmod\nLMOD_SETTARG_FULL_SUPPORT=no\nCONDA_DEFAULT_ENV=base\nMPI_EXA_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/examples\nHISTSIZE=1000\nLMOD_PKG=/opt/lmod/lmod\nLMOD_CMD=/opt/lmod/lmod/libexec/lmod\nLESSOPEN=||/usr/bin/lesspipe.sh %s\nOMP_NUM_THREADS=1\nLMOD_DIR=/opt/lmod/lmod/libexec\nBASH_FUNC_module%%=() {  eval $(/opt/bwhpc/common/admin/modules/module-wrapper/modulecmd bash $)\n}\nBASH_FUNC__module_raw%%=() {  unset _mlshdbg;\nif [ \"${MODULES_SILENT_SHELL_DEBUG:-0}\" = '1' ]; then\ncase \"$-\" in\nvx*)\nset +vx;\n_mlshdbg='vx'\n;;\nv)\nset +v;\n_mlshdbg='v'\n;;\nx)\nset +x;\nmlshdbg='x'\n;;\n)\n_mlshdbg=''\n;;\nesac;\nfi;\nunset _mlre _mlIFS;\nif [ -n \"${IFS+x}\" ]; then\n_mlIFS=$IFS;\nfi;\nIFS=' ';\nfor _mlv in ${MODULES_RUN_QUARANTINE:-};\ndo\nif [ \"${_mlv}\" = \"${_mlv##[!A-Za-z0-9]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\nif [ -n \"eval 'echo ${'$_mlv'+x}'\" ]; then\n_mlre=\"${_mlre:-}${_mlv}_modquar='eval 'echo ${'$_mlv'}'' \";\nfi;\nmlrv=\"MODULES_RUNENV${_mlv}\";\n_mlre=\"${_mlre:-}${_mlv}='eval 'echo ${'$_mlrv':-}'' \";\nfi;\ndone;\nif [ -n \"${_mlre:-}\" ]; then\neval eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash '\"$@\"';\nelse\neval /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\";\nfi;\n_mlstatus=$?;\nif [ -n \"${_mlIFS+x}\" ]; then\nIFS=$_mlIFS;\nelse\nunset IFS;\nfi;\nunset _mlre _mlv _mlrv _mlIFS;\nif [ -n \"${_mlshdbg:-}\" ]; then\nset -$_mlshdbg;\nfi;\nunset _mlshdbg;\nreturn $_mlstatus\n}\nBASH_FUNC_switchml%%=() {  typeset swfound=1;\nif [ \"${MODULES_USE_COMPAT_VERSION:-0}\" = '1' ]; then\ntypeset swname='main';\nif [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\ntypeset swfound=0;\nunset MODULES_USE_COMPAT_VERSION;\nfi;\nelse\ntypeset swname='compatibility';\nif [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\ntypeset swfound=0;\nMODULES_USE_COMPAT_VERSION=1;\nexport MODULES_USE_COMPAT_VERSION;\nfi;\nfi;\nif [ $swfound -eq 0 ]; then\necho \"Switching to Modules $swname version\";\nsource /usr/share/Modules/init/bash;\nelse\necho \"Cannot switch to Modules $swname version, command not found\";\nreturn 1;\nfi\n}\nBASH_FUNC_ml%%=() {  eval \"$($LMOD_DIR/ml_cmd \"$@\")\"\n}\n_=/usr/bin/env\n(base)",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4159082",
                          "updatedAt": "2022-11-16T16:42:37Z",
                          "publishedAt": "2022-11-16T16:42:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so your python is still the miniconda python\nyou still have a problem with compilers, your path contains an mpich you probably installed yourself in home/moose-compilers,\nand openmpi from the module\nwhat s worse is that that openmpi from the module is BROKEN. It gives you errors on every invocation.\nI would recommend:\n\nclean the environment further from both the openmpi and mpich in there\nfind a non-broken mpi module to use. It should not give these \"fabrics\" and \"symbol lookup\" errors",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4292601",
                          "updatedAt": "2022-12-02T13:46:31Z",
                          "publishedAt": "2022-12-02T13:46:30Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "The MPI compiler is broken it sends an error message.\n\nYou need to address this before you can build libmesh.\n\nIf this is something your cluster administrator installed for you then you ll need to contact them\n\u2026\n Le 16 nov. 2022 \u00e0 09:42, Joseph-0123 ***@***.***> a \u00e9crit :\n\n \ufeff\n hello\n\n It seems there are issues with the compiler.\n\n Could you please report the output of the diagnostics script in moose/scripts\n\n Thanks for your help. I get the following information.\n\n (base) ./scripts/diagnostics.sh\n Wed Nov 16 17:37:48 CET 2022\n\n System Arch: LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch Distributor ID: RedHatEnterprise Description: Red Hat Enterprise Linux release 8.4 (Ootpa) Release: 8.4 Codename: Ootpa\n\n MOOSE Package Version: Custom Build\n\n CPU Count: 80\n\n Memory Free: 320052.148 MB\n mpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\n\n Variable which $CC check:\n /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin/mpicc\n\n $CC --version:\n\n mpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\n mpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\n mpicc: symbol lookup error: /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\n ./scripts/diagnostics.sh: line 40: --version: command not found\n\n MPICC:\n which mpicc:\n /opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin/mpicc\n mpicc -show:\n\n COMPILER :\n\n Python:\n /home/miniconda3/bin/python\n Python 3.7.13\n\n MODULES:\n\n Currently Loaded Modules:\n\n compiler/gnu/10.2 2) mpi/openmpi/4.0\n PETSC_DIR not set\n\n ENVIRONMENT:\n CONDA_SHLVL=1\n LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:.tar=01;31:.tgz=01;31:.arc=01;31:.arj=01;31:.taz=01;31:.lha=01;31:.lz4=01;31:.lzh=01;31:.lzma=01;31:.tlz=01;31:.txz=01;31:.tzo=01;31:.t7z=01;31:.zip=01;31:.z=01;31:.dz=01;31:.gz=01;31:.lrz=01;31:.lz=01;31:.lzo=01;31:.xz=01;31:.zst=01;31:.tzst=01;31:.bz2=01;31:.bz=01;31:.tbz=01;31:.tbz2=01;31:.tz=01;31:.deb=01;31:.rpm=01;31:.jar=01;31:.war=01;31:.ear=01;31:.sar=01;31:.rar=01;31:.alz=01;31:.ace=01;31:.zoo=01;31:.cpio=01;31:.7z=01;31:.rz=01;31:.cab=01;31:.wim=01;31:.swm=01;31:.dwm=01;31:.esd=01;31:.jpg=01;35:.jpeg=01;35:.mjpg=01;35:.mjpeg=01;35:.gif=01;35:.bmp=01;35:.pbm=01;35:.pgm=01;35:.ppm=01;35:.tga=01;35:.xbm=01;35:.xpm=01;35:.tif=01;35:.tiff=01;35:.png=01;35:.svg=01;35:.svgz=01;35:.mng=01;35:.pcx=01;35:.mov=01;35:.mpg=01;35:.mpeg=01;35:.m2v=01;35:.mkv=01;35:.webm=01;35:.ogm=01;35:.mp4=01;35:.m4v=01;35:.mp4v=01;35:.vob=01;35:.qt=01;35:.nuv=01;35:.wmv=01;35:.asf=01;35:.rm=01;35:.rmvb=01;35:.flc=01;35:.avi=01;35:.fli=01;35:.flv=01;35:.gl=01;35:.dl=01;35:.xcf=01;35:.xwd=01;35:.yuv=01;35:.cgm=01;35:.emf=01;35:.ogv=01;35:.ogx=01;35:.aac=01;36:.au=01;36:.flac=01;36:.m4a=01;36:.mid=01;36:.midi=01;36:.mka=01;36:.mp3=01;36:.mpc=01;36:.ogg=01;36:.ra=01;36:.wav=01;36:.oga=01;36:.opus=01;36:.spx=01;36:.xspf=01;36:\n LD_LIBRARY_PATH=/home/moose-compilers/gcc-10.2.0/lib64:/home/moose-compilers/gcc-10.2.0/lib:/home/moose-compilers/gcc-10.2.0/lib/gcc/x86_64-pc-linux-gnu/10.2.0:/home/moose-compilers/gcc-10.2.0/libexec/gcc/x86_64-pc-linux-gnu/10.2.0:/home/moose-compilers/mpich-3.3/lib:/opt/bwhpc/common/compiler/gnu/10.2.0/lib:/opt/bwhpc/common/compiler/gnu/10.2.0/lib64:/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib:/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64\n CONDA_EXE=/home/miniconda3/bin/conda\n __LMOD_REF_COUNT_PATH=/opt/bwhpc/common/compiler/gnu/10.2.0/bin:1;/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin:1;/home/.local/bin:1;/home/bin:1;/software/all/bin:1;/usr/share/Modules/bin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1\n ModuleTable002=LAp9LApbIm1waS9vcGVubXBpIl0gPSB7CmZuID0gIi9zb2Z0d2FyZS9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29tcGlsZXIvZ251LzEwLjIvbXBpL29wZW5tcGkvNC4wLmx1YSIsCmZ1bGxOYW1lID0gIm1waS9vcGVubXBpLzQuMCIsCmxvYWRPcmRlciA9IDIsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAibXBpL29wZW5tcGkvNC4wIiwKd1YgPSAiMDAwMDAwMDA0Lip6ZmluYWwiLAp9LAp9LAptcGF0aEEgPSB7CiIvc29mdHdhcmUvYndocGMvY29tbW9uL21vZHVsZWZpbGVzL0NvbXBpbGVyL2dudS8xMC4yIiwgIi9vcHQvYndocGMva2l0L21vZHVsZWZpbGVzIiwgIi9vcHQvYndocGMvY29tbW9uL21vZHVsZWZp\n SSH_CONNECTION=2a00:1398:4:7e02:e5a8:991c:1523:d3e 49843 2a00:1398:4:1805::810d:3814 22\n MODULES_RUN_QUARANTINE=LD_LIBRARY_PATH LD_PRELOAD\n LANG=en_US.UTF-8\n LMOD_SYSTEM_NAME=uc2\n HISTTIMEFORMAT=%F %T\n GNU_HOME=/opt/bwhpc/common/compiler/gnu/10.2.0\n LMOD_FAMILY_COMPILER_VERSION=10.2\n HOSTNAME=uc2n995.localdomain\n OLDPWD=/home/projects/moose/scripts\n GNU_MAN_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/share/man\n C_INCLUDE_PATH=/home//moose-compilers/mpich-3.3/include:\n MPI_INC_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/include\n MPI_MAN_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/share/man\n FPATH=/home//moose-compilers/mpich-3.3/include:\n GNU_BIN_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/bin\n CONDA_PREFIX=/home/miniconda3\n __LMOD_REF_COUNT_LD_LIBRARY_PATH=/opt/bwhpc/common/compiler/gnu/10.2.0/lib:1;/opt/bwhpc/common/compiler/gnu/10.2.0/lib64:1;/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib:1;/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64:1\n MPIDIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2\n GNU_VERSION=10.2.0\n MPI_LIB_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib\n _CE_M=\n CC=mpicc\n PROJ_LIB=/home/miniconda3/share/proj\n XDG_SESSION_ID=11076\n MODULES_CMD=/usr/share/Modules/libexec/modulecmd.tcl\n USER=qz9211\n GNU_LIB_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/lib\n __LMOD_REF_COUNT_MODULEPATH=/software/bwhpc/common/modulefiles/Compiler/gnu/10.2:1;/opt/bwhpc/modulefiles:1;/opt/bwhpc/common/modulefiles/Core:1\n PWD=/home/projects/moose\n HOME=/home/\n TMP=/scratch\n CONDA_PYTHON_EXE=/home/miniconda3/bin/python\n SSH_CLIENT=2a00:1398:4:7e02:e5a8:991c:1523:d3e 49843 22\n LMOD_VERSION=8.6.16\n LMOD_PAGER=less\n FAMILY_COMPILER_VERSION=10.2\n F77=mpif77\n XDG_DATA_DIRS=/home/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share\n FAMILY_COMPILER=compiler/gnu\n _CE_CONDA=\n MKL_NUM_THREADS=1\n TMPDIR=/scratch\n LMOD_sys=Linux\n ModuleTable001=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IGZhbHNlLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpjb21waWxlciA9ICJjb21waWxlci9nbnUiLAp9LAptVCA9IHsKWyJjb21waWxlci9nbnUiXSA9IHsKZm4gPSAiL29wdC9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29yZS9jb21waWxlci9nbnUvMTAuMi5sdWEiLApmdWxsTmFtZSA9ICJjb21waWxlci9nbnUvMTAuMiIsCmxvYWRPcmRlciA9IDEsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAxLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY29tcGlsZXIvZ251LzEwLjIiLAp3ViA9ICJeMDAwMDAwMTAuMDAwMDAwMDAyLip6ZmluYWwi\n LOADEDMODULES=compiler/gnu/10.2:mpi/openmpi/4.0\n FC=mpif90\n __LMOD_REF_COUNT_MANPATH=/opt/bwhpc/common/compiler/gnu/10.2.0/share/man:1;/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/share/man:1;/opt/lmod/lmod/share/man:1\n SCRATCH=/scratch\n ModuleTable003=bGVzL0NvcmUiLAp9LApzeXN0ZW1CYXNlTVBBVEggPSAiL29wdC9id2hwYy9raXQvbW9kdWxlZmlsZXM6L29wdC9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29yZSIsCn0K\n LMOD_ROOT=/opt/lmod\n CONDA_PROMPT_MODIFIER=(base)\n SSH_TTY=/dev/pts/33\n MAIL=/var/spool/mail/qz9211\n MPI_VERSION=4.0.5-gnu-10.2\n CXX=mpicxx\n MPI_HOME=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2\n MPI_LIB64_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64\n SHELL=/bin/bash\n TERM=xterm\n LMOD_SITE_NAME=TIK\n ModuleTable_Sz=3\n LMOD_FAMILY_COMPILER=compiler/gnu\n LSDF=/lsdf\n TMOUT=36000\n GNU_LIB64_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/lib64\n MPI_BIN_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin\n SHLVL=2\n MANPATH=/home/moose-compilers/mpich-3.3/share/man:/opt/bwhpc/common/compiler/gnu/10.2.0/share/man:/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/share/man:/opt/lmod/lmod/share/man::\n TEMP=/scratch\n F90=mpif90\n MODULEPATH=/software/bwhpc/common/modulefiles/Compiler/gnu/10.2:/opt/modulefiles:/opt/bwhpc/common/modulefiles/Core\n LOGNAME=qz9211\n DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/231419/bus\n CLUSTER=uc2\n XDG_RUNTIME_DIR=/run/user/231419\n CPLUS_INCLUDE_PATH=/home/moose-compilers/mpich-3.3/include:\n MODULEPATH_modshare=/usr/share/modulefiles:1:/usr/share/Modules/modulefiles:1:/etc/modulefiles:1\n MODULEPATH_ROOT=/opt/modulefiles\n LMOD_PACKAGE_PATH=/etc/lmod/?.lua;;\n PATH=/home/miniconda3/bin:/home/miniconda3/condabin:/home/moose-compilers/gcc-10.2.0/bin:/home/moose-compilers/mpich-3.3/bin:/home/moose-compilers/miniconda/bin:/opt/bwhpc/common/compiler/gnu/10.2.0/bin:/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/bin:/home/.local/bin:/home/bin:/software/all/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin\n GNU_INC_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/include\n LMFILES=/opt/bwhpc/common/modulefiles/Core/compiler/gnu/10.2.lua:/software/bwhpc/common/modulefiles/Compiler/gnu/10.2/mpi/openmpi/4.0.lua\n MODULESHOME=/opt/lmod/lmod\n LMOD_SETTARG_FULL_SUPPORT=no\n CONDA_DEFAULT_ENV=base\n MPI_EXA_DIR=/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/examples\n HISTSIZE=1000\n LMOD_PKG=/opt/lmod/lmod\n LMOD_CMD=/opt/lmod/lmod/libexec/lmod\n LESSOPEN=||/usr/bin/lesspipe.sh %s\n OMP_NUM_THREADS=1\n LMOD_DIR=/opt/lmod/lmod/libexec\n BASH_FUNC_module%%=() { eval $(/opt/bwhpc/common/admin/modules/module-wrapper/modulecmd bash $)\n }\n BASH_FUNC__module_raw%%=() { unset _mlshdbg;\n if [ \"${MODULES_SILENT_SHELL_DEBUG:-0}\" = '1' ]; then\n case \"$-\" in\n vx*)\n set +vx;\n _mlshdbg='vx'\n ;;\n v)\n set +v;\n _mlshdbg='v'\n ;;\n x)\n set +x;\n mlshdbg='x'\n ;;\n )\n _mlshdbg=''\n ;;\n esac;\n fi;\n unset _mlre _mlIFS;\n if [ -n \"${IFS+x}\" ]; then\n _mlIFS=$IFS;\n fi;\n IFS=' ';\n for _mlv in ${MODULES_RUN_QUARANTINE:-};\n do\n if [ \"${_mlv}\" = \"${_mlv##[!A-Za-z0-9]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\n if [ -n \"eval 'echo ${'$_mlv'+x}'\" ]; then\n _mlre=\"${_mlre:-}${_mlv}_modquar='eval 'echo ${'$_mlv'}'' \";\n fi;\n mlrv=\"MODULES_RUNENV${_mlv}\";\n _mlre=\"${_mlre:-}${_mlv}='eval 'echo ${'$_mlrv':-}'' \";\n fi;\n done;\n if [ -n \"${_mlre:-}\" ]; then\n eval eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash '\"$@\"';\n else\n eval /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\";\n fi;\n _mlstatus=$?;\n if [ -n \"${_mlIFS+x}\" ]; then\n IFS=$_mlIFS;\n else\n unset IFS;\n fi;\n unset _mlre _mlv _mlrv _mlIFS;\n if [ -n \"${_mlshdbg:-}\" ]; then\n set -$_mlshdbg;\n fi;\n unset _mlshdbg;\n return $_mlstatus\n }\n BASH_FUNC_switchml%%=() { typeset swfound=1;\n if [ \"${MODULES_USE_COMPAT_VERSION:-0}\" = '1' ]; then\n typeset swname='main';\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\n typeset swfound=0;\n unset MODULES_USE_COMPAT_VERSION;\n fi;\n else\n typeset swname='compatibility';\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\n typeset swfound=0;\n MODULES_USE_COMPAT_VERSION=1;\n export MODULES_USE_COMPAT_VERSION;\n fi;\n fi;\n if [ $swfound -eq 0 ]; then\n echo \"Switching to Modules $swname version\";\n source /usr/share/Modules/init/bash;\n else\n echo \"Cannot switch to Modules $swname version, command not found\";\n return 1;\n fi\n }\n BASH_FUNC_ml%%=() { eval \"$($LMOD_DIR/ml_cmd \"$@\")\"\n }\n _=/usr/bin/env\n (base)\n\n \u2014\n Reply to this email directly, view it on GitHub, or unsubscribe.\n You are receiving this because you commented.",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4159518",
                  "updatedAt": "2022-11-16T17:32:08Z",
                  "publishedAt": "2022-11-16T17:32:08Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Joseph-0123"
                  },
                  "bodyText": "Thanks for your help, I have contacted the cluster administrator.",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4165736",
                  "updatedAt": "2022-11-17T09:56:35Z",
                  "publishedAt": "2022-11-17T09:56:34Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Joseph-0123"
                  },
                  "bodyText": "Sorry for my question again. I have the following errors (ending part of the contents) again when I run ./scripts/update_and_rebuild_libmesh.sh,\nizer_CodeTree::CodeTree<complex<long double> >, allocator<FPoptimizer_CodeTree::CodeTree<complex<long double> > > > >; _Size = long int; _Compare = __gnu_cxx::__ops::_Iter_comp_iter<FPoptimizer_CodeTree::ParamComparer<complex<long double> > >]\u2019 at /pfs/data5/software_uc2/bwhpc/common/compiler/gnu/12.1.0/include/c++/12.1.0/bits/stl_algo.h:1921:38:\nlib/autoptr.hh:51:19: warning: pointer used after \u2018void operator delete(void*, std::size_t)\u2019 [-Wuse-after-free]\nIn member function \u2018void FPOPT_autoptr<Ref>::Forget() [with Ref = FPoptimizer_CodeTree::CodeTreeData<std::complex<long double> >]\u2019,\n    inlined from \u2018void FPOPT_autoptr<Ref>::Set(Ref*) [with Ref = FPoptimizer_CodeTree::CodeTreeData<std::complex<long double> >]\u2019 at lib/autoptr.hh:62:11,\n    inlined from \u2018FPOPT_autoptr<Ref>& FPOPT_autoptr<Ref>::operator=(const FPOPT_autoptr<Ref>&) [with Ref = FPoptimizer_CodeTree::CodeTreeData<std::complex<long double> >]\u2019 at lib/autoptr.hh:16:60,\n    inlined from \u2018FPoptimizer_CodeTree::CodeTree<std::complex<long double> >& FPoptimizer_CodeTree::CodeTree<std::complex<long double> >::operator=(const FPoptimizer_CodeTree::CodeTree<std::complex<long double> >&)\u2019 at fpoptimizer/codetree.hh:37:11,\n    inlined from \u2018std::_Require<std::__not_<std::__is_tuple_like<_Tp> >, std::is_move_constructible<_Tp>, std::is_move_assignable<_Tp> > std::swap(_Tp&, _Tp&) [with _Tp = FPoptimizer_CodeTree::CodeTree<complex<long double> >]\u2019 at /pfs/data5/software_uc2/bwhpc/common/compiler/gnu/12.1.0/include/c++/12.1.0/bits/move.h:205:11,\n    inlined from \u2018void std::iter_swap(_ForwardIterator1, _ForwardIterator2) [with _ForwardIterator1 = __gnu_cxx::__normal_iterator<FPoptimizer_CodeTree::CodeTree<complex<long double> >*, vector<FPoptimizer_CodeTree::CodeTree<complex<long double> >, allocator<FPoptimizer_CodeTree::CodeTree<complex<long double> > > > >; _ForwardIterator2 = __gnu_cxx::__normal_iterator<FPoptimizer_CodeTree::CodeTree<complex<long double> >*, vector<FPoptimizer_CodeTree::CodeTree<complex<long double> >, allocator<FPoptimizer_CodeTree::CodeTree<complex<long double> > > > >]\u2019 at /pfs/data5/software_uc2/bwhpc/common/compiler/gnu/12.1.0/include/c++/12.1.0/bits/stl_algobase.h:182:11,\n    inlined from \u2018_RandomAccessIterator std::__unguarded_partition(_RandomAccessIterator, _RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = __gnu_cxx::__normal_iterator<FPoptimizer_CodeTree::CodeTree<complex<long double> >*, vector<FPoptimizer_CodeTree::CodeTree<complex<long double> >, allocator<FPoptimizer_CodeTree::CodeTree<complex<long double> > > > >; _Compare = __gnu_cxx::__ops::_Iter_comp_iter<FPoptimizer_CodeTree::ParamComparer<complex<long double> > >]\u2019 at /pfs/data5/software_uc2/bwhpc/common/compiler/gnu/12.1.0/include/c++/12.1.0/bits/stl_algo.h:1874:18,\n    inlined from \u2018_RandomAccessIterator std::__unguarded_partition_pivot(_RandomAccessIterator, _RandomAccessIterator, _Compare) [with _RandomAccessIterator = __gnu_cxx::__normal_iterator<FPoptimizer_CodeTree::CodeTree<complex<long double> >*, vector<FPoptimizer_CodeTree::CodeTree<complex<long double> >, allocator<FPoptimizer_CodeTree::CodeTree<complex<long double> > > > >; _Compare = __gnu_cxx::__ops::_Iter_comp_iter<FPoptimizer_CodeTree::ParamComparer<complex<long double> > >]\u2019 at /pfs/data5/software_uc2/bwhpc/common/compiler/gnu/12.1.0/include/c++/12.1.0/bits/stl_algo.h:1889:40,\n    inlined from \u2018void std::__introsort_loop(_RandomAccessIterator, _RandomAccessIterator, _Size, _Compare) [with _RandomAccessIterator = __gnu_cxx::__normal_iterator<FPoptimizer_CodeTree::CodeTree<complex<long double> >*, vector<FPoptimizer_CodeTree::CodeTree<complex<long double> >, allocator<FPoptimizer_CodeTree::CodeTree<complex<long double> > > > >; _Size = long int; _Compare = __gnu_cxx::__ops::_Iter_comp_iter<FPoptimizer_CodeTree::ParamComparer<complex<long double> > >]\u2019 at /pfs/data5/software_uc2/bwhpc/common/compiler/gnu/12.1.0/include/c++/12.1.0/bits/stl_algo.h:1921:38:\nlib/autoptr.hh:45:22: note: call to \u2018void operator delete(void*, std::size_t)\u2019 here\nIn member function \u2018void FPOPT_autoptr<Ref>::Forget() [with Ref = FPoptimizer_CodeTree::CodeTreeData<std::complex<long double> >]\u2019,\n    inlined from \u2018FPOPT_autoptr<Ref>::~FPOPT_autoptr() [with Ref = FPoptimizer_CodeTree::CodeTreeData<std::complex<long double> >]\u2019 at lib/autoptr.hh:23:30,\n    inlined from \u2018FPoptimizer_CodeTree::CodeTree<Value_t>::~CodeTree() [with Value_t = std::complex<long double>]\u2019 at fpoptimizer/codetree.cc:104:5,\n    inlined from \u2018void {anonymous}::CodeTreeParserData<Value_t>::AddConst(const Value_t&) [with Value_t = std::complex<long double>]\u2019 at fpoptimizer/readbytecode.cc:305:9,\n    inlined from \u2018void FPoptimizer_CodeTree::CodeTree<Value_t>::GenerateFrom(const typename FunctionParserBase<Value_t>::Data&, const std::vector<FPoptimizer_CodeTree::CodeTree<Value_t> >&, bool) [with Value_t = std::complex<long double>]\u2019 at fpoptimizer/readbytecode.cc:487:33:\nlib/autoptr.hh:44:8: warning: pointer used after \u2018void operator delete(void*, std::size_t)\u2019 [-Wuse-after-free]\nIn member function \u2018void FPOPT_autoptr<Ref>::Forget() [with Ref = FPoptimizer_CodeTree::CodeTreeData<std::complex<long double> >]\u2019,\n    inlined from \u2018FPOPT_autoptr<Ref>::~FPOPT_autoptr() [with Ref = FPoptimizer_CodeTree::CodeTreeData<std::complex<long double> >]\u2019 at lib/autoptr.hh:23:30,\n    inlined from \u2018FPoptimizer_CodeTree::CodeTree<Value_t>::~CodeTree() [with Value_t = std::complex<long double>]\u2019 at fpoptimizer/codetree.cc:104:5,\n    inlined from \u2018void {anonymous}::CodeTreeParserData<Value_t>::AddConst(const Value_t&) [with Value_t = std::complex<long double>]\u2019 at fpoptimizer/readbytecode.cc:304:17,\n    inlined from \u2018void FPoptimizer_CodeTree::CodeTree<Value_t>::GenerateFrom(const typename FunctionParserBase<Value_t>::Data&, const std::vector<FPoptimizer_CodeTree::CodeTree<Value_t> >&, bool) [with Value_t = std::complex<long double>]\u2019 at fpoptimizer/readbytecode.cc:487:33:\nlib/autoptr.hh:45:22: note: call to \u2018void operator delete(void*, std::size_t)\u2019 here\n  CXXLD    libdevel.la\n  CXX      libopt_la-fparser.lo\n  CXX      libopt_la-fparser_ad.lo\n  CXX      libopt_la-Faddeeva.lo\n  CXX      libopt_la-fpoptimizer.lo\n  CXXLD    libopt.la\n  CXX      liboprof_la-fparser.lo\n  CXX      liboprof_la-fparser_ad.lo\n  CXX      liboprof_la-Faddeeva.lo\n  CXX      liboprof_la-fpoptimizer.lo\n  CXXLD    liboprof.la\nmake[4]: Leaving directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/fparser'\nmake[3]: Leaving directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/fparser'\nmake[2]: Leaving directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/fparser'\nMaking all in nanoflann\nmake[2]: Entering directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/nanoflann'\nmake[2]: Nothing to be done for 'all'.\nmake[2]: Leaving directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/nanoflann'\nMaking all in timpi\nmake[2]: Entering directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/timpi'\nMaking all in src\nmake[3]: Entering directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/timpi/src'\n  CXX      apps/version.o\n  CXX      parallel/src/libtimpi_dbg_la-communicator.lo\n  CXX      parallel/src/libtimpi_dbg_la-message_tag.lo\n  CXX      parallel/src/libtimpi_dbg_la-request.lo\n  CXX      utilities/src/libtimpi_dbg_la-semipermanent.lo\n  CXX      utilities/src/libtimpi_dbg_la-timpi_assert.lo\n  CXX      utilities/src/libtimpi_dbg_la-timpi_init.lo\n  CXX      utilities/src/libtimpi_dbg_la-timpi_version.lo\n  CXXLD    libtimpi_dbg.la\n  CXXLD    timpi_version-dbg\nlibtool: warning: '-version-info' is ignored for programs\n/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64/libopen-pal.so: undefined reference to `fi_open@FABRIC_1.5'\ncollect2: error: ld returned 1 exit status\nmake[3]: *** [Makefile:950: timpi_version-dbg] Error 1\nmake[3]: Leaving directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/timpi/src'\nmake[2]: *** [Makefile:614: all-recursive] Error 1\nmake[2]: Leaving directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib/timpi'\nmake[1]: *** [Makefile:1033: all-recursive] Error 1\nmake[1]: Leaving directory '/pfs/data5/home/kit/agw/qz9211/projects/moose/libmesh/build/contrib'\nmake: *** [Makefile:33070: all-recursive] Error 1",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4283042",
                  "updatedAt": "2022-12-05T14:30:51Z",
                  "publishedAt": "2022-12-01T12:46:22Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "This is still this fabric library\n/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64/libopen-pal.so: undefined reference to `fi_open@FABRIC_1.5'",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4283572",
                          "updatedAt": "2022-12-01T13:55:08Z",
                          "publishedAt": "2022-12-01T13:55:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "This is still this fabric library\n/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64/libopen-pal.so: undefined reference to `fi_open@FABRIC_1.5'\n\n\nThanks for your reminder. Could you please tell me what are the detailed solutions for this problem? I do not find any similar answers about this problem in our Discussion community.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4283736",
                          "updatedAt": "2022-12-01T14:13:44Z",
                          "publishedAt": "2022-12-01T14:13:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "@roystgnr I am not sure what to make of this issue... PETSc seems to have built fine (you can see the link lines in the last post of this thread).\nDo you have any idea as to what may be going on? I think Joseph's environment is sound... Edit: I am responding here because the latest error results from building libMesh can be seen above.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4313787",
                          "updatedAt": "2022-12-05T14:34:23Z",
                          "publishedAt": "2022-12-05T14:32:54Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "roystgnr"
                          },
                          "bodyText": "Do you have any idea as to what may be going on?\n\nDifferent MPI versions getting mixed?  If that happens, anywhere, the eventual disaster looks something like this.  There can only be one MPI that can get invoked by libMesh, and it has to be the exact same binary that PETSc used, and if you try to build with one but don't clean out every library file before \"starting over\" with another, that still counts as different MPI versions getting mixed.\nmake V=1 might be useful to see what the linker thinks it's trying to do here.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4317685",
                          "updatedAt": "2022-12-05T22:22:00Z",
                          "publishedAt": "2022-12-05T22:21:59Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Joseph-0123"
                  },
                  "bodyText": "The output of the diagnostics script in moose/scripts is shown as follows. It looks like there are no errors.\nThu Dec  1 13:41:04 CET 2022\n\nSystem Arch: LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch Distributor ID: RedHatEnterprise Description: Red Hat Enterprise Linux release 8.4 (Ootpa) Release: 8.4 Codename: Ootpa\n\nMOOSE Package Version: Custom Build\n\nCPU Count: 80\n\nMemory Free: 290202.082 MB\n\n$CC not set\n\nMPICC:\nwhich mpicc:\n\t/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin/mpicc\nmpicc -show:\n\tgcc -I/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/include -pthread -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64 -L/usr/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64 -Wl,-rpath -Wl,/usr/lib64 -Wl,--enable-new-dtags -lmpi\n\nCOMPILER gcc:\ngcc (GCC) 12.1.0\nCopyright (C) 2022 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nPython:\n\t/home/miniconda3/bin/python\n\tPython 3.7.13\n\nMODULES:\n\nCurrently Loaded Modules:\n  1) compiler/gnu/12.1   2) mpi/openmpi/4.1\n\n \n\nPETSC_DIR not set\n\nENVIRONMENT:\nCONDA_SHLVL=1\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.m4a=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.oga=01;36:*.opus=01;36:*.spx=01;36:*.xspf=01;36:\nLD_LIBRARY_PATH=/opt/bwhpc/common/compiler/gnu/12.1.0/lib64:/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64:/home/moose-compilers/gcc-10.2.0/lib64:/home/moose-compilers/gcc-10.2.0/lib:/home/moose-compilers/gcc-10.2.0/lib/gcc/x86_64-pc-linux-gnu/10.2.0:/home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/libexec/gcc/x86_64-pc-linux-gnu/10.2.0:/home/kit/agw/qz9211/moose-compilers/mpich-3.3/lib\nCONDA_EXE=/home/kit/agw/qz9211/miniconda3/bin/conda\n__LMOD_REF_COUNT_PATH=/opt/bwhpc/common/compiler/gnu/12.1.0/bin:1;/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin:1;/home/kit/agw/qz9211/miniconda3/bin:1;/home/kit/agw/qz9211/miniconda3/condabin:1;/home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/bin:1;/home/kit/agw/qz9211/moose-compilers/mpich-3.3/bin:1;/home/kit/agw/qz9211/moose-compilers/miniconda/bin:1;/home/kit/agw/qz9211/.local/bin:1;/home/kit/agw/qz9211/bin:1;/software/all/bin:1;/usr/share/Modules/bin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1\n_ModuleTable002_=MS4qemZpbmFsIiwKfSwKWyJtcGkvb3Blbm1waSJdID0gewpmbiA9ICIvc29mdHdhcmUvYndocGMvY29tbW9uL21vZHVsZWZpbGVzL0NvbXBpbGVyL2dudS8xMi4xL21waS9vcGVubXBpLzQuMS5sdWEiLApmdWxsTmFtZSA9ICJtcGkvb3Blbm1waS80LjEiLApsb2FkT3JkZXIgPSAyLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gIm1waS9vcGVubXBpLzQuMSIsCndWID0gIjAwMDAwMDAwNC4wMDAwMDAwMDEuKnpmaW5hbCIsCn0sCn0sCm1wYXRoQSA9IHsKIi9zb2Z0d2FyZS9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29tcGlsZXIvZ251LzEyLjEiLCAiL29wdC9id2hwYy9raXQvbW9kdWxlZmlsZXMiLCAiL29wdC9i\nSSH_CONNECTION=2a00:1398:4:7e02:3dc8:2104:ec1c:ecb5 54776 2a00:1398:4:1805::810d:3813 22\nMODULES_RUN_QUARANTINE=LD_LIBRARY_PATH LD_PRELOAD\nLANG=en_US.UTF-8\nLMOD_SYSTEM_NAME=uc2\nHISTTIMEFORMAT=%F %T \nGNU_HOME=/opt/bwhpc/common/compiler/gnu/12.1.0\nLMOD_FAMILY_COMPILER_VERSION=12.1\nHOSTNAME=uc2n996.localdomain\nOLDPWD=/home/kit/agw/qz9211/projects/moose/petsc\nGNU_MAN_DIR=/opt/bwhpc/common/compiler/gnu/12.1.0/share/man\nC_INCLUDE_PATH=/home/kit/agw/qz9211/moose-compilers/mpich-3.3/include:\nMPI_INC_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/include\nMPI_MAN_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/share/man\nFPATH=/home/kit/agw/qz9211/moose-compilers/mpich-3.3/include:\nGNU_BIN_DIR=/opt/bwhpc/common/compiler/gnu/12.1.0/bin\nCONDA_PREFIX=/home/kit/agw/qz9211/miniconda3\n__LMOD_REF_COUNT_LD_LIBRARY_PATH=/opt/bwhpc/common/compiler/gnu/12.1.0/lib64:1;/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64:1;/home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/lib64:1;/home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/lib:1;/home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/lib/gcc/x86_64-pc-linux-gnu/10.2.0:1;/home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/libexec/gcc/x86_64-pc-linux-gnu/10.2.0:1;/home/kit/agw/qz9211/moose-compilers/mpich-3.3/lib:1\nMPIDIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1\nGNU_VERSION=12.1.0\nMPI_LIB_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64\n_CE_M=\nPROJ_LIB=/home/kit/agw/qz9211/miniconda3/share/proj\nXDG_SESSION_ID=12297\nMODULES_CMD=/usr/share/Modules/libexec/modulecmd.tcl\nUSER=qz9211\nGNU_LIB_DIR=/opt/bwhpc/common/compiler/gnu/12.1.0/lib64\n__LMOD_REF_COUNT_MODULEPATH=/software/bwhpc/common/modulefiles/Compiler/gnu/12.1:1;/opt/bwhpc/kit/modulefiles:1;/opt/bwhpc/common/modulefiles/Core:1\nPWD=/home/kit/agw/qz9211/projects/moose\nHOME=/home/kit/agw/qz9211\nTMP=/scratch\nCONDA_PYTHON_EXE=/home/kit/agw/qz9211/miniconda3/bin/python\nSSH_CLIENT=2a00:1398:4:7e02:3dc8:2104:ec1c:ecb5 54776 22\nLMOD_VERSION=8.6.16\nLMOD_PAGER=less\nKIT_FAMILY_COMPILER_VERSION=12.1\nF77=mpif77\nXDG_DATA_DIRS=/home/kit/agw/qz9211/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share\nKIT_FAMILY_COMPILER=compiler/gnu\n_CE_CONDA=\nMKL_NUM_THREADS=1\nTMPDIR=/scratch\nLMOD_sys=Linux\n_ModuleTable001_=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IDg2NDAwLApjX3Nob3J0VGltZSA9IDcuNjk0MTI4OTkwMTczMywKZGVwdGhUID0ge30sCmZhbWlseSA9IHsKY29tcGlsZXIgPSAiY29tcGlsZXIvZ251IiwKfSwKbVQgPSB7ClsiY29tcGlsZXIvZ251Il0gPSB7CmZuID0gIi9vcHQvYndocGMvY29tbW9uL21vZHVsZWZpbGVzL0NvcmUvY29tcGlsZXIvZ251LzEyLjEubHVhIiwKZnVsbE5hbWUgPSAiY29tcGlsZXIvZ251LzEyLjEiLApsb2FkT3JkZXIgPSAxLApwcm9wVCA9IHt9LApzdGFja0RlcHRoID0gMSwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gImNvbXBpbGVyL2dudS8xMi4xIiwKd1YgPSAiMDAwMDAwMDEyLjAwMDAwMDAw\nLOADEDMODULES=compiler/gnu/12.1:mpi/openmpi/4.1\nFC=mpif90\n__LMOD_REF_COUNT_MANPATH=/opt/bwhpc/common/compiler/gnu/12.1.0/share/man:1;/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/share/man:1;/home/kit/agw/qz9211/moose-compilers/mpich-3.3/share/man:1;/opt/lmod/lmod/share/man:1\nSCRATCH=/scratch\n_ModuleTable003_=d2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29yZSIsCn0sCnN5c3RlbUJhc2VNUEFUSCA9ICIvb3B0L2J3aHBjL2tpdC9tb2R1bGVmaWxlczovb3B0L2J3aHBjL2NvbW1vbi9tb2R1bGVmaWxlcy9Db3JlIiwKfQo=\nLMOD_ROOT=/opt/lmod\nCONDA_PROMPT_MODIFIER=(base) \nSSH_TTY=/dev/pts/5\nMAIL=/var/spool/mail/qz9211\nMPI_VERSION=4.1.4-gnu-12.1\nMPI_HOME=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1\nSHELL=/bin/bash\nTERM=xterm\nLMOD_SITE_NAME=KIT\n_ModuleTable_Sz_=3\nLMOD_FAMILY_COMPILER=compiler/gnu\nLSDF=/lsdf\nTMOUT=36000\nMPI_BIN_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin\nSHLVL=2\nMANPATH=/opt/bwhpc/common/compiler/gnu/12.1.0/share/man:/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/share/man:/home/kit/agw/qz9211/moose-compilers/mpich-3.3/share/man:/opt/lmod/lmod/share/man::\nTEMP=/scratch\nF90=mpif90\nMODULEPATH=/software/bwhpc/common/modulefiles/Compiler/gnu/12.1:/opt/bwhpc/kit/modulefiles:/opt/bwhpc/common/modulefiles/Core\nLOGNAME=qz9211\nDBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/231419/bus\nCLUSTER=uc2\nXDG_RUNTIME_DIR=/run/user/231419\nCPLUS_INCLUDE_PATH=/home/kit/agw/qz9211/moose-compilers/mpich-3.3/include:\nMODULEPATH_modshare=/usr/share/modulefiles:1:/usr/share/Modules/modulefiles:1:/etc/modulefiles:1\nMODULEPATH_ROOT=/opt/modulefiles\nLMOD_PACKAGE_PATH=/etc/lmod/?.lua;;\nPATH=/opt/bwhpc/common/compiler/gnu/12.1.0/bin:/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin:/home/kit/agw/qz9211/miniconda3/bin:/home/kit/agw/qz9211/miniconda3/condabin:/home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/bin:/home/kit/agw/qz9211/moose-compilers/mpich-3.3/bin:/home/kit/agw/qz9211/moose-compilers/miniconda/bin:/home/kit/agw/qz9211/.local/bin:/home/kit/agw/qz9211/bin:/software/all/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin\nGNU_INC_DIR=/opt/bwhpc/common/compiler/gnu/12.1.0/include\n_LMFILES_=/opt/bwhpc/common/modulefiles/Core/compiler/gnu/12.1.lua:/software/bwhpc/common/modulefiles/Compiler/gnu/12.1/mpi/openmpi/4.1.lua\nMODULESHOME=/opt/lmod/lmod\nLMOD_SETTARG_FULL_SUPPORT=no\nCONDA_DEFAULT_ENV=base\nMPI_EXA_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/examples\nHISTSIZE=1000\nLMOD_PKG=/opt/lmod/lmod\nLMOD_CMD=/opt/lmod/lmod/libexec/lmod\nLESSOPEN=||/usr/bin/lesspipe.sh %s\nOMP_NUM_THREADS=1\nLMOD_DIR=/opt/lmod/lmod/libexec\nBASH_FUNC_module%%=() {  eval $(/opt/bwhpc/common/admin/modules/module-wrapper/modulecmd bash $*)\n}\nBASH_FUNC__module_raw%%=() {  unset _mlshdbg;\n if [ \"${MODULES_SILENT_SHELL_DEBUG:-0}\" = '1' ]; then\n case \"$-\" in \n *v*x*)\n set +vx;\n _mlshdbg='vx'\n ;;\n *v*)\n set +v;\n _mlshdbg='v'\n ;;\n *x*)\n set +x;\n _mlshdbg='x'\n ;;\n *)\n _mlshdbg=''\n ;;\n esac;\n fi;\n unset _mlre _mlIFS;\n if [ -n \"${IFS+x}\" ]; then\n _mlIFS=$IFS;\n fi;\n IFS=' ';\n for _mlv in ${MODULES_RUN_QUARANTINE:-};\n do\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\n if [ -n \"`eval 'echo ${'$_mlv'+x}'`\" ]; then\n _mlre=\"${_mlre:-}${_mlv}_modquar='`eval 'echo ${'$_mlv'}'`' \";\n fi;\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\n _mlre=\"${_mlre:-}${_mlv}='`eval 'echo ${'$_mlrv':-}'`' \";\n fi;\n done;\n if [ -n \"${_mlre:-}\" ]; then\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash '\"$@\"'`;\n else\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\"`;\n fi;\n _mlstatus=$?;\n if [ -n \"${_mlIFS+x}\" ]; then\n IFS=$_mlIFS;\n else\n unset IFS;\n fi;\n unset _mlre _mlv _mlrv _mlIFS;\n if [ -n \"${_mlshdbg:-}\" ]; then\n set -$_mlshdbg;\n fi;\n unset _mlshdbg;\n return $_mlstatus\n}\nBASH_FUNC_switchml%%=() {  typeset swfound=1;\n if [ \"${MODULES_USE_COMPAT_VERSION:-0}\" = '1' ]; then\n typeset swname='main';\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\n typeset swfound=0;\n unset MODULES_USE_COMPAT_VERSION;\n fi;\n else\n typeset swname='compatibility';\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\n typeset swfound=0;\n MODULES_USE_COMPAT_VERSION=1;\n export MODULES_USE_COMPAT_VERSION;\n fi;\n fi;\n if [ $swfound -eq 0 ]; then\n echo \"Switching to Modules $swname version\";\n source /usr/share/Modules/init/bash;\n else\n echo \"Cannot switch to Modules $swname version, command not found\";\n return 1;\n fi\n}\nBASH_FUNC_ml%%=() {  eval \"$($LMOD_DIR/ml_cmd \"$@\")\"\n}\n_=/usr/bin/env",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4283062",
                  "updatedAt": "2022-12-01T14:03:16Z",
                  "publishedAt": "2022-12-01T12:48:47Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Your environment is not sane. I see multiple compilers and MPI wrappers being made available among your PATH:\nPATH=/opt/bwhpc/common/compiler/gnu/12.1.0/bin:/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin/home/kit/agw/qz9211/miniconda3/bin:/home/kit/agw/qz9211/miniconda3/condabin:/home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/bin:/home/kit/agw/qz9211/moose-compilers/mpich-3.3/bin:/home/kit/agw/qz9211/moose-compilers/miniconda/bin:/home/kit/agw/qz9211/.local/bin:/home/kit/agw/qz9211/bin:/software/all/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin\n\n(GCC 12 and 10 at: /opt/bwhpc/common/compiler/gnu/12.1.0/bin ,home/kit/agw/qz9211/moose-compilers/gcc-10.2.0/bin\n(OpenMPI and MPICH at /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin, /home/kit/agw/qz9211/moose-compilers/mpich-3.3/bin)\nIt looks like these two compilers are coming from the System and your implementation of Conda. My advice would be to remove the Conda portion and attempt to use your clusters primary OpenMPI compiler.\nAlso, you need to set CC, CXX, FC, F77, F90, otherwise libMesh will build using 'gcc' and not the necessary MPI wrapper 'mpicc':\nexport CC=mpicc CXX=mpicxx FC=mpif90 F77=mpif77 F90=mpif90\nI would expect when loading the following modules:\nCurrently Loaded Modules:\n  1) compiler/gnu/12.1   2) mpi/openmpi/4.1\n\n...that the above CC, CXX exports would be done for you, however they are not. So you'll need to remember to export these variables each time you wish to use this system's MPI wrapper.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4283742",
                          "updatedAt": "2022-12-01T14:16:18Z",
                          "publishedAt": "2022-12-01T14:14:08Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Joseph-0123"
                  },
                  "bodyText": "**Hello Milljm,\nThanks for your suggestion.\nAfter I received your email, I started to use the cluster's primary OpenMPI compiler and delete my downloaded compiler.**\nI delete my downloaded folders of miniconda3, moose-compilers, and mpich-3.3. And I change the .bashrc contents.\n**However, I get the same errors after I run  ./scripts/update_and_rebuild_libmesh.sh\nAnd I got the following infromation after I run the diagnostics script in moose/scripts. The deleted olders of miniconda3, moose-compilers, and mpich-3.3 shown again.**\n _**.bashrc contents**_\nif [ -f /etc/bashrc ]; then\n\t. /etc/bashrc\nfi\n\nPATH=\"$HOME/.local/bin:$HOME/bin:$PATH\"\nexport PATH\n\nPACKAGES_DIR=$/opt/bwhpc/common/\n\nexport CC=mpicc\nexport CXX=mpicxx\nexport F90=mpif90\nexport F77=mpif77\nexport FC=mpif90\n\nmodule load compiler/gnu/10.2\nmodule load mpi/openmpi/4.1\n\nexport PATH=$PACKAGES_DIR/compiler/gnu/10.2.0/bin:$PACKAGES_DIR/mpi/openmpi/4.1.4-gnu-10.2/bin:$PATH\nexport LD_LIBRARY_PATH=$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/bin:$LD_LIBRARY_PATH\nexport C_INCLUDE_PATH=$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include:$C_INCLUDE_PATH\nexport CPLUS_INCLUDE_PATH=$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include:$CPLUS_INCLUDE_PATH\nexport FPATH=$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include:$FPATH\nexport MANPATH=$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/share/man:$MANPATH\n\nMETHOD=dbg make -j 8\n\n\n\n\n_**diagnostics log**_\n\n\n\n\n\n\n\nFri Dec  2 13:25:12 CET 2022\n\nSystem Arch: LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch Distributor ID: RedHatEnterprise Description: Red Hat Enterprise Linux release 8.4 (Ootpa) Release: 8.4 Codename: Ootpa\n\nMOOSE Package Version: Custom Build\n\nCPU Count: 80\n\nMemory Free: 286905.648 MB\n\nVariable `which $CC` check:\n/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/bin/mpicc\n\n$CC --version:\ngcc (GCC) 10.2.0\nCopyright (C) 2020 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nMPICC:\nwhich mpicc:\n\t/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/bin/mpicc\nmpicc -show:\n\tgcc -I/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include -pthread -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/usr/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/usr/lib64 -Wl,--enable-new-dtags -lmpi\n\nCOMPILER gcc:\ngcc (GCC) 10.2.0\nCopyright (C) 2020 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nPython:\n\t/usr/bin/python\n\tPython 3.6.8\n\nMODULES:\n\nCurrently Loaded Modules:\n  1) compiler/gnu/10.2   2) mpi/openmpi/4.1\n\n \n\nPETSC_DIR not set\n\nENVIRONMENT:\nCONDA_SHLVL=1\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.m4a=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.oga=01;36:*.opus=01;36:*.spx=01;36:*.xspf=01;36:\nLD_LIBRARY_PATH=/opt/bwhpc/common/compiler/gnu/10.2.0/lib:/opt/bwhpc/common/compiler/gnu/10.2.0/lib64:/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64:$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin\nCONDA_EXE=/home/kit/agw/qz9211/miniconda3/bin/conda\n__LMOD_REF_COUNT_PATH=/opt/bwhpc/common/compiler/gnu/10.2.0/bin:1;/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/bin:1;/home/kit/agw/qz9211/miniconda3/bin:1;/home/kit/agw/qz9211/miniconda3/condabin:1;$/opt/bwhpc/common/compiler/gnu/12.1.0/bin:1;$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin:1;/home/kit/agw/qz9211/.local/bin:1;/home/kit/agw/qz9211/bin:1;/software/all/bin:1;/usr/share/Modules/bin:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1\n_ModuleTable002_=LAp9LApbIm1waS9vcGVubXBpIl0gPSB7CmZuID0gIi9zb2Z0d2FyZS9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29tcGlsZXIvZ251LzEwLjIvbXBpL29wZW5tcGkvNC4xLmx1YSIsCmZ1bGxOYW1lID0gIm1waS9vcGVubXBpLzQuMSIsCmxvYWRPcmRlciA9IDIsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAibXBpL29wZW5tcGkvNC4xIiwKd1YgPSAiMDAwMDAwMDA0LjAwMDAwMDAwMS4qemZpbmFsIiwKfSwKfSwKbXBhdGhBID0gewoiL3NvZnR3YXJlL2J3aHBjL2NvbW1vbi9tb2R1bGVmaWxlcy9Db21waWxlci9nbnUvMTAuMiIsICIvb3B0L2J3aHBjL2tpdC9tb2R1bGVmaWxlcyIsICIvb3B0L2J3aHBjL2NvbW1v\nSSH_CONNECTION=2a00:1398:4:7e02:8d30:91d9:4b2a:4edb 53157 2a00:1398:4:1805::810d:3813 22\nMODULES_RUN_QUARANTINE=LD_LIBRARY_PATH LD_PRELOAD\nLANG=en_US.UTF-8\nLMOD_SYSTEM_NAME=uc2\nHISTTIMEFORMAT=%F %T \nGNU_HOME=/opt/bwhpc/common/compiler/gnu/10.2.0\nLMOD_FAMILY_COMPILER_VERSION=10.2\nHOSTNAME=uc2n996.localdomain\nOLDPWD=/home/kit/agw/qz9211\nGNU_MAN_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/share/man\nC_INCLUDE_PATH=$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/include:\nMPI_INC_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include\nMPI_MAN_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/share/man\nFPATH=$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/include:\nGNU_BIN_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/bin\nCONDA_PREFIX=/home/kit/agw/qz9211/miniconda3\n__LMOD_REF_COUNT_LD_LIBRARY_PATH=/opt/bwhpc/common/compiler/gnu/10.2.0/lib:1;/opt/bwhpc/common/compiler/gnu/10.2.0/lib64:1;/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64:1;$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin:1\nMPIDIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2\nGNU_VERSION=10.2.0\nMPI_LIB_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64\n_CE_M=\nCC=mpicc\nPROJ_LIB=/home/kit/agw/qz9211/miniconda3/share/proj\nXDG_SESSION_ID=13033\nMODULES_CMD=/usr/share/Modules/libexec/modulecmd.tcl\nUSER=qz9211\nGNU_LIB_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/lib\n__LMOD_REF_COUNT_MODULEPATH=/software/bwhpc/common/modulefiles/Compiler/gnu/10.2:1;/opt/bwhpc/kit/modulefiles:1;/opt/bwhpc/common/modulefiles/Core:1\nPWD=/home/kit/agw/qz9211/projects/moose\nHOME=/home/kit/agw/qz9211\nTMP=/scratch\nCONDA_PYTHON_EXE=/home/kit/agw/qz9211/miniconda3/bin/python\nSSH_CLIENT=2a00:1398:4:7e02:8d30:91d9:4b2a:4edb 53157 22\nLMOD_VERSION=8.6.16\nLMOD_PAGER=less\nKIT_FAMILY_COMPILER_VERSION=10.2\nF77=mpif77\nXDG_DATA_DIRS=/home/kit/agw/qz9211/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share\nKIT_FAMILY_COMPILER=compiler/gnu\n_CE_CONDA=\nMKL_NUM_THREADS=1\nTMPDIR=/scratch\nLMOD_sys=Linux\n_ModuleTable001_=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IDg2NDAwLApjX3Nob3J0VGltZSA9IGZhbHNlLApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewpjb21waWxlciA9ICJjb21waWxlci9nbnUiLAp9LAptVCA9IHsKWyJjb21waWxlci9nbnUiXSA9IHsKZm4gPSAiL29wdC9id2hwYy9jb21tb24vbW9kdWxlZmlsZXMvQ29yZS9jb21waWxlci9nbnUvMTAuMi5sdWEiLApmdWxsTmFtZSA9ICJjb21waWxlci9nbnUvMTAuMiIsCmxvYWRPcmRlciA9IDEsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAxLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiY29tcGlsZXIvZ251LzEwLjIiLAp3ViA9ICJeMDAwMDAwMTAuMDAwMDAwMDAyLip6ZmluYWwi\nLOADEDMODULES=compiler/gnu/10.2:mpi/openmpi/4.1\nFC=mpif90\n__LMOD_REF_COUNT_MANPATH=/opt/bwhpc/common/compiler/gnu/10.2.0/share/man:1;/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/share/man:1;$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/share/man:1;/opt/lmod/lmod/share/man:1\nSCRATCH=/scratch\n_ModuleTable003_=bi9tb2R1bGVmaWxlcy9Db3JlIiwKfSwKc3lzdGVtQmFzZU1QQVRIID0gIi9vcHQvYndocGMva2l0L21vZHVsZWZpbGVzOi9vcHQvYndocGMvY29tbW9uL21vZHVsZWZpbGVzL0NvcmUiLAp9Cg==\nLMOD_ROOT=/opt/lmod\nCONDA_PROMPT_MODIFIER=(base) \nSSH_TTY=/dev/pts/10\nMAIL=/var/spool/mail/qz9211\nMPI_VERSION=4.1.4-gnu-10.2\nCXX=mpicxx\nMPI_HOME=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2\nSHELL=/bin/bash\nTERM=xterm\nLMOD_SITE_NAME=KIT\n_ModuleTable_Sz_=3\nLMOD_FAMILY_COMPILER=compiler/gnu\nLSDF=/lsdf\nTMOUT=36000\nGNU_LIB64_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/lib64\nMPI_BIN_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/bin\nSHLVL=2\nMANPATH=/opt/bwhpc/common/compiler/gnu/10.2.0/share/man:/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/share/man:$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/share/man:/opt/lmod/lmod/share/man::\nTEMP=/scratch\nF90=mpif90\nMODULEPATH=/software/bwhpc/common/modulefiles/Compiler/gnu/10.2:/opt/bwhpc/kit/modulefiles:/opt/bwhpc/common/modulefiles/Core\nLOGNAME=qz9211\nDBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/231419/bus\nCLUSTER=uc2\nXDG_RUNTIME_DIR=/run/user/231419\nCPLUS_INCLUDE_PATH=$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/include:\nMODULEPATH_modshare=/usr/share/modulefiles:1:/usr/share/Modules/modulefiles:1:/etc/modulefiles:1\nMODULEPATH_ROOT=/opt/modulefiles\nLMOD_PACKAGE_PATH=/etc/lmod/?.lua;;\nPATH=/opt/bwhpc/common/compiler/gnu/10.2.0/bin:/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/bin:/home/kit/agw/qz9211/miniconda3/bin:/home/kit/agw/qz9211/miniconda3/condabin:$/opt/bwhpc/common/compiler/gnu/12.1.0/bin:$/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/bin:/home/kit/agw/qz9211/.local/bin:/home/kit/agw/qz9211/bin:/software/all/bin:/usr/share/Modules/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin\nGNU_INC_DIR=/opt/bwhpc/common/compiler/gnu/10.2.0/include\n_LMFILES_=/opt/bwhpc/common/modulefiles/Core/compiler/gnu/10.2.lua:/software/bwhpc/common/modulefiles/Compiler/gnu/10.2/mpi/openmpi/4.1.lua\nMODULESHOME=/opt/lmod/lmod\nLMOD_SETTARG_FULL_SUPPORT=no\nCONDA_DEFAULT_ENV=base\nMPI_EXA_DIR=/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/examples\nHISTSIZE=1000\nLMOD_PKG=/opt/lmod/lmod\nLMOD_CMD=/opt/lmod/lmod/libexec/lmod\nLESSOPEN=||/usr/bin/lesspipe.sh %s\nOMP_NUM_THREADS=1\nLMOD_DIR=/opt/lmod/lmod/libexec\nBASH_FUNC_module%%=() {  eval $(/opt/bwhpc/common/admin/modules/module-wrapper/modulecmd bash $*)\n}\nBASH_FUNC__module_raw%%=() {  unset _mlshdbg;\n if [ \"${MODULES_SILENT_SHELL_DEBUG:-0}\" = '1' ]; then\n case \"$-\" in \n *v*x*)\n set +vx;\n _mlshdbg='vx'\n ;;\n *v*)\n set +v;\n _mlshdbg='v'\n ;;\n *x*)\n set +x;\n _mlshdbg='x'\n ;;\n *)\n _mlshdbg=''\n ;;\n esac;\n fi;\n unset _mlre _mlIFS;\n if [ -n \"${IFS+x}\" ]; then\n _mlIFS=$IFS;\n fi;\n IFS=' ';\n for _mlv in ${MODULES_RUN_QUARANTINE:-};\n do\n if [ \"${_mlv}\" = \"${_mlv##*[!A-Za-z0-9_]}\" -a \"${_mlv}\" = \"${_mlv#[0-9]}\" ]; then\n if [ -n \"`eval 'echo ${'$_mlv'+x}'`\" ]; then\n _mlre=\"${_mlre:-}${_mlv}_modquar='`eval 'echo ${'$_mlv'}'`' \";\n fi;\n _mlrv=\"MODULES_RUNENV_${_mlv}\";\n _mlre=\"${_mlre:-}${_mlv}='`eval 'echo ${'$_mlrv':-}'`' \";\n fi;\n done;\n if [ -n \"${_mlre:-}\" ]; then\n eval `eval ${_mlre} /usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash '\"$@\"'`;\n else\n eval `/usr/bin/tclsh /usr/share/Modules/libexec/modulecmd.tcl bash \"$@\"`;\n fi;\n _mlstatus=$?;\n if [ -n \"${_mlIFS+x}\" ]; then\n IFS=$_mlIFS;\n else\n unset IFS;\n fi;\n unset _mlre _mlv _mlrv _mlIFS;\n if [ -n \"${_mlshdbg:-}\" ]; then\n set -$_mlshdbg;\n fi;\n unset _mlshdbg;\n return $_mlstatus\n}\nBASH_FUNC_switchml%%=() {  typeset swfound=1;\n if [ \"${MODULES_USE_COMPAT_VERSION:-0}\" = '1' ]; then\n typeset swname='main';\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\n typeset swfound=0;\n unset MODULES_USE_COMPAT_VERSION;\n fi;\n else\n typeset swname='compatibility';\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\n typeset swfound=0;\n MODULES_USE_COMPAT_VERSION=1;\n export MODULES_USE_COMPAT_VERSION;\n fi;\n fi;\n if [ $swfound -eq 0 ]; then\n echo \"Switching to Modules $swname version\";\n source /usr/share/Modules/init/bash;\n else\n echo \"Cannot switch to Modules $swname version, command not found\";\n return 1;\n fi\n}\nBASH_FUNC_ml%%=() {  eval \"$($LMOD_DIR/ml_cmd \"$@\")\"\n}\n_=/usr/bin/env",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4292468",
                  "updatedAt": "2022-12-02T13:43:49Z",
                  "publishedAt": "2022-12-02T13:26:10Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "When you made these changes, did you go back and rebuild PETSc? If not, can you perform the following? I want to see how PETSc is being linked, since it too uses MPI in order to build (yet libMesh configure dies early on with MPI issues):\ncd home/projects/moose/petsc/arch-moose/lib\nldd libpetsc.3.16.so\n(I believe libpetsc.3.16.so is the correct filename). If the file does not exist, can you provide what is listed in this location?",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4313580",
                          "updatedAt": "2022-12-05T14:09:45Z",
                          "publishedAt": "2022-12-05T14:09:45Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "When you made these changes, did you go back and rebuild PETSc? If not, can you perform the following? I want to see how PETSc is being linked, since it too uses MPI in order to build (yet libMesh configure dies early on with MPI issues):\ncd home/projects/moose/petsc/arch-moose/lib\nldd libpetsc.3.16.so\n(I believe libpetsc.3.16.so is the correct filename). If the file does not exist, can you provide what is listed in this location?\n\nI have the libpetsc.so.3.16 in the folder of /projects/moose/petsc/arch-moose/lib.\nAnd I can get the following contents when I run ldd libpetsc.so.3.16.\n    linux-vdso.so.1 (0x00007fff9f9ef000)\n    libHYPRE-2.23.0.so => /home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib/libHYPRE-2.23.0.so (0x000015023c2ac000)\n    libstrumpack.so => /home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib/libstrumpack.so (0x000015023b7cf000)\n    libsuperlu_dist.so.7 => /home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib/libsuperlu_dist.so.7 (0x000015023b4ce000)\n    libhdf5_hl.so.200 => /home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib/libhdf5_hl.so.200 (0x000015023b2aa000)\n    libhdf5.so.200 => /home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib/libhdf5.so.200 (0x000015023ac2d000)\n    libparmetis.so => /home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib/libparmetis.so (0x000015023a9ed000)\n    libmetis.so => /home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib/libmetis.so (0x000015023a78a000)\n    libm.so.6 => /usr/lib64/libm.so.6 (0x000015023a408000)\n    libX11.so.6 => /usr/lib64/libX11.so.6 (0x000015023a0c5000)\n    libstdc++.so.6 => /opt/bwhpc/common/compiler/gnu/10.2.0/lib64/libstdc++.so.6 (0x0000150239ccf000)\n    libdl.so.2 => /usr/lib64/libdl.so.2 (0x0000150239acb000)\n    libmpi_usempif08.so.40 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libmpi_usempif08.so.40 (0x0000150239893000)\n    libmpi_usempi_ignore_tkr.so.40 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libmpi_usempi_ignore_tkr.so.40 (0x0000150239688000)\n    libmpi_mpifh.so.40 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libmpi_mpifh.so.40 (0x000015023941e000)\n    libmpi.so.40 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libmpi.so.40 (0x0000150238f4d000)\n    libgfortran.so.5 => /opt/bwhpc/common/compiler/gnu/10.2.0/lib64/libgfortran.so.5 (0x0000150238a82000)\n    libgcc_s.so.1 => /opt/bwhpc/common/compiler/gnu/10.2.0/lib64/libgcc_s.so.1 (0x0000150238869000)\n    libquadmath.so.0 => /opt/bwhpc/common/compiler/gnu/10.2.0/lib64/libquadmath.so.0 (0x0000150238621000)\n    libpthread.so.0 => /usr/lib64/libpthread.so.0 (0x0000150238401000)\n    librt.so.1 => /usr/lib64/librt.so.1 (0x00001502381f9000)\n    libgomp.so.1 => /opt/bwhpc/common/compiler/gnu/10.2.0/lib64/libgomp.so.1 (0x0000150237fb8000)\n    libc.so.6 => /usr/lib64/libc.so.6 (0x0000150237bf3000)\n    /lib64/ld-linux-x86-64.so.2 (0x000015023e25c000)\n    libxcb.so.1 => /lib64/libxcb.so.1 (0x00001502379ca000)\n    liblustreapi.so.1 => /lib64/liblustreapi.so.1 (0x0000150237798000)\n    libgpfs.so => /lib64/libgpfs.so (0x0000150237582000)\n    libopen-rte.so.40 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libopen-rte.so.40 (0x0000150237261000)\n    libopen-pal.so.40 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libopen-pal.so.40 (0x0000150236dae000)\n    libucp.so.0 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libucp.so.0 (0x0000150236a82000)\n    libuct.so.0 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libuct.so.0 (0x000015023683d000)\n    libucm.so.0 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libucm.so.0 (0x0000150236622000)\n    libucs.so.0 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libucs.so.0 (0x000015023627e000)\n    libfabric.so.1 => /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libfabric.so.1 (0x00001502357d3000)\n    librdmacm.so.1 => /lib64/librdmacm.so.1 (0x00001502355b8000)\n    libibverbs.so.1 => /lib64/libibverbs.so.1 (0x0000150235398000)\n    libpmi2.so.0 => /lib64/libpmi2.so.0 (0x0000150235180000)\n    libpmi.so.0 => /lib64/libpmi.so.0 (0x0000150234f7a000)\n    libutil.so.1 => /lib64/libutil.so.1 (0x0000150234d76000)\n    libz.so.1 => /lib64/libz.so.1 (0x0000150234b5e000)\n    libhwloc.so.15 => /lib64/libhwloc.so.15 (0x000015023490e000)\n    libevent_core-2.1.so.6 => /lib64/libevent_core-2.1.so.6 (0x00001502346d5000)\n    libevent_pthreads-2.1.so.6 => /lib64/libevent_pthreads-2.1.so.6 (0x00001502344d2000)\n    libXau.so.6 => /lib64/libXau.so.6 (0x00001502342ce000)\n    libreadline.so.7 => /lib64/libreadline.so.7 (0x000015023407f000)\n    libnuma.so.1 => /lib64/libnuma.so.1 (0x0000150233e73000)\n    libuuid.so.1 => /lib64/libuuid.so.1 (0x0000150233c6b000)\n    libatomic.so.1 => /opt/bwhpc/common/compiler/gnu/10.2.0/lib64/libatomic.so.1 (0x0000150233a63000)\n    libnl-3.so.200 => /lib64/libnl-3.so.200 (0x0000150233840000)\n    libnl-route-3.so.200 => /lib64/libnl-route-3.so.200 (0x00001502335ba000)\n    libresolv.so.2 => /lib64/libresolv.so.2 (0x00001502333a3000)\n    libslurm_pmi.so => /usr/lib64/slurm/libslurm_pmi.so (0x0000150232fc8000)\n    libcrypto.so.1.1 => /lib64/libcrypto.so.1.1 (0x0000150232ae2000)\n    libtinfo.so.6 => /lib64/libtinfo.so.6 (0x00001502328b5000)",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4313683",
                          "updatedAt": "2022-12-05T14:22:34Z",
                          "publishedAt": "2022-12-05T14:22:34Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Joseph-0123"
                  },
                  "bodyText": "Hello, @roystgnr , I get the following information after I run  make V=1 in the  projects/moose/petsc/ folder.\n==========================================\n \nSee documentation/faq.html and documentation/bugreporting.html\nfor help with installation problems.  Please send EVERYTHING\nprinted out below when reporting problems.  Please check the\nmailing list archives and consider subscribing.\n \n  https://petsc.org/release/community/mailing/\n \n==========================================\nStarting make run on uc2n994.localdomain at Tue, 06 Dec 2022 09:50:34 +0100\nMachine characteristics: Linux uc2n994.localdomain 4.18.0-305.65.1.el8_4.x86_64 #1 SMP Thu Sep 22 08:28:21 EDT 2022 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------\nUsing PETSc directory: /home/kit/agw/qz9211/projects/moose/petsc\nUsing PETSc arch: arch-moose\n-----------------------------------------\nPETSC_VERSION_RELEASE    1\nPETSC_VERSION_MAJOR      3\nPETSC_VERSION_MINOR      16\nPETSC_VERSION_SUBMINOR   6\nPETSC_VERSION_PATCH      0\nPETSC_VERSION_DATE       \"unknown\"\nPETSC_VERSION_GIT        \"unknown\"\nPETSC_VERSION_DATE_GIT   \"unknown\"\nPETSC_VERSION_EQ(MAJOR,MINOR,SUBMINOR) \\\nPETSC_VERSION_ PETSC_VERSION_EQ\nPETSC_VERSION_LT(MAJOR,MINOR,SUBMINOR)          \\\nPETSC_VERSION_LE(MAJOR,MINOR,SUBMINOR) \\\nPETSC_VERSION_GT(MAJOR,MINOR,SUBMINOR) \\\nPETSC_VERSION_GE(MAJOR,MINOR,SUBMINOR) \\\n-----------------------------------------\nUsing configure Options: --download-hypre=1 --with-shared-libraries=1 --download-hdf5=1 --download-hdf5-fortran-bindings=0   --with-debugging=no --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-strumpack=1 --download-scalapack=1 --download-slepc=1 --with-mpi=1 --with-openmp=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0 --with-64-bit-indices \nUsing configuration flags:\n#define INCLUDED_PETSCCONF_H\n#define PETSC_ARCH \"arch-moose\"\n#define PETSC_ATTRIBUTEALIGNED(size) __attribute((aligned(size)))\n#define PETSC_Alignx(a,b)   \n#define PETSC_BLASLAPACK_UNDERSCORE 1\n#define PETSC_CLANGUAGE_C 1\n#define PETSC_CXX_INLINE inline\n#define PETSC_CXX_RESTRICT __restrict\n#define PETSC_C_INLINE inline\n#define PETSC_C_RESTRICT __restrict\n#define PETSC_DEPRECATED_ENUM(why) __attribute((deprecated))\n#define PETSC_DEPRECATED_FUNCTION(why) __attribute((deprecated))\n#define PETSC_DEPRECATED_MACRO(why) _Pragma(why)\n#define PETSC_DEPRECATED_TYPEDEF(why) __attribute((deprecated))\n#define PETSC_DIR \"/home/kit/agw/qz9211/projects/moose/petsc\"\n#define PETSC_DIR_SEPARATOR '/'\n#define PETSC_FORTRAN_CHARLEN_T size_t\n#define PETSC_FORTRAN_TYPE_INITIALIZE  = -2\n#define PETSC_FUNCTION_NAME_C __func__\n#define PETSC_FUNCTION_NAME_CXX __func__\n#define PETSC_HAVE_ACCESS 1\n#define PETSC_HAVE_ATOLL 1\n#define PETSC_HAVE_ATTRIBUTEALIGNED 1\n#define PETSC_HAVE_BUILTIN_EXPECT 1\n#define PETSC_HAVE_BZERO 1\n#define PETSC_HAVE_C99_COMPLEX 1\n#define PETSC_HAVE_CLOCK 1\n#define PETSC_HAVE_CXX 1\n#define PETSC_HAVE_CXX_COMPLEX 1\n#define PETSC_HAVE_CXX_COMPLEX_FIX 1\n#define PETSC_HAVE_CXX_DIALECT_CXX03 1\n#define PETSC_HAVE_CXX_DIALECT_CXX11 1\n#define PETSC_HAVE_DLADDR 1\n#define PETSC_HAVE_DLCLOSE 1\n#define PETSC_HAVE_DLERROR 1\n#define PETSC_HAVE_DLFCN_H 1\n#define PETSC_HAVE_DLOPEN 1\n#define PETSC_HAVE_DLSYM 1\n#define PETSC_HAVE_DOUBLE_ALIGN_MALLOC 1\n#define PETSC_HAVE_DRAND48 1\n#define PETSC_HAVE_DYNAMIC_LIBRARIES 1\n#define PETSC_HAVE_ERF 1\n#define PETSC_HAVE_FBLASLAPACK 1\n#define PETSC_HAVE_FCNTL_H 1\n#define PETSC_HAVE_FENV_H 1\n#define PETSC_HAVE_FLOAT_H 1\n#define PETSC_HAVE_FORK 1\n#define PETSC_HAVE_FORTRAN_FLUSH 1\n#define PETSC_HAVE_FORTRAN_GET_COMMAND_ARGUMENT 1\n#define PETSC_HAVE_FORTRAN_TYPE_STAR 1\n#define PETSC_HAVE_FORTRAN_UNDERSCORE 1\n#define PETSC_HAVE_GETCWD 1\n#define PETSC_HAVE_GETDOMAINNAME 1\n#define PETSC_HAVE_GETHOSTBYNAME 1\n#define PETSC_HAVE_GETHOSTNAME 1\n#define PETSC_HAVE_GETPAGESIZE 1\n#define PETSC_HAVE_GETRUSAGE 1\n#define PETSC_HAVE_HDF5 1\n#define PETSC_HAVE_HYPRE 1\n#define PETSC_HAVE_IMMINTRIN_H 1\n#define PETSC_HAVE_INTTYPES_H 1\n#define PETSC_HAVE_ISINF 1\n#define PETSC_HAVE_ISNAN 1\n#define PETSC_HAVE_ISNORMAL 1\n#define PETSC_HAVE_LGAMMA 1\n#define PETSC_HAVE_LOG2 1\n#define PETSC_HAVE_LSEEK 1\n#define PETSC_HAVE_MALLOC_H 1\n#define PETSC_HAVE_MEMALIGN 1\n#define PETSC_HAVE_MEMMOVE 1\n#define PETSC_HAVE_METIS 1\n#define PETSC_HAVE_MMAP 1\n#define PETSC_HAVE_MPIEXEC_ENVIRONMENTAL_VARIABLE OMP\n#define PETSC_HAVE_MPIIO 1\n#define PETSC_HAVE_MPI_COMBINER_CONTIGUOUS 1\n#define PETSC_HAVE_MPI_COMBINER_DUP 1\n#define PETSC_HAVE_MPI_COMBINER_NAMED 1\n#define PETSC_HAVE_MPI_EXSCAN 1\n#define PETSC_HAVE_MPI_F90MODULE 1\n#define PETSC_HAVE_MPI_F90MODULE_VISIBILITY 1\n#define PETSC_HAVE_MPI_FEATURE_DYNAMIC_WINDOW 1\n#define PETSC_HAVE_MPI_FINALIZED 1\n#define PETSC_HAVE_MPI_GET_ACCUMULATE 1\n#define PETSC_HAVE_MPI_GET_LIBRARY_VERSION 1\n#define PETSC_HAVE_MPI_GPU_AWARE 1\n#define PETSC_HAVE_MPI_IALLREDUCE 1\n#define PETSC_HAVE_MPI_IBARRIER 1\n#define PETSC_HAVE_MPI_INIT_THREAD 1\n#define PETSC_HAVE_MPI_INT64_T 1\n#define PETSC_HAVE_MPI_IN_PLACE 1\n#define PETSC_HAVE_MPI_LONG_DOUBLE 1\n#define PETSC_HAVE_MPI_NEIGHBORHOOD_COLLECTIVES 1\n#define PETSC_HAVE_MPI_NONBLOCKING_COLLECTIVES 1\n#define PETSC_HAVE_MPI_ONE_SIDED 1\n#define PETSC_HAVE_MPI_PROCESS_SHARED_MEMORY 1\n#define PETSC_HAVE_MPI_REDUCE_LOCAL 1\n#define PETSC_HAVE_MPI_REDUCE_SCATTER 1\n#define PETSC_HAVE_MPI_REDUCE_SCATTER_BLOCK 1\n#define PETSC_HAVE_MPI_RGET 1\n#define PETSC_HAVE_MPI_TYPE_DUP 1\n#define PETSC_HAVE_MPI_TYPE_GET_ENVELOPE 1\n#define PETSC_HAVE_MPI_WIN_CREATE 1\n#define PETSC_HAVE_MUMPS 1\n#define PETSC_HAVE_NANOSLEEP 1\n#define PETSC_HAVE_NETDB_H 1\n#define PETSC_HAVE_NETINET_IN_H 1\n#define PETSC_HAVE_OMPI_MAJOR_VERSION 4\n#define PETSC_HAVE_OMPI_MINOR_VERSION 1\n#define PETSC_HAVE_OMPI_RELEASE_VERSION 4\n#define PETSC_HAVE_OPENMP 1\n#define PETSC_HAVE_PACKAGES \":blaslapack:fblaslapack:hdf5:hypre:mathlib:metis:mpi:mumps:openmp:parmetis:pthread:ptscotch:regex:scalapack:strumpack:superlu_dist:x11:\"\n#define PETSC_HAVE_PARMETIS 1\n#define PETSC_HAVE_POPEN 1\n#define PETSC_HAVE_PTHREAD 1\n#define PETSC_HAVE_PTHREAD_BARRIER_T 1\n#define PETSC_HAVE_PTHREAD_H 1\n#define PETSC_HAVE_PTSCOTCH 1\n#define PETSC_HAVE_PWD_H 1\n#define PETSC_HAVE_RAND 1\n#define PETSC_HAVE_READLINK 1\n#define PETSC_HAVE_REALPATH 1\n#define PETSC_HAVE_REAL___FLOAT128 1\n#define PETSC_HAVE_REGEX 1\n#define PETSC_HAVE_RTLD_GLOBAL 1\n#define PETSC_HAVE_RTLD_LAZY 1\n#define PETSC_HAVE_RTLD_LOCAL 1\n#define PETSC_HAVE_RTLD_NOW 1\n#define PETSC_HAVE_SCALAPACK 1\n#define PETSC_HAVE_SCHED_CPU_SET_T 1\n#define PETSC_HAVE_SCOTCH_PARMETIS_V3_NODEND 1\n#define PETSC_HAVE_SETJMP_H 1\n#define PETSC_HAVE_SLEEP 1\n#define PETSC_HAVE_SLEPC 1\n#define PETSC_HAVE_SNPRINTF 1\n#define PETSC_HAVE_SOCKET 1\n#define PETSC_HAVE_SO_REUSEADDR 1\n#define PETSC_HAVE_STDINT_H 1\n#define PETSC_HAVE_STRCASECMP 1\n#define PETSC_HAVE_STRINGS_H 1\n#define PETSC_HAVE_STRUCT_SIGACTION 1\n#define PETSC_HAVE_STRUMPACK 1\n#define PETSC_HAVE_SUPERLU_DIST 1\n#define PETSC_HAVE_SYSINFO 1\n#define PETSC_HAVE_SYS_PARAM_H 1\n#define PETSC_HAVE_SYS_PROCFS_H 1\n#define PETSC_HAVE_SYS_RESOURCE_H 1\n#define PETSC_HAVE_SYS_SOCKET_H 1\n#define PETSC_HAVE_SYS_SYSCTL_H 1\n#define PETSC_HAVE_SYS_SYSINFO_H 1\n#define PETSC_HAVE_SYS_TIMES_H 1\n#define PETSC_HAVE_SYS_TIME_H 1\n#define PETSC_HAVE_SYS_TYPES_H 1\n#define PETSC_HAVE_SYS_UTSNAME_H 1\n#define PETSC_HAVE_SYS_WAIT_H 1\n#define PETSC_HAVE_TGAMMA 1\n#define PETSC_HAVE_TIME 1\n#define PETSC_HAVE_TIME_H 1\n#define PETSC_HAVE_UNAME 1\n#define PETSC_HAVE_UNISTD_H 1\n#define PETSC_HAVE_USLEEP 1\n#define PETSC_HAVE_VA_COPY 1\n#define PETSC_HAVE_VSNPRINTF 1\n#define PETSC_HAVE_X 1\n#define PETSC_HAVE_XMMINTRIN_H 1\n#define PETSC_HDF5_HAVE_PARALLEL 1\n#define PETSC_IS_COLORING_MAX USHRT_MAX\n#define PETSC_IS_COLORING_VALUE_TYPE short\n#define PETSC_IS_COLORING_VALUE_TYPE_F integer2\n#define PETSC_LEVEL1_DCACHE_LINESIZE 64\n#define PETSC_LIB_DIR \"/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib\"\n#define PETSC_MAX_PATH_LEN 4096\n#define PETSC_MEMALIGN 16\n#define PETSC_MPICC_SHOW \"gcc -I/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include -pthread -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/usr/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/usr/lib64 -Wl,--enable-new-dtags -lmpi\"\n#define PETSC_MPIU_IS_COLORING_VALUE_TYPE MPI_UNSIGNED_SHORT\n#define PETSC_PREFETCH_HINT_NTA _MM_HINT_NTA\n#define PETSC_PREFETCH_HINT_T0 _MM_HINT_T0\n#define PETSC_PREFETCH_HINT_T1 _MM_HINT_T1\n#define PETSC_PREFETCH_HINT_T2 _MM_HINT_T2\n#define PETSC_PYTHON_EXE \"/usr/bin/python\"\n#define PETSC_Prefetch(a,b,c) _mm_prefetch((const char*)(a),(c))\n#define PETSC_REPLACE_DIR_SEPARATOR '\\\\'\n#define PETSC_SIGNAL_CAST  \n#define PETSC_SIZEOF_ENUM 4\n#define PETSC_SIZEOF_INT 4\n#define PETSC_SIZEOF_LONG 8\n#define PETSC_SIZEOF_LONG_LONG 8\n#define PETSC_SIZEOF_SHORT 2\n#define PETSC_SIZEOF_SIZE_T 8\n#define PETSC_SIZEOF_VOID_P 8\n#define PETSC_SLSUFFIX \"so\"\n#define PETSC_UINTPTR_T uintptr_t\n#define PETSC_UNUSED __attribute((unused))\n#define PETSC_USE_64BIT_INDICES 1\n#define PETSC_USE_AVX512_KERNELS 1\n#define PETSC_USE_BACKWARD_LOOP 1\n#define PETSC_USE_CTABLE 1\n#define PETSC_USE_DEBUGGER \"gdb\"\n#define PETSC_USE_INFO 1\n#define PETSC_USE_ISATTY 1\n#define PETSC_USE_LOG 1\n#define PETSC_USE_MALLOC_COALESCED 1\n#define PETSC_USE_PROC_FOR_SIZE 1\n#define PETSC_USE_REAL_DOUBLE 1\n#define PETSC_USE_SHARED_LIBRARIES 1\n#define PETSC_USE_SINGLE_LIBRARY 1\n#define PETSC_USE_SOCKET_VIEWER 1\n#define PETSC_USE_VISIBILITY_C 1\n#define PETSC_USE_VISIBILITY_CXX 1\n#define PETSC_USING_64BIT_PTR 1\n#define PETSC_USING_F2003 1\n#define PETSC_USING_F90FREEFORM 1\n#define PETSC_VERSION_BRANCH_GIT \"HEAD\"\n#define PETSC_VERSION_DATE_GIT \"2022-07-19 17:19:37 -0500\"\n#define PETSC_VERSION_GIT \"v3.16.6-1-g477e44bbb55\"\n#define PETSC__BSD_SOURCE 1\n#define PETSC__DEFAULT_SOURCE 1\n#define PETSC__GNU_SOURCE 1\n-----------------------------------------\nUsing C compile: mpicc -o .o -c -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O -fopenmp    \nmpicc -show: gcc -I/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include -pthread -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/usr/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/usr/lib64 -Wl,--enable-new-dtags -lmpi\nC compiler version: gcc (GCC) 10.2.0\nUsing C++ compile: mpicxx -o .o -c -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O -fopenmp  -fPIC -std=gnu++11  -fopenmp  -I/home/kit/agw/qz9211/projects/moose/petsc/include -I/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/include  -fopenmp \nmpicxx -show: g++ -I/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include -pthread -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/usr/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/usr/lib64 -Wl,--enable-new-dtags -lmpi\nC++ compiler version: g++ (GCC) 10.2.0\nUsing Fortran compile: mpif90 -o .o -c -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O  -fopenmp   -I/home/kit/agw/qz9211/projects/moose/petsc/include -I/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/include   \nmpif90 -show: gfortran -I/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include -pthread -I/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/usr/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/usr/lib64 -Wl,--enable-new-dtags -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi\nFortran compiler version: GNU Fortran (GCC) 10.2.0\n-----------------------------------------\nUsing C/C++ linker: mpicc\nUsing C/C++ flags: -fopenmp -fopenmp   -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O -fopenmp\nUsing Fortran linker: mpif90\nUsing Fortran flags: -fopenmp -fopenmp   -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O  -fopenmp\n-----------------------------------------\nUsing system modules: compiler/gnu/10.2:mpi/openmpi/4.1\nUsing mpi.h: # 1 \"/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include/mpi.h\" 1\n-----------------------------------------\nUsing libraries: -Wl,-rpath,/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib -L/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib -Wl,-rpath,/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib -L/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib -Wl,-rpath,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -Wl,-rpath,/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib/gcc/x86_64-pc-linux-gnu/10.2.0 -L/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib/gcc/x86_64-pc-linux-gnu/10.2.0 -Wl,-rpath,/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib/gcc -L/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib/gcc -Wl,-rpath,/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib64 -L/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib64 -Wl,-rpath,/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib -L/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib -lpetsc -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lstrumpack -lscalapack -lsuperlu_dist -lflapack -lfblas -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lhdf5_hl -lhdf5 -lparmetis -lmetis -lm -lX11 -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lrt -lquadmath -lstdc++ -ldl\n------------------------------------------\nUsing mpiexec: mpiexec --oversubscribe\n------------------------------------------\nUsing MAKE: /usr/bin/gmake\nUsing MAKEFLAGS: -j44 -l113.6  --no-print-directory -- PETSC_DIR=/home/kit/agw/qz9211/projects/moose/petsc PETSC_ARCH=arch-moose V=1\n==========================================\ngmake[3]: Nothing to be done for 'libs'.\n*** Building SLEPc ***\nChecking environment... \nCleaning arch dir /pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc/arch-moose...\ndone\nChecking PETSc installation... \nxxx==========================================================================xxx\nWARNING: Your PETSc and SLEPc repos may not be in sync (more than 30 days apart)\nxxx==========================================================================xxx\ndone\nChecking LAPACK library... done\nChecking SCALAPACK... done\nWriting various configuration files... done\n \n================================================================================\nSLEPc Configuration\n================================================================================\n\nSLEPc directory:\n  /pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc\n  It is a git repository on branch: tags/v3.16.2^0\nSLEPc prefix directory:\n  /pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose\nPETSc directory:\n  /home/kit/agw/qz9211/projects/moose/petsc\n  It is a git repository on branch: tags/v3.17.4~4^2\nArchitecture \"arch-moose\" with double precision real numbers\nSCALAPACK from SCALAPACK linked by PETSc\n\nxxx==========================================================================xxx\n Configure stage complete. Now build the SLEPc library with:\n   make SLEPC_DIR=/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc PETSC_DIR=/home/kit/agw/qz9211/projects/moose/petsc PETSC_ARCH=arch-moose\nxxx==========================================================================xxx\n\n==========================================\nStarting make run on uc2n994.localdomain at Tue, 06 Dec 2022 09:51:10 +0100\nMachine characteristics: Linux uc2n994.localdomain 4.18.0-305.65.1.el8_4.x86_64 #1 SMP Thu Sep 22 08:28:21 EDT 2022 x86_64 x86_64 x86_64 GNU/Linux\n-----------------------------------------\nUsing SLEPc directory: /pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc\nUsing PETSc directory: /home/kit/agw/qz9211/projects/moose/petsc\nUsing PETSc arch: arch-moose\n-----------------------------------------\nSLEPC_VERSION_RELEASE    1\nSLEPC_VERSION_MAJOR      3\nSLEPC_VERSION_MINOR      16\nSLEPC_VERSION_SUBMINOR   2\nSLEPC_VERSION_PATCH      0\nSLEPC_VERSION_DATE       \"unknown\"\nSLEPC_VERSION_GIT        \"unknown\"\nSLEPC_VERSION_DATE_GIT   \"unknown\"\nSLEPC_VERSION_EQ(MAJOR,MINOR,SUBMINOR) \\\nSLEPC_VERSION_ SLEPC_VERSION_EQ\nSLEPC_VERSION_LT(MAJOR,MINOR,SUBMINOR)          \\\nSLEPC_VERSION_LE(MAJOR,MINOR,SUBMINOR) \\\nSLEPC_VERSION_GT(MAJOR,MINOR,SUBMINOR) \\\nSLEPC_VERSION_GE(MAJOR,MINOR,SUBMINOR) \\\n-----------------------------------------\nUsing SLEPc configure options: --with-clean=1 --prefix=/home/kit/agw/qz9211/projects/moose/petsc/arch-moose\nUsing SLEPc configuration flags:\n#define INCLUDED_SLEPCCONF_H\n#define SLEPC_PETSC_DIR \"/home/kit/agw/qz9211/projects/moose/petsc\"\n#define SLEPC_PETSC_ARCH \"arch-moose\"\n#define SLEPC_DIR \"/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc\"\n#define SLEPC_LIB_DIR \"/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib\"\n#define SLEPC_VERSION_GIT \"v3.16.2\"\n#define SLEPC_VERSION_DATE_GIT \"2022-02-01 11:25:32 +0100\"\n#define SLEPC_VERSION_BRANCH_GIT \"tags/v3.16.2^0\"\n#define SLEPC_MISSING_LAPACK_GGSVD3 1\n#define SLEPC_HAVE_SCALAPACK 1\n#define SLEPC_SCALAPACK_HAVE_UNDERSCORE 1\n#define SLEPC_HAVE_PACKAGES \":scalapack:\"\n-----------------------------------------\nPETSC_VERSION_RELEASE    1\nPETSC_VERSION_MAJOR      3\nPETSC_VERSION_MINOR      16\nPETSC_VERSION_SUBMINOR   6\nPETSC_VERSION_PATCH      0\nPETSC_VERSION_DATE       \"unknown\"\nPETSC_VERSION_GIT        \"unknown\"\nPETSC_VERSION_DATE_GIT   \"unknown\"\nPETSC_VERSION_EQ(MAJOR,MINOR,SUBMINOR) \\\nPETSC_VERSION_ PETSC_VERSION_EQ\nPETSC_VERSION_LT(MAJOR,MINOR,SUBMINOR)          \\\nPETSC_VERSION_LE(MAJOR,MINOR,SUBMINOR) \\\nPETSC_VERSION_GT(MAJOR,MINOR,SUBMINOR) \\\nPETSC_VERSION_GE(MAJOR,MINOR,SUBMINOR) \\\n-----------------------------------------\nUsing PETSc configure options: --download-hypre=1 --with-shared-libraries=1 --download-hdf5=1 --download-hdf5-fortran-bindings=0   --with-debugging=no --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-strumpack=1 --download-scalapack=1 --download-slepc=1 --with-mpi=1 --with-openmp=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0 --with-64-bit-indices \nUsing PETSc configuration flags:\n#define INCLUDED_PETSCCONF_H\n#define PETSC_ARCH \"arch-moose\"\n#define PETSC_ATTRIBUTEALIGNED(size) __attribute((aligned(size)))\n#define PETSC_Alignx(a,b)   \n#define PETSC_BLASLAPACK_UNDERSCORE 1\n#define PETSC_CLANGUAGE_C 1\n#define PETSC_CXX_INLINE inline\n#define PETSC_CXX_RESTRICT __restrict\n#define PETSC_C_INLINE inline\n#define PETSC_C_RESTRICT __restrict\n#define PETSC_DEPRECATED_ENUM(why) __attribute((deprecated))\n#define PETSC_DEPRECATED_FUNCTION(why) __attribute((deprecated))\n#define PETSC_DEPRECATED_MACRO(why) _Pragma(why)\n#define PETSC_DEPRECATED_TYPEDEF(why) __attribute((deprecated))\n#define PETSC_DIR \"/home/kit/agw/qz9211/projects/moose/petsc\"\n#define PETSC_DIR_SEPARATOR '/'\n#define PETSC_FORTRAN_CHARLEN_T size_t\n#define PETSC_FORTRAN_TYPE_INITIALIZE  = -2\n#define PETSC_FUNCTION_NAME_C __func__\n#define PETSC_FUNCTION_NAME_CXX __func__\n#define PETSC_HAVE_ACCESS 1\n#define PETSC_HAVE_ATOLL 1\n#define PETSC_HAVE_ATTRIBUTEALIGNED 1\n#define PETSC_HAVE_BUILTIN_EXPECT 1\n#define PETSC_HAVE_BZERO 1\n#define PETSC_HAVE_C99_COMPLEX 1\n#define PETSC_HAVE_CLOCK 1\n#define PETSC_HAVE_CXX 1\n#define PETSC_HAVE_CXX_COMPLEX 1\n#define PETSC_HAVE_CXX_COMPLEX_FIX 1\n#define PETSC_HAVE_CXX_DIALECT_CXX03 1\n#define PETSC_HAVE_CXX_DIALECT_CXX11 1\n#define PETSC_HAVE_DLADDR 1\n#define PETSC_HAVE_DLCLOSE 1\n#define PETSC_HAVE_DLERROR 1\n#define PETSC_HAVE_DLFCN_H 1\n#define PETSC_HAVE_DLOPEN 1\n#define PETSC_HAVE_DLSYM 1\n#define PETSC_HAVE_DOUBLE_ALIGN_MALLOC 1\n#define PETSC_HAVE_DRAND48 1\n#define PETSC_HAVE_DYNAMIC_LIBRARIES 1\n#define PETSC_HAVE_ERF 1\n#define PETSC_HAVE_FBLASLAPACK 1\n#define PETSC_HAVE_FCNTL_H 1\n#define PETSC_HAVE_FENV_H 1\n#define PETSC_HAVE_FLOAT_H 1\n#define PETSC_HAVE_FORK 1\n#define PETSC_HAVE_FORTRAN_FLUSH 1\n#define PETSC_HAVE_FORTRAN_GET_COMMAND_ARGUMENT 1\n#define PETSC_HAVE_FORTRAN_TYPE_STAR 1\n#define PETSC_HAVE_FORTRAN_UNDERSCORE 1\n#define PETSC_HAVE_GETCWD 1\n#define PETSC_HAVE_GETDOMAINNAME 1\n#define PETSC_HAVE_GETHOSTBYNAME 1\n#define PETSC_HAVE_GETHOSTNAME 1\n#define PETSC_HAVE_GETPAGESIZE 1\n#define PETSC_HAVE_GETRUSAGE 1\n#define PETSC_HAVE_HDF5 1\n#define PETSC_HAVE_HYPRE 1\n#define PETSC_HAVE_IMMINTRIN_H 1\n#define PETSC_HAVE_INTTYPES_H 1\n#define PETSC_HAVE_ISINF 1\n#define PETSC_HAVE_ISNAN 1\n#define PETSC_HAVE_ISNORMAL 1\n#define PETSC_HAVE_LGAMMA 1\n#define PETSC_HAVE_LOG2 1\n#define PETSC_HAVE_LSEEK 1\n#define PETSC_HAVE_MALLOC_H 1\n#define PETSC_HAVE_MEMALIGN 1\n#define PETSC_HAVE_MEMMOVE 1\n#define PETSC_HAVE_METIS 1\n#define PETSC_HAVE_MMAP 1\n#define PETSC_HAVE_MPIEXEC_ENVIRONMENTAL_VARIABLE OMP\n#define PETSC_HAVE_MPIIO 1\n#define PETSC_HAVE_MPI_COMBINER_CONTIGUOUS 1\n#define PETSC_HAVE_MPI_COMBINER_DUP 1\n#define PETSC_HAVE_MPI_COMBINER_NAMED 1\n#define PETSC_HAVE_MPI_EXSCAN 1\n#define PETSC_HAVE_MPI_F90MODULE 1\n#define PETSC_HAVE_MPI_F90MODULE_VISIBILITY 1\n#define PETSC_HAVE_MPI_FEATURE_DYNAMIC_WINDOW 1\n#define PETSC_HAVE_MPI_FINALIZED 1\n#define PETSC_HAVE_MPI_GET_ACCUMULATE 1\n#define PETSC_HAVE_MPI_GET_LIBRARY_VERSION 1\n#define PETSC_HAVE_MPI_GPU_AWARE 1\n#define PETSC_HAVE_MPI_IALLREDUCE 1\n#define PETSC_HAVE_MPI_IBARRIER 1\n#define PETSC_HAVE_MPI_INIT_THREAD 1\n#define PETSC_HAVE_MPI_INT64_T 1\n#define PETSC_HAVE_MPI_IN_PLACE 1\n#define PETSC_HAVE_MPI_LONG_DOUBLE 1\n#define PETSC_HAVE_MPI_NEIGHBORHOOD_COLLECTIVES 1\n#define PETSC_HAVE_MPI_NONBLOCKING_COLLECTIVES 1\n#define PETSC_HAVE_MPI_ONE_SIDED 1\n#define PETSC_HAVE_MPI_PROCESS_SHARED_MEMORY 1\n#define PETSC_HAVE_MPI_REDUCE_LOCAL 1\n#define PETSC_HAVE_MPI_REDUCE_SCATTER 1\n#define PETSC_HAVE_MPI_REDUCE_SCATTER_BLOCK 1\n#define PETSC_HAVE_MPI_RGET 1\n#define PETSC_HAVE_MPI_TYPE_DUP 1\n#define PETSC_HAVE_MPI_TYPE_GET_ENVELOPE 1\n#define PETSC_HAVE_MPI_WIN_CREATE 1\n#define PETSC_HAVE_MUMPS 1\n#define PETSC_HAVE_NANOSLEEP 1\n#define PETSC_HAVE_NETDB_H 1\n#define PETSC_HAVE_NETINET_IN_H 1\n#define PETSC_HAVE_OMPI_MAJOR_VERSION 4\n#define PETSC_HAVE_OMPI_MINOR_VERSION 1\n#define PETSC_HAVE_OMPI_RELEASE_VERSION 4\n#define PETSC_HAVE_OPENMP 1\n#define PETSC_HAVE_PACKAGES \":blaslapack:fblaslapack:hdf5:hypre:mathlib:metis:mpi:mumps:openmp:parmetis:pthread:ptscotch:regex:scalapack:strumpack:superlu_dist:x11:\"\n#define PETSC_HAVE_PARMETIS 1\n#define PETSC_HAVE_POPEN 1\n#define PETSC_HAVE_PTHREAD 1\n#define PETSC_HAVE_PTHREAD_BARRIER_T 1\n#define PETSC_HAVE_PTHREAD_H 1\n#define PETSC_HAVE_PTSCOTCH 1\n#define PETSC_HAVE_PWD_H 1\n#define PETSC_HAVE_RAND 1\n#define PETSC_HAVE_READLINK 1\n#define PETSC_HAVE_REALPATH 1\n#define PETSC_HAVE_REAL___FLOAT128 1\n#define PETSC_HAVE_REGEX 1\n#define PETSC_HAVE_RTLD_GLOBAL 1\n#define PETSC_HAVE_RTLD_LAZY 1\n#define PETSC_HAVE_RTLD_LOCAL 1\n#define PETSC_HAVE_RTLD_NOW 1\n#define PETSC_HAVE_SCALAPACK 1\n#define PETSC_HAVE_SCHED_CPU_SET_T 1\n#define PETSC_HAVE_SCOTCH_PARMETIS_V3_NODEND 1\n#define PETSC_HAVE_SETJMP_H 1\n#define PETSC_HAVE_SLEEP 1\n#define PETSC_HAVE_SLEPC 1\n#define PETSC_HAVE_SNPRINTF 1\n#define PETSC_HAVE_SOCKET 1\n#define PETSC_HAVE_SO_REUSEADDR 1\n#define PETSC_HAVE_STDINT_H 1\n#define PETSC_HAVE_STRCASECMP 1\n#define PETSC_HAVE_STRINGS_H 1\n#define PETSC_HAVE_STRUCT_SIGACTION 1\n#define PETSC_HAVE_STRUMPACK 1\n#define PETSC_HAVE_SUPERLU_DIST 1\n#define PETSC_HAVE_SYSINFO 1\n#define PETSC_HAVE_SYS_PARAM_H 1\n#define PETSC_HAVE_SYS_PROCFS_H 1\n#define PETSC_HAVE_SYS_RESOURCE_H 1\n#define PETSC_HAVE_SYS_SOCKET_H 1\n#define PETSC_HAVE_SYS_SYSCTL_H 1\n#define PETSC_HAVE_SYS_SYSINFO_H 1\n#define PETSC_HAVE_SYS_TIMES_H 1\n#define PETSC_HAVE_SYS_TIME_H 1\n#define PETSC_HAVE_SYS_TYPES_H 1\n#define PETSC_HAVE_SYS_UTSNAME_H 1\n#define PETSC_HAVE_SYS_WAIT_H 1\n#define PETSC_HAVE_TGAMMA 1\n#define PETSC_HAVE_TIME 1\n#define PETSC_HAVE_TIME_H 1\n#define PETSC_HAVE_UNAME 1\n#define PETSC_HAVE_UNISTD_H 1\n#define PETSC_HAVE_USLEEP 1\n#define PETSC_HAVE_VA_COPY 1\n#define PETSC_HAVE_VSNPRINTF 1\n#define PETSC_HAVE_X 1\n#define PETSC_HAVE_XMMINTRIN_H 1\n#define PETSC_HDF5_HAVE_PARALLEL 1\n#define PETSC_IS_COLORING_MAX USHRT_MAX\n#define PETSC_IS_COLORING_VALUE_TYPE short\n#define PETSC_IS_COLORING_VALUE_TYPE_F integer2\n#define PETSC_LEVEL1_DCACHE_LINESIZE 64\n#define PETSC_LIB_DIR \"/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib\"\n#define PETSC_MAX_PATH_LEN 4096\n#define PETSC_MEMALIGN 16\n#define PETSC_MPICC_SHOW \"gcc -I/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/include -pthread -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/usr/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -Wl,-rpath -Wl,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath -Wl,/usr/lib64 -Wl,--enable-new-dtags -lmpi\"\n#define PETSC_MPIU_IS_COLORING_VALUE_TYPE MPI_UNSIGNED_SHORT\n#define PETSC_PREFETCH_HINT_NTA _MM_HINT_NTA\n#define PETSC_PREFETCH_HINT_T0 _MM_HINT_T0\n#define PETSC_PREFETCH_HINT_T1 _MM_HINT_T1\n#define PETSC_PREFETCH_HINT_T2 _MM_HINT_T2\n#define PETSC_PYTHON_EXE \"/usr/bin/python\"\n#define PETSC_Prefetch(a,b,c) _mm_prefetch((const char*)(a),(c))\n#define PETSC_REPLACE_DIR_SEPARATOR '\\\\'\n#define PETSC_SIGNAL_CAST  \n#define PETSC_SIZEOF_ENUM 4\n#define PETSC_SIZEOF_INT 4\n#define PETSC_SIZEOF_LONG 8\n#define PETSC_SIZEOF_LONG_LONG 8\n#define PETSC_SIZEOF_SHORT 2\n#define PETSC_SIZEOF_SIZE_T 8\n#define PETSC_SIZEOF_VOID_P 8\n#define PETSC_SLSUFFIX \"so\"\n#define PETSC_UINTPTR_T uintptr_t\n#define PETSC_UNUSED __attribute((unused))\n#define PETSC_USE_64BIT_INDICES 1\n#define PETSC_USE_AVX512_KERNELS 1\n#define PETSC_USE_BACKWARD_LOOP 1\n#define PETSC_USE_CTABLE 1\n#define PETSC_USE_DEBUGGER \"gdb\"\n#define PETSC_USE_INFO 1\n#define PETSC_USE_ISATTY 1\n#define PETSC_USE_LOG 1\n#define PETSC_USE_MALLOC_COALESCED 1\n#define PETSC_USE_PROC_FOR_SIZE 1\n#define PETSC_USE_REAL_DOUBLE 1\n#define PETSC_USE_SHARED_LIBRARIES 1\n#define PETSC_USE_SINGLE_LIBRARY 1\n#define PETSC_USE_SOCKET_VIEWER 1\n#define PETSC_USE_VISIBILITY_C 1\n#define PETSC_USE_VISIBILITY_CXX 1\n#define PETSC_USING_64BIT_PTR 1\n#define PETSC_USING_F2003 1\n#define PETSC_USING_F90FREEFORM 1\n#define PETSC_VERSION_BRANCH_GIT \"HEAD\"\n#define PETSC_VERSION_DATE_GIT \"2022-07-19 17:19:37 -0500\"\n#define PETSC_VERSION_GIT \"v3.16.6-1-g477e44bbb55\"\n#define PETSC__BSD_SOURCE 1\n#define PETSC__DEFAULT_SOURCE 1\n#define PETSC__GNU_SOURCE 1\n-----------------------------------------\nUsing C/C++ include paths: -I/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc/include -I/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc/arch-moose/include      -I/home/kit/agw/qz9211/projects/moose/petsc/include -I/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/include\nUsing C/C++ compiler: mpicc -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O -fopenmp    \nUsing Fortran include/module paths: -I/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc/include -I/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc/arch-moose/include -I/home/kit/agw/qz9211/projects/moose/petsc/include -I/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/include\nUsing Fortran compiler: mpif90 -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O  -fopenmp   \n-----------------------------------------\nUsing C/C++ linker: mpicc\nUsing C/C++ flags: -fopenmp -fopenmp   -fPIC -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -fstack-protector -fvisibility=hidden -g -O -fopenmp\nUsing Fortran linker: mpif90\nUsing Fortran flags: -fopenmp -fopenmp   -fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument -g -O  -fopenmp\n-----------------------------------------\nUsing libraries: -Wl,-rpath,/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc/arch-moose/lib -L/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc/arch-moose/lib -lslepc         -Wl,-rpath,/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib -L/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib -Wl,-rpath,/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib -L/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/lib -Wl,-rpath,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64 -Wl,-rpath,/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -L/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib -Wl,-rpath,/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib/gcc/x86_64-pc-linux-gnu/10.2.0 -L/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib/gcc/x86_64-pc-linux-gnu/10.2.0 -Wl,-rpath,/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib/gcc -L/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib/gcc -Wl,-rpath,/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib64 -L/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib64 -Wl,-rpath,/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib -L/pfs/data5/software_uc2/bwhpc/common/compiler/gnu/10.2.0/lib -lpetsc -lHYPRE -lcmumps -ldmumps -lsmumps -lzmumps -lmumps_common -lpord -lstrumpack -lscalapack -lsuperlu_dist -lflapack -lfblas -lptesmumps -lptscotchparmetis -lptscotch -lptscotcherr -lesmumps -lscotch -lscotcherr -lhdf5_hl -lhdf5 -lparmetis -lmetis -lm -lX11 -lstdc++ -ldl -lmpi_usempif08 -lmpi_usempi_ignore_tkr -lmpi_mpifh -lmpi -lgfortran -lm -lgfortran -lm -lgcc_s -lquadmath -lpthread -lrt -lquadmath -lstdc++ -ldl\n------------------------------------------\nUsing mpiexec: mpiexec --oversubscribe\n------------------------------------------\nUsing MAKEFLAGS: -j44 -l113.6  --no-print-directory -- SLEPC_DIR=/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc V=1 PETSC_ARCH=arch-moose PETSC_DIR=/home/kit/agw/qz9211/projects/moose/petsc\n==========================================\n/usr/bin/python /home/kit/agw/qz9211/projects/moose/petsc/config/gmakegen.py --petsc-arch=arch-moose --pkg-dir=/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc --pkg-name=slepc --pkg-pkgs=sys,eps,svd,pep,nep,mfn,lme --pkg-arch=arch-moose\n/usr/bin/python /home/kit/agw/qz9211/projects/moose/petsc/config/gmakegentest.py --petsc-dir=/home/kit/agw/qz9211/projects/moose/petsc --petsc-arch=arch-moose --testdir=./arch-moose/tests --srcdir=/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc/src --pkg-name=slepc --pkg-pkgs=sys,eps,svd,pep,nep,mfn,lme --pkg-arch=arch-moose --pkg-dir=/pfs/data5/home/kit/agw/qz9211/projects/moose/petsc/arch-moose/externalpackages/git.slepc",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4320936",
                  "updatedAt": "2022-12-06T13:34:04Z",
                  "publishedAt": "2022-12-06T08:55:33Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I believe what Roy is trying to tell us, is that by not having:\nexport CC=mpicc CXX=mpicxx F77=mpif77 F90=mpif90\n...set before running ./update_and_rebuild_petsc.sh and only during our last attempt at building libMesh, the two libraries have been built differently. It looks fine to me by way of what PETSc is being linked to. But it is probably a good idea to start over by building PETSc. Making sure the above exports are being done. And also now with Conda's MPI library truly out of the way.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4323216",
                          "updatedAt": "2022-12-06T13:44:00Z",
                          "publishedAt": "2022-12-06T13:43:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "roystgnr"
                          },
                          "bodyText": "I'm wondering about make V=1 in the libMesh folder, actually.  The linker there is what's complaining, and I'm curious about what it's really trying to do under the make hood.\nIt's interesting to see not a mention of fabric in those PETSc results, though.  That at least seems to clarify that it's something being brought in by MPI and not at a higher level.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4327838",
                          "updatedAt": "2022-12-06T23:44:24Z",
                          "publishedAt": "2022-12-06T23:44:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "I'm wondering about make V=1 in the libMesh folder, actually. The linker there is what's complaining, and I'm curious about what it's really trying to do under the make hood.\nIt's interesting to see not a mention of fabric in those PETSc results, though. That at least seems to clarify that it's something being brought in by MPI and not at a higher level.\n\nThanks for your tip. When I make V=1 in the libMesh folder, I get\nmake: *** No targets specified and no makefile found.  Stop.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4327874",
                          "updatedAt": "2022-12-06T23:53:58Z",
                          "publishedAt": "2022-12-06T23:53:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "when using the ./update_and_rebuild_libmesh.sh script, when it dies, you'll need to enter the following directory before running make V=1:\n/home/kit/agw/qz9211/projects/moose/libmesh/build\nI can install OpenMPI 4.1.4 on one of our test machines, see if we can replicate. I'll report back with my results...",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4333015",
                          "updatedAt": "2022-12-07T13:28:36Z",
                          "publishedAt": "2022-12-07T13:28:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Well, OpenMPI 4.1.4 with GCC 10.2.0, on Rocky-8 (as close as I can get to RHEL 8.x) worked for me, unfortunately.\nMy PETSc links:\n[rocky-8][~/projects/2nd_moose/petsc/arch-moose/lib]> ldd libpetsc.so\n\tlinux-vdso.so.1 (0x00007ffd15f7e000)\n\tlibHYPRE-2.23.0.so => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libHYPRE-2.23.0.so (0x00007f8553ad6000)\n\tlibstrumpack.so => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libstrumpack.so (0x00007f8552ff9000)\n\tlibsuperlu_dist.so.7 => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libsuperlu_dist.so.7 (0x00007f8552cf8000)\n\tlibhdf5_hl.so.200 => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libhdf5_hl.so.200 (0x00007f8552ad4000)\n\tlibhdf5.so.200 => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libhdf5.so.200 (0x00007f8552456000)\n\tlibparmetis.so => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libparmetis.so (0x00007f8552216000)\n\tlibmetis.so => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libmetis.so (0x00007f8551fb3000)\n\tlibm.so.6 => /lib64/libm.so.6 (0x00007f8551c31000)\n\tlibX11.so.6 => /lib64/libX11.so.6 (0x00007f85518ee000)\n\tlibstdc++.so.6 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libstdc++.so.6 (0x00007f8551513000)\n\tlibdl.so.2 => /lib64/libdl.so.2 (0x00007f855130f000)\n\tlibmpi_usempif08.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libmpi_usempif08.so.40 (0x00007f85510ce000)\n\tlibmpi_usempi_ignore_tkr.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libmpi_usempi_ignore_tkr.so.40 (0x00007f8550ebf000)\n\tlibmpi_mpifh.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libmpi_mpifh.so.40 (0x00007f8550c51000)\n\tlibmpi.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libmpi.so.40 (0x00007f855073d000)\n\tlibgfortran.so.5 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libgfortran.so.5 (0x00007f85502a0000)\n\tlibgcc_s.so.1 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libgcc_s.so.1 (0x00007f8550087000)\n\tlibquadmath.so.0 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libquadmath.so.0 (0x00007f854fe3f000)\n\tlibpthread.so.0 => /lib64/libpthread.so.0 (0x00007f854fc1f000)\n\tlibrt.so.1 => /lib64/librt.so.1 (0x00007f854fa17000)\n\tlibgomp.so.1 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libgomp.so.1 (0x00007f854f7d7000)\n\tlibc.so.6 => /lib64/libc.so.6 (0x00007f854f412000)\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f8555a83000)\n\tlibxcb.so.1 => /lib64/libxcb.so.1 (0x00007f854f1e9000)\n\tlibopen-rte.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libopen-rte.so.40 (0x00007f854eec6000)\n\tlibopen-pal.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libopen-pal.so.40 (0x00007f854ebc2000)\n\tlibpmix.so.2 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/pmix-4.1.2-4pykhszatp6kjdo6wbskirkpu7ey5x6s/lib/libpmix.so.2 (0x00007f854e7d3000)\n\tlibutil.so.1 => /lib64/libutil.so.1 (0x00007f854e5cf000)\n\tlibz.so.1 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/zlib-1.2.13-p7bwlngjhz5hp73lkes7efviqft5tkdi/lib/libz.so.1 (0x00007f854e3b7000)\n\tlibhwloc.so.15 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/hwloc-2.8.0-cvypsspkurpffm54wjuadcasfsyvco47/lib/libhwloc.so.15 (0x00007f854e159000)\n\tlibevent_core-2.1.so.7 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libevent-2.1.12-xgyfn7vgkuendutqklq2u34f2ksqjmuj/lib/libevent_core-2.1.so.7 (0x00007f854df23000)\n\tlibevent_pthreads-2.1.so.7 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libevent-2.1.12-xgyfn7vgkuendutqklq2u34f2ksqjmuj/lib/libevent_pthreads-2.1.so.7 (0x00007f854dd20000)\n\tlibXau.so.6 => /lib64/libXau.so.6 (0x00007f854db1c000)\n\tlibpciaccess.so.0 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libpciaccess-0.16-idrbcbu2dgirjpz5pw5uhofkpvljr6vi/lib/libpciaccess.so.0 (0x00007f854d913000)\n\tlibxml2.so.2 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libxml2-2.10.1-e65t5nvaqvefmhcckjw6cnhczewjyyja/lib/libxml2.so.2 (0x00007f854d5a8000)\n\tliblzma.so.5 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/xz-5.2.7-wcfmgp6kbiza7qsued5ylgqctrn6jkwt/lib/liblzma.so.5 (0x00007f854d381000)\n\tlibiconv.so.2 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libiconv-1.16-vxufhfcwlegaeg64whhmea76sldq2ejv/lib/libiconv.so.2 (0x00007f854d083000)\n\nMy libmesh links:\n[rocky-8][~/projects/2nd_moose/libmesh/build/.libs]> ldd libmesh_opt.so\n\tlinux-vdso.so.1 (0x00007fff5e1f4000)\n\tlibnetcdf.so.13 => /home/milljm/projects/2nd_moose/libmesh/build/contrib/netcdf/v4/liblib/.libs/libnetcdf.so.13 (0x00007f2164d6b000)\n\tlibtimpi_opt.so.11 => /home/milljm/projects/2nd_moose/libmesh/build/contrib/timpi/src/.libs/libtimpi_opt.so.11 (0x00007f2164b5d000)\n\tlibglpk.so.40 => /lib64/libglpk.so.40 (0x00007f216487c000)\n\tlibz.so.1 => /lib64/libz.so.1 (0x00007f2164664000)\n\tlibslepc.so.3.16 => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libslepc.so.3.16 (0x00007f21640ae000)\n\tlibpetsc.so.3.16 => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libpetsc.so.3.16 (0x00007f2162716000)\n\tlibHYPRE-2.23.0.so => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libHYPRE-2.23.0.so (0x00007f2162101000)\n\tlibstrumpack.so => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libstrumpack.so (0x00007f2161624000)\n\tlibsuperlu_dist.so.7 => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libsuperlu_dist.so.7 (0x00007f2161323000)\n\tlibhdf5_hl.so.200 => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libhdf5_hl.so.200 (0x00007f21610ff000)\n\tlibhdf5.so.200 => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libhdf5.so.200 (0x00007f2160a81000)\n\tlibparmetis.so => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libparmetis.so (0x00007f2160841000)\n\tlibmetis.so => /home/milljm/projects/2nd_moose/petsc/arch-moose/lib/libmetis.so (0x00007f21605de000)\n\tlibX11.so.6 => /lib64/libX11.so.6 (0x00007f216029b000)\n\tlibmpi_usempif08.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libmpi_usempif08.so.40 (0x00007f216005a000)\n\tlibmpi_usempi_ignore_tkr.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libmpi_usempi_ignore_tkr.so.40 (0x00007f215fe4b000)\n\tlibmpi_mpifh.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libmpi_mpifh.so.40 (0x00007f215fbdd000)\n\tlibgfortran.so.5 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libgfortran.so.5 (0x00007f215f740000)\n\tlibrt.so.1 => /lib64/librt.so.1 (0x00007f215f538000)\n\tlibquadmath.so.0 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libquadmath.so.0 (0x00007f215f2f0000)\n\tlibdl.so.2 => /lib64/libdl.so.2 (0x00007f215f0ec000)\n\tlibtirpc.so.3 => /lib64/libtirpc.so.3 (0x00007f215eeb9000)\n\tlibmpi.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libmpi.so.40 (0x00007f215e9a5000)\n\tlibstdc++.so.6 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libstdc++.so.6 (0x00007f215e5ca000)\n\tlibm.so.6 => /lib64/libm.so.6 (0x00007f215e248000)\n\tlibpthread.so.0 => /lib64/libpthread.so.0 (0x00007f215e028000)\n\tlibc.so.6 => /lib64/libc.so.6 (0x00007f215dc63000)\n\tlibgcc_s.so.1 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libgcc_s.so.1 (0x00007f215da4a000)\n\tlibgomp.so.1 => /data/spack/opt/spack/linux-rocky8-zen/gcc-8.5.0/gcc-10.2.0-r7okwsndnoebrrrrttgmvkzye3qw2uhf/lib64/libgomp.so.1 (0x00007f215d80a000)\n\tlibamd.so.2 => /lib64/libamd.so.2 (0x00007f215d5ff000)\n\tlibcolamd.so.2 => /lib64/libcolamd.so.2 (0x00007f215d3f7000)\n\tlibgmp.so.10 => /lib64/libgmp.so.10 (0x00007f215d15f000)\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f21667c7000)\n\tlibxcb.so.1 => /lib64/libxcb.so.1 (0x00007f215cf36000)\n\tlibopen-rte.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libopen-rte.so.40 (0x00007f215cc13000)\n\tlibopen-pal.so.40 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/openmpi-4.1.4-rfpxfrutyaulpwhk4unnq4now4sw2fjk/lib/libopen-pal.so.40 (0x00007f215c90f000)\n\tlibpmix.so.2 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/pmix-4.1.2-4pykhszatp6kjdo6wbskirkpu7ey5x6s/lib/libpmix.so.2 (0x00007f215c520000)\n\tlibutil.so.1 => /lib64/libutil.so.1 (0x00007f215c31c000)\n\tlibhwloc.so.15 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/hwloc-2.8.0-cvypsspkurpffm54wjuadcasfsyvco47/lib/libhwloc.so.15 (0x00007f215c0be000)\n\tlibevent_core-2.1.so.7 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libevent-2.1.12-xgyfn7vgkuendutqklq2u34f2ksqjmuj/lib/libevent_core-2.1.so.7 (0x00007f215be88000)\n\tlibevent_pthreads-2.1.so.7 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libevent-2.1.12-xgyfn7vgkuendutqklq2u34f2ksqjmuj/lib/libevent_pthreads-2.1.so.7 (0x00007f215bc85000)\n\tlibgssapi_krb5.so.2 => /lib64/libgssapi_krb5.so.2 (0x00007f215ba30000)\n\tlibkrb5.so.3 => /lib64/libkrb5.so.3 (0x00007f215b746000)\n\tlibk5crypto.so.3 => /lib64/libk5crypto.so.3 (0x00007f215b52f000)\n\tlibcom_err.so.2 => /lib64/libcom_err.so.2 (0x00007f215b32b000)\n\tlibsuitesparseconfig.so.4 => /lib64/libsuitesparseconfig.so.4 (0x00007f215b128000)\n\tlibXau.so.6 => /lib64/libXau.so.6 (0x00007f215af24000)\n\tlibpciaccess.so.0 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libpciaccess-0.16-idrbcbu2dgirjpz5pw5uhofkpvljr6vi/lib/libpciaccess.so.0 (0x00007f215ad1b000)\n\tlibxml2.so.2 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libxml2-2.10.1-e65t5nvaqvefmhcckjw6cnhczewjyyja/lib/libxml2.so.2 (0x00007f215a9b0000)\n\tlibkrb5support.so.0 => /lib64/libkrb5support.so.0 (0x00007f215a79f000)\n\tlibkeyutils.so.1 => /lib64/libkeyutils.so.1 (0x00007f215a59b000)\n\tlibcrypto.so.1.1 => /lib64/libcrypto.so.1.1 (0x00007f215a0b2000)\n\tlibresolv.so.2 => /lib64/libresolv.so.2 (0x00007f2159e9b000)\n\tliblzma.so.5 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/xz-5.2.7-wcfmgp6kbiza7qsued5ylgqctrn6jkwt/lib/liblzma.so.5 (0x00007f2159c74000)\n\tlibiconv.so.2 => /data/spack/opt/spack/linux-rocky8-zen2/gcc-10.2.0/libiconv-1.16-vxufhfcwlegaeg64whhmea76sldq2ejv/lib/libiconv.so.2 (0x00007f2159976000)\n\tlibselinux.so.1 => /lib64/libselinux.so.1 (0x00007f215974c000)\n\tlibpcre2-8.so.0 => /lib64/libpcre2-8.so.0 (0x00007f21594c8000)\n\nAttaching libmesh_diagnostics.log\nlibmesh_diagnostic.log",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4334470",
                          "updatedAt": "2022-12-07T15:57:03Z",
                          "publishedAt": "2022-12-07T15:56:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "In all of my links, I do not see me linking to libfabric.\nI do not even see this library in my lib directory for OpenMPI. Hmm, I'll do a bit more research on just what this library is, that seems to come with OpenMPI. I suppose its an argument option I am not supplying while building OpenMPI.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4334573",
                          "updatedAt": "2022-12-07T16:07:24Z",
                          "publishedAt": "2022-12-07T16:07:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "It would seem libFabric is separate from OpenMPI. My guess is when your cluster updated to OpenMPI 4.1.4, they failed to also build libFabric using GCC 10.2.0. This is my laymen explanation... Perhaps there exists an OpenMPI module you can load, which is using an older GCC?\nNormally when a cluster updates their MPI wrappers, what they are actually doing is adding to an existing bunch. Is it possible to load an early version of OpenMPI?\nCurrently Loaded Modules:\n\n    1) compiler/gnu/10.2 2) mpi/openmpi/4.0\n\nIs there perhaps mpi/openmpi/3? (strange that 4.0 means 4.1.4).",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4334678",
                          "updatedAt": "2022-12-07T16:17:57Z",
                          "publishedAt": "2022-12-07T16:17:56Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I am rebuilding openmpi with fabrics=ofi, that should get me a libfabrics library. However, this library is seriously tailored for the backpane interconnect of the nodes, specialized hardware, for your cluster. It will be different for mine... So I am not sure how close I can replicate your environment for testing.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4334797",
                          "updatedAt": "2022-12-07T16:29:35Z",
                          "publishedAt": "2022-12-07T16:29:35Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "1runer"
                          },
                          "bodyText": "It was OpenMPI 4.0 for quite some time, but after the last larger system update (we got noticed by the administrators) 4.0 isn't compatible anymore and they decided to recompile petsc\nand openmpi using Gnu10.2 and openmpi 4.1.\nAt least for my attempt to compile libmesh, I'm sure that I used proper modules loaded and linked:\nlibmesh_build_2022-12-06.16%3A44%3A43.log",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4334836",
                          "updatedAt": "2022-12-07T16:34:00Z",
                          "publishedAt": "2022-12-07T16:34:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I rebuilt openmpi to make use of libfabric. However, while I was expecting libopen-pal.so to link to a version of libfabric I installed using spack, it instead linked to my system's versions:\n{ [milljm@sawtooth2] [3b55c2e] } [/apps/local/spack/software/gcc-8.4.0/openmpi-4.0.2-xkgotunrhknpvlly4cuq66owczlchudz/lib]\n$ ldd libopen-pal.so \n\tlinux-vdso.so.1 =>  (0x00007fffedb02000)\n\tlibfabric.so.1 => /usr/lib64/libfabric.so.1 (0x00007fffed05c000)      <------------- RIGHT HERE\n\tlibrdmacm.so.1 => /usr/lib64/librdmacm.so.1 (0x00007fffece40000)\n\tlibibverbs.so.1 => /usr/lib64/libibverbs.so.1 (0x00007fffecc27000)\n\tlibpsm2.so.2 => /usr/lib64/libpsm2.so.2 (0x00007fffec9c1000)\n\tlibrt.so.1 => /usr/lib64/librt.so.1 (0x00007fffec7b9000)\n\tlibutil.so.1 => /usr/lib64/libutil.so.1 (0x00007fffec5b6000)\n\tlibhwloc.so.15 => /apps/local/spack/software/gcc-8.4.0/hwloc-2.1.0-c37ukynvqpcswa3y6dppqyjc5qea2zon/lib/libhwloc.so.15 (0x00007fffec365000)\n\tlibudev.so.1 => /usr/lib64/libudev.so.1 (0x00007fffec14f000)\n\tlibpciaccess.so.0 => /apps/local/spack/software/gcc-8.4.0/libpciaccess-0.13.5-fjixzxnwbotbtdrb4dvldiyc3kul37ic/lib/libpciaccess.so.0 (0x00007fffebf46000)\n\tlibxml2.so.2 => /apps/local/spack/software/gcc-8.4.0/libxml2-2.9.10-odes7obyblstttzxon4o3foz5czn5und/lib/libxml2.so.2 (0x00007fffebbe5000)\n\tlibdl.so.2 => /usr/lib64/libdl.so.2 (0x00007fffeb9e1000)\n\tlibz.so.1 => /apps/local/spack/software/gcc-8.4.0/zlib-1.2.11-iode5iweyrwrwqi7vc4wbtvtszsqba5g/lib/libz.so.1 (0x00007fffeb7ca000)\n\tliblzma.so.5 => /apps/local/spack/software/gcc-8.4.0/xz-5.2.4-egk7d7shjuuzm4jl4nyoen64hgu6ft4f/lib/liblzma.so.5 (0x00007fffeb5a4000)\n\tlibiconv.so.2 => /apps/local/spack/software/gcc-8.4.0/libiconv-1.16-gxhwn4i3pmaieairp27f7dphdysssabc/lib/libiconv.so.2 (0x00007fffeb2a8000)\n\tlibm.so.6 => /usr/lib64/libm.so.6 (0x00007fffeafa6000)\n\tlibpthread.so.0 => /usr/lib64/libpthread.so.0 (0x00007fffead8a000)\n\tlibc.so.6 => /usr/lib64/libc.so.6 (0x00007fffea9bc000)\n\tlibnl-3.so.200 => /usr/lib64/libnl-3.so.200 (0x00007fffea79b000)\n\tlibnl-route-3.so.200 => /usr/lib64/libnl-route-3.so.200 (0x00007fffea52e000)\n\tlibpsm_infinipath.so.1 => /usr/lib64/libpsm_infinipath.so.1 (0x00007fffea2d8000)\n\tlibgcc_s.so.1 => /apps/local/spack/software/gcc-4.8.5/gcc-8.4.0-jacdabugmghmzya2rpfqpym2bpzjomy2/lib64/libgcc_s.so.1 (0x00007fffea0c0000)\n\t/lib64/ld-linux-x86-64.so.2 (0x00007fffed8e3000)\n\tlibnuma.so.1 => /apps/local/spack/software/gcc-8.4.0/numactl-2.0.13-ewpw3l3emcqvu55ii2tylucw54lltg35/lib/libnuma.so.1 (0x00007fffe9eb5000)\n\tlibcap.so.2 => /usr/lib64/libcap.so.2 (0x00007fffe9cb0000)\n\tlibdw.so.1 => /usr/lib64/libdw.so.1 (0x00007fffe9a5f000)\n\tlibinfinipath.so.4 => /usr/lib64/libinfinipath.so.4 (0x00007fffe9850000)\n\tlibuuid.so.1 => /usr/lib64/libuuid.so.1 (0x00007fffe964b000)\n\tlibattr.so.1 => /usr/lib64/libattr.so.1 (0x00007fffe9446000)\n\tlibelf.so.1 => /usr/lib64/libelf.so.1 (0x00007fffe922e000)\n\tlibbz2.so.1 => /usr/lib64/libbz2.so.1 (0x00007fffe901e000)\n\nCan you perhaps see what libfabric your libopen-pal.so is linked to (please do this):\nldd /opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libopen-pal.so",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4335481",
                          "updatedAt": "2022-12-07T17:25:13Z",
                          "publishedAt": "2022-12-07T17:25:13Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "1runer"
                  },
                  "bodyText": "Dear all,\nI also wanna step in this discussion, because I have more or less the same issues with the cluster.\nInstead of Joseph-0123, I used the build-in versions of Petsc and OpenMPI.\nSome weeks ago, the administrators updated OpenMPI from 4.0 to 4.1 which makes it necessary to recompile moose/libmesh. WIth the newer version of OpenMPI I couldn't recompile libmesh failing at one of the very last steps.\nThe errors tells something  like @roystgnr mentioned regarding fabric.\n/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libopen-pal.so: undefined reference to fi_open@FABRIC_1.5'`\nI attached the log file to this message.\nRecompilation_MOOSE.txt\nBest regards",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4330167",
                  "updatedAt": "2022-12-07T07:28:06Z",
                  "publishedAt": "2022-12-07T07:26:40Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "milljm"
                  },
                  "bodyText": "I am not sue what is going on on this machine. I am reading past threads in this post, and I am realizing we have used three different compiler combinations:\n/opt/bwhpc/common/mpi/openmpi/4.0.5-gnu-10.2/lib64/libfabric.so.1: undefined symbol: rdma_establish, version RDMACM_1.0\n/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-12.1/lib64/libopen-pal.so: undefined reference to `fi_open@FABRIC_1.5'\n/opt/bwhpc/common/mpi/openmpi/4.1.4-gnu-10.2/lib64/libopen-pal.so: undefined reference to `fi_open@FABRIC_1.5'\n                              ^^^^^^^^^^^^^^ 3 different combos in play\n\nHow are you guys selecting these different compilers?  Modules? By hand somehow? Are your admins making changes to what is being sourced on-the-fly over the course of these past few days?",
                  "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4344154",
                  "updatedAt": "2022-12-08T15:00:05Z",
                  "publishedAt": "2022-12-08T14:59:34Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "1runer"
                          },
                          "bodyText": "That happens when you try to solve an issue by recompiling Moose with different combinations of openmpi/gnu/petsc.\nMaybe some history about our HPC:\nWe used MOOSE for quite some time successfully until November using OpenMPI 4.0 and Gnu 10.2.\nThen they decided to go for hardware and software upgrade resulting in a non-working (but still existing) old openmpi 4.0, which explains the very first message (undefined symbol: rdma_establish).\nSo they recommended to recompile MOOSE using their pre-built openmpi 4.1 which actually fails due to \"undefined reference to `fi_open@FABRIC_1.5'\".\nI hope I was able to clarify some things even if I didn't solve it.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4344250",
                          "updatedAt": "2022-12-08T15:13:13Z",
                          "publishedAt": "2022-12-08T15:13:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I spoke to our HPC admins. They looked over this issue and spoke to me at length that they too run (or have ran) into this issue in the past. That it's highly dependent on the cluster hardware and fabric and libraries.\nOne thing we can try to do, is have you build your own MPI wrapper and use that, instead of the wrapper being supplied by your cluster. Doing so would produce a wrapper not using fabrics at all.\nIf you have not done this before, building/installing/using your own MPI wrapper follows the same ./configure, make, make install paradigm -its pretty straight forward. I can try and help with this if you wish.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4346232",
                          "updatedAt": "2022-12-08T19:16:17Z",
                          "publishedAt": "2022-12-08T19:16:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "I spoke to our HPC admins. They looked over this issue and spoke to me at length that they too run (or have ran) into this issue in the past. That it's highly dependent on the cluster hardware and fabric and libraries.\nOne thing we can try to do, is have you build your own MPI wrapper and use that, instead of the wrapper being supplied by your cluster. Doing so would produce a wrapper not using fabrics at all.\nIf you have not done this before, building/installing/using your own MPI wrapper follows the same ./configure, make, make install paradigm -its pretty straight forward. I can try and help with this if you wish.\n\nHello, Milljm. I do appreciate your generous help in solving this problem. I will try to build my own MPI wrapper.",
                          "url": "https://github.com/idaholab/moose/discussions/22720#discussioncomment-4346940",
                          "updatedAt": "2022-12-08T21:10:34Z",
                          "publishedAt": "2022-12-08T21:10:33Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Help understanding how vector variables are handled in auxkernels",
          "author": {
            "login": "Eilloo"
          },
          "bodyText": "Hi all,\nI've been trying to learn about auxkernels, with a view to extract velocity gradient components at an arbitrary node ID (not necessarily the node currently under consideration).\nFrom the documentation on the 'couplable' interface, I was hoping that I'd be able to do something along the lines of:\n_vel_grad_x(coupledGradient(\"velocity\", index))\nwhere \"velocity\" is an input vector variable, and 'index' is 0, 1, or 2 for x, y, and z.\nSuffice to say this didn't work, and so I began using some cout << statements in my auxkernel to try and work out how to index my velocity vector properly.\nUsing something along the lines of:\nstd::cout << _velocity[currentNodeID]\nprinted values looking like this:\n(x,y,z)=(2.18065e-314, 2.47033e-323, 2.18065e-314)\nor for some lines:\n(x,y,z)=(0, 4.82411e+228, 1.90859e+185)\nThe e-300 values I thought could be rounding errors and the velocity is zero. However, I can't explain the very large values, and none of the output velocities look close to those in the output file, whose velocity magnitudes, vary between 0 and 8e-2.\nMy question is, what have I been printing? My hope is that this will clarify how I should set about extracting the velocity gradient components properly.\nThanks!\nBelow are some snippets from the .h and .C files to clarify how I've defined things in more detail:\nIn the header file, under 'protected':\nconst VectorVariableValue & _velocity;\nconst VariableGradient & _vel_grad_x;\nIn the initialiser list in the .C file:\n_velocity(coupledVectorValue(\"velocity\")),\n_vel_grad_x(coupledGradient(\"velocity\", 0)),\nThe cout statement:\nstd::cout << _velocity[_current_node->id()] << \"\\n\";\nVelocity is defined with family=\"LAGRANGE_VEC\" in the input file.\nI'll also add that my simulation is 2D, so seeing any value in the 'z' component is surprising!",
          "url": "https://github.com/idaholab/moose/discussions/22895",
          "updatedAt": "2023-01-03T23:22:37Z",
          "publishedAt": "2022-12-06T16:37:56Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou are reading out of bounds, so there could be any value there.\nIf velocity is a vector variable, then to retrieve the gradient of it (the whole vector) you will need to save it in a\nconst VectorVariableGradient & _vel_grad\nand use\nconst ADVectorVariableGradient &\nto fill it.\nThen you will want to access each component with 0, 1, 2 for X Y Z\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/22895#discussioncomment-4325086",
                  "updatedAt": "2022-12-06T16:58:30Z",
                  "publishedAt": "2022-12-06T16:58:29Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Eilloo"
                  },
                  "bodyText": "Ah, I hadn't seen that VectorVariableGradient is a type!\nTo make sure I understand, I shouldn't use the 'coupleable' interface to populate it, but instead look to access the ADVectorVariableGradient values directly?\nI see now that the nodeID's aren't what I thought and hence are out of bounds garbage when I try to index into _velocity using them... is there a way to convert from a node ID to an index so I can pull out the corresponding variable values?\nThanks for your help!",
                  "url": "https://github.com/idaholab/moose/discussions/22895#discussioncomment-4325240",
                  "updatedAt": "2022-12-06T17:18:08Z",
                  "publishedAt": "2022-12-06T17:18:07Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You should use the coupleable interface, but you need to use the right routine to get the vector variable gradient\nSo that part is tough. The coupleable interface only gets you the local variable value and for some items the neighbor values. It wont get you the general values of the variable everywhere based on the node ID.\nYou can use the variable pointer directly (use getVariable to retrieve it) and if you know where to index you can directly access the solution vector (wont work for higher orders very easily)\nhttps://mooseframework.inl.gov/docs/doxygen/moose/classMooseVariableFE.html#acbbf516d918a5de55a3b59832908cf98\nor you can use the functor base class of variables. Functors are a base class of variable and help with geometry-based evaluations (like element face, or particular point in space). That hasnt been explored much so far for node IDs, you ll need to do some development\nhttps://mooseframework.inl.gov/docs/doxygen/moose/classMoose_1_1FunctorBase.html",
                          "url": "https://github.com/idaholab/moose/discussions/22895#discussioncomment-4325716",
                          "updatedAt": "2022-12-06T18:04:15Z",
                          "publishedAt": "2022-12-06T18:04:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "with a view to extract velocity gradient components at an arbitrary node ID (not necessarily the node currently under consideration).\n\nbasically this is difficult to do in MOOSE right now.\nSome solutions will either be hacky or involve developments\nIf you can formulate your problem to always consider the local node, or nodes around the local element, then it will be easier",
                          "url": "https://github.com/idaholab/moose/discussions/22895#discussioncomment-4325722",
                          "updatedAt": "2022-12-06T18:05:43Z",
                          "publishedAt": "2022-12-06T18:05:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Eilloo"
                          },
                          "bodyText": "Thanks for the info @GiudGiud - I thought I'd say what I've ended up doing, for the sake of completeness:\nThere's a handy 'find_nodal_neighbors' method in MeshTools, so once you know which node you're interested in, it's easy to get a list of its neighbour nodes using this.\nYou can then get the dof associated with each velocity component at each of these neighbours using 'dof_number', and changing the 'component' argument. Getting the actual values is achieved in a similar way to the existing 'NearestNodeValue' Auxkernel.\nSubtracting these components from those associated with the node of interest essentially gives 'delta_velocity_x' (for, say, the x component), and you can divide this by the difference in x/y/z co-ordinates to get all 4 (2D) or 9 (3D) velocity gradient components.\nAveraging each component by summing and dividing by the number of neighbour nodes used to calculate them gives one set of velocity gradient components, associated with the original node of interest.\nNow, this isn't very elegant and involves a lot of nested loops, so I suspect things will get very slow very soon when I start looking at more complex meshes... but initial impressions are that it seems to work, in case anyone is interested!",
                          "url": "https://github.com/idaholab/moose/discussions/22895#discussioncomment-4342538",
                          "updatedAt": "2022-12-08T11:53:37Z",
                          "publishedAt": "2022-12-08T11:53:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Yes that seems workable. Thank you for sharing",
                          "url": "https://github.com/idaholab/moose/discussions/22895#discussioncomment-4343669",
                          "updatedAt": "2022-12-08T14:07:10Z",
                          "publishedAt": "2022-12-08T14:07:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "This won\u2019t work for all variable types obviously, only a subset of nodal variables",
                          "url": "https://github.com/idaholab/moose/discussions/22895#discussioncomment-4343678",
                          "updatedAt": "2022-12-08T14:07:52Z",
                          "publishedAt": "2022-12-08T14:07:52Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to add TensorMechanics into eigenvalue calculation",
          "author": {
            "login": "js-jixu"
          },
          "bodyText": "Hi, experts.\nI'm doing an eigenvalue calculation. My input file clearly contains TensorMechanics, but the calculated result does not contain TensorMechanics. When I look at the non-linear iteration residuals, other variables have values, only disp_x/y/z are 0:\n 0 Nonlinear |R| = 4.649184e+03\n    |residual|_2 of individual variables:\n                   velocity: 4647.58\n                   p:        4.07466e-05\n                   Tf:       8.7437e-13\n                   Ts:       122.101\n                   disp_x:   0\n                   disp_y:   0\n                   disp_z:   0\n\nHere is my blocks about TensorMechanics(Variables and BCs are omitted):\n[Kernels]\n  [disp_x_diff]\n    type = Diffusion\n    variable = disp_x\n    block = fluid\n    use_displaced_mesh = true\n  []\n  [disp_y_diff]\n    type = Diffusion\n    variable = disp_y\n    block = fluid\n    use_displaced_mesh = true\n  []\n  [disp_z_diff]\n    type = Diffusion\n    variable = disp_z\n    block = fluid\n    use_displaced_mesh = true\n  []\n[]\n\n[Modules/TensorMechanics/Master]\n  displacements = 'disp_x disp_y disp_z'\n  strain = FINITE\n  material_output_order = FIRST\n  generate_output = 'vonmises_stress stress_xx stress_yy stress_zz strain_xx strain_yy strain_zz'\n  [mechanics]\n    block = 'solid'\n    temperature = Ts\n    displacements = 'disp_x disp_y disp_z'\n    automatic_eigenstrain_names = true\n  []\n[]\n\n[Materials]\n  [elasticity_solid]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 2e7\n    poissons_ratio = 0.32\n    block = 'solid'\n    use_displaced_mesh = true\n  []\n  [thermal_expansion_solid]\n    type = ComputeThermalExpansionEigenstrain\n    temperature = Ts\n    thermal_expansion_coeff = 1e-5\n    stress_free_temperature = 273\n    eigenstrain_name = thermal_expansion\n    block = 'solid'\n    use_displaced_mesh = true\n  []\n  [stress_solid]\n    type = ComputeFiniteStrainElasticStress\n    block = 'solid'\n  []\n[]\n\nAnd I use InversePowerMethod in Executioner block:\n[Executioner]\n  type = InversePowerMethod\n  max_power_iterations = 50\n\n  normalization = 'powernorm'\n  normal_factor = 21230\n\n  bx_norm = 'bnorm'\n  k0 = 1\n  l_max_its = 200\n  eig_check_tol = 1e-3\n\n  solve_type = 'PJFNK'\n  petsc_options = '-snes_converged_reason -ksp_converged_reason -snes_linesearch_monitor'\n  petsc_options_iname = '-pc_type -pc_factor_shift_type -pc_factor_mat_solver_package'\n  petsc_options_value = 'lu       NONZERO               superlu_dist'\n\n  line_search = none\n  automatic_scaling = true\n[]",
          "url": "https://github.com/idaholab/moose/discussions/22721",
          "updatedAt": "2023-01-03T03:12:42Z",
          "publishedAt": "2022-11-16T13:04:50Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "js-jixu"
                  },
                  "bodyText": "Is there a kind guy to take a look at my question?\ud83d\udc40",
                  "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4162945",
                  "updatedAt": "2022-11-17T01:58:33Z",
                  "publishedAt": "2022-11-17T01:58:32Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nYou should not need to have [Modules/ ] before TensorMechanics.\nThis syntax of adding Modules has been deprecated\nUse the [Debug] show_action parameter to show if the action is running\nJudging by the 0 residual there's nothing acting on the displacement.\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4179274",
                          "updatedAt": "2022-11-18T17:38:16Z",
                          "publishedAt": "2022-11-18T17:38:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "Hi, Guillaume, sorry for the late reply.\n\nYou should not need to have [Modules/ ] before TensorMechanics.\n\nI removed [Modules/ ], but the terminal reported that section 'TensorMechanics' does not have an associated \"Action\". It may be that my moose version is too low. I haven't updated since September. So to get it working I added [Modules/ ] again.\n\nUse the [Debug] show_action parameter to show if the action is running.\n\nI added show_actions = true in [Debug]. It seems that TensorMechanics is taken into calculation:\n[DBG][ACT] TASK (             meta_action) TYPE (     CommonTensorMechanicsAction) NAME (          Master) Memory usage 68MB\n[DBG][ACT] TASK (             meta_action) TYPE (           TensorMechanicsAction) NAME (       mechanics) Memory usage 68MB\n[DBG][ACT] TASK (   check_copy_nodal_vars) TYPE (             CopyNodalVarsAction) NAME (          disp_x) Memory usage 68MB\n[DBG][ACT] TASK (   check_copy_nodal_vars) TYPE (             CopyNodalVarsAction) NAME (          disp_y) Memory usage 68MB\n[DBG][ACT] TASK (   check_copy_nodal_vars) TYPE (             CopyNodalVarsAction) NAME (          disp_z) Memory usage 68MB\n[DBG][ACT] TASK (                  add_bc) TYPE (                     AddBCAction) NAME (          pin1_y) Memory usage 105MB\n[DBG][ACT] TASK (                  add_bc) TYPE (                     AddBCAction) NAME (          pin1_x) Memory usage 105MB\n[DBG][ACT] TASK (                  add_bc) TYPE (                     AddBCAction) NAME (          pin2_y) Memory usage 106MB\n[DBG][ACT] TASK (                  add_bc) TYPE (                     AddBCAction) NAME (          pin2_x) Memory usage 106MB\n[DBG][ACT] TASK (                  add_bc) TYPE (                     AddBCAction) NAME (          pin3_y) Memory usage 106MB\n[DBG][ACT] TASK (                  add_bc) TYPE (                     AddBCAction) NAME (         axial_z) Memory usage 106MB\n[DBG][ACT] TASK (                  add_bc) TYPE (                     AddBCAction) NAME (        radial_x) Memory usage 106MB\n[DBG][ACT] TASK (                  add_bc) TYPE (                     AddBCAction) NAME (        radial_y) Memory usage 106MB\n\nBut as I said before, disp_x/y/z are all 0 in the calculation results. This is not appropriate, because the temperature in different parts of the system changes. Then disp_x/y/z should also change.\nThere are figures for displacement and temperature distribution of results:",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4200707",
                          "updatedAt": "2022-11-22T02:17:15Z",
                          "publishedAt": "2022-11-22T02:16:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "I wonder if the problem will appear here. Since I have used InversePowerMethod to solve an eigenvalue problem. The ComputeThermalExpansionEigenstrain in the TensorMechanics module is also an eigenvalue solving problem. So maybe an eigenvalue problem cannot be nested within an eigenvalue problem?\nWhen I use executioner Transient or Steady for ComputeThermalExpansionEigenstrain, everything works fine. This problem occurs only when using InversePowerMethod.\nHowever, when I use NonlinearEigen, disp_x/y/z are not involved in the calculation in the power iteration. But at the end of the power iteration, when the nonlinear iteration is performed, disp_x/y/z participates in the calculation again. Unfortunately, the results of the calculations are still problematic. So I want TensorMechanics to be able to participate in power iteration.",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4205850",
                          "updatedAt": "2022-11-22T13:34:41Z",
                          "publishedAt": "2022-11-22T13:34:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lynnmunday"
                          },
                          "bodyText": "ComputeThermalExpansionEigenstrain is not solving an eigenvalue problem.  Eigenstrain is any stress-free strain, I don't know why someone named it eigen-strain because I don't think it has anything to do with eigenvectors.",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4256415",
                          "updatedAt": "2022-11-28T19:02:10Z",
                          "publishedAt": "2022-11-28T19:02:09Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "lynnmunday"
                  },
                  "bodyText": "I don't understand your problem, you have diffusion and mechanics acting on the same variables.  What kind of physics are you modeling?  I haven't seen any examples for solving natural frequencies of a mechanical system in MOOSE.  You should look at the electromagnetics module in moose for tests that use the eigensolver.  Let me know of you get this to work, I want to solve mechanics eigenvalue problems too.",
                  "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4198808",
                  "updatedAt": "2022-11-21T20:11:20Z",
                  "publishedAt": "2022-11-21T20:09:59Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "Hi, Lynn.\n\nyou have diffusion and mechanics acting on the same variables. What kind of physics are you modeling?\n\nI want to use disp_x/y/z for simulating thermal expansion due to temperature rise. My model has fluid regions and solid regions. The solid region will expand due to the increase in temperature, which will also change the size of the fluid region. So I set Diffusion for dispxyz in the fluid area. Finally get such a result:\n\nI'm solving an eigenvalue problem which determines power distributions. I want to know the effect of temperature and displacement on power distribution, and power distribution in turn affects temperature and displacement. So I put the Kernels and BCs of temperature and displacement into the eigenvalue problem. But from the results, the temperature has changed, but disp_x/y/z has not changed.",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4200859",
                          "updatedAt": "2022-11-22T02:44:07Z",
                          "publishedAt": "2022-11-22T02:44:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so the eigenvalue problem is not the tensor mechanics one, it's the one that determines the power distribution, which is coupled to fluid flow + mechanics?\nI would build a working input with a multiapp for either the eigenvalue problem or the tensor mechanics + fluid problem.\nThen when this works, you may consider a fully coupled solve",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4207912",
                          "updatedAt": "2022-11-22T16:04:16Z",
                          "publishedAt": "2022-11-22T16:04:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "so the eigenvalue problem is not the tensor mechanics one, it's the one that determines the power distribution, which is coupled to fluid flow + mechanics?\n\nYeah! I want to solve an eigenvalue problem which determines the distribution of power, and this problem has to be coupled with tensormechanics and fluid flow..\n\nI would build a working input with a multiapp for either the eigenvalue problem or the tensor mechanics + fluid problem. Then when this works, you may consider a fully coupled solve.\n\nI have a input file which couple the eigenvalue problem and fluid. It works well.\nAnd I have a steady input file which couples tensormechanics and fluid. Its power isn't solved by an eigenvalue problem but given as a function of z coordinate. This input file also works fine.\nSo I want to couple tensormechanics into the original input file which couples eigenvalue problem and fluid.",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4208044",
                          "updatedAt": "2022-11-23T01:13:15Z",
                          "publishedAt": "2022-11-22T16:16:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "Is there any method for TensorMechanics to run in InversePowerMethod\uff1f",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4211662",
                          "updatedAt": "2022-11-23T01:30:17Z",
                          "publishedAt": "2022-11-23T01:30:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "Tensor mechanics eigen problem  works for me\n[GlobalParams]\n  displacements = 'disp_x disp_y'\n[]\n\n[Mesh]\n  [gmg]\n    type = GeneratedMeshGenerator\n    dim = 2\n    xmax = 10\n    nx = 20\n    ymax = 1\n    ny = 2\n  []\n[]\n\n[Modules]\n  [TensorMechanics]\n    [Master]\n      [all]\n        add_variables = true\n        new_system = true\n        formulation = TOTAL\n        volumetric_locking_correction = true\n      []\n    []\n  []\n[]\n\n[Kernels]\n  [x]\n    type = MassEigenKernel\n    variable = disp_x\n  []\n  [y]\n    type = MassEigenKernel\n    variable = disp_y\n  []\n[]\n\n[BCs]\n  [left_x]\n    type = DirichletBC\n    variable = disp_x\n    boundary = left\n    value = 0\n  []\n  [left_y]\n    type = DirichletBC\n    variable = disp_y\n    boundary = left\n    value = 0\n  []\n[]\n\n[Materials]\n  [C]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 1\n    poissons_ratio = 0.25\n  []\n  [stress]\n    type = ComputeLagrangianLinearElasticStress\n  []\n[]\n\n[AuxVariables]\n  [unorm_old]\n    [AuxKernel]\n      type = ParsedAux\n      function = 'sqrt(disp_x^2+disp_y^2)'\n      args = 'disp_x disp_y'\n      execute_on = 'INITIAL TIMESTEP_END'\n    []\n  []\n  [unorm]\n    [AuxKernel]\n      type = ParsedAux\n      function = 'sqrt(disp_x^2+disp_y^2)'\n      args = 'disp_x disp_y'\n      execute_on = 'LINEAR TIMESTEP_END'\n    []\n  []\n[]\n\n[Postprocessors]\n  [unorm]\n    type = ElementIntegralVariablePostprocessor\n    variable = unorm\n    execute_on = 'LINEAR'\n  []\n  [udiff]\n    type = ElementL2Difference\n    variable = unorm\n    other_variable = unorm_old\n    execute_on = 'LINEAR'\n  []\n[]\n\n[Preconditioning]\n  [smp]\n    type = SMP\n    full = true\n  []\n[]\n\n[Executioner]\n  type = InversePowerMethod\n\n  solve_type = PJFNK\n  petsc_options_iname = '-pc_type'\n  petsc_options_value = 'lu'\n\n  bx_norm = unorm\n  normalization = unorm\n  xdiff = udiff\n\n  min_power_iterations = 11\n  max_power_iterations = 400\n  Chebyshev_acceleration_on = true\n  eig_check_tol = 1e-12\n  k0 = 1000\n[]\n\n[Outputs]\n  exodus = true\n  print_linear_residuals = false\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4212000",
                          "updatedAt": "2022-11-23T02:46:13Z",
                          "publishedAt": "2022-11-23T02:46:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "Thank you for your input file @hugary1995 . I think maybe TensorMechanics can't be compatible with my eigenvalue problem. Gary's input file with TensorMechanics can run with InversePowerMethod, but when I substitute the TensorMechanics part of gary's input file with mine, it doesn't work. I mean, when I use TensorMechanics part of gary's input file + eigenvalue problem of my input file, the  disp_x/y/z will not take part in the calculation.\nI don't know why, but I have a guess that I mentioned before. From ComputeThermalExpansionEigenstrain, I think the calculation of displacement field is also an EigenValue calculation. And the eigenvalue calculation cannot be nested in another eigenvalue calculation. So when I do an eigenvalue calculation, I can't nest TensorMechanics into it.\ud83e\udd14",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4241396",
                          "updatedAt": "2022-11-26T06:46:33Z",
                          "publishedAt": "2022-11-26T06:46:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "@YaqiWang knows a lot about our eigen systems",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4243574",
                          "updatedAt": "2022-11-26T17:47:00Z",
                          "publishedAt": "2022-11-26T17:47:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "I changed the initial condition of dispxyz, and now the residual of dispxyz in eigenvalue calculation will not be 0.\n 0 Nonlinear |R| = 4.649051e+03\n    |residual|_2 of individual variables:\n                   velocity: 4647.58\n                   p:        0.000258578\n                   Tf:       8.43338e-13\n                   Ts:       116.934\n                   disp_x:   0.0106197\n                   disp_y:   0.0106197\n                   disp_z:   0.000100545\n\nBut that doesn't solve my problem. First, the calculation results are actually related to the initial value of dispxyz. That is, I set different initial values of dispxyz then get different calculation results. What I do is an eigenvalue calculation. The final result should be independent of the initial value. Second, from the output results, the distribution of dispxyz is very abnormal. It doesn't seem like a reasonable result.\nThe first figure is the output of disp_x solved by the eigenvalue problem and the second figure is the reasonable results.",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4245500",
                          "updatedAt": "2022-11-27T07:37:32Z",
                          "publishedAt": "2022-11-27T07:05:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "if the residuals are not 0, the problem is not converged and there is no reason for the results to be good.\nI d reduce the case down to a small mesh and try to get convergence with LU, then work on a field split",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4246987",
                          "updatedAt": "2022-11-27T15:00:42Z",
                          "publishedAt": "2022-11-27T15:00:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "The final residual will reach 0. I just intercepted the residual at the beginning of the first step. The problem I encountered at the beginning was that the residual of dispxyz in eigenvalue calculation was 0 from beginning to end, which was strange. Later, I changed the initial value of dispxyz. At the beginning of eigenvalue calculation, the residual error of dispxyz is not 0 (I think this indicates that dispxyz has participated in the calculation). But when the calculation is completed, the result is similar to the first picture. dispx and dispy is asymmetrical and very strange. I think the ideal result should be similar to the second picture",
                          "url": "https://github.com/idaholab/moose/discussions/22721#discussioncomment-4247180",
                          "updatedAt": "2022-11-27T15:41:34Z",
                          "publishedAt": "2022-11-27T15:41:33Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "libmesh-config missing after the installation via conda",
          "author": {
            "login": "max-hassani"
          },
          "bodyText": "Bug Description\nI followed the instruction to install moose via conda. Afterwards, I cloned the moose repository, and inside the test folder, make -j 4 and received the following error:\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/libtool: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/libtool: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\nChecking if header needs updating: /home/muhammad/gitRepos/moose/framework/include/base/MooseRevision.h...\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\nCompiling C++ (in opt mode) /home/muhammad/gitRepos/moose/framework/build/unity_src/src_Unity.C...\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/libtool: not found\n/bin/sh: 1: /home/muhammad/gitRepos/moose/libmesh/installed/contrib/bin/libmesh-config: not found\nmake: *** [/home/muhammad/gitRepos/moose/framework/build.mk:145: /home/muhammad/gitRepos/moose/framework/build/unity_src/src_Unity..opt.lo] Error 127\n\nIs there any other step that I missed?\nI am running on WSL2 (ubuntu).\nThe conda environment includes the following packages:\n_libgcc_mutex             0.1                 conda_forge    conda-forge\n_openmp_mutex             4.5                       2_gnu    conda-forge\n_sysroot_linux-64_curr_repodata_hack 3                   h5bd9786_13    conda-forge\natk-1.0                   2.36.0               ha1a6a79_0\nautoconf                  2.69            pl5321hd708f79_11    conda-forge\nautomake                  1.16.1          pl5320ha770c72_1004    conda-forge\nbeautifulsoup4            4.11.1          py310h06a4308_0\nbinutils_impl_linux-64    2.36.1               h193b22a_2    conda-forge\nbinutils_linux-64         2.36                hf3e587d_10    conda-forge\nblas                      1.0                    openblas\nblosc                     1.21.0               h4ff587b_1\nbottleneck                1.3.5           py310ha9d4c09_0\nbrotli                    1.0.9                h5eee18b_7\nbrotli-bin                1.0.9                h5eee18b_7\nbrunsli                   0.1                  h2531618_0\nbzip2                     1.0.8                h7f98852_4    conda-forge\nc-ares                    1.18.1               h7f98852_0    conda-forge\nca-certificates           2022.6.15            ha878542_0    conda-forge\ncairo                     1.16.0               h19f5f5c_2\ncfitsio                   3.470                h5893167_7\ncharls                    2.2.0                h2531618_0\nclang-format-12           12.0.1          default_ha53f305_4    conda-forge\ncloudpickle               2.0.0              pyhd3eb1b0_0\ncmake                     3.24.1               h5432695_0    conda-forge\ncycler                    0.11.0             pyhd3eb1b0_0\ncytoolz                   0.11.0          py310h7f8727e_0\ndask-core                 2022.7.0        py310h06a4308_0\ndbus                      1.13.18              hb2f20db_0\ndeepdiff                  5.8.1              pyhd8ed1ab_0    conda-forge\nexpat                     2.4.8                h27087fc_0    conda-forge\nfont-ttf-dejavu-sans-mono 2.37                 hd3eb1b0_0\nfont-ttf-inconsolata      2.001                hcb22688_0\nfont-ttf-source-code-pro  2.030                hd3eb1b0_0\nfont-ttf-ubuntu           0.83                 h8b1ccd4_0\nfontconfig                2.13.1               h6c09931_0\nfonts-anaconda            1                    h8fa9717_0\nfonts-conda-ecosystem     1                    hd3eb1b0_0\nfonttools                 4.25.0             pyhd3eb1b0_0\nfreetype                  2.11.0               h70c0345_0\nfribidi                   1.0.10               h7b6447c_0\nfsspec                    2022.7.1        py310h06a4308_0\ngcc_impl_linux-64         10.4.0              h7ee1905_16    conda-forge\ngcc_linux-64              10.4.0              h9215b83_10    conda-forge\ngdk-pixbuf                2.42.8               h433bba3_0\ngfortran_impl_linux-64    10.4.0              h44b2e72_16    conda-forge\ngfortran_linux-64         10.4.0              h69d5af5_10    conda-forge\ngiflib                    5.2.1                h7b6447c_0\nglib                      2.69.1               h4ff587b_1\ngmp                       6.2.1                h295c915_3\ngmpy2                     2.1.2           py310heeb90bb_0\ngobject-introspection     1.72.0          py310hbb6d50b_0\ngraphite2                 1.3.14               h295c915_1\ngraphviz                  2.50.0               h3cd0ef9_0\ngst-plugins-base          1.14.0               h8213a91_2\ngstreamer                 1.14.0               h28cd5cc_2\ngtk2                      2.24.33              h73c1081_2\ngts                       0.7.6                hb67d8dd_3\ngxx_impl_linux-64         10.4.0              h7ee1905_16    conda-forge\ngxx_linux-64              10.4.0              h6e491c6_10    conda-forge\nharfbuzz                  4.3.0                hd55b92a_0\nhdf5                      1.12.1          mpi_mpich_h08b82f9_4    conda-forge\nicu                       58.2                 he6710b0_3\nimagecodecs               2021.8.26       py310hecf7e94_1\nimageio                   2.19.3          py310h06a4308_0\njinja2                    3.0.3              pyhd3eb1b0_0\njpeg                      9e                   h7f8727e_0\njxrlib                    1.1                  h7b6447c_2\nkernel-headers_linux-64   3.10.0              h4a8ded7_13    conda-forge\nkeyutils                  1.6.1                h166bdaf_0    conda-forge\nkiwisolver                1.4.2           py310h295c915_0\nkrb5                      1.19.3               h3790be6_0    conda-forge\nlatexcodec                2.0.1              pyh9f0ad1d_0    conda-forge\nlcms2                     2.12                 h3be6417_0\nld_impl_linux-64          2.36.1               hea4e1c9_2    conda-forge\nlerc                      3.0                  h295c915_0\nlibaec                    1.0.4                he6710b0_1\nlibblas                   3.9.0           15_linux64_openblas    conda-forge\nlibbrotlicommon           1.0.9                h5eee18b_7\nlibbrotlidec              1.0.9                h5eee18b_7\nlibbrotlienc              1.0.9                h5eee18b_7\nlibcblas                  3.9.0           15_linux64_openblas    conda-forge\nlibclang                  10.0.1          default_hb85057a_2\nlibclang-cpp12            12.0.1          default_ha53f305_4    conda-forge\nlibcurl                   7.83.1               h7bff187_0    conda-forge\nlibdeflate                1.8                  h7f8727e_5\nlibdrm-cos7-x86_64        2.4.97            h9b0a68f_1105    conda-forge\nlibedit                   3.1.20191231         he28a2e2_2    conda-forge\nlibev                     4.33                 h516909a_1    conda-forge\nlibevent                  2.1.12               h8f2d780_0\nlibffi                    3.3                  he6710b0_2\nlibgcc-devel_linux-64     10.4.0              h74af60c_16    conda-forge\nlibgcc-ng                 12.1.0              h8d9b700_16    conda-forge\nlibgd                     2.3.3                h695aa2c_1\nlibgfortran-ng            12.1.0              h69a702a_16    conda-forge\nlibgfortran5              12.1.0              hdcd56e2_16    conda-forge\nlibglu                    9.0.0             he1b5a44_1001    conda-forge\nlibglvnd-cos7-x86_64      1.0.1             h9b0a68f_1105    conda-forge\nlibglvnd-glx-cos7-x86_64  1.0.1             h9b0a68f_1105    conda-forge\nlibgomp                   12.1.0              h8d9b700_16    conda-forge\nlibice-cos7-x86_64        1.0.9             h9b0a68f_1105    conda-forge\nlibice-devel-cos7-x86_64  1.0.9             h9b0a68f_1105    conda-forge\nliblapack                 3.9.0           15_linux64_openblas    conda-forge\nlibllvm10                 10.0.1               hbcb73fb_5\nlibllvm12                 12.0.1               hf817b99_2    conda-forge\nlibnghttp2                1.47.0               hdcd2b5c_1    conda-forge\nlibnsl                    2.0.0                h7f98852_0    conda-forge\nlibopenblas               0.3.20               h043d6bf_1\nlibpng                    1.6.37               hbc83047_0\nlibpq                     12.9                 h16c4e8d_3\nlibrsvg                   2.54.4               h19fe530_0\nlibsanitizer              10.4.0              hde28e3b_16    conda-forge\nlibsm-cos7-x86_64         1.2.2             h9b0a68f_1105    conda-forge\nlibsm-devel-cos7-x86_64   1.2.2             h9b0a68f_1105    conda-forge\nlibssh2                   1.10.0               haa6b8db_3    conda-forge\nlibstdcxx-devel_linux-64  10.4.0              h74af60c_16    conda-forge\nlibstdcxx-ng              12.1.0              ha89aaad_16    conda-forge\nlibtiff                   4.4.0                hecacb30_0\nlibtool                   2.4.6             h9c3ff4c_1008    conda-forge\nlibuuid                   1.0.3                h7f8727e_2\nlibuv                     1.44.2               h166bdaf_0    conda-forge\nlibwebp                   1.2.2                h55f646e_0\nlibwebp-base              1.2.2                h7f8727e_0\nlibx11-common-cos7-x86_64 1.6.7             h9b0a68f_1105    conda-forge\nlibx11-cos7-x86_64        1.6.7             h9b0a68f_1105    conda-forge\nlibx11-devel-cos7-x86_64  1.6.7             h9b0a68f_1105    conda-forge\nlibxcb                    1.15                 h7f8727e_0\nlibxext-cos7-x86_64       1.3.3             h9b0a68f_1105    conda-forge\nlibxext-devel-cos7-x86_64 1.3.3             h9b0a68f_1105    conda-forge\nlibxkbcommon              1.0.1                hfa300c1_0\nlibxml2                   2.9.14               h74e7548_0\nlibxslt                   1.1.35               h4e12654_0\nlibxt-cos7-x86_64         1.1.5             h9b0a68f_1105    conda-forge\nlibxt-devel-cos7-x86_64   1.1.5             h9b0a68f_1105    conda-forge\nlibzlib                   1.2.12               h166bdaf_2    conda-forge\nlibzopfli                 1.0.3                he6710b0_0\nlivereload                2.6.3              pyh9f0ad1d_0    conda-forge\nlocket                    1.0.0           py310h06a4308_0\nlxml                      4.9.1           py310h1edc446_0\nlz4-c                     1.9.3                h295c915_1\nm4                        1.4.18            h516909a_1001    conda-forge\nmake                      4.3                  hd18ef5c_1    conda-forge\nmako                      1.1.4              pyhd3eb1b0_0\nmarkupsafe                2.1.1           py310h7f8727e_0\nmatplotlib                3.5.2           py310h06a4308_0\nmatplotlib-base           3.5.2           py310hf590b9c_0\nmesa-khr-devel-cos7-x86_64 18.3.4            h9b0a68f_1105    conda-forge\nmesa-libgl-cos7-x86_64    18.3.4            h9b0a68f_1105    conda-forge\nmesa-libgl-devel-cos7-x86_64 18.3.4            h9b0a68f_1105    conda-forge\nmesa-libglapi-cos7-x86_64 18.3.4            h9b0a68f_1105    conda-forge\nmesalib                   18.3.1               h590aaf7_0    conda-forge\nmock                      4.0.3              pyhd3eb1b0_0\nmoose-libmesh             2022.09.09              build_0    https://conda.software.inl.gov/public\nmoose-libmesh-vtk         9.1.0               h94e2b84_12    https://conda.software.inl.gov/public\nmoose-mpich               4.0.2                   build_2    https://conda.software.inl.gov/public\nmoose-petsc               3.16.6                  build_1    https://conda.software.inl.gov/public\nmoose-tools               2022.07.18      py310haa4b876_0    https://conda.software.inl.gov/public\nmpc                       1.1.0                h10f8cd9_1\nmpfr                      4.0.2                hb69a4c5_1\nmpi                       1.0                       mpich\nmpich                     4.0.2              h846660c_100    conda-forge\nmpich-mpicc               4.0.2              hb600da9_100    conda-forge\nmpich-mpicxx              4.0.2              h166bdaf_100    conda-forge\nmpich-mpifort             4.0.2              h924138e_100    conda-forge\nmpmath                    1.2.1                    pypi_0    pypi\nmunkres                   1.1.4                      py_0\nncurses                   6.3                  h27087fc_1    conda-forge\nnetworkx                  2.8.4           py310h06a4308_0\nninja                     1.10.2               h06a4308_5\nninja-base                1.10.2               hd09550d_5\nnspr                      4.33                 h295c915_0\nnss                       3.74                 h0370c37_0\nnumexpr                   2.8.3           py310h757a811_0\nnumpy                     1.23.1          py310hac523dd_0\nnumpy-base                1.23.1          py310h375b286_0\nopenjpeg                  2.4.0                h3ad879b_0\nopenssl                   1.1.1q               h166bdaf_0    conda-forge\nordered-set               4.1.0              pyhd8ed1ab_0    conda-forge\npackaging                 21.3               pyhd3eb1b0_0\npandas                    1.4.4           py310h6a678d5_0\npango                     1.50.7               h05da053_0\npartd                     1.2.0              pyhd3eb1b0_1\npcre                      8.45                 h295c915_0\nperl                      5.32.1          2_h7f98852_perl5    conda-forge\npillow                    9.2.0           py310hace64e9_1\npip                       22.2.2          py310h06a4308_0\npixman                    0.40.0               h7f8727e_1\npkg-config                0.29.2               h1bed415_8\nply                       3.11            py310h06a4308_0\npybtex                    0.24.0             pyhd8ed1ab_2    conda-forge\npylatexenc                2.10               pyhd8ed1ab_0    conda-forge\npyparsing                 3.0.9           py310h06a4308_0\npyqt                      5.15.7          py310h6a678d5_1\npyqt5-sip                 12.11.0                  pypi_0    pypi\npython                    3.10.4               h12debd9_0\npython-dateutil           2.8.2              pyhd3eb1b0_0\npython_abi                3.10                    2_cp310    conda-forge\npytz                      2022.1          py310h06a4308_0\npywavelets                1.3.0           py310h7f8727e_0\npyyaml                    6.0                      pypi_0    pypi\nqt-main                   5.15.2               h327a75a_7\nqt-webengine              5.15.9               hd2b0992_4\nqtwebkit                  5.212                h4eab89a_4\nreadline                  8.1.2                h7f8727e_1\nrhash                     1.4.3                h166bdaf_0    conda-forge\nscikit-image              0.19.2          py310h00e6091_0\nscipy                     1.9.1           py310hdfbd76f_0    conda-forge\nsetuptools                59.8.0          py310hff52083_1    conda-forge\nsip                       6.6.2           py310h6a678d5_0\nsix                       1.16.0             pyhd3eb1b0_1\nsnappy                    1.1.9                h295c915_0\nsoupsieve                 2.3.1              pyhd3eb1b0_0\nsqlite                    3.39.3               h5082296_0\nsympy                     1.10.1          py310h06a4308_0\nsysroot_linux-64          2.17                h4a8ded7_13    conda-forge\ntifffile                  2021.7.2           pyhd3eb1b0_2\ntk                        8.6.12               h1ccaba5_0\ntoml                      0.10.2             pyhd3eb1b0_0\ntoolz                     0.11.2             pyhd3eb1b0_0\ntornado                   6.2             py310h5eee18b_0\ntzdata                    2022c                h04d1e81_0\nwheel                     0.37.1             pyhd3eb1b0_0\nxorg-x11-proto-devel-cos7-x86_64 2018.4            h9b0a68f_1105    conda-forge\nxz                        5.2.6                h166bdaf_0    conda-forge\nyaml                      0.2.5                h7b6447c_0\nzfp                       0.5.5                h295c915_6\nzlib                      1.2.12               h166bdaf_2    conda-forge\nzstd                      1.5.2                h6239696_4    conda-forge",
          "url": "https://github.com/idaholab/moose/discussions/22911",
          "updatedAt": "2022-12-10T06:48:09Z",
          "publishedAt": "2022-10-02T14:07:29Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@muh-hassani sorry we missed this. We monitor the Discussions for build issues, not the issues.\nYou likely missed activating the moose environment before compiling",
                  "url": "https://github.com/idaholab/moose/discussions/22911#discussioncomment-4337343",
                  "updatedAt": "2022-12-07T21:17:49Z",
                  "publishedAt": "2022-12-07T21:17:48Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}