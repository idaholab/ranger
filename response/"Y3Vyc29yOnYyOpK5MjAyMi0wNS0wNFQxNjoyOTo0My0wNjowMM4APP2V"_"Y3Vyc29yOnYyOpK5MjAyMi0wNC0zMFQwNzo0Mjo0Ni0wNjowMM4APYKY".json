{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMi0wNC0zMFQwNzo0Mjo0Ni0wNjowMM4APYKY"
    },
    "edges": [
      {
        "node": {
          "title": "How to read nodeset from the file?",
          "author": {
            "login": "avtarsinghh1991"
          },
          "bodyText": "Hello MOOSE Experts\nI want to know if there is any way to read the NODESETS from the file different from the mesh input file.\nLets say I have 4 files, for which I want to generate 4 NODESETS means each nodeset for each file.\nAny suggestions?\nThanks in advance.\nBest\nAvtar",
          "url": "https://github.com/idaholab/moose/discussions/20939",
          "updatedAt": "2022-06-15T14:11:54Z",
          "publishedAt": "2022-05-03T13:50:00Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou can read 4 mesh files and then stitch them or just combine them if they are separate.\nYou cant read most of the mesh from one file and the nodesets from another currently.\nBut you could also implement a mesh generator that does that, it's not very hard.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20939#discussioncomment-2681207",
                  "updatedAt": "2022-06-15T14:11:53Z",
                  "publishedAt": "2022-05-03T15:35:43Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Thanks. I tried to generate the ExtraNodesetGenerator by giving the node numbers but its nor working properly as nodes and element renumbering occurs. I tried allow_renumbering = false but still its not working.\nAny suggestions?",
                          "url": "https://github.com/idaholab/moose/discussions/20939#discussioncomment-2683021",
                          "updatedAt": "2022-06-15T14:12:04Z",
                          "publishedAt": "2022-05-03T20:55:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "no that should work\nwhat kind of simulation are you doing? distributed? split mesh? renumbering only happens on selective occasions",
                          "url": "https://github.com/idaholab/moose/discussions/20939#discussioncomment-2683066",
                          "updatedAt": "2022-06-15T14:12:05Z",
                          "publishedAt": "2022-05-03T21:05:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "how did you get the node numbers? From the exodus file?",
                          "url": "https://github.com/idaholab/moose/discussions/20939#discussioncomment-2683282",
                          "updatedAt": "2022-06-15T14:12:07Z",
                          "publishedAt": "2022-05-03T21:46:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Thanks. Its a structured mesh contains multiple grains. I wrote the .msh file using matlab subroutine. And from the matlab subroutine itself I got to know the node numbers for nodesets.\nPlease find the attached mesh file (.msh) and the moose input file of mesh (.i)\nFile.zip\n.",
                          "url": "https://github.com/idaholab/moose/discussions/20939#discussioncomment-2688011",
                          "updatedAt": "2022-06-18T18:46:04Z",
                          "publishedAt": "2022-05-04T15:29:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "then why dont you include the nodesets in the .msh file?",
                          "url": "https://github.com/idaholab/moose/discussions/20939#discussioncomment-2688772",
                          "updatedAt": "2022-06-18T18:46:04Z",
                          "publishedAt": "2022-05-04T17:16:45Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Thanks Guillaume.\nYes, I can do that. I only thing is that I am still figuring out the syntax to add the nodesets in .msh file. However, thanks for your suggestions.\nBest\nAvtar",
                          "url": "https://github.com/idaholab/moose/discussions/20939#discussioncomment-2688908",
                          "updatedAt": "2022-06-18T18:46:04Z",
                          "publishedAt": "2022-05-04T17:37:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Sounds good. If you find it easier to add a side set in gmsh you can have moose convert a side set to a node set. But I suspect node set is easier in gmsh",
                          "url": "https://github.com/idaholab/moose/discussions/20939#discussioncomment-2689059",
                          "updatedAt": "2022-06-18T18:46:14Z",
                          "publishedAt": "2022-05-04T18:03:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "@GiudGiud\nCan I provide two files i.e., .msh and .geo to MOOSE for mesh data and nodeset data?",
                          "url": "https://github.com/idaholab/moose/discussions/20939#discussioncomment-2689124",
                          "updatedAt": "2022-06-18T18:46:16Z",
                          "publishedAt": "2022-05-04T18:13:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "not for the same parts of the domain currently.",
                          "url": "https://github.com/idaholab/moose/discussions/20939#discussioncomment-2690176",
                          "updatedAt": "2022-06-18T18:46:20Z",
                          "publishedAt": "2022-05-04T21:48:36Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Example of dendritic crystal growth",
          "author": {
            "login": "KangChenRui"
          },
          "bodyText": "Hi, everyone.\nI want to reproduce snowflake growth using 2 order parameters\uff0clike op = eta1, op = eta2,\nthe kenerl is ACInterfaceKobayashi1, ACInterfaceKobayashi2 and InterfaceOrientationMaterial.\nBut it  doesn't work\uff1a\n*** ERROR ***\nThe following material properties are declared on block 0 by multiple materials:\nMaterial Property             Material Objects\nddepsdgrad_op                 material material_2\ndeps                          material material_2\ndepsdgrad_op                  material material_2\neps                           material material_2\nHope to get your help.\nKang C R",
          "url": "https://github.com/idaholab/moose/discussions/20882",
          "updatedAt": "2022-06-09T07:39:04Z",
          "publishedAt": "2022-04-27T01:04:22Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "laagesen"
                  },
                  "bodyText": "Please see #20886",
                  "url": "https://github.com/idaholab/moose/discussions/20882#discussioncomment-2689602",
                  "updatedAt": "2022-06-09T07:39:10Z",
                  "publishedAt": "2022-05-04T19:49:43Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "A problem about MOOSE mesh reading",
          "author": {
            "login": "js-jixu"
          },
          "bodyText": "Hi, everyone.\nI've use gmsh to generate a mesh file for input file, but MOOSE has problem in reading sidesets. The error report screenshot is here.\n\nWhen I use \"--mesh-only\" command to check the mesh file used by input file, it seems that the sidesets are provided. The check screenshots are provided here.\n\n\nI don't know why MOOSE has identified the sidesets but failed to read them. The MOOSE input file named framework.i, gmsh geometry file named model_geo and mesh file named model are attached, they are all txt files and can be used by modified extension name. The group constants file is also attached.\nframework.txt\nmodel_geo.txt\nmodel.txt\ngroup_constants.zip",
          "url": "https://github.com/idaholab/moose/discussions/20924",
          "updatedAt": "2022-06-15T14:11:40Z",
          "publishedAt": "2022-04-30T06:57:18Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "js-jixu"
                  },
                  "bodyText": "The problem is the commas in BCs block in line 152 and 163. I have to remove the commas.",
                  "url": "https://github.com/idaholab/moose/discussions/20924#discussioncomment-2684054",
                  "updatedAt": "2022-06-15T14:11:44Z",
                  "publishedAt": "2022-05-04T02:24:10Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Glad you found the problem",
                          "url": "https://github.com/idaholab/moose/discussions/20924#discussioncomment-2684299",
                          "updatedAt": "2022-06-15T14:11:47Z",
                          "publishedAt": "2022-05-04T04:09:32Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Apply BC only to nodes in contact",
          "author": {
            "login": "Flolaffel"
          },
          "bodyText": "Hello,\nI'd like to apply a BC only to nodes that are currently in contact. Is there a way to do this in MOOSE?",
          "url": "https://github.com/idaholab/moose/discussions/20931",
          "updatedAt": "2022-06-02T06:47:43Z",
          "publishedAt": "2022-05-02T11:09:13Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "So boundary conditions on nodes that are in between two blocks? Internal to the mesh?",
                  "url": "https://github.com/idaholab/moose/discussions/20931#discussioncomment-2674093",
                  "updatedAt": "2022-06-02T06:47:43Z",
                  "publishedAt": "2022-05-02T14:28:29Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Flolaffel"
                          },
                          "bodyText": "BCs only on nodes from one block that are currently in contact. See my description for @dschwen",
                          "url": "https://github.com/idaholab/moose/discussions/20931#discussioncomment-2674342",
                          "updatedAt": "2022-06-02T06:47:43Z",
                          "publishedAt": "2022-05-02T15:03:17Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "dschwen"
                  },
                  "bodyText": "Can you elaborate on what physics you are trying to model? We have something like this in mortar based pressure dependent gap heat conduction.",
                  "url": "https://github.com/idaholab/moose/discussions/20931#discussioncomment-2674135",
                  "updatedAt": "2022-06-02T06:48:08Z",
                  "publishedAt": "2022-05-02T14:32:54Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Flolaffel"
                          },
                          "bodyText": "Of course. I'm doing a tensor mechanics, contact, porous flow simulation. My goal is to indent a porous medium from the top so fluid is squeezed out. At the same time the top surface is the only unconfined surface, so I'm applying a BC there so the porepressure is 0 and there can be outflow. But I want to restrict the outflow at the contact area because fluid wouldn't be able to flow out where contact is established. So I was wondering if MOOSE defines a nodeset that contains the nodes in contact which I could to use to prohibit outflow on these specific nodes.",
                          "url": "https://github.com/idaholab/moose/discussions/20931#discussioncomment-2674319",
                          "updatedAt": "2022-06-02T06:49:54Z",
                          "publishedAt": "2022-05-02T14:59:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "recuero"
                          },
                          "bodyText": "You can define a nodeset or sideset and then probably add a tailored mortar constraint that would do what you want. It doesn't sound like your variable is temperature, so the object may look slightly different from what we already have in MOOSE.",
                          "url": "https://github.com/idaholab/moose/discussions/20931#discussioncomment-2677077",
                          "updatedAt": "2022-06-02T06:49:56Z",
                          "publishedAt": "2022-05-02T23:33:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Flolaffel"
                          },
                          "bodyText": "The variable would be the porepressure and it should be 0 on the free surface but unrestricted where contact is established. I just noticed that would mean I'd have to set my BC only on the nodes NOT in contact, not like I formulated my question initially.\nI will look into the mortar heat conductance.",
                          "url": "https://github.com/idaholab/moose/discussions/20931#discussioncomment-2679342",
                          "updatedAt": "2022-06-02T06:49:56Z",
                          "publishedAt": "2022-05-03T10:32:39Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Solving Eigenvalue Problems within Eigenvalue Executioner",
          "author": {
            "login": "LagrangeW"
          },
          "bodyText": "Respected experts and peers\uff0c\nI'm solving a simple multi-group neutron diffusion problem within Eigenvalue Executioner, while couldn't get correct eigenvalue by imitating the given example input file. Is there some more detailed input syntax guidance of this executioner?\nAny help would be appreciated, thanks!",
          "url": "https://github.com/idaholab/moose/discussions/20239",
          "updatedAt": "2022-06-19T01:40:32Z",
          "publishedAt": "2022-02-06T13:04:15Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThis is the best documentation we have on solving eigenvalue problems.\nhttps://mooseframework.inl.gov/source/problems/EigenProblem.html\nInput file syntax wise, there is this page:\nhttps://mooseframework.inl.gov/source/executioners/Eigenvalue.html\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2126141",
                  "updatedAt": "2022-06-19T01:40:36Z",
                  "publishedAt": "2022-02-07T15:16:35Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "LagrangeW"
                          },
                          "bodyText": "thanks so much for your reply\uff01",
                          "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2155222",
                          "updatedAt": "2022-06-19T01:40:37Z",
                          "publishedAt": "2022-02-11T06:52:40Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "AnthonyB08"
                  },
                  "bodyText": "Hi LagrangeW,\nDid you have any success in modeling multi-group neutron transport? I too am having difficulty with the eigenvalue problem. Mine seems to not want to converge.",
                  "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2642434",
                  "updatedAt": "2022-06-19T01:40:36Z",
                  "publishedAt": "2022-04-26T23:06:30Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "LagrangeW"
                          },
                          "bodyText": "Hello\uff0c\nI\u2018ve done some simple 2D diffusion benchmarks, and the the calculated value is in good agreement with the reference.\nI met some non-converge problems before also, and the everytime it was found to be input error, I give wrong group constants or group cross section.\nMaybe you can try to change your input parameters.",
                          "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2644078",
                          "updatedAt": "2022-08-10T03:07:53Z",
                          "publishedAt": "2022-04-27T06:40:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "AnthonyB08"
                          },
                          "bodyText": "When you found your eigenvalue, lambda, how did you go about solving the criticality diffusion problem using MOOSE. All of my simulations come back 0",
                          "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2650661",
                          "updatedAt": "2022-08-10T03:07:58Z",
                          "publishedAt": "2022-04-27T23:10:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "AnthonyB08"
                          },
                          "bodyText": "I understand that the criticality solves for the differential to be equal to zero, but how does one then analyze the transport for the flux? Is this just not a problem of finding criticality for the respected macroscopic cross-section and diffusion coefficient?",
                          "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2650749",
                          "updatedAt": "2022-08-10T03:07:58Z",
                          "publishedAt": "2022-04-27T23:42:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "LagrangeW"
                          },
                          "bodyText": "I use eigenvalue executioner to solve this type of eigenvalue problem, as @GiudGiud suggests, which could solve out eigenvalue together with it's eigenvector, or neutron flux.",
                          "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2651272",
                          "updatedAt": "2022-08-10T03:08:16Z",
                          "publishedAt": "2022-04-28T02:23:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "AnthonyB08"
                          },
                          "bodyText": "One last question Largrange W, what kernel did you use to couple to two groups together? This is my governing problem",
                          "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2663822",
                          "updatedAt": "2022-08-10T03:08:17Z",
                          "publishedAt": "2022-04-29T20:09:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "LagrangeW"
                          },
                          "bodyText": "I think diffusion kernel https://mooseframework.inl.gov/source/kernels/Diffusion.html can be used in neutron diffusion term(leakage term), and reaction term https://mooseframework.inl.gov/source/kernels/Reaction.html can be used in other terms (removal, fission and scattering).",
                          "url": "https://github.com/idaholab/moose/discussions/20239#discussioncomment-2677391",
                          "updatedAt": "2022-08-10T03:08:18Z",
                          "publishedAt": "2022-05-03T01:31:40Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Decoupled Navier-Stokes and energy equations using multiapp",
          "author": {
            "login": "am-tc01"
          },
          "bodyText": "Hi,\nI am trying to solve steady state Navier Stokes and energy equations in a decoupled manner using multiapp such that the main app solves for the temperature (T) and the sub-app solves for the velocity (v) and pressure (P). The driving force in my case is temperature (like in welding or AM) so the two systems are tightly coupled i.e. the temperature distribution depends on the flow field, while the flow field is driven by the gradient in temperature (Marangoni effect). My question now is, given that it is a steady state problem, do I need one sub-app like I described above or two sub-apps in the following way:\n\nMain app solves for T without v, transfers T to sub-app\nSub-app solves for v and P with T as known, transfers v to sub-sub-app\nSub-sub-app solves for T with v as known\n\nThanks!",
          "url": "https://github.com/idaholab/moose/discussions/20887",
          "updatedAt": "2022-06-05T05:27:11Z",
          "publishedAt": "2022-04-27T10:30:07Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou dont need to have 2 subapps or 3 apps in total. You just need to iterate this:\n\nMain app solves for T with v (starts with initial guess), transfers T to sub-app\nSub-app solves for v and P with T as known, transfers v to main app\n\nEven if the driving force is T, you can solve for velocity as the main app and solve for T in the subapp btw. The decision on subapp/main app depends on which app needs to take the smallest time steps.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20887#discussioncomment-2649980",
                  "updatedAt": "2022-06-05T05:27:13Z",
                  "publishedAt": "2022-04-27T20:31:36Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "am-tc01"
                          },
                          "bodyText": "Hi Guillaume,\nThanks! Given the fact that it's a steady state problem, would it still work with 1 subapp? Do I need Fixed Point iteration in that case?",
                          "url": "https://github.com/idaholab/moose/discussions/20887#discussioncomment-2652432",
                          "updatedAt": "2022-06-05T05:27:14Z",
                          "publishedAt": "2022-04-28T07:14:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You still need fixed point in that problem. Fluid flow is often easier to solve with relaxation-to-steady-state transients. DId you get a solution for steady state already?",
                          "url": "https://github.com/idaholab/moose/discussions/20887#discussioncomment-2655420",
                          "updatedAt": "2022-06-05T05:27:14Z",
                          "publishedAt": "2022-04-28T14:46:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "am-tc01"
                          },
                          "bodyText": "Yes I did get a solution with steady state but the convergence for the fluid flow app is really slow.\nI am not sure if I understood what you meant by \"relaxation-to-steady-state transients\"? Solve the steady state fluid flow as a transient problem with steady_state_detection, or?\nBR.",
                          "url": "https://github.com/idaholab/moose/discussions/20887#discussioncomment-2662875",
                          "updatedAt": "2022-06-05T05:27:14Z",
                          "publishedAt": "2022-04-29T16:56:22Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Yes that's the idea. The flow can reach the steady solution through a transient.\nThis generally works better than trying to solve the steady problem directly.",
                          "url": "https://github.com/idaholab/moose/discussions/20887#discussioncomment-2674349",
                          "updatedAt": "2022-06-12T05:00:59Z",
                          "publishedAt": "2022-05-02T15:04:33Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "am-tc01"
                          },
                          "bodyText": "I see. But I assume it would be slower than solving for steady state directly, given that one can actually get a solution with steady state, right? I should try it anyway. Thanks for the tip!",
                          "url": "https://github.com/idaholab/moose/discussions/20887#discussioncomment-2676102",
                          "updatedAt": "2022-06-12T05:00:59Z",
                          "publishedAt": "2022-05-02T19:46:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Yes it's slower, but it ll work more reliably.\ngood luck, let us know how the coupling goes!",
                          "url": "https://github.com/idaholab/moose/discussions/20887#discussioncomment-2676126",
                          "updatedAt": "2022-06-12T05:00:59Z",
                          "publishedAt": "2022-05-02T19:50:27Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Execution of RVE sub-model for all macro-gauss points",
          "author": {
            "login": "abarun22"
          },
          "bodyText": "Dear all,\nI am working on a computational homogenization scheme where i am required to run my RVE sub-model for each of the macro gauss point and return the homogenized stress measures to the relevant gauss points. For a mesh with 100 elements and 8 GP's per element, i may need to launch 800 sub-apps for performing the computation and each of them will return the results to the individual gauss points. Obviously it's computationally intensive, but parallelization might help here. I wonder whether any activity of this kind has been attempted before. I welcome any related ideas/suggestions here that would help me come up with right simulation framework for my model.\nKind regards,\nArun",
          "url": "https://github.com/idaholab/moose/discussions/20213",
          "updatedAt": "2022-06-14T20:47:44Z",
          "publishedAt": "2022-02-02T18:56:47Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "hello\nThe multiapps system is designed for this. it should spread the multiapps across the processes nicely.\nOne thing that can help performance is to create the smallest type of app possible. So if you can limit your subapp to use a single MOOSE module, heat conduction for example, then specifying app_type=HeatConductionApp to the Multiapp will speed things up.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2099350",
                  "updatedAt": "2022-06-14T20:47:45Z",
                  "publishedAt": "2022-02-02T19:44:24Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Hi Guillaume,\nThanks for the suggestions. Concerning my requirements, i guess 'CentroidMultiApp' is the closest one i can think of doing the job, but however executed only at the element centroids. I want the apps to be called at the element gauss points instead of the centroids and so can i look at adapting 'CentroidMultiApp' to implement this scenario. Also i need a transfer facility to copy the stress tensor at each gauss point on to the sub-apps and return the homogenized stress tensor from the sub-apps on to the macro gauss points.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2105368",
                          "updatedAt": "2022-06-14T20:47:45Z",
                          "publishedAt": "2022-02-03T17:12:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nYes you will have to create a QuadraturePointMultiApp I think.\nSome of the trasnfers will still work if you are working with quadrature points instead of centroids.\nFor example, towards the multiapp, I suppose you will use this one https://mooseframework.inl.gov/source/transfers/MultiAppVariableValueSamplePostprocessorTransfer.html, and since it is based on the location of the multiapp, it should work.\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2105810",
                          "updatedAt": "2022-06-14T20:47:45Z",
                          "publishedAt": "2022-02-03T18:14:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "I started with QuadraturePointMultiApp. Towards that i am looking at methods to access the spatial locations of the quadrature points in the whole domain. Quite a few approaches are available to access them, but most of them start with solving a system of equations which eventually branch out to objects relating to the QP locations. As far as i know there aren't any direct methods which can do this job, say for example a start from the mesh connectivity and then reaching out to individual QP locations.\nThe following code might do the trick in a straight forward manner, but for some reasons i ended up with a seg fault, when it reaches get_elem().\n  System & system = _fe_problem.getNonlinearSystemBase().system();\n  std::unique_ptr<FEMContext> mesh_context = libmesh_make_unique<FEMContext>(system);\n  unsigned short dim = mesh_context->get_elem().dim();\n  FEBase * fe_mesh = nullptr;\n  mesh_context->get_element_fe(0, fe_mesh, dim);\n  std::vector<Point> quad_point = fe_mesh->get_xyz();\n\nAny ideas on getting this done in a neater way is most welcome.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2157326",
                          "updatedAt": "2022-07-05T19:00:03Z",
                          "publishedAt": "2022-02-11T13:28:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nSo what kind of variable are you using? If the quadrature is low-order enough you could possibly get away with using the nodes. It's just a thought though.\nMost likely, what you need to compute quadrature points is to \"reinit\" at quadrature points.\nSee an example here:\n\n  \n    \n      moose/framework/src/base/Assembly.C\n    \n    \n         Line 3817\n      in\n      1bf7c59\n    \n  \n  \n    \n\n        \n          \n           Assembly::elementVolume(const Elem * elem) const \n        \n    \n  \n\n\nthere should be examples starting from just an element, which is what you are probably having a 'for' loop on in this context.\nI ll see if i find one.\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2170733",
                          "updatedAt": "2022-07-11T09:16:37Z",
                          "publishedAt": "2022-02-14T05:43:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "yeah better example here:\n\n  \n    \n      moose/framework/src/loops/ComputeUserObjectsThread.C\n    \n    \n         Line 115\n      in\n      4ee8eed\n    \n  \n  \n    \n\n        \n          \n           _fe_problem.reinitElem(elem, _tid); \n        \n    \n  \n\n\nthis is while looping over elements for user objects. Every time we 'enter' a new element, we 'reinit' it, which computes the quadrature points.\nIn which routine are you adding code exactly?",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2170743",
                          "updatedAt": "2022-07-11T09:16:36Z",
                          "publishedAt": "2022-02-14T05:45:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "As was the case conventionally, i want this implementation to be based on quadrature points alone. I tried the first approach as outlined in Assembly.C, but to my dismay the quadrature points object (q_points) was never populated with QP locations. Please below the related code snippet.\n  MooseMesh & master_mesh = _fe_problem.mesh();\n  auto & mesh = master_mesh.getMesh();\n  for (auto & elem : mesh.active_local_element_ptr_range()){\n    FEType fe_type(elem->default_order(),LAGRANGE);\n    std::unique_ptr<FEBase> fe(FEBase::build(elem->dim(), fe_type));\n    const std::vector<Point> & q_points = fe->get_xyz();\n\nAre you sure the second approach works? I do not see any lines of code that calculates the coordinates whenever we reinit elements.\nI am adding this code in a new source file QuadraturePointMultiapp.C which is an exact replica of CentroidMultiApp.",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2182976",
                          "updatedAt": "2022-07-11T09:20:36Z",
                          "publishedAt": "2022-02-15T18:02:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nYou dont pass enough information to get qps in that construction call. Only the element order and dimension are passed.\nYou need to re-init the FEBase with the element.\nSee https://libmesh.github.io/doxygen/classlibMesh_1_1FEGenericBase.html\nand this function, the reinit for example\nreinit\u00a0(const\u00a0Elem\u00a0*elem, const std::vector<\u00a0Point\u00a0> *const pts=nullptr, const std::vector<\u00a0Real\u00a0> *const weights=nullptr)=0\n\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2183737",
                          "updatedAt": "2022-07-11T09:20:45Z",
                          "publishedAt": "2022-02-15T20:17:58Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "abarun22"
                  },
                  "bodyText": "Hi Guillaume,\nThe code below works.\n MooseMesh & master_mesh = _fe_problem.mesh();\n  auto & mesh = master_mesh.getMesh();\n  for (auto & elem : mesh.active_local_element_ptr_range()){\n    const FEFamily mapping_family = FEMap::map_fe_type(*elem);\n    FEType fe_type(elem->default_order(),mapping_family);\n    std::unique_ptr<FEBase> fe(FEBase::build(elem->dim(), fe_type));\n    const std::vector<Point> & q_points = fe->get_xyz();\n    const int extraorder = 0;\n    std::unique_ptr<QBase> qrule (fe_type.default_quadrature_rule (2, extraorder));\n    fe->attach_quadrature_rule (qrule.get());\n    fe->reinit(elem);\n     for (auto i=0; i<q_points.size(); ++i){ \n      _positions.push_back(q_points[i]);\n}\n  }\n\nThe key here is the positioning of reinit and get_xyz().\nKind regards,\nArun",
                  "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2199099",
                  "updatedAt": "2022-07-11T09:16:30Z",
                  "publishedAt": "2022-02-17T18:09:53Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Hi Guillaume,\nI am currently working on a transfer scheme very similar to 'MultiAppVariableValueSamplePostprocessorTransfer' but for a tensor value across the multiapp system. The issue here being that this tensor (deformation gradient) is to be extracted for a collection of quadrature points across whole of the mesh and these are required to be transferred to individual sub-apps fixed to each quadrature point. I have the following questions in mind that needs clarification before i move on to the implementation phase.\n\nThe deformation gradient for all quadrature points can be stored in a single array like defgrad_tensor(3,3,tnqp) and then transfer individual tensors to the sub-apps with the following segment of code.\n\nfor (unsigned int i = 0; i < _multi_app->numGlobalApps(); i++){\nif (_multi_app->hasLocalApp(i))\n          _multi_app->appProblemBase(i).setPostprocessorValueByName(_postprocessor_name, defgrad_tensor(3,3,i));\n}\n\nI do not want to go this way as this array will grow in size when large meshes are used. Are there any efficient approaches we have in place to map the individual tensors on to the problem base in the sub-apps without having to go for bulky arrays. Some sort of functional mapping or a variable substitution method that can connect the tensors and sub-app problem base would be beneficial here.\n\nThis question mainly depend on the type of approach we choose for data transfer. How do we consolidate the collection of deformation gradients, given that these are material properties and are only referenced at a particular quadrature point when called from material routines? Could we compile them together for the whole set of integration points from a single place?\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2295815",
                          "updatedAt": "2022-07-11T09:21:07Z",
                          "publishedAt": "2022-03-04T15:16:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "So this transfer goes:\nvariable (single valued field) to postprocessor (single value for an app)\nAnd you want to go:\ntensor (multi-valued field, 9 values?) to a few postprocessors (they can only hold one value)?\nYou could consider using a vectorpostprocessor on the receiving hand, would cut down on the number of transfers\nHow bulky do you expect these arrays that you want to transfer at each Qp ?\nIf you want to store the values only once, that's a no-go. Subapp should have its own copy, not retrieve it from the main app every time. Subapps do not know that they are subapps, they behave exactly as if they were the main application.\nSo the deformation gradients are material properties?\nMaterial properties typically are re-computed every time you consider a new element. They are not stored for the whole domain at once. You could transfer each component of the tensor to an auxiliary variable.",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2298824",
                          "updatedAt": "2022-07-11T09:21:10Z",
                          "publishedAt": "2022-03-05T03:21:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Hi Guillaume,\nYes i want deformation gradient tensor(3x3=9 values, yes they are material properties) to be passed on to whatever sub-app data fields. In return i want PK1 stress tensor (3x3) to be passed from individual sub-apps to the macro quadrature points.  I see that 'MultiAppVariableValueSamplePostprocessorTransfer' does not implement the return transfer for some reasons. So you say that each set of tensor specific to a quadrature point can be saved on to an auxiliary variable in the sub-app side? Given below is a typical input file that is supposed to run on all the sub-apps. I am interested in setting the parameter 'targets' with the tensor values we are transferring from the macro quadrature points. Currently this has been done manually within the inputfile through functions . Can this be done more efficiently collecting the values from all QP's?\n# 3D test with just strain control\n\n[GlobalParams]\n  displacements = 'disp_x disp_y disp_z'\n  large_kinematics = true\n  constraint_types = 'strain strain strain strain strain strain strain strain strain'\n  ndim = 3\n  macro_gradient = hvar\n[]\n\n[Mesh]\n  [base]\n    type = FileMeshGenerator\n    file = '3d.e'\n  []\n\n  [sidesets]\n    type = SideSetsFromNormalsGenerator\n    input = base\n    normals = '-1 0 0\n                1 0 0\n                0 -1 0\n                0 1 0\n              '\n              ' 0 0 -1\n                0 0  1  '\n    fixed_normal = true\n    new_boundary = 'left right bottom top back front'\n  []\n[]\n\n[Variables]\n  [disp_x]\n  []\n  [disp_y]\n  []\n  [disp_z]\n  []\n  [hvar]\n    family = SCALAR\n    order = NINTH\n  []\n[]\n\n[AuxVariables]\n  [s11]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [s21]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [s31]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [s12]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [s22]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [s32]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [s13]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [s23]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [s33]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n\n  [F11]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [F21]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [F31]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [F12]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [F22]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [F32]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [F13]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [F23]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n  [F33]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n[]\n\n[AuxKernels]\n  [s11]\n    type = RankTwoAux\n    variable = s11\n    rank_two_tensor = pk1_stress\n    index_i = 0\n    index_j = 0\n  []\n  [s21]\n    type = RankTwoAux\n    variable = s21\n    rank_two_tensor = pk1_stress\n    index_i = 1\n    index_j = 0\n  []\n  [s31]\n    type = RankTwoAux\n    variable = s31\n    rank_two_tensor = pk1_stress\n    index_i = 2\n    index_j = 0\n  []\n  [s12]\n    type = RankTwoAux\n    variable = s12\n    rank_two_tensor = pk1_stress\n    index_i = 0\n    index_j = 1\n  []\n  [s22]\n    type = RankTwoAux\n    variable = s22\n    rank_two_tensor = pk1_stress\n    index_i = 1\n    index_j = 1\n  []\n  [s32]\n    type = RankTwoAux\n    variable = s32\n    rank_two_tensor = pk1_stress\n    index_i = 2\n    index_j = 1\n  []\n  [s13]\n    type = RankTwoAux\n    variable = s13\n    rank_two_tensor = pk1_stress\n    index_i = 0\n    index_j = 2\n  []\n  [s23]\n    type = RankTwoAux\n    variable = s23\n    rank_two_tensor = pk1_stress\n    index_i = 1\n    index_j = 2\n  []\n  [s33]\n    type = RankTwoAux\n    variable = s33\n    rank_two_tensor = pk1_stress\n    index_i = 2\n    index_j = 2\n  []\n\n  [F11]\n    type = RankTwoAux\n    variable = F11\n    rank_two_tensor = deformation_gradient\n    index_i = 0\n    index_j = 0\n  []\n  [F21]\n    type = RankTwoAux\n    variable = F21\n    rank_two_tensor = deformation_gradient\n    index_i = 1\n    index_j = 0\n  []\n  [F31]\n    type = RankTwoAux\n    variable = F31\n    rank_two_tensor = deformation_gradient\n    index_i = 2\n    index_j = 0\n  []\n  [F12]\n    type = RankTwoAux\n    variable = F12\n    rank_two_tensor = deformation_gradient\n    index_i = 0\n    index_j = 1\n  []\n  [F22]\n    type = RankTwoAux\n    variable = F22\n    rank_two_tensor = deformation_gradient\n    index_i = 1\n    index_j = 1\n  []\n  [F32]\n    type = RankTwoAux\n    variable = F32\n    rank_two_tensor = deformation_gradient\n    index_i = 2\n    index_j = 1\n  []\n  [F13]\n    type = RankTwoAux\n    variable = F13\n    rank_two_tensor = deformation_gradient\n    index_i = 0\n    index_j = 2\n  []\n  [F23]\n    type = RankTwoAux\n    variable = F23\n    rank_two_tensor = deformation_gradient\n    index_i = 1\n    index_j = 2\n  []\n  [F33]\n    type = RankTwoAux\n    variable = F33\n    rank_two_tensor = deformation_gradient\n    index_i = 2\n    index_j = 2\n  []\n[]\n\n[UserObjects]\n  [integrator]\n    type = HomogenizationConstraintIntegral\n    targets = 'strain11 strain21 strain31 \n\t             strain12 strain22 strain32 \n\t\t     strain13 strain23 strain33'\n    execute_on = 'initial linear'\n  []\n[]\n\n[Kernels]\n  [sdx]\n    type = HomogenizedTotalLagrangianStressDivergence\n    variable = disp_x\n    component = 0\n  []\n  [sdy]\n    type = HomogenizedTotalLagrangianStressDivergence\n    variable = disp_y\n    component = 1\n  []\n  [sdz]\n    type = HomogenizedTotalLagrangianStressDivergence\n    variable = disp_z\n    component = 2\n  []\n[]\n\n[ScalarKernels]\n  [enforce]\n    type = HomogenizationConstraintScalarKernel\n    variable = hvar\n    integrator = integrator\n  []\n[]\n\n[Functions]\n  [strain11]\n    type = ParsedFunction\n    value = '8.0e-2*t'\n  []\n  [strain22]\n    type = ParsedFunction\n    value = '-4.0e-2*t'\n  []\n  [strain33]\n    type = ParsedFunction\n    value = '8.0e-2*t'\n  []\n  [strain23]\n    type = ParsedFunction\n    value = '2.0e-2*t'\n  []\n  [strain13]\n    type = ParsedFunction\n    value = '-7.0e-2*t'\n  []\n  [strain12]\n    type = ParsedFunction\n    value = '1.0e-2*t'\n  []\n  [strain32]\n    type = ParsedFunction\n    value = '1.0e-2*t'\n  []\n  [strain31]\n    type = ParsedFunction\n    value = '2.0e-2*t'\n  []\n  [strain21]\n    type = ParsedFunction\n    value = '-1.5e-2*t'\n  []\n  [zero]\n    type = ConstantFunction\n    value = 0\n  []\n[]\n\n[BCs]\n  [Periodic]\n    [x]\n      variable = disp_x\n      auto_direction = 'x y z'\n    []\n    [y]\n      variable = disp_y\n      auto_direction = 'x y z'\n    []\n    [z]\n      variable = disp_z\n      auto_direction = 'x y z'\n    []\n  []\n\n  [fix1_x]\n    type = DirichletBC\n    boundary = \"fix_all\"\n    variable = disp_x\n    value = 0\n  []\n  [fix1_y]\n    type = DirichletBC\n    boundary = \"fix_all\"\n    variable = disp_y\n    value = 0\n  []\n  [fix1_z]\n    type = DirichletBC\n    boundary = \"fix_all\"\n    variable = disp_z\n    value = 0\n  []\n\n  [fix2_x]\n    type = DirichletBC\n    boundary = \"fix_xy\"\n    variable = disp_x\n    value = 0\n  []\n  [fix2_y]\n    type = DirichletBC\n    boundary = \"fix_xy\"\n    variable = disp_y\n    value = 0\n  []\n\n  [fix3_z]\n    type = DirichletBC\n    boundary = \"fix_z\"\n    variable = disp_z\n    value = 0\n  []\n[]\n\n[Materials]\n  [elastic_tensor_1]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 100000.0\n    poissons_ratio = 0.3\n    block = '1'\n  []\n  [elastic_tensor_2]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 120000.0\n    poissons_ratio = 0.21\n    block = '2'\n  []\n  [elastic_tensor_3]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 80000.0\n    poissons_ratio = 0.4\n    block = '3'\n  []\n  [elastic_tensor_4]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 76000.0\n    poissons_ratio = 0.11\n    block = '4'\n  []\n  [compute_stress]\n    type = ComputeLagrangianLinearElasticStress\n  []\n  [compute_strain]\n    type = ComputeLagrangianStrain\n    homogenization_gradient_names = 'homogenization_gradient'\n  []\n  [compute_homogenization_gradient]\n    type = ComputeHomogenizedLagrangianStrain\n  []\n[]\n\n[Preconditioning]\n  [smp]\n    type = SMP\n    full = true\n  []\n[]\n\n[Postprocessors]\n  [s11]\n    type = ElementAverageValue\n    variable = s11\n    execute_on = 'initial timestep_end'\n  []\n  [s21]\n    type = ElementAverageValue\n    variable = s21\n    execute_on = 'initial timestep_end'\n  []\n  [s31]\n    type = ElementAverageValue\n    variable = s31\n    execute_on = 'initial timestep_end'\n  []\n  [s12]\n    type = ElementAverageValue\n    variable = s12\n    execute_on = 'initial timestep_end'\n  []\n  [s22]\n    type = ElementAverageValue\n    variable = s22\n    execute_on = 'initial timestep_end'\n  []\n  [s32]\n    type = ElementAverageValue\n    variable = s32\n    execute_on = 'initial timestep_end'\n  []\n  [s13]\n    type = ElementAverageValue\n    variable = s13\n    execute_on = 'initial timestep_end'\n  []\n  [s23]\n    type = ElementAverageValue\n    variable = s23\n    execute_on = 'initial timestep_end'\n  []\n  [s33]\n    type = ElementAverageValue\n    variable = s33\n    execute_on = 'initial timestep_end'\n  []\n\n  [F11]\n    type = ElementAverageValue\n    variable = F11\n    execute_on = 'initial timestep_end'\n  []\n  [F21]\n    type = ElementAverageValue\n    variable = F21\n    execute_on = 'initial timestep_end'\n  []\n  [F31]\n    type = ElementAverageValue\n    variable = F31\n    execute_on = 'initial timestep_end'\n  []\n  [F12]\n    type = ElementAverageValue\n    variable = F12\n    execute_on = 'initial timestep_end'\n  []\n  [F22]\n    type = ElementAverageValue\n    variable = F22\n    execute_on = 'initial timestep_end'\n  []\n  [F32]\n    type = ElementAverageValue\n    variable = F32\n    execute_on = 'initial timestep_end'\n  []\n  [F13]\n    type = ElementAverageValue\n    variable = F13\n    execute_on = 'initial timestep_end'\n  []\n  [F23]\n    type = ElementAverageValue\n    variable = F23\n    execute_on = 'initial timestep_end'\n  []\n  [F33]\n    type = ElementAverageValue\n    variable = F33\n    execute_on = 'initial timestep_end'\n  []\n[]\n\n[Executioner]\n  type = Transient\n\n  solve_type = 'newton'\n  line_search = none\n\n  petsc_options_iname = '-pc_type'\n  petsc_options_value = 'lu'\n\n  l_max_its = 2\n  l_tol = 1e-14\n  nl_max_its = 20\n  nl_rel_tol = 1e-8\n  nl_abs_tol = 1e-10\n\n  start_time = 0.0\n  dt = 0.2\n  dtmin = 0.2\n  end_time = 0.2\n[]\n\n[Outputs]\n#  file_base = strain_3d\n  exodus = true\n  csv = true\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2318782",
                          "updatedAt": "2022-07-11T09:21:09Z",
                          "publishedAt": "2022-03-08T18:36:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so there is no material property transfers as far as I know.\nSo copy everything to aux variables then transfer.\nOr you'll have to code something to make material property transfers work.\nIn the direction:\nauxvariable -> postprocessor (so main app to subapp, from the material property first) use\nhttps://mooseframework.inl.gov/source/transfers/MultiAppVariableValueSamplePostprocessorTransfer.html\npostprocessor -> auxvariable (so subapp to main app, use\nhttps://mooseframework.inl.gov/source/transfers/MultiAppPostprocessorInterpolationTransfer.html",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2319862",
                          "updatedAt": "2022-11-10T23:41:21Z",
                          "publishedAt": "2022-03-08T22:10:57Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "I guess the material property transfers are really hard to implement. Howver i am currently looking in to the other option you mentioned.\nI am kind of caught up at how to copy the deformation gradient (present in ComputeFiniteStrain.C) on to an auxvariable. Do you see the tensors to be copied seperately for each quadrature point or put everything in a single array and copy once. The possibility of later is debatable given that ComputeFiniteStrain::computeProperties() is called in an outer loop of elements and therefore collection in a single array will be tricky. Once we are clear of this method, we could then think about altering MultiAppVariableValueSamplePostprocessorTransfer and MultiAppPostprocessorInterpolationTransfer to transfer the data across the multiapps.\nIf you could provide some examples of copying material properties on to auxvariables, i would be pleased.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2373004",
                          "updatedAt": "2022-11-10T23:41:21Z",
                          "publishedAt": "2022-03-16T13:49:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "I further see another problem while returning the homogenized stresses from the sub model to the macro quadrature points. A look at the material module below suggest that i should be reading and writing the stresses at the class ComputeFiniteStrainElasticStress\n[Materials]\n  [./elasticity_tensor]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 2.1e5\n    poissons_ratio = 0.3\n  [../]\n  [./stress]\n    type = ComputeFiniteStrainElasticStress\n  [../]\n[]\n\n\nThe issue here being that we are reading (the input stress tensor) and writing (homogenized stresses) at the same place in the macro model and more importantly through a single variable _stress[_qp] which creates a problem while calling the sub-model. After reading this tensor we may want to transfer focus to the sub-model, compute the homogenized stress tensor and return to the main model and use that to overwrite _stress[qp]. This action must repeat for all of the integration points in the macro model. My first question here is that how do we break from the main model at this place, run sub-model  and return to the same place to resume the macro computation? Could we make use of EXEC_NONLINEAR to implement this? How about usage of custom transfers ? Will that be of any help here? Another problem  with this approach being the difficulty associated with the parallelization, as we are addressing the workflow QP by QP and never see the creation of multiple number of apps.\nAlternatively we could collect the tensors for all of the QP's, create equivalent number of multiapps, solve the sub-models and return the homogenized stresses to _stress[qp]. This way the parallelization is ensured. The main problem i see here is with regarding to the re-writing of stresses as we are already out of this location where this should happen. But i'm sure there should be a workaround to this, possibly by function overriding or something similar.\nAny suggestion/pointers on adopting the right approach here would be extremely helpful.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2416602",
                          "updatedAt": "2022-11-10T23:41:22Z",
                          "publishedAt": "2022-03-22T17:32:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Just thought i should make further progress in to this deadlock. The intention here is to read and over-write the stresses (in ComputeFiniteStrainElasticStress) within the scope of a macro timestep. For this to happen we need (a) collection of stresses from the macro model (b) a proper transfer mechanism (c) proper choice of a control parameter (execute_on) in QuadraturePointMultiapp that executes the sub-model upon occurence of a certain action. We can somehow come up with an idea to implement (a) and (b). My concern is that what parameter we should use in (c) to signal the execution of sub-model. Do you think CUSTOM or NONLINEAR options for 'execute_on' will be of any help here. This is very much essential as it not only initiates the sub-model at the right place, but also help us maintain MOOSE's natural way of working, especially not disturbing the inbuilt parallelization capabilities of the multi-apps system. Any suggestions here would be much appreciated.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2480431",
                          "updatedAt": "2022-11-10T23:41:58Z",
                          "publishedAt": "2022-03-31T18:05:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nOk seems I missed three messages here.\n1)\nYou're working with a matrix here right? For the tensor.\nIf so then this auxkernel\nhttps://mooseframework.inl.gov/source/auxkernels/MaterialRankTwoTensorAux.html\nwill copy the material property into an auxvariable.\nThen\n\nMultiAppVariableValueSamplePostprocessorTransfer will do the transfer of the auxvariable value at each point to a postprocessor\nMultiAppPostprocessorInterpolationTransfer can be used to send data back (as postprocessors) from the subapp to an auxvariable\n\n\nThere's a lot to unpack there. I ll try to provide some elements.\n\nAre you expecting to have to converge this coupling at every non linear iteration? Like the stress the main app send to the subapp influences the stress received from the subapp, and it impacts the stress in the main app ?\nOr is this a one-off calculation where the stress in the main app is simply overwritten, then you move on. I think in that case you do not want to run on EXEC_NONLINEAR, simply on TIMESTEP_END.\nI would not worry about parallelization for this, MOOSE will take care of running each subapp in an orderly fashion, at every quadrature point one after the other, using as many processes in parallel as it can (given the arguments to mpirun)\nYou want one multiapp, with subapps at every quadrature point.\nMy main concern is actually writing to _stress. I dont think we'll be able to do that in a straight forward way. You ll definitely have to dig in the code to make it happen. Possibly write in a if statement in the stress computation : if(before multiapps, do this, else: load this)\n\n\n\n(a) and (b) should be OK now\n(c) it depends on your answer for the question above",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2481448",
                          "updatedAt": "2022-11-10T23:42:04Z",
                          "publishedAt": "2022-03-31T21:16:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Yes we are dealing mainly with tensors essentially the matrices. I think MaterialRankTwoTensorAux might be a good reference to carry the material properties on to auxvariables. I do not think the coupling happens in the midst of some non-linear iterations. It would be a one-off calculation with an intention to overwrite the macro stresses with the homogenized stresses from the sub-model. It can possibly happen with 'TIMESTEP_END' parameter in QuadraturePointMultiapp. When this object is called you are ready to create the sub-apps for equivalent number of integration points and you would have collected the stresses that needs transfer across these apps. These sub-apps are now in a position to return the stresses back to the integration points. The issue now is we have now lost access to the material module (ComputeFiniteStrainElasticStress) where the over-writing should happen. This is where the purpose of if condition for determining the status of multiapps will not work, as we will never be able to get in to the element computation loop. But we could do a workaround here. If we understand how these stresses are used for further calculation before the start of next time step we might try and develop that additional bit of code and proceed with further timesteps. In my opinion this will be a highly unlikely task given how implicity MOOSE is handling those element computations behind the scenes. The main problem being that we are out of the element loop that does the basic maths such as element integrations and extrapolations needed for the nodal stress calculations. These are my thoughts and i welcome your suggestions here.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2487224",
                          "updatedAt": "2022-11-10T23:42:23Z",
                          "publishedAt": "2022-04-01T16:43:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I dont think you need to worry about the basic math acts here. You ll just need to create a new material to handle this switch between values from the main app and values of stress from the subapps, stored in aux variables.\nHow far along are you? Do you have the:\n\nsaving stress in auxvariables\nquadraturepoint multiapp\ntransfers of stress to the subapp\ntransfers of stress to the main app\n?",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2487657",
                          "updatedAt": "2022-11-10T23:42:28Z",
                          "publishedAt": "2022-04-01T17:49:32Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "srinath-chakravarthy"
                  },
                  "bodyText": "I have a question for a similar problem, however my mesh is exactly the\nsame for the rve and the only thing that changes from rve to rve are the\nBCS which come from the master app. It seems an awful waste to make n qp\nsub apps for such an application. It seems to me that perhaps we could\nconsolidate all it into a single subapp and.lrescibe BCS through the\ntransfer mechanism\n\nAny suggestions are most welcome.\n\nCheers\n\u2026\nOn Fri, Apr 29, 2022, 11:16 AM Guillaume Giudicelli < ***@***.***> wrote:\n If you want to avoid using 48 auxkernels you will have to use an array\n auxvariables, or seriously hack auxkernels to make them act on more than\n one variable. The latter is not recommended.\n\n You can write an array auxkernel to work on array auxvariables. The build\n array variable aux is actually a good example of that\n\n I dont think there's a setVectorPostprocessorByName. I think looking at\n Reporters is a good idea, it's better at dealing with miscealenous data\n types like a vector here.\n\n https://mooseframework.inl.gov/source/transfers/MultiAppReporterTransfer.html\n is able to transfer vectorpostprocessors, treating them as reporters\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#20213 (reply in thread)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/ACYC6LKGEWARASSHAZYK6BTVHP4L3ANCNFSM5NM327XQ>\n .\n You are receiving this because you are subscribed to this thread.Message\n ID: ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2664541",
                  "updatedAt": "2022-07-11T09:23:16Z",
                  "publishedAt": "2022-04-30T00:12:10Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "This means that the RVE computations for all the quadrature points meant to be happening sequentiually in a single sub-app. I am not pretty sure whether the parallel computation makes any impact here.",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2672367",
                          "updatedAt": "2022-07-11T09:23:16Z",
                          "publishedAt": "2022-05-02T09:26:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Just a terminology point, we are planning to use one MultiApp, creating multiple subapp, one for each quadrature point.\nThe subapps are going to be ran in parallel, with usually near perfect scaling.\nSee the docs here https://mooseframework.inl.gov/syntax/MultiApps/",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2674128",
                          "updatedAt": "2022-07-11T09:23:24Z",
                          "publishedAt": "2022-05-02T14:32:12Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "srinath-chakravarthy"
                  },
                  "bodyText": "@abarun22 Not sure what you are referring to, the parallel part of the\ncomputation has 2 parts, solution of the governing equations for each\nsub-app and splitting sub-apps in parallel. What I would like to have\nhappen is have a single-sub-app for all qp's and the resulting governing\nequations of the sub-app will be solved in parallel. Is there a mechanism\nin moose to accomplish this?\n\n@GiudGiud i cannot understand the scaling part, are you suggesting that in\nan example with 100 qp's and n procs, the 100 sub-app solution with n procs\n(n < number of apps) will have the same time scaling as a single sub-app\nwith n proc solution ??\n\u2026\nOn Mon, May 2, 2022 at 5:26 AM abarun22 ***@***.***> wrote:\n This means that the RVE computations for all the quadrature points meant\n to be happening sequentiually in a single sub-app. I am not pretty sure\n whether the parallel computation makes any impact here.\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#20213 (reply in thread)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/ACYC6LKHZPDDPUVA76KCBVDVH6NT5ANCNFSM5NM327XQ>\n .\n You are receiving this because you commented.Message ID:\n ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2674414",
                  "updatedAt": "2022-07-11T09:23:17Z",
                  "publishedAt": "2022-05-02T15:12:59Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "So the form of scaling I was referring to was weak scaling. In a problem with N qps and n<N processors, if N and n go to infinity at the same rate, the solution time stays roughly constant.\nWhat you are describing is a mix of strong scaling and some other concept. Typically I would expect 100 subapps with n=10 procs to solve faster than 1 app with 10 procs. The nonlinear system for each subapp is smaller hence cheaper. But it could go either way, as in the first case the setup is done 10 times per app, whereas in the latter the setup is done once, and with 10 procs.",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2674459",
                          "updatedAt": "2022-07-11T09:23:30Z",
                          "publishedAt": "2022-05-02T15:19:03Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "srinath-chakravarthy"
                  },
                  "bodyText": "Thanks for the explanation. i just a did a quick manual check. I will post\nthe results in a day or so, where i manually created the sub-apps for a 1d\nsystem at all the qp's since the qp locations are known. I also\nmanually created a single sub-app with a combinergenerator to create the\npattern of interest. Initial results seem as if the sub-app is considerably\nslower.\n\nWhat about a mechanism for creating a sub-app like what i am suggesting.\nCan such a thing be done ?\n\nCheers\n\u2026\nOn Mon, May 2, 2022 at 11:19 AM Guillaume Giudicelli < ***@***.***> wrote:\n So the form of scaling I was referring to was weak scaling. In a problem\n with N qps and n<N processors, if N and n go to infinity at the same rate,\n the solution time stays roughly constant.\n\n What you are describing is a mix of strong scaling and some other concept.\n Typically I would expect 100 subapps with n procs to solve faster than 1\n app with 10 procs. The nonlinear system for each subapp is smaller hence\n cheaper. But it could go either way, as in the first case the setup is done\n 10 times per app, whereas in the latter the setup is done once, and with 10\n procs.\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#20213 (reply in thread)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/ACYC6LJ2V3AOPC3NCA5PESLVH7W7HANCNFSM5NM327XQ>\n .\n You are receiving this because you commented.Message ID:\n ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2674583",
                  "updatedAt": "2022-07-11T09:23:24Z",
                  "publishedAt": "2022-05-02T15:38:35Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "For sure. If you want to automate a complex process you can create an action for it:\nhttps://mooseframework.inl.gov/source/actions/Action.html\nThis will be some specialized syntax that creates the objects you need. You could write a MultiApp in an action though it's not usually done.\nIn the single sub-app approach, you can probably get more performance by working on the solve method. Each qp's solve is independent so you should use some sort block-method to solve it.",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2674624",
                          "updatedAt": "2022-07-11T09:23:21Z",
                          "publishedAt": "2022-05-02T15:44:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Thats a nice suggestion having a single sub-app and allowing all the governing equations per qp to be happening in a parallel fashion. Given that the sub-app computation is generally quicker makes it sensible to do this way. I am not very sure about the features in MOOSE to handle this type of computation.",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2674640",
                          "updatedAt": "2022-07-11T09:23:21Z",
                          "publishedAt": "2022-05-02T15:46:01Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "srinath-chakravarthy"
                  },
                  "bodyText": "Thanks, I am going to start working on it soon.\nJust to clarify, what you are suggesting would amount to the following\n1) i would follow something similar to @abarun22 method of extracting the\nqp positions (posted here earlier),\n2) Instead of populating the positions parameters in the multi-app, i would\nuse it to programmatically create an input file for the single sub-app\nbased on these xyz positions ...(either using generated meshes or read an\ninput mesh... etc)\nSo in effect this would be an action inside the multiapp\n\n@guidgiud, are the petsc solvers with LU preconditioning not fast on block\ndiagonal matrices ? I assumed that since you are using such solvers for the\nFV part in MOOSE. Is this not true ??\n\nCheers\n\u2026\nOn Mon, May 2, 2022 at 11:46 AM abarun22 ***@***.***> wrote:\n Thats a nice suggestion having a single sub-app and allowing all the\n governing equations per qp to be happening in a parallel fashion. Given\n that the sub-app computation is generally quicker makes it sensible to do\n this way. I am not very sure about the features in MOOSE to handle this\n type of computation.\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#20213 (reply in thread)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/ACYC6LJGP44GPZUP6XVWVBTVH72EJANCNFSM5NM327XQ>\n .\n You are receiving this because you commented.Message ID:\n ***@***.***>",
                  "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2674970",
                  "updatedAt": "2022-07-11T09:23:23Z",
                  "publishedAt": "2022-05-02T16:33:34Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "There's a lot more blocks here than we have in the finite volume of the Navier Stokes equations.\nLU works well for us as a placeholder while we are working on segregated solvers",
                          "url": "https://github.com/idaholab/moose/discussions/20213#discussioncomment-2675016",
                          "updatedAt": "2022-11-10T23:21:19Z",
                          "publishedAt": "2022-05-02T16:40:14Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "[Multiapp][Turn off subapp]",
          "author": {
            "login": "coskrrb2002"
          },
          "bodyText": "Dear users,\nIs there any way to turn off subapp in the mainapp at the certain time or at the certain value?\nCheers",
          "url": "https://github.com/idaholab/moose/discussions/20901",
          "updatedAt": "2022-07-08T08:00:14Z",
          "publishedAt": "2022-04-28T12:26:58Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "You can use the Controls system (https://mooseframework.inl.gov/moose/syntax/Controls/) to change the enabled flag on a MultiApp.",
                  "url": "https://github.com/idaholab/moose/discussions/20901#discussioncomment-2654647",
                  "updatedAt": "2022-07-08T08:00:14Z",
                  "publishedAt": "2022-04-28T13:11:41Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "coskrrb2002"
                          },
                          "bodyText": "Thank you for your advice!",
                          "url": "https://github.com/idaholab/moose/discussions/20901#discussioncomment-2671373",
                          "updatedAt": "2022-07-08T08:00:14Z",
                          "publishedAt": "2022-05-02T04:58:02Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Boundary condition that couples Neumann data to the time derivative of a variable",
          "author": {
            "login": "aaelmeli"
          },
          "bodyText": "Hi\nI need to define the following boundary condition:\ndu/dx=coeff * du/dt\nCorrect me if I am wrong, CoupledVarNeumannBC will not be useful in this case.\nAny ideas!",
          "url": "https://github.com/idaholab/moose/discussions/20850",
          "updatedAt": "2022-06-15T15:32:24Z",
          "publishedAt": "2022-04-22T17:34:24Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou could still use CoupledVarNeumannBC, if the coupled variable was an auxvariable equal to the derivative. I dont see an auxkernel to do that but it d be pretty easy. Please let know if you want to do that\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2617699",
                  "updatedAt": "2022-06-15T15:32:26Z",
                  "publishedAt": "2022-04-22T18:36:03Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aaelmeli"
                          },
                          "bodyText": "Hello\nYou could still use CoupledVarNeumannBC, if the coupled variable was an auxvariable equal to the derivative. I dont see an auxkernel to do that but it d be pretty easy. Please let know if you want to do that\nGuillaume\n\nThanks, @GiudGiud. Yes, that would be great.",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2617796",
                          "updatedAt": "2022-06-25T15:49:26Z",
                          "publishedAt": "2022-04-22T18:51:57Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Ok I ll make an object today and make a PR to MOOSE",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2618176",
                          "updatedAt": "2022-06-25T15:49:33Z",
                          "publishedAt": "2022-04-22T20:02:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Could you please try out the code in #20853\nand see if it works for you.",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2618971",
                          "updatedAt": "2022-06-25T15:49:33Z",
                          "publishedAt": "2022-04-22T23:55:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aaelmeli"
                          },
                          "bodyText": "Could you please try out the code in #20853 and see if it works for you.\n\nHi @GiudGiud\nWhat command should I use to pull this? I think I am doing something wrong when pulling this to my local.",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2622368",
                          "updatedAt": "2022-08-23T16:04:26Z",
                          "publishedAt": "2022-04-23T23:36:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "There's multiple ways. One would be\n\nadd my branch as remote\nfetch it\nadd the two commits from my PR\n\ngit remote add guillaume git@github.com:GiudGiud/moose.git\ngit fetch guillaume\ngit cherry-pick 3f51a3271b1e7c2637aa4e7607118955bf5b66e1\ngit cherry-pick b233d0b0a77ade27c811d550d274f7891da2f663",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2622399",
                          "updatedAt": "2022-08-23T16:04:26Z",
                          "publishedAt": "2022-04-23T23:56:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aaelmeli"
                          },
                          "bodyText": "Thank you, but I am still getting this error.\n(moose) aaelmeli@CCEE-DT-284:~/projects/moose$ git remote add guillaume git@github.com:GiudGiud/moose.git\n(moose) aaelmeli@CCEE-DT-284:~/projects/moose$ git fetch guillaume\nWarning: Permanently added the ECDSA host key for IP address '140.82.112.3' to the list of known hosts.\ngit@github.com: Permission denied (publickey).\nfatal: Could not read from remote repository.\n\nPlease make sure you have the correct access rights\nand the repository exists.",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2622462",
                          "updatedAt": "2022-08-23T16:04:33Z",
                          "publishedAt": "2022-04-24T00:42:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aaelmeli"
                          },
                          "bodyText": "This is a screenshot of the branches that I have. I think I mistakenly made the moose.git branch with some command. I do not know what is the significance of the \"+\" sign on its left. This might be the problem.\nI tried to delete it but I am getting the following error message:\nerror: Cannot delete branch 'moose.git' checked out at '/home/aaelmeli/projects/moose/git@github.com:GiudGiud/moose.git'",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2622470",
                          "updatedAt": "2022-08-23T16:04:33Z",
                          "publishedAt": "2022-04-24T00:51:08Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "never seen that\nwhat does git remote -v return?\ndid you clone MOOSE with ssh or with https?",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2622563",
                          "updatedAt": "2022-08-23T16:04:35Z",
                          "publishedAt": "2022-04-24T01:47:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aaelmeli"
                          },
                          "bodyText": "never seen that what does git remote -v return?\n\nabdorepo        https://github.com/aaelmeli/moose.git (fetch)\nabdorepo        https://github.com/aaelmeli/moose.git (push)\ngithub-desktop-giudgiud https://github.com/GiudGiud/moose.git (fetch)\ngithub-desktop-giudgiud https://github.com/GiudGiud/moose.git (push)\nguid    git@github.com:GiudGiud/moose.git (fetch)\nguid    git@github.com:GiudGiud/moose.git (push)\nguillaume       git@github.com:GiudGiud/moose.git (fetch)\nguillaume       git@github.com:GiudGiud/moose.git (push)\norigin  https://github.com/idaholab/moose.git (fetch)\norigin  https://github.com/aaelmeli/moose_remote_aaelmeli.git (push)\norigin2 git@github.com:aaelmeli/moose_remote_aaelmeli.git (fetch)\norigin2 git@github.com:aaelmeli/moose_remote_aaelmeli.git (push)\nupstream        https://github.com/idaholab/moose.git (fetch)\nupstream        https://github.com/idaholab/moose.git (push)\n\n\ndid you clone MOOSE with ssh or with https?\n\nI have tried both actually with no success.\nI could get you pr with the following command\ngit fetch https://github.com/GiudGiud/moose.git PR_timeaux:timeaux\nupdated moose and conda, and I am getting another interesting error when do make as follows:\n(moose) aaelmeli@CCEE-DT-284:~/projects/first_test$ make\nRebuilding symlinks in /home/aaelmeli/projects/first_test/build/header_symlinks\nCompiling C++ (in opt mode) /home/aaelmeli/projects/moose/framework/build/unity_src/actions_Unity.C...\nIn file included from /home/aaelmeli/projects/moose/framework/build/header_symlinks/MaterialBase.h:34,\n                 from /home/aaelmeli/projects/moose/framework/build/header_symlinks/Material.h:13,\n                 from /home/aaelmeli/projects/moose/framework/src/actions/CheckOutputAction.C:12,\n                 from /home/aaelmeli/projects/moose/framework/build/unity_src/actions_Unity.C:49:\n/home/aaelmeli/projects/moose/framework/build/header_symlinks/Assembly.h:24:10: fatal error: libmesh/elem_side_builder.h: No such file or directory\n   24 | #include \"libmesh/elem_side_builder.h\"\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nmake: *** [/home/aaelmeli/projects/moose/framework/build.mk:145: /home/aaelmeli/projects/moose/framework/build/unity_src/actions_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2622646",
                          "updatedAt": "2022-08-23T16:04:37Z",
                          "publishedAt": "2022-04-24T02:30:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so to make the ssh cloning work you ll have to go to Github settings and add your ssh public key from your machine.\nThis does look like libmesh is out of date.\nWhat does mamba list | grep moose return?",
                          "url": "https://github.com/idaholab/moose/discussions/20850#discussioncomment-2622697",
                          "updatedAt": "2022-08-23T16:04:40Z",
                          "publishedAt": "2022-04-24T02:57:55Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Frictional contact with locking correction has bad convergence",
          "author": {
            "login": "Flolaffel"
          },
          "bodyText": "Hello,\nI'm running a simulation where frictional contact is applied between two bodies. Without locking correction, the results are incorrect (I have an Abaqus simulation for comparison) and the convergence is good. When I turn locking correction on, the results are correct but the convergence takes a huge hit. The solve time increases from 3 to 10 minutes. I already played with a lot of options like the ones in the contact-block, scaling and PETsc preconditioning but I can't get it to converge properly.\nAny ideas to fix this behaviour?",
          "url": "https://github.com/idaholab/moose/discussions/20856",
          "updatedAt": "2022-06-02T06:52:26Z",
          "publishedAt": "2022-04-24T19:44:55Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nDo you know what methods Abaqus is using to solve that same problem?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2625631",
                  "updatedAt": "2022-06-02T06:52:26Z",
                  "publishedAt": "2022-04-24T20:02:50Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Flolaffel"
                          },
                          "bodyText": "Abaqus uses so called hybrid elements that introduce an additional DOF to compute the pressure stress in the elements to model (nearly) incompressible materials (I'm using a rubber part in my contact).",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2629001",
                          "updatedAt": "2022-06-02T06:52:42Z",
                          "publishedAt": "2022-04-25T11:15:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "how does it solve the nonlinear problem? like the numerical method (newton? pjfnk? and the linear problems at each step if one of those)",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2630765",
                          "updatedAt": "2022-06-02T06:52:41Z",
                          "publishedAt": "2022-04-25T15:06:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Flolaffel"
                          },
                          "bodyText": "Abaqus used the Newton method with a direct solver based on Gauss elimination for the linear equations.",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2631534",
                          "updatedAt": "2022-06-02T06:52:42Z",
                          "publishedAt": "2022-04-25T17:01:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "recuero"
                          },
                          "bodyText": "We don't do many hyperelastic material simulations: Do you know it's friction that triggers this behavior? I.e. is it harder for the solver to converge with frictionless contact?",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2631650",
                          "updatedAt": "2022-06-02T06:54:32Z",
                          "publishedAt": "2022-04-25T17:18:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Flolaffel"
                          },
                          "bodyText": "Frictionless contact convergence is a lot better than frictional. Same simulation with frictionless formulation only takes 3 minutes instead of 10 and needs a lot less iterations.",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2631747",
                          "updatedAt": "2022-06-02T06:54:31Z",
                          "publishedAt": "2022-04-25T17:33:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "recuero"
                          },
                          "bodyText": "Yeah, but I was mostly referring to the interaction with the correction to volumetric locking.",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2631776",
                          "updatedAt": "2022-06-02T06:54:32Z",
                          "publishedAt": "2022-04-25T17:37:33Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "recuero"
                          },
                          "bodyText": "Also, could you paste your contact block?",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2631808",
                          "updatedAt": "2022-06-02T06:54:32Z",
                          "publishedAt": "2022-04-25T17:42:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Flolaffel"
                          },
                          "bodyText": "I've run some tests and here are the numbers I obtained:\n\nfrictionless, no locking correction: 83 nonlinear iterations, 229 linear iterations\nfrictionless, locking correction: 83 nonlinear iteration, 285 linear iterations\nfrictional, no locking correction: 111 nonlinear iterations, 2961 linear iterations\nfrictional, locking correction: 191 nonlinear iterations, 13115 linear iterations\n\nAs you can see, the frictional contact convergence definitely takes a bigger hit from the locking correction than the frictionless contact. Frictionless contact convergence remains fine.\nMy contact block looks as follows\n[Contact]\n  [kontakt]\n    primary = top\n    secondary = rmax\n    formulation = penalty\n    penalty = 1e4\n    model = frictionless/coulomb\n    (friction_coefficient = 0.6)\n    normal_smoothing_distance = 0.1\n  []\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2639163",
                          "updatedAt": "2022-06-02T06:54:32Z",
                          "publishedAt": "2022-04-26T13:37:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "bwspenc"
                          },
                          "bodyText": "I don't know what to say about the connection between volumetric locking and poor convergence. Frictional contact problems are always tough, so it doesn't surprise me that you're in general having more trouble with friction.\nIt's interesting that your nonlinear convergence isn't too bad, but the linear convergence is what's really suffering. That indicates that your preconditioner isn't very effective. That could be due to a poor preconditioning method or a poor approximation of the Jacobian provided by your mechanics kernels (or a combination of the two). You'll have your best luck with a direct solver like superlu_dist and a full preconditioning matrix. I'm not sure what your settings are in your mechanics materials/kernels, but there are some options there that can help as well.",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2640863",
                          "updatedAt": "2022-06-02T06:55:43Z",
                          "publishedAt": "2022-04-26T17:13:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Flolaffel"
                          },
                          "bodyText": "I played with a lot of different preconditioning settings. The results posted above were obtained with PJFNK, full SMP and superlu_dist. When using fricitonless contact the best settings seem to be Newton, -ksp_type preonly and mumps. But these fail to converge with frictional contact.\nCould you please provide some insight on the settings you mentioned?",
                          "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2640980",
                          "updatedAt": "2022-06-02T06:55:59Z",
                          "publishedAt": "2022-04-26T17:35:18Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "recuero"
                  },
                  "bodyText": "I'd recommend following the setup of existing frictional cases in the repository.",
                  "url": "https://github.com/idaholab/moose/discussions/20856#discussioncomment-2666225",
                  "updatedAt": "2022-06-02T06:52:41Z",
                  "publishedAt": "2022-04-30T13:42:46Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}