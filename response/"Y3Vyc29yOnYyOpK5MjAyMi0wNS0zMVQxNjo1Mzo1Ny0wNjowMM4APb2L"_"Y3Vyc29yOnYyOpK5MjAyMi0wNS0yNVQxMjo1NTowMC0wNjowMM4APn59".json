{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMi0wNS0yNVQxMjo1NTowMC0wNjowMM4APn59"
    },
    "edges": [
      {
        "node": {
          "title": "Add multiple points as boundaries",
          "author": {
            "login": "Joseph-0123"
          },
          "bodyText": "Hello everyone, I want to add multiple points (~1k) as boundaries.\nI get used to using the ExtraNodesetGenerator to create point boundaries. But it's a huge task for thousands point,\nIs there a similar way like ReporterPointSource for adding multiple points (~1k) as boundaries?\nhttps://mooseframework.inl.gov/blackbear/source/dirackernels/ReporterPointSource.html\nThanks a lot,",
          "url": "https://github.com/idaholab/moose/discussions/21046",
          "updatedAt": "2022-05-31T09:07:53Z",
          "publishedAt": "2022-05-18T11:30:01Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou're right that it's too much to do manually. Is there anything in common between those points ? Like they are nodes on faces with the same normal? Or they are between two separate subdomains? Or on the outside of known subdomains?\nTypically we use these geometric characteristics in the mesh generator system to generate sidesets. Nodesets can be obtained from sidesets with just a parameter to Mesh.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21046#discussioncomment-2776623",
                  "updatedAt": "2022-06-14T08:46:16Z",
                  "publishedAt": "2022-05-18T14:48:59Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "Hello, Guillaume, thanks for your responses.\nYes. A thousand points set as boundaries are located at the 'Line block' (in red color).  Their interval distance is 10m. They belong to the same block.\nThanks for your help again,",
                          "url": "https://github.com/idaholab/moose/discussions/21046#discussioncomment-2776736",
                          "updatedAt": "2022-05-31T09:07:41Z",
                          "publishedAt": "2022-05-18T15:00:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nI'd say there's two options :\n\nmake the inner cube a different block (it s totally ok wrt to defining materials and kernels to spread them on two blocks rather than one) then use a sidesetbetweensubdomaingenerator\nhttps://mooseframework.inl.gov/source/meshgenerators/SideSetsBetweenSubdomainsGenerator.html\nOR\nuse one (or more) parsed expressions to specify the sidesets. Since it's straight it's just  'x<1000.1 & x>999.9 & y>-3000.1' for example for the expression\nhttps://mooseframework.inl.gov/source/meshgenerators/ParsedGenerateSideset.html\n\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/21046#discussioncomment-2777062",
                          "updatedAt": "2022-05-18T15:43:00Z",
                          "publishedAt": "2022-05-18T15:43:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "Great. They all look useful. Thanks, Guillaume, I will follow their way.\nCheers,",
                          "url": "https://github.com/idaholab/moose/discussions/21046#discussioncomment-2777500",
                          "updatedAt": "2022-05-31T09:07:29Z",
                          "publishedAt": "2022-05-18T16:39:24Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "MOOSE parallelsation doesn't scale linearly for my problem?",
          "author": {
            "login": "Traiwit"
          },
          "bodyText": "Hi guys,\nI'm doing sensitivity analysis to see what would be the optimum number of mpi-cores/threads I should use for my problem:\nso below is the size and general info of my model, the module is [Porousflow].\n\nby the look in here, it's supposed to scale linearly with the number of mpi-core: https://mooseframework.inl.gov/getting_started/examples_and_tutorials/tutorial01_app_development/step07_parallel.html\nbut this is what I got for run-time in second (the first number is mpicore, the second number is threads)\n11+0\t1125.03\n11+2\t956.082\n11+4\t931.994\n11+6\t1064.535\n11+8\t1254.079\n11+12\t2192.663\n8+2          1001.693\n4+2\t        1220.607\n4+4\t        1043.648\n2+2\t        1667.109\n6+4\t        967.795\n3+4\t        1169.202\nmy system: AMD\u00ae Ryzen threadripper 3970x 32-core processor \u00d7 64 , 251.6 GB of RAM\nThe  max mpi-core I can use is ~11 cores, my simulation requires ~20gb per mpi-core due to heavy linear_constraints.\nin this case, I could have just run 3 simulations with 3 cores simultaneously to save time? or something isn't right with my setup?\nKind regards,\nTraiwit",
          "url": "https://github.com/idaholab/moose/discussions/20822",
          "updatedAt": "2022-06-12T18:58:49Z",
          "publishedAt": "2022-04-20T05:37:00Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThreading parallelization is hard to get right. Some sections are easier to do than others, so not all of your simulation may be multithreaded. For example, mesh generation is often not multithreaded. Many solve methods (petsc parts) are not multithreaded either. If you want to spend time on evaluating this, I recommend you use the perf graph https://mooseframework.inl.gov/source/utils/PerfGraph.html and plot the timings for all the sections of your simulation. Then you can identify what scales and what does not.\nMPI or distributed parallelization should work a lot better in MOOSE. It does look like your simulation is capping out at 3 MPI ranks, which is not normal. So again, let's use the PerfGraph and figure out what part of your simulation is not scaling.\nFrom other threads, I gather you are using replicated meshes? This typically wont scale very far. Not at all in memory but also limited in CPU time.\nDo you have a list of objects that do not work for you for distributed meshes?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/20822#discussioncomment-2602821",
                  "updatedAt": "2022-06-12T18:58:51Z",
                  "publishedAt": "2022-04-20T16:35:49Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "Hi @GiudGiud\nI have done some testing there are two main objects that stop me from using distributed meshes.\nfirst of all the LinearNodalConstraints action:\nThis is the error\n===================================================================================\n=   BAD TERMINATION OF ONE OF YOUR APPLICATION PROCESSES\n=   PID 2823469 RUNNING AT moose-System-Product-Name\n=   EXIT CODE: 139\n=   CLEANING UP REMAINING PROCESSES\n=   YOU CAN IGNORE THE BELOW CLEANUP MESSAGES\n===================================================================================\nYOUR APPLICATION TERMINATED WITH THE EXIT STRING: Segmentation fault (signal 11)\nThis typically refers to a problem with your application.\nPlease see the FAQ page for debugging suggestions\n\nAs I suspect, node-pair might be on the different meshes (after splitting)\nSecond, so I ran the simulation without LinearNodalConstraints action, this is the new error that I just found\nit is the use of assigning element properties from the CSV files (combination of ElementPropertyReadFile and my own version of perm/porosity) [the example is shown below]\n      [./permeability_1]\n        type = PorousFlowPermeabilityConstCSV\n        read_prop_user_object = perm_read\n        damage = damage\n      [../]\n\nand this is the error:\n*** ERROR ***\nThe following error occurred in the object \"poro_read\", of type \"ElementPropertyReadFile\".\n\nElement ID18928880 greater than than total number of element in mesh 4598664\n\n\nI suspect that after the mesh is split, each of the small meshes contains a portion of elements, hence the number of properties values from ElementPropertyReadFile and the material properties are not matched.\nWithout these 2 objects (removing constraints and using the default homogenous material properties), my simulation can be run using the distributed-mesh option with no problem.\nDo you think these are possible to solve? feel like this is more of a fundamental/core system problem rather than modifying the objects kinda problem.\nnote: I will post perf graph output from the replicated parallel shortly\nThank you!\nKind regards,\nTraiwit",
                          "url": "https://github.com/idaholab/moose/discussions/20822#discussioncomment-2606163",
                          "updatedAt": "2022-06-12T18:58:52Z",
                          "publishedAt": "2022-04-21T04:54:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "This is the perf graph output from run with the replicated parallel (3 mpi-cores and 4 threads)\nPerformance Graph:\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|                                 Section                                | Calls |   Self(s)  |   Avg(s)   |    %   | Mem(MB) |  Total(s)  |   Avg(s)   |    %   | Mem(MB) |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| traiTestApp (main)                                                     |     1 |      0.035 |      0.035 |   0.00 |       4 |   1580.082 |   1580.082 | 100.00 |   20957 |\n|   RankMap::construct                                                   |     1 |      0.000 |      0.000 |   0.00 |       1 |      0.000 |      0.000 |   0.00 |       1 |\n|   MooseApp::setup                                                      |     1 |      0.000 |      0.000 |   0.00 |       0 |    320.839 |    320.839 |  20.31 |   19107 |\n|     Action::SetupMeshAction::Mesh::SetupMeshAction::act::setup_mesh    |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     Action::SetupMeshAction::Mesh::SetupMeshAction::act::set_mesh_base |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|       MeshGeneratorMesh::init                                          |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     Action::SetupMeshCompleteAction::Mesh::completeSetupUndisplaced    |     2 |      0.000 |      0.000 |   0.00 |       0 |     11.209 |      5.605 |   0.71 |     109 |\n|       MeshGeneratorMesh::prepare                                       |     2 |      0.801 |      0.400 |   0.05 |       0 |     11.209 |      5.605 |   0.71 |     109 |\n|     Action::SetupMeshCompleteAction::Mesh::uniformRefine               |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     Action::SetupMeshCompleteAction::Mesh::deleteRemoteElems           |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     MooseApp::executeMeshGenerators                                    |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     FEProblem::init                                                    |     1 |      0.119 |      0.119 |   0.01 |     127 |     45.270 |     45.270 |   2.87 |    1247 |\n|       FEProblem::EquationSystems::Init                                 |     1 |     23.222 |     23.222 |   1.47 |     883 |     29.152 |     29.152 |   1.84 |    1100 |\n|   MooseApp::execute                                                    |     1 |      0.000 |      0.000 |   0.00 |       0 |   1259.207 |   1259.207 |  79.69 |    1845 |\n|     FEProblem::initialSetup                                            |     1 |      0.001 |      0.001 |   0.00 |       0 |     51.274 |     51.274 |   3.25 |    1359 |\n|       FEProblem::projectSolution                                       |     1 |      4.647 |      4.647 |   0.29 |       0 |      4.647 |      4.647 |   0.29 |       0 |\n|       NonlinearSystemBase::nlInitialSetup                              |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.006 |      0.006 |   0.00 |       0 |\n|         NonlinearSystemBase::kernelsInitialSetup                       |     1 |      0.006 |      0.006 |   0.00 |       0 |      0.006 |      0.006 |   0.00 |       0 |\n|         NonlinearSystemBase::mortarSetup                               |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|       FEProblem::initialSetupTransfers                                 |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     FEProblem::outputStep                                              |     1 |      0.035 |      0.035 |   0.00 |       0 |      0.101 |      0.101 |   0.01 |       0 |\n|       Console::outputStep                                              |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|       Exodus::outputStep                                               |     1 |      0.066 |      0.066 |   0.00 |       0 |      0.066 |      0.066 |   0.00 |       0 |\n|     Steady::PicardSolve                                                |     1 |      0.000 |      0.000 |   0.00 |       0 |   1185.522 |   1185.522 |  75.03 |     271 |\n|       FEProblem::outputStep                                            |     2 |      0.068 |      0.034 |   0.00 |       0 |      0.068 |      0.034 |   0.00 |       0 |\n|         Console::outputStep                                            |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|       FEProblem::solve                                                 |     1 |    346.052 |    346.052 |  21.90 |      68 |   1182.552 |   1182.552 |  74.84 |     271 |\n|         Console::outputStep                                            |   438 |     59.453 |      0.136 |   3.76 |       0 |     59.453 |      0.136 |   3.76 |       0 |\n|         FEProblem::computeResidualInternal                             |     1 |      0.000 |      0.000 |   0.00 |       0 |      7.843 |      7.843 |   0.50 |     127 |\n|         FEProblem::computeResidualInternal                             |    59 |      0.001 |      0.000 |   0.00 |       0 |    444.198 |      7.529 |  28.11 |       1 |\n|         NonlinearSystemBase::initialBCs                                |     1 |      0.114 |      0.114 |   0.01 |       0 |      0.114 |      0.114 |   0.01 |       0 |\n|         FEProblem::computeJacobianInternal                             |    43 |      0.001 |      0.000 |   0.00 |       0 |    324.890 |      7.556 |  20.56 |      75 |\n|     Steady::final                                                      |     1 |      0.000 |      0.000 |   0.00 |       0 |     22.302 |     22.302 |   1.41 |     215 |\n|       FEProblem::outputStep                                            |     1 |      0.034 |      0.034 |   0.00 |       0 |     22.302 |     22.302 |   1.41 |     215 |\n|         Console::outputStep                                            |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|         Exodus::outputStep                                             |     1 |     22.268 |     22.268 |   1.41 |     215 |     22.268 |     22.268 |   1.41 |     215 |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nHeaviest Branch:\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|                              Section                             | Calls |   Self(s)  |   Avg(s)   |    %   | Mem(MB) |  Total(s)  |   Avg(s)   |    %   | Mem(MB) |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| traiTestApp (main)                                               |     1 |      0.035 |      0.035 |   0.00 |       4 |   1580.082 |   1580.082 | 100.00 |   20957 |\n|   MooseApp::run                                                  |     1 |      0.000 |      0.000 |   0.00 |       0 |   1580.047 |   1580.047 | 100.00 |   20952 |\n|     MooseApp::execute                                            |     1 |      0.000 |      0.000 |   0.00 |       0 |   1259.207 |   1259.207 |  79.69 |    1845 |\n|       MooseApp::executeExecutioner                               |     1 |      0.007 |      0.007 |   0.00 |       0 |   1259.207 |   1259.207 |  79.69 |    1845 |\n|         Steady::PicardSolve                                      |     1 |      0.000 |      0.000 |   0.00 |       0 |   1185.522 |   1185.522 |  75.03 |     271 |\n|           FEProblem::solve                                       |     1 |    346.052 |    346.052 |  21.90 |      68 |   1182.552 |   1182.552 |  74.84 |     271 |\n|             FEProblem::computeResidualSys                        |    59 |      0.001 |      0.000 |   0.00 |       0 |    444.199 |      7.529 |  28.11 |       1 |\n|               FEProblem::computeResidualInternal                 |    59 |      0.001 |      0.000 |   0.00 |       0 |    444.198 |      7.529 |  28.11 |       1 |\n|                 FEProblem::computeResidualTags                   |    59 |      0.015 |      0.000 |   0.00 |       0 |    444.197 |      7.529 |  28.11 |       1 |\n|                   NonlinearSystemBase::nl::computeResidualTags   |    59 |      0.751 |      0.013 |   0.05 |       1 |    254.352 |      4.311 |  16.10 |       1 |\n|                     NonlinearSystemBase::computeResidualInternal |    59 |     41.840 |      0.709 |   2.65 |       0 |    246.928 |      4.185 |  15.63 |       0 |\n|                       NonlinearSystemBase::Kernels               |    59 |    203.653 |      3.452 |  12.89 |       0 |    203.653 |      3.452 |  12.89 |       0 |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nHeaviest Sections:\n-------------------------------------------------------------------------------------------------\n|                  Section                 | Calls |   Self(s)  |    Avg.    |    %   | Mem(MB) |\n-------------------------------------------------------------------------------------------------\n| FEProblem::solve                         |     1 |    346.052 |    346.052 |  21.90 |      68 |\n| NonlinearSystemBase::computeJacobianTags |    43 |    283.396 |      6.591 |  17.94 |      75 |\n| NonlinearSystemBase::Kernels             |    60 |    207.236 |      3.454 |  13.12 |     127 |\n| AuxiliarySystem::computeElementalVars    |   107 |    193.693 |      1.810 |  12.26 |       0 |\n| Action::ExecuteMeshGenerators::act       |     1 |    134.774 |    134.774 |   8.53 |    1580 |\n| Console::outputStep                      |   443 |     59.453 |      0.134 |   3.76 |       0 |\n| Action::MPCbe::MPCbe::act                |     1 |     52.677 |     52.677 |   3.33 |   14037 |\n-------------------------------------------------------------------------------------------------",
                          "url": "https://github.com/idaholab/moose/discussions/20822#discussioncomment-2606222",
                          "updatedAt": "2022-06-12T18:59:09Z",
                          "publishedAt": "2022-04-21T05:13:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "if you post 4 MPI 1 thread and 1 MPI 1 thread we can have a basic look at what is scaling and what is not.\nNote that the PerfGraphOutput can help you process this data easily if you want to draw scalability plots:\nhttps://mooseframework.inl.gov/source/outputs/PerfGraphOutput.html\nwrt to these 2 objects, I think they are both adaptable to distributed meshes. The second one trivially so. We just need to relax some of the bounds checking on the mesh vs the CSV file.\nFor the LinearNodalConstraint we need to take a serious look at it. It should work, there is code inside it aimed at working in parallel.",
                          "url": "https://github.com/idaholab/moose/discussions/20822#discussioncomment-2606376",
                          "updatedAt": "2022-06-12T18:59:34Z",
                          "publishedAt": "2022-04-21T06:03:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "will get back with 4 MPI 1 thread and 1 MPI 1 thread soon, thank you @GiudGiud!\nLet's have a look into both objects as well after that (if you have time)\nRegards,\nTraiwit",
                          "url": "https://github.com/idaholab/moose/discussions/20822#discussioncomment-2606388",
                          "updatedAt": "2022-06-12T18:59:53Z",
                          "publishedAt": "2022-04-21T06:06:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "**edit: the one above was 3 mpi 4 threads.\nso 1 MPI 1 thread >>  7229.056 sec\nand 3 MPI 1 thread >> 3430.557 sec (also converged better, for some reason)\nPerformance Graph:\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|                                 Section                                | Calls |   Self(s)  |   Avg(s)   |    %   | Mem(MB) |  Total(s)  |   Avg(s)   |    %   | Mem(MB) |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| traiTestApp (main)                                                     |     1 |      0.039 |      0.039 |   0.00 |       5 |   7229.056 |   7229.056 | 100.00 |   21545 |\n|   RankMap::construct                                                   |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|   MooseApp::setup                                                      |     1 |      0.000 |      0.000 |   0.00 |       0 |    245.984 |    245.984 |   3.40 |   19623 |\n|     Action::SetupMeshAction::Mesh::SetupMeshAction::act::setup_mesh    |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     Action::SetupMeshAction::Mesh::SetupMeshAction::act::set_mesh_base |     2 |      0.003 |      0.001 |   0.00 |       0 |      0.003 |      0.001 |   0.00 |       0 |\n|       MeshGeneratorMesh::init                                          |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     Action::SetupMeshCompleteAction::Mesh::completeSetupUndisplaced    |     2 |      0.001 |      0.000 |   0.00 |       0 |     10.681 |      5.341 |   0.15 |     137 |\n|       MeshGeneratorMesh::prepare                                       |     2 |      0.960 |      0.480 |   0.01 |       0 |     10.680 |      5.340 |   0.15 |     137 |\n|     Action::SetupMeshCompleteAction::Mesh::uniformRefine               |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     Action::SetupMeshCompleteAction::Mesh::deleteRemoteElems           |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     MooseApp::executeMeshGenerators                                    |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     FEProblem::init                                                    |     1 |      0.014 |      0.014 |   0.00 |       0 |     40.806 |     40.806 |   0.56 |    1757 |\n|       FEProblem::EquationSystems::Init                                 |     1 |     23.832 |     23.832 |   0.33 |    1422 |     29.276 |     29.276 |   0.40 |    1737 |\n|   MooseApp::execute                                                    |     1 |      0.000 |      0.000 |   0.00 |       0 |   6983.034 |   6983.034 |  96.60 |    1917 |\n|     FEProblem::initialSetup                                            |     1 |      0.001 |      0.001 |   0.00 |       0 |     58.967 |     58.967 |   0.82 |       0 |\n|       FEProblem::projectSolution                                       |     1 |     39.463 |     39.463 |   0.55 |       0 |     39.463 |     39.463 |   0.55 |       0 |\n|       NonlinearSystemBase::nlInitialSetup                              |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.005 |      0.005 |   0.00 |       0 |\n|         NonlinearSystemBase::kernelsInitialSetup                       |     1 |      0.005 |      0.005 |   0.00 |       0 |      0.005 |      0.005 |   0.00 |       0 |\n|         NonlinearSystemBase::mortarSetup                               |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|       FEProblem::initialSetupTransfers                                 |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     FEProblem::outputStep                                              |     1 |      0.013 |      0.013 |   0.00 |       0 |      0.076 |      0.076 |   0.00 |       9 |\n|       Console::outputStep                                              |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|       Exodus::outputStep                                               |     1 |      0.062 |      0.062 |   0.00 |       9 |      0.062 |      0.062 |   0.00 |       9 |\n|     Steady::PicardSolve                                                |     1 |      0.000 |      0.000 |   0.00 |       0 |   6881.051 |   6881.051 |  95.19 |    1832 |\n|       FEProblem::outputStep                                            |     2 |      0.029 |      0.014 |   0.00 |       0 |      0.029 |      0.014 |   0.00 |       0 |\n|         Console::outputStep                                            |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|       FEProblem::solve                                                 |     1 |    421.155 |    421.155 |   5.83 |    -859 |   6852.621 |   6852.621 |  94.79 |    1832 |\n|         Console::outputStep                                            |   406 |    189.606 |      0.467 |   2.62 |    1473 |    189.606 |      0.467 |   2.62 |    1473 |\n|         FEProblem::computeResidualInternal                             |     1 |      0.000 |      0.000 |   0.00 |       0 |     67.603 |     67.603 |   0.94 |    1042 |\n|         FEProblem::computeResidualInternal                             |    58 |      0.001 |      0.000 |   0.00 |       0 |   3870.429 |     66.732 |  53.54 |       7 |\n|         NonlinearSystemBase::initialBCs                                |     1 |      0.378 |      0.378 |   0.01 |       0 |      0.378 |      0.378 |   0.01 |       0 |\n|         FEProblem::computeJacobianInternal                             |    42 |      0.001 |      0.000 |   0.00 |       0 |   2303.447 |     54.844 |  31.86 |     169 |\n|     Steady::final                                                      |     1 |      0.000 |      0.000 |   0.00 |       0 |     42.934 |     42.934 |   0.59 |      76 |\n|       FEProblem::outputStep                                            |     1 |      0.016 |      0.016 |   0.00 |       0 |     42.934 |     42.934 |   0.59 |      76 |\n|         Console::outputStep                                            |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|         Exodus::outputStep                                             |     1 |     42.918 |     42.918 |   0.59 |      76 |     42.918 |     42.918 |   0.59 |      76 |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nHeaviest Branch:\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|                              Section                             | Calls |   Self(s)  |   Avg(s)   |    %   | Mem(MB) |  Total(s)  |   Avg(s)   |    %   | Mem(MB) |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| traiTestApp (main)                                               |     1 |      0.039 |      0.039 |   0.00 |       5 |   7229.056 |   7229.056 | 100.00 |   21545 |\n|   MooseApp::run                                                  |     1 |      0.000 |      0.000 |   0.00 |       0 |   7229.018 |   7229.018 | 100.00 |   21540 |\n|     MooseApp::execute                                            |     1 |      0.000 |      0.000 |   0.00 |       0 |   6983.034 |   6983.034 |  96.60 |    1917 |\n|       MooseApp::executeExecutioner                               |     1 |      0.006 |      0.006 |   0.00 |       0 |   6983.034 |   6983.034 |  96.60 |    1917 |\n|         Steady::PicardSolve                                      |     1 |      0.000 |      0.000 |   0.00 |       0 |   6881.051 |   6881.051 |  95.19 |    1832 |\n|           FEProblem::solve                                       |     1 |    421.155 |    421.155 |   5.83 |    -859 |   6852.621 |   6852.621 |  94.79 |    1832 |\n|             FEProblem::computeResidualSys                        |    58 |      0.001 |      0.000 |   0.00 |       0 |   3870.430 |     66.732 |  53.54 |       7 |\n|               FEProblem::computeResidualInternal                 |    58 |      0.001 |      0.000 |   0.00 |       0 |   3870.429 |     66.732 |  53.54 |       7 |\n|                 FEProblem::computeResidualTags                   |    58 |      0.010 |      0.000 |   0.00 |       0 |   3870.428 |     66.732 |  53.54 |       7 |\n|                   NonlinearSystemBase::nl::computeResidualTags   |    58 |      0.090 |      0.002 |   0.00 |       0 |   2032.153 |     35.037 |  28.11 |       7 |\n|                     NonlinearSystemBase::computeResidualInternal |    58 |     43.217 |      0.745 |   0.60 |       7 |   2009.771 |     34.651 |  27.80 |       7 |\n|                       NonlinearSystemBase::Kernels               |    58 |   1966.239 |     33.901 |  27.20 |       0 |   1966.239 |     33.901 |  27.20 |       0 |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nHeaviest Sections:\n-------------------------------------------------------------------------------------------------\n|                  Section                 | Calls |   Self(s)  |    Avg.    |    %   | Mem(MB) |\n-------------------------------------------------------------------------------------------------\n| NonlinearSystemBase::computeJacobianTags |    42 |   2303.232 |     54.839 |  31.86 |     169 |\n| NonlinearSystemBase::Kernels             |    59 |   2000.882 |     33.913 |  27.68 |    1017 |\n| AuxiliarySystem::computeElementalVars    |   105 |   1898.357 |     18.080 |  26.26 |       0 |\n| FEProblem::solve                         |     1 |    421.155 |    421.155 |   5.83 |    -859 |\n| Console::outputStep                      |   411 |    189.606 |      0.461 |   2.62 |    1473 |\n| Action::ExecuteMeshGenerators::act       |     1 |    102.167 |    102.167 |   1.41 |    1555 |\n| Action::MPCbe::MPCbe::act                |     1 |     49.306 |     49.306 |   0.68 |   14035 |\n-------------------------------------------------------------------------------------------------\n\n\nPerformance Graph:\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|                                 Section                                | Calls |   Self(s)  |   Avg(s)   |    %   | Mem(MB) |  Total(s)  |   Avg(s)   |    %   | Mem(MB) |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| traiTestApp (main)                                                     |     1 |      0.048 |      0.048 |   0.00 |       4 |   3430.557 |   3430.557 | 100.00 |   20837 |\n|   RankMap::construct                                                   |     1 |      0.000 |      0.000 |   0.00 |       1 |      0.000 |      0.000 |   0.00 |       1 |\n|   MooseApp::setup                                                      |     1 |      0.000 |      0.000 |   0.00 |       0 |    287.476 |    287.476 |   8.38 |   18926 |\n|     Action::SetupMeshAction::Mesh::SetupMeshAction::act::setup_mesh    |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     Action::SetupMeshAction::Mesh::SetupMeshAction::act::set_mesh_base |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|       MeshGeneratorMesh::init                                          |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     Action::SetupMeshCompleteAction::Mesh::completeSetupUndisplaced    |     2 |      0.000 |      0.000 |   0.00 |       0 |     11.118 |      5.559 |   0.32 |     109 |\n|       MeshGeneratorMesh::prepare                                       |     2 |      0.793 |      0.397 |   0.02 |       0 |     11.118 |      5.559 |   0.32 |     109 |\n|     Action::SetupMeshCompleteAction::Mesh::uniformRefine               |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     Action::SetupMeshCompleteAction::Mesh::deleteRemoteElems           |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     MooseApp::executeMeshGenerators                                    |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     FEProblem::init                                                    |     1 |      0.106 |      0.106 |   0.00 |     127 |     44.171 |     44.171 |   1.29 |    1080 |\n|       FEProblem::EquationSystems::Init                                 |     1 |     25.317 |     25.317 |   0.74 |     691 |     31.402 |     31.402 |   0.92 |     933 |\n|   MooseApp::execute                                                    |     1 |      0.000 |      0.000 |   0.00 |       0 |   3143.033 |   3143.033 |  91.62 |    1906 |\n|     FEProblem::initialSetup                                            |     1 |      0.001 |      0.001 |   0.00 |       0 |     63.526 |     63.526 |   1.85 |    1363 |\n|       FEProblem::projectSolution                                       |     1 |     13.898 |     13.898 |   0.41 |       0 |     13.898 |     13.898 |   0.41 |       0 |\n|       NonlinearSystemBase::nlInitialSetup                              |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.006 |      0.006 |   0.00 |       0 |\n|         NonlinearSystemBase::kernelsInitialSetup                       |     1 |      0.006 |      0.006 |   0.00 |       0 |      0.006 |      0.006 |   0.00 |       0 |\n|         NonlinearSystemBase::mortarSetup                               |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|       FEProblem::initialSetupTransfers                                 |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     FEProblem::outputStep                                              |     2 |      0.069 |      0.035 |   0.00 |       0 |      1.416 |      0.708 |   0.04 |       0 |\n|       Console::outputStep                                              |     3 |      1.278 |      0.426 |   0.04 |       0 |      1.278 |      0.426 |   0.04 |       0 |\n|       Exodus::outputStep                                               |     1 |      0.069 |      0.069 |   0.00 |       0 |      0.069 |      0.069 |   0.00 |       0 |\n|     Steady::PicardSolve                                                |     1 |      0.001 |      0.001 |   0.00 |       0 |   3054.898 |   3054.898 |  89.05 |     255 |\n|       FEProblem::outputStep                                            |     1 |      0.033 |      0.033 |   0.00 |       0 |      0.033 |      0.033 |   0.00 |       0 |\n|         Console::outputStep                                            |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|       FEProblem::solve                                                 |     1 |    844.542 |    844.542 |  24.62 |     247 |   3033.303 |   3033.303 |  88.42 |     255 |\n|         Console::outputStep                                            |   463 |     60.398 |      0.130 |   1.76 |       0 |     60.398 |      0.130 |   1.76 |       0 |\n|         FEProblem::computeResidualInternal                             |     1 |      0.000 |      0.000 |   0.00 |       0 |     25.251 |     25.251 |   0.74 |       0 |\n|         FEProblem::computeResidualInternal                             |    48 |      0.001 |      0.000 |   0.00 |       0 |   1188.957 |     24.770 |  34.66 |       0 |\n|         NonlinearSystemBase::initialBCs                                |     1 |      0.122 |      0.122 |   0.00 |       0 |      0.122 |      0.122 |   0.00 |       0 |\n|         FEProblem::computeJacobianInternal                             |    45 |      0.001 |      0.000 |   0.00 |       0 |    914.032 |     20.312 |  26.64 |       8 |\n|       FEProblem::computeUserObjects                                    |     2 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|     Steady::final                                                      |     1 |      0.000 |      0.000 |   0.00 |       0 |     23.169 |     23.169 |   0.68 |     288 |\n|       FEProblem::outputStep                                            |     1 |      0.041 |      0.041 |   0.00 |       0 |     23.169 |     23.169 |   0.68 |     288 |\n|         Console::outputStep                                            |     1 |      0.000 |      0.000 |   0.00 |       0 |      0.000 |      0.000 |   0.00 |       0 |\n|         Exodus::outputStep                                             |     1 |     23.128 |     23.128 |   0.67 |     288 |     23.128 |     23.128 |   0.67 |     288 |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nHeaviest Branch:\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|                              Section                             | Calls |   Self(s)  |   Avg(s)   |    %   | Mem(MB) |  Total(s)  |   Avg(s)   |    %   | Mem(MB) |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n| traiTestApp (main)                                               |     1 |      0.048 |      0.048 |   0.00 |       4 |   3430.557 |   3430.557 | 100.00 |   20837 |\n|   MooseApp::run                                                  |     1 |      0.000 |      0.000 |   0.00 |       0 |   3430.509 |   3430.509 | 100.00 |   20832 |\n|     MooseApp::execute                                            |     1 |      0.000 |      0.000 |   0.00 |       0 |   3143.034 |   3143.034 |  91.62 |    1906 |\n|       MooseApp::executeExecutioner                               |     1 |      0.023 |      0.023 |   0.00 |       0 |   3143.034 |   3143.034 |  91.62 |    1906 |\n|         Steady::PicardSolve                                      |     1 |      0.001 |      0.001 |   0.00 |       0 |   3054.898 |   3054.898 |  89.05 |     255 |\n|           FEProblem::solve                                       |     1 |    844.542 |    844.542 |  24.62 |     247 |   3033.303 |   3033.303 |  88.42 |     255 |\n|             FEProblem::computeResidualSys                        |    48 |      0.001 |      0.000 |   0.00 |       0 |   1188.958 |     24.770 |  34.66 |       0 |\n|               FEProblem::computeResidualInternal                 |    48 |      0.001 |      0.000 |   0.00 |       0 |   1188.957 |     24.770 |  34.66 |       0 |\n|                 FEProblem::computeResidualTags                   |    48 |      0.010 |      0.000 |   0.00 |       0 |   1188.956 |     24.770 |  34.66 |       0 |\n|                   NonlinearSystemBase::nl::computeResidualTags   |    48 |      0.600 |      0.013 |   0.02 |       0 |    641.161 |     13.358 |  18.69 |       0 |\n|                     NonlinearSystemBase::computeResidualInternal |    48 |     34.229 |      0.713 |   1.00 |       0 |    635.008 |     13.229 |  18.51 |       0 |\n|                       NonlinearSystemBase::Kernels               |    48 |    600.470 |     12.510 |  17.50 |       0 |    600.470 |     12.510 |  17.50 |       0 |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nHeaviest Sections:\n-------------------------------------------------------------------------------------------------\n|                  Section                 | Calls |   Self(s)  |    Avg.    |    %   | Mem(MB) |\n-------------------------------------------------------------------------------------------------\n| NonlinearSystemBase::computeJacobianTags |    45 |    913.823 |     20.307 |  26.64 |       8 |\n| FEProblem::solve                         |     1 |    844.542 |    844.542 |  24.62 |     247 |\n| NonlinearSystemBase::Kernels             |    49 |    613.280 |     12.516 |  17.88 |       0 |\n| AuxiliarySystem::computeElementalVars    |    99 |    580.843 |      5.867 |  16.93 |       0 |\n| Action::ExecuteMeshGenerators::act       |     1 |    135.038 |    135.038 |   3.94 |    1582 |\n| Console::outputStep                      |   468 |     61.676 |      0.132 |   1.80 |       0 |\n| Action::MPCbe::MPCbe::act                |     1 |     52.237 |     52.237 |   1.52 |   14036 |\n-------------------------------------------------------------------------------------------------",
                          "url": "https://github.com/idaholab/moose/discussions/20822#discussioncomment-2608411",
                          "updatedAt": "2022-06-12T18:59:54Z",
                          "publishedAt": "2022-04-21T12:07:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so summarizing this:\n\n\n\n\n\n\n<style>\n\n</style>\n\n\n\n\n\n\n\u00a0\navg\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\n\n\n\nSection\n3 MPI 1 thread\n3 MPI 4 threads\n1 MPI 1 thread\nSpeedup - MPI\nSpeedup - threads\n\n\ncomputeJacobinaTags\n20.307\n6.591\n54.839\n2.700497365\n2.70049737\n\n\nkernels\n12.516\n3.454\n33.913\n2.709571748\n2.70957175\n\n\ncomputeElementalVars\n5.867\n1.81\n18.08\n3.081643088\n3.08164309\n\n\nsolve\n844.542\n346\n421.155\n0.498678574\n0.49867857\n\n\noutputStep\n0.132\n0.134\n0.461\n3.492424242\n3.49242424\n\n\nmeshGen\n135.038\n134.774\n102.167\n0.756579629\n0.75657963\n\n\nMPCbe\n52.237\n52.677\n49.306\n0.943890346\n0.94389035\n\n\n\n\n\nthe solve isnt scaling like it should.\nWhat s your Executioner block like?",
                          "url": "https://github.com/idaholab/moose/discussions/20822#discussioncomment-2610063",
                          "updatedAt": "2022-06-12T19:00:04Z",
                          "publishedAt": "2022-04-21T16:18:22Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "mesh generation and constraint action dont scale at all because they are replicated right now. So that is expected.\nScaling is pretty good to 3 otherwise.\nLet s not get too deep / spend too much time in studying scalability though if we want to improve this by going distributed.",
                          "url": "https://github.com/idaholab/moose/discussions/20822#discussioncomment-2610088",
                          "updatedAt": "2022-06-12T19:00:05Z",
                          "publishedAt": "2022-04-21T16:21:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "the solve isnt scaling like it should. What s your Executioner block like?\n\n[Executioner]\n  type = Steady\n  solve_type = NEWTON\n  petsc_options = '-snes_converged_reason'\n  petsc_options_iname = '-pc_type -pc_hypre_type'\n  petsc_options_value = 'hypre    boomeramg'\n\n  nl_rel_tol = 1e-15\n  nl_abs_tol = 1.65e3\n  l_tol = 1e-06\n  l_max_its = 150\n  nl_max_its = 50\n[]\n\nit's very specific to this particular case, I cannot relax it otherwise the results won't match Abaqus, so basically most of my simulations I run it until it fails then I use this block to extra the results just before it fails\n    [./run_abaqus_compare_step9_v1_new]\n      file_base = GNKT_short_9steps/step_1\n      type = Exodus\n      execute_on = 'final'\n    [../]\n\n\nLet s not get too deep / spend too much time in studying scalability though if we want to improve this by going distributed.\n\nI think so @GiudGiud\nRegards,\nTraiwit",
                          "url": "https://github.com/idaholab/moose/discussions/20822#discussioncomment-2612506",
                          "updatedAt": "2022-06-12T19:00:06Z",
                          "publishedAt": "2022-04-22T01:50:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Well multigrid methods should scale, so something isnt going right.\nInside the solve method you have these timed sections:\n|       FEProblem::solve                                                 |     1 |    844.542 |    844.542 |  24.62 |     247 |   3033.303 |   3033.303 |  88.42 |     255 |\n|         Console::outputStep                                            |   463 |     60.398 |      0.130 |   1.76 |       0 |     60.398 |      0.130 |   1.76 |       0 |\n|         FEProblem::computeResidualInternal                             |     1 |      0.000 |      0.000 |   0.00 |       0 |     25.251 |     25.251 |   0.74 |       0 |\n|         FEProblem::computeResidualInternal                             |    48 |      0.001 |      0.000 |   0.00 |       0 |   1188.957 |     24.770 |  34.66 |       0 |\n|         NonlinearSystemBase::initialBCs                                |     1 |      0.122 |      0.122 |   0.00 |       0 |      0.122 |      0.122 |   0.00 |       0 |\n|         FEProblem::computeJacobianInternal                             |    45 |      0.001 |      0.000 |   0.00 |       0 |    914.032 |     20.312 |  26.64 |       8 |\n|\n\ndo you mind doing what I did to see which ones dont scale?\nGood news is my PR for reworking the ElementPropertyReadFile got in (in next, devel maybe by tomorrow), so we can rework it again now.\nThis object should actually work in parallel. As long as you pass allow_renumbering = false to the [Mesh] block.\nCould you please try that?",
                          "url": "https://github.com/idaholab/moose/discussions/20822#discussioncomment-2616996",
                          "updatedAt": "2022-06-12T19:00:21Z",
                          "publishedAt": "2022-04-22T16:33:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "Sorry for the late reply @GiudGiud it was a public holiday in Australia yesterday.\nFirst of all regarding PropertyReadFile with distributed mesh, when I set allow_renumbering = false I got this error\nThe following error occurred in the object \"poro_read\", of type \"PropertyReadFile\".\n\nElement ID 4598847 greater than than total number of element in mesh: 4598664. Elements should be numbered consecutively.\n\nbut when I have allow_renumbering = true I got this warning, but it still let me go into solving stage:\n*** Warning ***\nThe following warning occurred in the object \"perm_read\", of type \"PropertyReadFile\".\n\nCSV data is sorted by element, but mesh element renumbering is on, be careful!\n\nHowever, the properties map is defs incorrect\n\nit should look like this",
                          "url": "https://github.com/idaholab/moose/discussions/20822#discussioncomment-2635873",
                          "updatedAt": "2022-06-12T19:00:24Z",
                          "publishedAt": "2022-04-26T05:03:13Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Developement of Solid Beam element formulation",
          "author": {
            "login": "abdullah2146"
          },
          "bodyText": "Hi,\nI want to develop the three-dimensional Solid beam formulation which is developed in this paper.\nhttps://www.sciencedirect.com/science/article/pii/S0045782513001618\nHas the MOOSE already developed this formulation? If not, can i develop this formulation on MOOSE?\nThis formulation have different matrics which need to define like\n\nCan you please answer this question?\nThanks,",
          "url": "https://github.com/idaholab/moose/discussions/21049",
          "updatedAt": "2022-06-09T01:44:41Z",
          "publishedAt": "2022-05-18T19:56:07Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@jiangwen84 @dschwen",
                  "url": "https://github.com/idaholab/moose/discussions/21049#discussioncomment-2778966",
                  "updatedAt": "2022-05-18T20:13:35Z",
                  "publishedAt": "2022-05-18T20:13:34Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "recuero"
                  },
                  "bodyText": "No, MOOSE doesn't have a solid beam formulation. The existing beam formulation makes use of the typical centerline plus cross section rotation approach (i.e. Timoshenko) with torsion (see https://mooseframework.inl.gov/modules/tensor_mechanics/beam_vandv.html).",
                  "url": "https://github.com/idaholab/moose/discussions/21049#discussioncomment-2785660",
                  "updatedAt": "2022-05-19T18:18:38Z",
                  "publishedAt": "2022-05-19T18:18:38Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "abdullah2146"
                          },
                          "bodyText": "Thanks for your reply. As it's not developed on MOOSE, Can I develop this Solid beam formulation on my own?",
                          "url": "https://github.com/idaholab/moose/discussions/21049#discussioncomment-2834993",
                          "updatedAt": "2022-05-27T15:09:00Z",
                          "publishedAt": "2022-05-27T15:08:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Certainly.\nAnd contributions are always welcome if this goes well.",
                          "url": "https://github.com/idaholab/moose/discussions/21049#discussioncomment-2838821",
                          "updatedAt": "2022-05-28T01:23:46Z",
                          "publishedAt": "2022-05-28T01:23:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abdullah2146"
                          },
                          "bodyText": "@GiudGiud Can you share any C++ files or guidance how we can develop formulation in MOOSE?",
                          "url": "https://github.com/idaholab/moose/discussions/21049#discussioncomment-2844511",
                          "updatedAt": "2022-05-29T18:18:07Z",
                          "publishedAt": "2022-05-29T18:18:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "If you are a beginner in MOOSE you should follow the tutorials here first\nhttps://mooseframework.inl.gov/index.html\nIf not you should consult the tensor mechanics documentation to see how various formulations were implemented\nhttps://mooseframework.inl.gov/modules/tensor_mechanics/index.html",
                          "url": "https://github.com/idaholab/moose/discussions/21049#discussioncomment-2846225",
                          "updatedAt": "2022-05-30T04:40:16Z",
                          "publishedAt": "2022-05-30T04:40:16Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "[Constraints] Feeding multiple secondary nodes to LinearNodalConstraint action",
          "author": {
            "login": "Traiwit"
          },
          "bodyText": "Hi guys,\nso LinearNodalConstraint typically you can add multiple secondary_node_ids for a single pirmary_node, for example\n  [./pair]\n    type = LinearNodalConstraint\n    variable = porepressure\n    primary_node_ids = 3\n    secondary_node_ids = '5 6 7'\n    weights = 1\n  [../]\n\nI have created an action, so that I can feed multiple Constraint pairs at once (I have more than 500k pairs, so it's better to do it this way)\nHowever my action input can only feed 1 to 1 node pair, so for the example above, it looks like this:\n[MPCbe]\n    primary_node_ids = ' 3 3 3'\n    secondary_node_ids  = '5 6 7'\n[]\n\njust wondering how do I make it such that 1 primary node can take multiple secondary nodes like the original version.\nBelow is my action script, it basically wrap LinearNodalConstraint with a for-loop\nregisterMooseAction(\"traiApp\", MPCbe, \"add_constraint\");\n\nInputParameters\nMPCbe::validParams()\n{\n  InputParameters params = Action::validParams();\n  params.addParam<std::vector<unsigned int>>(\"primary_node_ids\", \"The primary node IDs.\");\n  params.addParam<std::vector<unsigned int>>(\"secondary_node_ids\",\n                                           \"The list of secondary node ids\");\n\n  return params;\n}\nMPCbe::MPCbe(const InputParameters & params)\n  : Action(params),\nprimary_node_ids(getParam<std::vector<unsigned int>>(\"primary_node_ids\")),\nsecondary_node_ids(getParam<std::vector<unsigned int>>(\"secondary_node_ids\"))\n\n{\n}\nvoid\nMPCbe::act()\n\n{\nfor (unsigned cur_num = 0; cur_num<primary_node_ids.size() ; cur_num++)\n{\n\n  std::vector<Real> weights_in (1);\n  std::vector<unsigned int> primary_node_ids_in (1);\n  std::vector<unsigned int> secondary_node_ids_in (1);\n\n  weights_in.at(0) = 1;\n  primary_node_ids_in.at(0) = primary_node_ids[cur_num];\n  secondary_node_ids_in.at(0) = secondary_node_ids[cur_num];\n\n  InputParameters params = _factory.getValidParams(\"LinearNodalConstraint\");\n  params.set<NonlinearVariableName>(\"variable\") = \"porepressure\";\n  params.set<std::vector<Real>>(\"weights\")= weights_in;\n  params.set<std::vector<unsigned int>>(\"primary\") = primary_node_ids_in;\n  params.set<std::vector<unsigned int>>(\"secondary_node_ids\") = secondary_node_ids_in;\n  params.set<Real>(\"penalty\") = 1e10;\n  _problem->addConstraint(\"LinearNodalConstraint\", \"MPCbe\" + Moose::stringify(cur_num), params);\n\n}\n}\n\nI tried secondary_node_ids  = ' \"241 244\" 240 242' or secondary_node_ids  = ' [241 244] 240 242', and none of them works.\ncannot convert '\"241' to int \nmy C++ is very very basic too :(\nPlease help if you have any idea\n@GiudGiud I tag you since you know about general stuff the most here.\nKind regards,\nTraiwit",
          "url": "https://github.com/idaholab/moose/discussions/20972",
          "updatedAt": "2022-07-05T06:37:24Z",
          "publishedAt": "2022-05-06T06:16:25Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "Traiwit"
                  },
                  "bodyText": "Hi guys,\nI think instead of directly inputting vectors of nodes into the action, I think it might be easier to feed it via CSV, where column A contains only the primary node and column B contains the list of secondary nodes.\nis there any example of this? Thank you!",
                  "url": "https://github.com/idaholab/moose/discussions/20972#discussioncomment-2711060",
                  "updatedAt": "2022-07-05T06:42:27Z",
                  "publishedAt": "2022-05-09T01:16:14Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "there's examples of CSV reading yes.\nThe CSVReader postprocessor is one, the PropertyReadFile UO is another",
                          "url": "https://github.com/idaholab/moose/discussions/20972#discussioncomment-2718049",
                          "updatedAt": "2022-07-05T06:42:28Z",
                          "publishedAt": "2022-05-09T22:18:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "Hi @GiudGiud, how do I feed data from CSVReader into my action?\nsays I want primary_node_ids to take the first column, then secondary_node_ids takes all the numbers from the second to n columns.\n(please refer to my action code above).\nthis is an example of my CSV:\n\nAlso, I don't think PropertyReadFile works since it has to be assigned as properties (tied to element/nodes/block)\nKind regards,\nTraiwit",
                          "url": "https://github.com/idaholab/moose/discussions/20972#discussioncomment-2718628",
                          "updatedAt": "2022-07-05T06:42:28Z",
                          "publishedAt": "2022-05-10T01:06:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "yeah PropertyReadFile isnt structured the right way.\nActually the CSVReader is an example of using the DelimitedFileReader class in the code. It's not meant to be used directly.\nsome docs\nhttps://mooseframework.inl.gov/source/utils/MooseUtils.html",
                          "url": "https://github.com/idaholab/moose/discussions/20972#discussioncomment-2719207",
                          "updatedAt": "2022-07-05T06:42:41Z",
                          "publishedAt": "2022-05-10T03:38:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "any luck with calling the DelimitedFileReader ?",
                          "url": "https://github.com/idaholab/moose/discussions/20972#discussioncomment-2755451",
                          "updatedAt": "2022-07-05T20:11:04Z",
                          "publishedAt": "2022-05-15T21:21:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "Hi @GiudGiud yeh took some time to get it to work properly\nit roughly looks like this (not finished yet)\nMPCbe2::act()\n\n{\n\n\n  MooseUtils::DelimitedFileReader csv_reader(getParam<FileName>(\"csv_file\"), &_communicator);\n  if (isParamValid(\"header\"))\n  csv_reader.setHeaderFlag(getParam<bool>(\"header\")\n                               ? MooseUtils::DelimitedFileReader::HeaderFlag::ON\n                               : MooseUtils::DelimitedFileReader::HeaderFlag::OFF);\n\n  csv_reader.read();\n  const std::vector<std::string> & names = csv_reader.getNames();\n    const std::vector<std::vector<double>> & data = csv_reader.getData();\n\n    int column = (sizeof(data)*data.size())+data.size();\n\n // for (unsigned cur_num = 0; cur_num<secondary_node_ids.size() ; cur_num++)\n for (unsigned cur_num = 0; cur_num<column ; cur_num++)\n{\n\n  std::vector<Real> weights_in (1);\n  std::vector<unsigned int> primary_node_ids_in (1);\n  std::vector<unsigned int> secondary_node_ids_in_1 (1);\n  std::vector<unsigned int> secondary_node_ids_in_2 (1);\n\n\n  weights_in.at(0) = 1;\n  primary_node_ids_in.at(0) = data[0][cur_num];\n  secondary_node_ids_in_1.at(0) =  data[1][cur_num];\n\nbut funny that it seems like my CVS got transposed, as you can see from the CSV input above it should loop at the row, but I got the correct solution when I loop the column instead.\n  primary_node_ids_in.at(0) = data[0][cur_num];\n  secondary_node_ids_in_1.at(0) =  data[1][cur_num];\n\nOne super quick question though, is there a way to print an integer (for example number of column/row) during the run of the main input file?\nfor example I want to print  int column = (sizeof(data)*data.size())+data.size();, but I don't think I can just simply have printf in my source code.\nAnyways, I'm almost there. Thank you very much!\nKind regards,\nTraiwit",
                          "url": "https://github.com/idaholab/moose/discussions/20972#discussioncomment-2755969",
                          "updatedAt": "2022-07-05T20:11:04Z",
                          "publishedAt": "2022-05-16T00:45:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nSo the way to print in moose is like this:\n_console << \"message\" << std::endl;\nThe header for that routine you are using says:\n  /**\n   * Return the rows/columns of data.\n   *\n   * The outer vector is column and the inner the rows.\n   */\n  const std::vector<std::vector<double>> & getData() const;\n\nso indeed this seems flipped.",
                          "url": "https://github.com/idaholab/moose/discussions/20972#discussioncomment-2760274",
                          "updatedAt": "2022-07-05T20:11:04Z",
                          "publishedAt": "2022-05-16T14:33:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "@GiudGiud a stupid question, so what do I do to make it not flip?",
                          "url": "https://github.com/idaholab/moose/discussions/20972#discussioncomment-2763628",
                          "updatedAt": "2022-07-05T20:11:05Z",
                          "publishedAt": "2022-05-17T00:02:45Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "@GiudGiud I just noticed that I cannot compile with\n    for (std::size_t i = 0; i < data.size(); ++i)\n    {\n      _column_data[names[i]] = &declareVector(names[i]);\n      _column_data[names[i]]->assign(data[i].begin(), data[i].end());\n    }\n\nsimilar to what in CSVReader source code\ni got this error\nerror: 'declareVector' was not declared in this scope\n   92 |       _column_data[names[i]] = &declareVector(names[i]);\n      |                                 ^~~~~~~~~~~~~\n/home/moose/projects/trai/src/kernels/MPCbe2.C:93:31: error: 'class MPCbe2' has no member named 'assign'\n   93 |       _column_data[names[i]]->assign(data[i].begin(), data[i].end());\n\nKind regards,\nTraiwit",
                          "url": "https://github.com/idaholab/moose/discussions/20972#discussioncomment-2764567",
                          "updatedAt": "2022-07-05T20:11:05Z",
                          "publishedAt": "2022-05-17T04:40:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "okay guys, so this is the current version of the code\nApparently, the  FEProblem::computeJacobianInternal takes significantly longer than the original version of the code, however, the RAM usage is 1/5 of the previous version. not sure what is the issue here\nso the new version, the number of loops (number of actions) is 1/5 smaller than before but the number of secondary node id for each loop increases [as explained in the original post at the top].\nregisterMooseAction(\"traiApp\", MPCbe2, \"add_constraint\");\n\nInputParameters\nMPCbe2::validParams()\n{\n  InputParameters params = Action::validParams();\n  params.addRequiredParam<FileName>(\"csv_file\",\n                                    \"The name of the CSV file to read. Currently, with \"\n                                    \"the exception of the header row, only numeric \"\n                                    \"values are supported.\");\nparams.addParam<bool>(\"header\",\n                              \"When true it is assumed that the first row contains column headers, these \"\n                              \"headers are used as the VectorPostprocessor vector names. If false the \"\n                              \"file is assumed to contain only numbers and the vectors are named \"\n                             \"automatically based on the column number (e.g., 'column_0000', \"\n                              \"'column_0001'). If not supplied the reader attempts to auto detect the \"\n                              \"headers.\");\n\n  return params;\n}\n\nMPCbe2::MPCbe2(const InputParameters & params)\n  : Action(params)\n{\n}\nvoid\nMPCbe2::act()\n\n{\n  MooseUtils::DelimitedFileReader csv_reader(getParam<FileName>(\"csv_file\"), &_communicator);\n  if (isParamValid(\"header\"))\n  csv_reader.setHeaderFlag(getParam<bool>(\"header\")\n                               ? MooseUtils::DelimitedFileReader::HeaderFlag::ON\n                               : MooseUtils::DelimitedFileReader::HeaderFlag::OFF);\n\n  csv_reader.read();\n    const std::vector<std::string> & names = csv_reader.getNames();\n    const std::vector<std::vector<double>> & data = csv_reader.getData();\n\n\n    int column =  119178; // number of row\n    int row = data.size();\n\n    _console << \"number of row   \" << column << std::endl;\n\n    _console << \"number of column   \" <<  row << std::endl;\n\n for (unsigned cur_num = 0; cur_num<column ; cur_num++)\n{\n\n  std::vector<Real> weights_in (1);\n  std::vector<unsigned int> primary_node_ids_in (1);\n  std::vector<unsigned int> secondary_node_ids_in_1 (1);\n  std::vector<unsigned int> secondary_node_ids_in_2 (1);\n\n\n  weights_in.at(0) = 1;\n  primary_node_ids_in.at(0) = data[0][cur_num];\n  secondary_node_ids_in_1.at(0) =  data[1][cur_num];\n  std::vector<unsigned int> secondary_node_ids_in_final(secondary_node_ids_in_1);\n\n  for (unsigned n = 2; n<row ; n++)\n  {\n\n  secondary_node_ids_in_2.at(0) =  data[n][cur_num];\n\n  secondary_node_ids_in_final.insert(secondary_node_ids_in_final.end(), secondary_node_ids_in_2.begin(), secondary_node_ids_in_2.end());\n  }\n\n  sort( secondary_node_ids_in_final.begin(), secondary_node_ids_in_final.end() );\n  secondary_node_ids_in_final.erase( unique( secondary_node_ids_in_final.begin(), secondary_node_ids_in_final.end() ), secondary_node_ids_in_final.end() );\n\n  InputParameters params = _factory.getValidParams(\"LinearNodalConstraint\");\n  params.set<NonlinearVariableName>(\"variable\") = \"porepressure\";\n  params.set<std::vector<Real>>(\"weights\")= weights_in;\n  params.set<std::vector<unsigned int>>(\"primary\") = primary_node_ids_in;\n  params.set<std::vector<unsigned int>>(\"secondary_node_ids\") = secondary_node_ids_in_final;\n  params.set<Real>(\"penalty\") = 1e5;\n  _problem->addConstraint(\"LinearNodalConstraint\", \"MPCbe2\" + Moose::stringify(cur_num), params);\n\n}\n\n\n_console << \"MPC done!\" << std::endl;\n}",
                          "url": "https://github.com/idaholab/moose/discussions/20972#discussioncomment-2765043",
                          "updatedAt": "2022-07-05T20:11:05Z",
                          "publishedAt": "2022-05-17T06:38:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\n\n\nnot sure. I d have to check the text in the docstring isnt wrong first\n\n\ndeclareVector is a vector postprocessor routine, using by the CSVReader which is a VPP. It does a few things like make sure the base class keeps track of what to output, or that the problem knows about the name of the vector, so it can be retrieved from elsewhere.\nFor you I suspect you only need storage right? So you just want a standard allocation? Or are you trying to make a VPP?\nIn the case that you are not making a VPP, something like this works\n\n\n    for (std::size_t i = 0; i < data.size(); ++i)\n    {\n      _column_data[names[i]].resize(data[i].size());\n      _column_data[names[i]] = data[i];\n    }\n\nif _column_data is a map or something that will properly allocate when you do _column_data[names[i]] (insert in a map should be preferred)\nand if data is a vector, so that the copy can just be done with =.\n\nSo the change that slowed down the code is going from:\nmany constraints with only 2 nodes specified\nless constraints with groups of nodes specified\nright?\n\nAre they strictly equivalent wrt physics? Or do you end up specifying more inter-node constraints in the second case?\nPS: You should add more comments to your code. It helps us understand what you are doing and it ll help you too in a few months when you look back.",
                          "url": "https://github.com/idaholab/moose/discussions/20972#discussioncomment-2768460",
                          "updatedAt": "2022-07-05T20:11:09Z",
                          "publishedAt": "2022-05-17T14:56:14Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Creating an application on moose",
          "author": {
            "login": "Kenziekaia"
          },
          "bodyText": "When trying to create an an application on MOOSE it can not run jobs and submit output files. When performing ./Bash_CSV.sh and then checking what the errors are via VIM log_slurm.o<job number>  however all previous tests have passed and worked. Every time I try to submit these jobs I get an error message that saying there is a segmentation fault and the user direction have been aborted.\n`Loading petsc/3.12.5\nLoading requirement: openmpi/gcc8/4.0.5 hypre/2.19.0 python37\nPrimary job  terminated normally, but 1 process returned\na non-zero exit code. Per user-direction, the job has been aborted.\n\nmpiexec noticed that process rank 0 with PID 139056 on node cpu102 exited on signal 11 (Segmentation fault).`\nIf anyone has any suggestions it would be greatly appreciated. Thank you",
          "url": "https://github.com/idaholab/moose/discussions/21164",
          "updatedAt": "2022-06-09T01:41:34Z",
          "publishedAt": "2022-05-27T15:23:09Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Please review the posting guidelines. There isnt nearly enough information to help you here.\nAlso your petsc is really old.",
                  "url": "https://github.com/idaholab/moose/discussions/21164#discussioncomment-2835159",
                  "updatedAt": "2022-05-27T15:34:40Z",
                  "publishedAt": "2022-05-27T15:34:36Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Kenziekaia"
                          },
                          "bodyText": "Ok, I updated my discussion post with more info. Do I need to update my petsc?\nThanks,\nKenzie",
                          "url": "https://github.com/idaholab/moose/discussions/21164#discussioncomment-2835384",
                          "updatedAt": "2022-05-27T16:10:48Z",
                          "publishedAt": "2022-05-27T16:10:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I think that's one thing to try.\nThere isnt that much more information in your post.\nWhat installation method did you follow? What compilers are you using?\nIs this running well on interactive jobs/locally and crashing on submissions to the queue?",
                          "url": "https://github.com/idaholab/moose/discussions/21164#discussioncomment-2835478",
                          "updatedAt": "2022-05-27T16:28:26Z",
                          "publishedAt": "2022-05-27T16:28:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Kenziekaia"
                          },
                          "bodyText": "Ok thank you! I am sorry for the confusion/ lack of information it is my first time using MOOSE so I am trying to figuring out all of the terminology.\nThe installation method I used: https://docs.google.com/document/d/1Hx5T7jtIdFMsvupkmKVPF4dMJ-s6DbYqh7XuOm4gsL0/edit?usp=sharing\nHow I am making my application: https://docs.google.com/document/d/1Hx5T7jtIdFMsvupkmKVPF4dMJ-s6DbYqh7XuOm4gsL0/edit?usp=sharing\nI have been compiling using libmesh and PETSC\nI believe the job is running well locally due to the job number being created but doesn't submit to the queue. But I am not sure.",
                          "url": "https://github.com/idaholab/moose/discussions/21164#discussioncomment-2835631",
                          "updatedAt": "2022-05-27T16:59:11Z",
                          "publishedAt": "2022-05-27T16:59:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "But running well locally I mean does it pass the tests? If you run ./run_tests in framework/test",
                          "url": "https://github.com/idaholab/moose/discussions/21164#discussioncomment-2835731",
                          "updatedAt": "2022-05-27T17:17:56Z",
                          "publishedAt": "2022-05-27T17:17:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Kenziekaia"
                          },
                          "bodyText": "I will try that after I finished updating PETSC and let you know",
                          "url": "https://github.com/idaholab/moose/discussions/21164#discussioncomment-2835746",
                          "updatedAt": "2022-05-27T17:19:48Z",
                          "publishedAt": "2022-05-27T17:19:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nSo this looks fine but it doesnt explain why you have such an old petsc. My guess is that your environment is contaminated by other modules you are loading knowingly or not.\nWhat does module list return?\nYou have shared the same document twice btw\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/21164#discussioncomment-2835793",
                          "updatedAt": "2022-05-27T17:27:30Z",
                          "publishedAt": "2022-05-27T17:27:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Kenziekaia"
                          },
                          "bodyText": "Yeah I am not sure why I have such an old PETSC as we are using what is provided on BSU's borah. Here is the second document I meant to share which is how I am attempting to make an application: https://docs.google.com/document/d/1fuLiyAIhguw5ypJlsOeOStZ3S5PVF6z0WeVHaKWivPk/edit?usp=sharing\nThanks,\nKenzie",
                          "url": "https://github.com/idaholab/moose/discussions/21164#discussioncomment-2835854",
                          "updatedAt": "2022-05-27T17:40:11Z",
                          "publishedAt": "2022-05-27T17:40:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "you need to remove that petsc from the environment.\nwhat does module list return?",
                          "url": "https://github.com/idaholab/moose/discussions/21164#discussioncomment-2836124",
                          "updatedAt": "2022-05-27T18:34:03Z",
                          "publishedAt": "2022-05-27T18:34:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Kenziekaia"
                          },
                          "bodyText": "How could I remove the PETSC from the environment.\nThe module list command in my application returns:\n`Currently Loaded Modulefiles:\n\ngcc/9.2.0   2) slurm/slurm/19.05.8   3) gcc/8.2.0   4) openmpi/gcc8/4.1.2   5) cmake/gcc8/3.18.0   6) valgrind/3.16.1   7) moose-dev-gcc  `\n\nThe ./run_tests application passed and resulted in\n`test:kernels/simple_diffusion.test ........................................................................ OK\nrun_cr0.05_co0.05_T853/test:kernels/simple_diffusion.test ................................................. OK\nrun_cr0.05_co0.1_T853/test:kernels/simple_diffusion.test .................................................. OK\nRan 3 tests in 1.6 seconds. Average test time 0.5 seconds, maximum test time 0.5 seconds.\n3 passed, 0 skipped, 0 pending, 0 failed`",
                          "url": "https://github.com/idaholab/moose/discussions/21164#discussioncomment-2836847",
                          "updatedAt": "2022-05-27T20:59:04Z",
                          "publishedAt": "2022-05-27T20:59:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "you have 2 gcc loaded, that is an issue as well. Use module unload to remove one of them.\nPETSC did not come from the modules it seems. It must be in your PATH or some other environment variable",
                          "url": "https://github.com/idaholab/moose/discussions/21164#discussioncomment-2838282",
                          "updatedAt": "2022-05-27T22:13:04Z",
                          "publishedAt": "2022-05-27T22:13:04Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Why can't I change the grid number?",
          "author": {
            "login": "abc-hy"
          },
          "bodyText": "Hi everyone,\nI don't know why I cannot change the grid number even though I change the value of n\ndf_0dot7_R3_30.csv\nx in the GeneratedMesh block. Why is this?\nThe number of points can only be 1001, but I actually want to refine the mesh size. How to make the nx become larger?\nCan anyone give me some advice?\nThank you!\nThe following is my input file:\n[Mesh]\n  type = GeneratedMesh\n  dim = 1\n  nx = 4004\n  xmax = 6\n  allow_renumbering = false\n  # uniform_refine = 3\n\n[]\n\n[Variables]\n  [./c]\n  [../]\n  [./w]\n  [../]\n[]\n# aux varaibles to track the free energy change (must decrease with time)\n[AuxVariables]\n  [./total_F]\n    order = CONSTANT\n    family = MONOMIAL\n  [../]\n  # the chemical potential gradients\n[]\n\n[ICs]\n  [./IC_c]\n    x1=0\n    y1=0\n    x2=3\n    y2=0\n    inside = 1\n    outside = -1 # Matrix is supersaturated with solute atoms\n    variable = c\n    type = BoundingBoxIC\n  [../]\n[]\n\n\n# [BCs]\n#   [./Periodic]\n#     [./cx]\n#       variable = c\n#       auto_direction = 'x'\n#     [../]\n#     [./wx]\n#       variable = w\n#       auto_direction = 'x'\n#     [../]\n#   [../]\n#\n# []\n\n\n\n[Kernels]\n  # Split form of Cahn-Hilliard equation with eta as coupled variable\n  # w is the chemical potential\n  [./c_dot]\n    type = CoupledTimeDerivative\n    variable = w\n    v = c\n  [../]\n  [./c_res]\n    type = SplitCHParsed\n    variable = c\n    f_name = F\n    kappa_name = kappa_c\n    w = w\n  [../]\n\n  [./w_res]\n    type = SplitCHWRes\n    args = 'c'\n    variable = w\n    mob_name = M\n  [../]\n\n[]\n\n[AuxKernels]\n  [./total_F]\n    type = TotalFreeEnergy\n    variable = total_F\n    interfacial_vars = c\n    kappa_names = kappa_c\n  [../]\n[]\n\n\n\n\n[Materials]\n\n  [./mobility]\n    type = DerivativeParsedMaterial\n    f_name = M\n    args = 'c'\n    function = 1\n    derivative_order = 1\n  [../]\n\n  [./kappa_c]\n    type = GenericConstantMaterial\n    prop_names  = 'kappa_c'\n    prop_values = '8.0e-4'\n  [../]\n\n  [./free_energy]\n    # equivalent to `MathFreeEnergy`\n    type = DerivativeParsedMaterial\n    f_name = F\n    args = 'c'\n    function = 2.1437*c^2*(1.3469-3.67346*c^2+2*c^4)+0.7\n  [../]\n\n\n\n[]\n\n\n\n[Postprocessors]\n\n  [./ElementInt_c]\n    type = ElementIntegralVariablePostprocessor\n    variable = c\n  [../]\n\n  [./total_F]\n    type = ElementIntegralVariablePostprocessor\n    variable = total_F\n  [../]\n[]\n\n# [VectorPostprocessors]\n#  # The numerical values of the variables/auxvariables across the centerline\n#  [./line_values]\n#    type =  LineValueSampler\n#    start_point = '0 0 0'\n#    end_point = '6 0 0'\n#    variable = 'c'\n#    num_points = 361\n#    sort_by = x\n#    #execute_on = 'FINAL'\n#  [../]\n# []\n\n\n ##[VectorPostprocessors]\n  # The numerical values of the variables/auxvariables across the centerline\n  ##[./line_values]\n   ##type =  LineValueSampler\n    ##start_point = '0'\n    #end_point = '9'\n    #variable = 'c w j_tot'\n    #num_points = 199\n    #sort_by =  id\n    #execute_on = 'TIMESTEP_END'\n  #[../]\n#[]\n\n\n\n[Preconditioning]\n  # active = ' '\n  [./SMP]\n    type = SMP\n    full = true\n  [../]\n[]\n\n[Executioner]\n  type = Transient\n  scheme = 'BDF2'\n  #petsc_options = '-snes_mf'\n\n  #Preconditioned JFNK (default)\n  solve_type = 'NEWTON'\n\n  petsc_options_iname = '-pc_type -ksp_grmres_restart -sub_ksp_type -sub_pc_type -pc_asm_overlap'\n  petsc_options_value = 'asm      31                  preonly       lu           1'\n  #petsc_options_iname = '-pc_type'\n  #petsc_options_value = 'lu'\n\n  l_max_its = 30 # maximum linear iterations\n  l_tol = 1.0e-6 # 0.001 Linear Tolerance\n\n  nl_max_its = 50 # maximum number of nonlinear iterations. exceed will cut dt.\n  nl_rel_tol = 1.0e-9 # -8 nonlinear relative tolerance\n  line_search = 'none'\n  #num_steps = 2\n\n  # [./TimeStepper]\n  #   type = ConstantDT\n  #   dt = 1e-4\n  #   cutback_factor_at_failure = 0.67\n  #   growth_factor = 1.5\n  # [../]\n  [./TimeStepper]\n    type = IterationAdaptiveDT\n    dt = 1e-6\n    cutback_factor = 0.67\n    growth_factor = 1.5   # 1.6\n    optimal_iterations = 20\n    iteration_window = 2\n  [../]\n\n\n  # [./Adaptivity]\n  #   coarsen_fraction = 0.1\n  #   refine_fraction = 0.7\n  #   max_h_level = 3\n  #   interval = 1\n  # [../]\n\n\n[]\n\n[Debug]\n  show_var_residual_norms = true\n[]\n\n[Outputs]\n  exodus = true\n  console = true\n  csv = true\n  interval = 1\n  checkpoint = true\n  [./console]\n    type = Console\n    max_rows = 10\n  [../]\n[]",
          "url": "https://github.com/idaholab/moose/discussions/21149",
          "updatedAt": "2022-06-09T01:40:51Z",
          "publishedAt": "2022-05-25T22:55:32Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nI dont understand what you want.\nIf you change nx, you can get more or less elements, I dont see why you say you have to have 1001 elements.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21149#discussioncomment-2823701",
                  "updatedAt": "2022-05-25T23:51:34Z",
                  "publishedAt": "2022-05-25T23:51:33Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Error in make -j 4",
          "author": {
            "login": "leosmith36"
          },
          "bodyText": "I am trying to test my MOOSE build using the make -j 4 command. I use Windows 10. I continue to get the error pasted below. I am not sure where to go to fix this error, so any advice would be greatly appreciated!\nChecking if header needs updating: /bsuhome/leosmith/scratch/projects/moose/framework/include/base/MooseRevision.h...\nLinking Library /bsuhome/leosmith/scratch/projects/moose/framework/libmoose-opt.la...\n/usr/bin/ld: cannot find -lX11\n/usr/bin/ld: skipping incompatible /lib/libdl.so when searching for -ldl\n/usr/bin/ld: skipping incompatible /lib/librt.so when searching for -lrt\n/usr/bin/ld: skipping incompatible /lib/libutil.so when searching for -lutil\n/usr/bin/ld: skipping incompatible /lib/libm.so when searching for -lm\n/usr/bin/ld: skipping incompatible /lib/libpthread.so when searching for -lpthread\ncollect2: error: ld returned 1 exit status\nmake: *** [/bsuhome/leosmith/scratch/projects/moose/framework/libmoose-opt.la] Error 1",
          "url": "https://github.com/idaholab/moose/discussions/21148",
          "updatedAt": "2022-05-25T22:08:23Z",
          "publishedAt": "2022-05-25T21:35:53Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "leosmith36"
                  },
                  "bodyText": "Solved: export LIBRARY_PATH=/cm/shared/software/opt/linux-centos7-x86_64/gcc-9.2.0/libx11-1.7.0-u5s2reu2nx5vykhfimela2hmutk2vrg3/lib:${LIBRARY_PATH}",
                  "url": "https://github.com/idaholab/moose/discussions/21148#discussioncomment-2823405",
                  "updatedAt": "2022-05-25T22:08:21Z",
                  "publishedAt": "2022-05-25T22:08:21Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Dirichlet BC for Constant Monomial Variable",
          "author": {
            "login": "AhmedAlmetwally"
          },
          "bodyText": "Bug Description\nNodal Dirichlet BC for a constant monomial variable is not working because of the absence of the nodal/quadrature points on the boundary for constant variable. It works only when we have higher order variables.\nSteps to Reproduce: Example to generate the bug... A data file, failed to solve with a constant monomial variable, and the same  input solved successfully with first order variable.\n[Mesh]\n  type = GeneratedMesh\n  parallel_type = replicated # Until RayTracing.C is fixed\n  dim = 2\n  nx = 10\n  ny = 10\n[]\n\n[Variables]\n  [u]\n    family = MONOMIAL\n    order = CONSTANT\n  []\n[]\n\n[Kernels]\n  [diff]\n    type = Diffusion\n    variable = u\n  []\n[]\n\n[BCs]\n  [left]\n    type = DirichletBC\n    variable = u\n    boundary = left\n    value = 0\n  []\n  [right]\n    type = DirichletBC\n    variable = u\n    boundary = right\n    value = 1\n  []\n  [bottom]\n    type = DirichletBC\n    variable = u\n    boundary = bottom\n    value = 0\n  []\n  [top]\n    type = DirichletBC\n    variable = u\n    boundary = top\n    value = 1\n  []\n[]\n\n[VectorPostprocessors]\n  [elems]\n    type = ElementsAlongLine\n    start = '0.05 0 0'\n    end = '0.405 0 0'\n  []\n[]\n\n[Executioner]\n  type = Steady\n  solve_type = PJFNK\n  petsc_options_iname = '-pc_type -pc_hypre_type'\n  petsc_options_value = 'hypre boomeramg'\n[]\n\n[Outputs]\n  exodus = true\n  csv = true\n[]\n\nLinear Variable Data File:\n[Mesh]\n  type = GeneratedMesh\n  parallel_type = replicated # Until RayTracing.C is fixed\n  dim = 2\n  nx = 10\n  ny = 10\n[]\n\n[Variables]\n  [u]\n  []\n[]\n\n[Kernels]\n  [diff]\n    type = Diffusion\n    variable = u\n  []\n[]\n\n[BCs]\n  [left]\n    type = DirichletBC\n    variable = u\n    boundary = left\n    value = 0\n  []\n  [right]\n    type = DirichletBC\n    variable = u\n    boundary = right\n    value = 1\n  []\n  [bottom]\n    type = DirichletBC\n    variable = u\n    boundary = bottom\n    value = 0\n  []\n  [top]\n    type = DirichletBC\n    variable = u\n    boundary = top\n    value = 1\n  []\n\n[]\n\n[VectorPostprocessors]\n  [elems]\n    type = ElementsAlongLine\n    start = '0.05 0 0'\n    end = '0.405 0 0'\n  []\n[]\n\n[Executioner]\n  type = Steady\n  solve_type = PJFNK\n  petsc_options_iname = '-pc_type -pc_hypre_type'\n  petsc_options_value = 'hypre boomeramg'\n[]\n\n[Outputs]\n  exodus = true\n  csv = true\n[]\n\nImpact\nCell-Centered Continuous/Discontinuous Galerkin is needed to enforce mass conservation for flow. Dirichlet BC should be enforced as a constant or through reflection elements as handled in the FV Dirichlet BC.",
          "url": "https://github.com/idaholab/moose/discussions/21353",
          "updatedAt": "2022-06-19T22:47:37Z",
          "publishedAt": "2022-05-17T19:34:37Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "AhmedAlmetwally"
                  },
                  "bodyText": "@dschwen",
                  "url": "https://github.com/idaholab/moose/discussions/21353#discussioncomment-2981817",
                  "updatedAt": "2022-06-19T22:44:16Z",
                  "publishedAt": "2022-05-25T18:48:53Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "dschwen"
                  },
                  "bodyText": "You can use PenaltyDirichletBC here instead. That one is integrated rather than nodal and will work for monomial variables.",
                  "url": "https://github.com/idaholab/moose/discussions/21353#discussioncomment-2981818",
                  "updatedAt": "2022-06-19T22:44:16Z",
                  "publishedAt": "2022-05-25T19:59:20Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Using Distributed Mesh with UserObject prescribed properties",
          "author": {
            "login": "lukuna"
          },
          "bodyText": "Hi All,\nI am trying to run some simulations using an application built on the MOOSE framework (specifically: https://github.com/ngrilli/c_pfor_am).  It works wonderfully for my purposes but I have come across a strange issue.\nI am trying to run some fairly large calculations (500K+ Voxels), and am seeing a lot of overhead when I run on many nodes.  To clarify, the same input file running 250 cores runs much faster on a cluster where I have 126 cores/node vs 48 cores/node.  To the point where running my calculation on more than 2 nodes actually slows it down (!!!).  In an effort to remedy this I looked to try to use the --distributed-mesh option and have come across a strange issue.  Before I get to the issue let me introduce why I think it may be happening.  In these calculations UserObjects are used to read in microstructure (Euler angle) and temperature data.\nI believe the images below clearly show the issue:\nDistributed:\n\nNot Distributed:\n\nThe way that the temperature is being read onto the mesh seems be messed up by the distributed mesh.  My mesh is as follows:\n  [./gen]\n    type = GeneratedMeshGenerator\n    dim = 3\n    nx = 40\n    ny = 46\n    nz = 18\n    xmax = 240.0\n    ymax = 276.0\n    zmax = 108.0\n    elem_type = HEX8\n    displacements = 'disp_x disp_y disp_z'\n  [../]\n\nand I am using the two UserObjects located in the c_pfor_am app:\n[./prop_read]\n  type = GrainPropertyReadFile\n  prop_file_name = 'CA3DVoxSizeTestLargeAngles.txt'\n  # Enter file data as prop#1, prop#2, .., prop#nprop\n  nprop = 3\n  read_type = element\n[../]\n\n[./temperature_read]\n  type = LaserTempReadFile\n  temperature_file_name = 'VoxSizeTestLargeTemp.txt'\n  temperature_num_step = 7\n[../]\n\nFound here and here.  Does anybody have any ideas or suggestions as to how this can be remedied?  Is this something internal with MOOSE, or just a slight adjustment that needs to be made within the app?\nThanks for your time,\nLukasz Kuna\nThe Executioner I am using for anyone curious regarding the HPC slowdown is:\n\n[Executioner]\n  type = Transient\n  solve_type = 'NEWTON'\n  petsc_options_iname = '-pc_hypre_type'\n  petsc_options_value = 'boomerang'\n  line_search = 'default'\n  automatic_scaling = true\n\n  l_max_its = 30\n  nl_max_its = 15\n  nl_rel_tol = 1e-3\n  nl_abs_tol = 1e-3\n  l_tol = 1e-5\n\n  [./TimeStepper]\n    type = FunctionDTGrowth\n    function = dts\n    cutback_factor_at_failure = 0.2\n    growth_factor = 1.5\n  [../]\n\n  start_time = 0.0\n  end_time = 2800\n  dtmin = 1.0e-99\n  timestep_tolerance = 1.0e-99\n\n[]\n\n\nI had used lu before, but it was maxing out the memory on the HPC nodes I was using so I switched to hypre\nAny suggestions regarding this would also be greatly appreciated.",
          "url": "https://github.com/idaholab/moose/discussions/21124",
          "updatedAt": "2022-05-25T19:06:11Z",
          "publishedAt": "2022-05-24T21:36:47Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nyou need to turn off mesh renumbering to use this user object with a distrivuted mesh. Don\u2019t you get a warning?\nor did you do this already?\nguillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21124#discussioncomment-2815746",
                  "updatedAt": "2022-05-24T23:21:42Z",
                  "publishedAt": "2022-05-24T23:21:42Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "allow_mesh_renumbering = false in Mesh block",
                          "url": "https://github.com/idaholab/moose/discussions/21124#discussioncomment-2815747",
                          "updatedAt": "2022-05-24T23:22:07Z",
                          "publishedAt": "2022-05-24T23:22:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lukuna"
                          },
                          "bodyText": "This goes inside the [Mesh] block or inside of [./gen]?\nWhen placed in [Mesh] I am seeing:\n/Users/lukasz/projects/c_pfor_am/VoxSizeTestLargeWall.i:6: unused parameter 'Mesh/allow_mesh_renumbering'\nand obviously no difference in the resulting mesh.",
                          "url": "https://github.com/idaholab/moose/discussions/21124#discussioncomment-2815802",
                          "updatedAt": "2022-05-24T23:41:02Z",
                          "publishedAt": "2022-05-24T23:40:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "It s just allow_renumbering it turns out. I had it wrong",
                          "url": "https://github.com/idaholab/moose/discussions/21124#discussioncomment-2816816",
                          "updatedAt": "2022-05-25T04:10:25Z",
                          "publishedAt": "2022-05-25T04:10:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lukuna"
                          },
                          "bodyText": "Thank you Guillaume.  This has solved my problem.  It would be very useful if this was documented along with the Mesh System (or wherever the distributed mesh documentation is).",
                          "url": "https://github.com/idaholab/moose/discussions/21124#discussioncomment-2819887",
                          "updatedAt": "2022-05-25T12:51:45Z",
                          "publishedAt": "2022-05-25T12:51:45Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "This is more of a problem with the user object you are using to map the CSV file to the mesh. I ll add it to the user object documentation",
                          "url": "https://github.com/idaholab/moose/discussions/21124#discussioncomment-2822563",
                          "updatedAt": "2022-05-25T19:06:12Z",
                          "publishedAt": "2022-05-25T19:06:11Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "lukuna"
                  },
                  "bodyText": "@ngrilli This is the solution to the distributed-mesh issue I was having.  Just FYI.",
                  "url": "https://github.com/idaholab/moose/discussions/21124#discussioncomment-2819900",
                  "updatedAt": "2022-05-25T12:53:09Z",
                  "publishedAt": "2022-05-25T12:53:08Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "AuxKernel to get an average within a radius",
          "author": {
            "login": "maxnezdyur"
          },
          "bodyText": "Is there a system in MOOSE to get an average value of the variable within a radius of the element. Say I have a variable V that gets solved and for every element, I want to know what the V_avg is where the V is taken from elements within radius R of the element center. I saw the user objects with the layered average which seems to be close to what I want but only gets the average in one direction I believe. Thanks.",
          "url": "https://github.com/idaholab/moose/discussions/21112",
          "updatedAt": "2022-06-09T01:40:38Z",
          "publishedAt": "2022-05-24T14:20:35Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nthe layered average is indeed what you want to look for.\ni think this one might do what you want\nhttps://mooseframework.inl.gov/source/userobject/NearestRadiusLayeredAverage.html\nYou should double check the code and or the docs to see what it does exactly. I remember being surprised that it didn\u2019t take into account the quadrature or the volumes, I don\u2019t remember.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21112#discussioncomment-2813361",
                  "updatedAt": "2022-05-24T15:55:00Z",
                  "publishedAt": "2022-05-24T15:54:59Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "Yes, that looks pretty close to what I want. I might have to update part of it because it only allows an average over a single direction, according to the docs. Looking through the code more to fully understand it.",
                          "url": "https://github.com/idaholab/moose/discussions/21112#discussioncomment-2819504",
                          "updatedAt": "2022-05-25T11:56:05Z",
                          "publishedAt": "2022-05-25T11:56:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so what kind of angular discretization do you need? like a uniform quadrature?",
                          "url": "https://github.com/idaholab/moose/discussions/21112#discussioncomment-2822489",
                          "updatedAt": "2022-05-25T18:55:00Z",
                          "publishedAt": "2022-05-25T18:55:00Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}