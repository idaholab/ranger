{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMS0wMS0wOFQwODoyMzowNS0wNzowMM4AGOue"
    },
    "edges": [
      {
        "node": {
          "title": "Moose plastic hardening simulation - Computationaly slower than ANSYS",
          "author": {
            "login": "abarun22"
          },
          "bodyText": "Dear all,\nI recently used MOOSE  to predict the hardening behaviour of certain class of steel specimens subjected to tensile loads. A PieceWise linear function was used to model the hardening response. The results were matching precisely with ANSYS until the start of necking, with some differences noticed thereafter, nevertheless a seemingly realistic behaviour overall. However the problem is that MOOSE is extremely slow in comparison to ANSYS's computational effort.  For a model with say ~1500 nodes, Ansys took only 5-6 mins, whereas MOOSE clocks it in 2.5hrs, which is exhorbitant, considering that we may need to go for a constrained optimization problem thereafter. We typically expect this problem to be solved in 10 iterations and that might take more than a day to complete the whole analysis. From the compiler perspective, we've chosen the right optimization (O3) that enables a faster computation. Attached please find the input file i was using to simulate this model. Any advice on how to achieve a computational time comparable to ANSYS would be greatly appreciated?\nKind regards,\nPieceWise_test.txt\nArun",
          "url": "https://github.com/idaholab/moose/discussions/16372",
          "updatedAt": "2022-06-10T14:39:03Z",
          "publishedAt": "2020-12-02T17:45:53Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "jiangwen84"
                  },
                  "bodyText": "Could you quickly check whether ANSYS and MOOSE are solving the same number of DOFs?\nHow many time steps does MOOSE and ANSYS take?\nFor each time step, how many nonlinear iteration does MOOSE and ANSYS?\nWhat solver does ANSYS use? How long does ANSYS and MOOSE to solve per iteration and time step?",
                  "url": "https://github.com/idaholab/moose/discussions/16372#discussioncomment-146194",
                  "updatedAt": "2022-06-10T14:39:17Z",
                  "publishedAt": "2020-12-03T16:09:15Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "srinath-chakravarthy"
                  },
                  "bodyText": "my 2c. In my experience, MOOSE is generally slower than either ANSYS or ABAQUS.  For non-AD solutions in plasticity i have found moose to be about 2 - 2.5 times slower with a LU preconditioner and newton solution.\nThe relative convergence criteria are about 1e-3 in ANSYS and ABAQUS. Also double check the elements you are using. The defaults in commercial software, use hybrid elements with an extra pressure degree of freedom, that is not yet implemented in MOOSE. Also turn on volumetric locking to get better convergence and speed.\nIn your input file, change the solution to NEWTON instead of PJFNK and see if it helps.\nCheers\nSrinath",
                  "url": "https://github.com/idaholab/moose/discussions/16372#discussioncomment-146203",
                  "updatedAt": "2022-06-10T14:39:17Z",
                  "publishedAt": "2020-12-03T16:17:42Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "For nonlinear mechanics problem, we do not implement exact Jacobians, so Newton should take take more iterations than PJFNK to converge. In our experience, for non-AD, NEWTON is usually slower than PJFNK. I am curious to know your experience on the speed of NEWTON and PJFNK for non-AD.",
                          "url": "https://github.com/idaholab/moose/discussions/16372#discussioncomment-146222",
                          "updatedAt": "2022-07-02T13:23:35Z",
                          "publishedAt": "2020-12-03T16:30:31Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "dschwen"
                  },
                  "bodyText": "Your IsotropicPlasticityStressUpdate is set to run on the displaced mesh. I don't think that's right. You might try using AD versions of all mechanics models ADIsotropicPlasticityStressUpdate, ADComputeMultipleInelasticStress, ADComputeIsotropicElasticityTensor, and use_asutomatic_differentiation = true in the Master action. (you may need to recompile your moose with a larger AD vector size for 3D...)\nBut the execution time difference is too substantial to be explained away by that most likely. Without the mesh and function files we cannot run your input though.\n1500 nodes is a really tiny problem and should absolutely not take that long to run at all.",
                  "url": "https://github.com/idaholab/moose/discussions/16372#discussioncomment-146395",
                  "updatedAt": "2022-07-02T13:23:38Z",
                  "publishedAt": "2020-12-03T17:06:39Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Dear all,\nThanks for the suggestions and indeed they worked very well. On top of that i resized my ANSYS mesh (~5600 degrees of freedom) to match closely with my MOOSE mesh. Also i was not exactly specifying the number of cores in my parallel execution launcher earlier and that took me to the serial mode.  With all these modifications i am now getting comparables times.  Ansys took 26 mins to finish the computation, while moose did that in 33mins, which seems quite satisfying going in to subsequent phase of activities. For your reference i am attaching here the required files for running the computation. Is there any way that i could tune the execution options to achieve an enhanced performance.\nKind regards,\nArun\nPlasticityModelling_files.zip",
                          "url": "https://github.com/idaholab/moose/discussions/16372#discussioncomment-146676",
                          "updatedAt": "2022-07-02T13:23:46Z",
                          "publishedAt": "2020-12-03T19:58:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Thanks for all your suggestions. With much hope i am now testing my plasticity model for large meshes say of the order of 50k Dof's. Moose crossed that in 32 hrs. I've not exactly quantified the speed in Ansys as i am trying to sort out certain issues associated with the batch job execution internaly. My speculation is that ANSYS scales much slower than MOOSE for bigger models, perhaps would take double the time attained by MOOSE. I shall give you a clear update in this as we progress with ANSYS testing.\nAnother issue i noted recently was that the newer version of MOOSE seems to be much slower than the earlier version (dated May'2020) with the former one requiring 55mins to complete the execution for the smaller test case i mentioned intially. From my tests I see  that the petsc versions are not influencing the performance. I wonder whether the new developments in MOOSE are reponsible for this degraded performance. Any advise on this would help me decide on the performance aspects of the code before going in to the subsequent phase of developments.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/16372#discussioncomment-277247",
                          "updatedAt": "2022-07-02T13:24:03Z",
                          "publishedAt": "2021-01-12T17:56:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "@abarun22 It's probably better to start a new discussion on the slow down using recent moose, so that people know this is unanswered.",
                          "url": "https://github.com/idaholab/moose/discussions/16372#discussioncomment-279652",
                          "updatedAt": "2022-08-17T22:06:11Z",
                          "publishedAt": "2021-01-13T16:55:40Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to preserve sideset names with FancyExtruderGenerator",
          "author": {
            "login": "aprilnovak"
          },
          "bodyText": "Hi all,\nI'd like to make a pincell mesh, starting from a circle mesh generated with the AnnularMeshGenerator, that is then extruded in the z-direction. I want to apply a boundary condition on the surface of the pincell, but am unsure how to access that sideset following the extrusion process.\nAnnularMeshGenerator labels the circle surface as a sideset with name rmax. But once I extrude it in the z-direction and try to apply a BC on rmax, I get a: the following side set ids do not exist on the mesh error. I feel like I'm just missing something basic here, but none of the sideset generators jump out at me as being capable of creating a sideset for the curved cylinder surface (b/c the normal isn't fixed). All the tests for the extrude-type mesh generators also only apply BCs on the bottom/top sidesets that you can name in the generator itself.\nAny suggestions for how to get a valid sideset on the cylinder surface? Here's my mesh:\n[Mesh]\n  [circle]\n    type = AnnularMeshGenerator\n    nr = 8\n    nt = 8\n    rmin = 0\n    rmax = 0.4E-2\n    growth_r = 1.2\n  []\n\n  [extrude]\n    type = FancyExtruderGenerator\n    input = circle\n    heights = '0.8'\n    num_layers = '10'\n    direction = '0 0 1'\n    bottom_sideset = '100'\n    top_sideset = '101'\n  []\n[]\n\n[Outputs]\n  exodus = true\n[]\n\nThanks!\n-April",
          "url": "https://github.com/idaholab/moose/discussions/16623",
          "updatedAt": "2023-04-05T23:17:01Z",
          "publishedAt": "2021-01-06T23:49:37Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hi April,\n@friedmud made such meshes for BEAVRS, you may see them in the mockingbird repository. I can't add you rn, maybe @smharper ?\n@smharper may also be able to help directly as he used the FancyExtruder recently.\nBest,\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/16623#discussioncomment-266719",
                  "updatedAt": "2023-04-05T23:17:06Z",
                  "publishedAt": "2021-01-07T11:10:47Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aprilnovak"
                          },
                          "bodyText": "I figured out my problem! It seems that the FancyExtruder doesn't retain sideset names when you extrude, but does preserve sideset numbers. I was trying to apply BCs based on names, which I had assumed would transfer correctly.\nIs there a reason why the sideset names aren't also transferred? I think it could be very helpful to transfer those as well as the sideset IDs.",
                          "url": "https://github.com/idaholab/moose/discussions/16623#discussioncomment-277483",
                          "updatedAt": "2023-04-05T23:17:06Z",
                          "publishedAt": "2021-01-12T19:45:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "Probably just an oversight.",
                          "url": "https://github.com/idaholab/moose/discussions/16623#discussioncomment-279440",
                          "updatedAt": "2023-04-05T23:17:06Z",
                          "publishedAt": "2021-01-13T15:37:51Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Installing MOOSE with OpenMPI3.1.3 and GCC 7",
          "author": {
            "login": "jinca"
          },
          "bodyText": "Hello,\nI used the following modules:\nmodule purge\nmodule load slurm\nmodule load dot\nmodule load turbovnc/2.0.1\nmodule load vgl/2.5.1/64\nmodule load singularity/current\nmodule load rhel7/global\nmodule load cmake/latest\nmodule load gcc/7\nmodule load openmpi-3.1.3-gcc-7.2.0-b5ihosk\nI followed the procedures of the Website:\nsource ~/.moose_profile\necho $CC\nwhich mpicc\ncd\nmkdir projects\ngit clone https://github.com/idaholab/moose\ncd moose\ngit checkout master\ncd ~/projects/moose\nunset PETSC_DIR PETSC_ARCH\n./scripts/update_and_rebuild_petsc.sh --download-mumps=0 --with-64-bit-indices=1\ncd petsc/\nmake PETSC_DIR=/home/ir-inca1/moose/scripts/../petsc PETSC_ARCH=arch-moose check\ncd ..\n./scripts/update_and_rebuild_libmesh.sh\ncd ~/projects/moose/test\nmake -j 4\n./run_tests -j 4\nI was able to build PETSC and LibMesh, but when it comes to run the tests, I got the following errors:\npostprocessors/num_elems.test_split ............................................ [min_cpus=4] FAILED (CSVDIFF)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor .............. [min_cpus=3] FAILED (EXODIFF)\noutputs/xml.parallel/distributed ............................................... [min_cpus=3] FAILED (XMLDIFF)\nmesh/splitting.use_split ....................................................... [min_cpus=3] FAILED (EXODIFF)\nrelationship_managers/evaluable.evaluable_neighbors_replicated ................. [min_cpus=3] FAILED (EXODIFF)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor_3D ........... [min_cpus=3] FAILED (EXODIFF)\nmesh/centroid_partitioner.centroid_partitioner_test ............................ [min_cpus=4] FAILED (EXODIFF)\nrelationship_managers/evaluable.edge_neighbor .................................. [min_cpus=3] FAILED (EXODIFF)\nRan 2416 tests in 772.2 seconds.\n2366 passed, 96 skipped, 0 pending, 50 FAILED\nMAX FAILURES REACHED",
          "url": "https://github.com/idaholab/moose/discussions/16544",
          "updatedAt": "2022-08-02T05:54:50Z",
          "publishedAt": "2020-12-18T18:26:46Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "There should be some more detailed error information farther up in your terminal window. I suspect there is a problem with the MPI.",
                  "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-235821",
                  "updatedAt": "2022-08-02T05:55:22Z",
                  "publishedAt": "2020-12-23T00:10:06Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "ngrilli"
                  },
                  "bodyText": "@jinca\nWhich operating system are you using?\nI suggest you try the manual installation with gcc 9.2.0 and mpich 3.3 on the website\nIt usually works fine for us\nNicol\u00f2",
                  "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-256245",
                  "updatedAt": "2022-08-02T05:55:22Z",
                  "publishedAt": "2021-01-02T05:36:08Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jinca"
                          },
                          "bodyText": "The OS is Scientific Linux release 7.9 (Nitrogen)",
                          "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-277556",
                          "updatedAt": "2022-08-02T05:55:25Z",
                          "publishedAt": "2021-01-12T20:30:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "50 Errors (the MAX before it quits) usually means an easy fix. Something fundamentally wrong. Not necessarily a compiler version or the like.\nAs Andrew asked, seeing the actual error helps tremendously. The details of these errors can be found if you scroll up in your terminal history.",
                          "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-277597",
                          "updatedAt": "2022-08-02T05:55:25Z",
                          "publishedAt": "2021-01-12T20:45:37Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "jinca"
                  },
                  "bodyText": "OK, When I set the following modules:\nmodule load rhel7/global\nmodule load slurm\nmodule load gcc/7\nmodule load openmpi-3.1.3-gcc-7.2.0-b5ihosk\nmodule load python/3.8\nI got this error while building petsc:\n===============================================================================                                                                                                                     Configuring STRUMPACK with cmake; this may take several minutes                                                                                                                         ===============================================================================                                                                                                                                                                                                                                                                                                             *******************************************************************************\nUNABLE to CONFIGURE with GIVEN OPTIONS    (see configure.log for details):\nError configuring STRUMPACK with cmake\n\nThere was an error. Exiting...",
                  "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-277621",
                  "updatedAt": "2022-08-02T05:55:25Z",
                  "publishedAt": "2021-01-12T21:01:21Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Without PETSc, you wouldn't (shouldn't) be able to build libMesh. Without libMesh... you wouldn't be able to build MOOSE for an attempt at ./run_test. I see lots of tests passed as well as reaching the max failure. The only thing I can think of, is that your moose repo was previously built, and has a partial successful build.\nIts hard to tell if you copied exactly what you attempted, or if there is an error during your attempt:\nwhich mpicc\ncd\nmkdir projects                 # <-------- failed to enter projects before cloning moose\ngit clone https://github.com/idaholab/moose\ncd moose\ngit checkout master\ncd ~/projects/moose            # <-------- not the moose you just cloned. A possible older version of moose?",
                          "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-277700",
                          "updatedAt": "2022-08-02T05:55:25Z",
                          "publishedAt": "2021-01-12T21:34:46Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "jinca"
                  },
                  "bodyText": "Thank you so much for the quick replies guys. Many days ago, I was able to run MOOSE only with 7 FAILS. Now I am trying to reinstalling again, I can not install at least PETSC. I am very aware that I have to build PETSC, then LibMesh and at the end MOOSE.\nI will clean and delete all, turn off the machine and do it again in order to send the info today asap. Thanks again.",
                  "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-277720",
                  "updatedAt": "2022-08-02T05:55:25Z",
                  "publishedAt": "2021-01-12T21:44:29Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "jinca"
                  },
                  "bodyText": "Hi guys, this is the configuration I used:\nmodule purge\nmodule load slurm\nmodule load dot\nmodule load turbovnc/2.0.1\nmodule load vgl/2.5.1/64\nmodule load singularity/current\nmodule load rhel7/global\nmodule load cmake/latest\nmodule load gcc/7\nmodule load openmpi-3.1.3-gcc-7.2.0-b5ihosk\nmodule load python/3.8\nThe PETSC was built as the libMesh, and therefore MOOSE.\nI am attaching the output of the tests I've done recently.\nHope you can help me soon.\nBest,\nJulita Inca\nmesh/checkpoint.test_8 .................................................. [insufficient slots,min_cpus=8] SKIP\nmesh/checkpoint.test_8a ................................................. [insufficient slots,min_cpus=8] SKIP\npartitioners/random_partitioner.test ......................................................... [min_cpus=4] OK\nmesh/nemesis.nemesis_repartitioning_test ..................................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.ptscotch_weight_both .......................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.parmetis ...................................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.parmetis_weight_element ....................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.parmetis_weight_side .......................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.parmetis_weight_both .......................................... [min_cpus=4] OK\npartitioners/petsc_partitioner.parmetis_presplit_mesh ........................................ [min_cpus=2] OK\nauxkernels/ghosting_aux.no_algebraic_ghosting ................................................ [min_cpus=4] OK\npostprocessors/num_nodes.test_split .......................................................... [min_cpus=4] OK\nreporters/mesh_info.info/files ........................................... [min_cpus=2] FAILED (MISSING FILES)\noutputs/json/distributed.info/default .................................... [min_cpus=2] FAILED (MISSING FILES)\nvectorpostprocessors/csv_reader.tester_fail ..................... [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\nmisc/exception.parallel_error_residual_transient_non_zero_rank .. [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank ................ [min_cpus=2] FAILED (NO CRASH)\nbcs/dmg_periodic.1d ............................................................. [min_cpus=2] FAILED (ERRMSG)\nvectorpostprocessors/work_balance.work_balance/replicated ...................... [min_cpus=2] FAILED (CSVDIFF)\noutputs/variables.nemesis_hide ................................................. [min_cpus=2] FAILED (EXODIFF)\nmeshgenerators/distributed_rectilinear/generator.3D_hierarch ................... [min_cpus=4] FAILED (EXODIFF)\nmeshgenerators/distributed_rectilinear/generator.3d_scomm_out .................. [min_cpus=3] FAILED (EXODIFF)\nRan 2490 tests in 586.0 seconds.\n2480 passed, 96 skipped, 0 pending, 10 FAILED",
                  "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-278002",
                  "updatedAt": "2022-08-02T05:55:25Z",
                  "publishedAt": "2021-01-13T00:27:43Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Wrap three ticks around text when wanting to use a pre-rendered block (instead of one). Somehow, doing that entire thing around one broke GitHub! As I only got most of it, from email.\ntick tick tick\npre-rendered block\n\ntick tick tick\nHere is an excerpt of what was sent to me by email:\nproblems/eigen_problem/eigensolvers.nonlinear_power ....................................................... OK\nics/depend_on_uo.ic_depend_on_uo: Working Directory: /home/ir-inca1/projects/moose/test/tests/ics/depend_on_uo\nics/depend_on_uo.ic_depend_on_uo: Running command: mpiexec -n 2 /home/ir-inca1/projects/moose/test/moose_test-opt -i geometric_neighbors_ic.i --error --error-unused --error-override --no-gdb-backtrace\nics/depend_on_uo.ic_depend_on_uo: srun: error: Unable to allocate resources: Invalid account or account/partition combination specified\nics/depend_on_uo.ic_depend_on_uo: srun: error: Unable to allocate resources: Invalid account or account/partition combination specified\nics/depend_on_uo.ic_depend_on_uo: Running exodiff: /home/ir-inca1/projects/moose/framework/contrib/exodiff/exodiff -m -F 1e-10 -t 5.5e-06 /home/ir-inca1/projects/moose/test/tests/ics/depend_on_uo/gold/geometric_neighbors_ic_out.e /home/ir-inca1/projects/moose/test/tests/ics/depend_on_uo/geometric_neighbors_ic_out.e\nics/depend_on_uo.ic_depend_on_uo: ERROR:\nics/depend_on_uo.ic_depend_on_uo: *****************************************************************\nics/depend_on_uo.ic_depend_on_uo: EXODIFF (Version: 2.90) Modified: 2018-02-15\nics/depend_on_uo.ic_depend_on_uo: Authors: Richard Drake, rrdrake@sandia.gov\nics/depend_on_uo.ic_depend_on_uo: Greg Sjaardema, gdsjaar@sandia.gov\nics/depend_on_uo.ic_depend_on_uo: Run on 2021/01/13 00:11:20 GMT\nics/depend_on_uo.ic_depend_on_uo: *****************************************************************\nics/depend_on_uo.ic_depend_on_uo:\nics/depend_on_uo.ic_depend_on_uo: exodiff: ERROR: Couldn't open file \"/home/ir-inca1/projects/moose/test/tests/ics/depend_on_uo/geometric_neighbors_ic_out.e\".\nics/depend_on_uo.ic_depend_on_uo: Reading first file ...\nics/depend_on_uo.ic_depend_on_uo: Reading second file ...\nics/depend_on_uo.ic_depend_on_uo: exodiff: ERROR: exodiff: ERROR: Couldn't open file \"/home/ir-inca1/projects/moose/test/tests/ics/depend_on_uo/geometric_neighbors_ic_out.e\". File does not exist.\nics/depend_on_uo.ic_depend_on_uo:\nics/depend_on_uo.ic_depend_on_uo: ################################################################################\nics/depend_on_uo.ic_depend_on_uo: Tester failed, reason: EXODIFF\nics/depend_on_uo.ic_depend_on_uo:\nics/depend_on_uo.ic_depend_on_uo ............................................... [min_cpus=2] FAILED (EXODIFF)\n\nThe issue is with srun:\nsrun: error: Unable to allocate resources: Invalid account or account/partition combination specified\nsrun: error: Unable to allocate resources: Invalid account or account/partition combination specified\n\nWhatever this is, it prevented the following command from running:\nmpiexec -n 2 /home/ir-inca1/projects/moose/test/moose_test-opt -i geometric_neighbors_ic.i --error --error-unused --error-override --no-gdb-backtrace\n\nWhich means, there would be no exodiff file generated, which would spawn the error:\nexodiff: ERROR: exodiff: ERROR: Couldn't open file \"/home/ir-inca1/projects/moose/test/tests/ics/depend_on_uo/geometric_neighbors_ic_out.e\". File does not exist.\n\nThe same is happening with every error posted (srun: error: Unable to allocate resources: Invalid account or account/partition combination specified). It would seem something to do with the machine you are running on. Allocating resources incorrectly, something like that.",
                          "url": "https://github.com/idaholab/moose/discussions/16544#discussioncomment-279284",
                          "updatedAt": "2022-08-02T05:56:05Z",
                          "publishedAt": "2021-01-13T14:41:53Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Finite strain simulations with the Maxwell Viscoelastic model",
          "author": {
            "login": "veevee1208"
          },
          "bodyText": "Hello MOOSE team,\nI have been running some finite strain simulations with the viscoelastic Kelvin Voigt model and I do not have any issues with convergence. However, when I try to run the finite strain simulation with the Maxwell model, I seem to run into a convergence issue . I tried using the PJFNK and AD but they seem to be of no effect. I was wondering if someone has experience running viscoelastic models on MOOSE and have any suggestions ? I have attached a copy of my input file.\nAny help will be appreciated.\nBest Regards\nVishal\nMaxwell-finite.txt",
          "url": "https://github.com/idaholab/moose/discussions/16600",
          "updatedAt": "2022-07-14T06:45:54Z",
          "publishedAt": "2020-12-30T20:28:24Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "crswong888"
                  },
                  "bodyText": "Hi @veevee1208, I tried playing around with your input file and I improved a bit the performance, but I think the problem you are having is related to very high strains. Please make sure that all of your units are consistent and that the forces and boundary conditions you're applying are realistic. I changed the axial force in the x-direction from 10e3 to just 10, and at 30 seconds the strain in the x-direction is 380%, which is a really high number for most solid materials (even viscoelastic ones) and leads to poor conditioning in the Jacobian.\nYou might find my version of your input file helpful: github.com/crswong888/scorpion/blob/master/inputs/users/Maxwell-finite.i. It diverges at about 30 seconds though.\nIf you continue to have issues, it may be helpful for me to take a look at the version of your input file for which you had none.",
                  "url": "https://github.com/idaholab/moose/discussions/16600#discussioncomment-275818",
                  "updatedAt": "2023-01-19T21:27:39Z",
                  "publishedAt": "2021-01-12T03:15:33Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Redirect external app std::cout to _console?",
          "author": {
            "login": "aprilnovak"
          },
          "bodyText": "Hi all,\nI'm running an external app (nekRS) through the ExternalProblem interface, and it would be nice to redirect nekRS's std::cout through MOOSE's _console to make the output prettier.\nFor instance, nekRS uses std::cout, which when run as a sub-app, looks like this on my console. Lines 4 and 5 come from nekRS.\nnek0: Time Step 17, time = 0.029, dt = 0.002\nnek0: Sending heat flux to nekRS boundary 1...\nnek0: Normalizing total nekRS flux of 389.376 to the conserved MOOSE value of 386.989...\nstep= 17  t= 2.90000000e-02  dt=2.0e-03  C= 0.05  UVW: 6  P: 201  S: 157  eTime= 6.60e-01, 1.08991e+01 s\ncopying solution to nek\nnek0: Extracting nekRS temperature from boundary 1...\nnek0: Interpolated temperature min/max values on interface: 628.15, 628.952\nnek0:  Solve Converged!\nDoes MOOSE have a capability like this? If this is obvious, my apologies - nothing jumped out at me on _console!\nThanks!\n-April",
          "url": "https://github.com/idaholab/moose/discussions/16647",
          "updatedAt": "2022-06-02T14:54:56Z",
          "publishedAt": "2021-01-08T21:11:17Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "I don't know how that could be done. @permcody do you know if this would be possible?",
                  "url": "https://github.com/idaholab/moose/discussions/16647#discussioncomment-270226",
                  "updatedAt": "2022-06-02T14:55:16Z",
                  "publishedAt": "2021-01-08T21:29:34Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "permcody"
                          },
                          "bodyText": "To the best of my knowledge a simple redirect definitely isn't possible. The _console is a C++ object that manages it's own stream. It may be possible to share the underlying stream but even that would take a bit of gymnastics.\nI think a better idea would be to add a new API (extern C linkage) that developers of other applications could call that would invoke the MOOSE console object. This would be fairly trivial to implement and would automatically give you the coloring, indentation, and input file based control we are accustomed to. The big disadvantage is that this would be very disruptive for applications that have output spread all over through their application as it would require them to go through their application and update everything. Worse yet, you'd have to have make this a macro to handle the cases when you are wrapped in MOOSE or not.\nAgain there are ways of sharing file system streams at the OS level, but I'm not sure of a way we could put all of the MOOSE functionality into one of those and share it. It may be doable, but we'd need to build a demo project to try a few things out.",
                          "url": "https://github.com/idaholab/moose/discussions/16647#discussioncomment-274706",
                          "updatedAt": "2022-06-02T14:55:20Z",
                          "publishedAt": "2021-01-11T15:58:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aprilnovak"
                          },
                          "bodyText": "Thanks for explaining everything! Unfortunately, nekRS has cout scattered over the entire project, so I agree that this would be pretty tricky to implement. And the nekRS devs are pretty against having any trace of \"MOOSE\" things in their project from a modularization perspective, understandably.\nThis was just something I thought would be nice for formatting, so this is very low priority anyways. Just wanted to check that that feature didn't exist yet!\nThanks!",
                          "url": "https://github.com/idaholab/moose/discussions/16647#discussioncomment-275573",
                          "updatedAt": "2022-06-02T14:55:23Z",
                          "publishedAt": "2021-01-11T23:09:09Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Solve failure with no information",
          "author": {
            "login": "dcolema4"
          },
          "bodyText": "My solve fails after multiple time steps without producing any error messages. The solution fails on the second nonlinear iteration. Here is the output using '-snes_converged_reason -ksp_converged_reason -ksp_monitor_true_residual -ksp_view':\nPerforming automatic scaling calculation\n0 Nonlinear |R| = [32m6.158493e+01[39m\n|residual|2 of individual variables:\ntemperature:     61.5849\nbinder_fraction: 0.00398115\n0 Linear |R| = [32m6.158493e+01[39m\n0 KSP unpreconditioned resid norm 6.158493137943e+01 true resid norm 6.158493137943e+01 ||r(i)||/||b|| 1.000000000000e+00\n1 Linear |R| = [32m1.528537e-02[39m\n1 KSP unpreconditioned resid norm 1.528537014084e-02 true resid norm 1.528537014084e-02 ||r(i)||/||b|| 2.481998404230e-04\n2 Linear |R| = [32m2.259913e-05[39m\n2 KSP unpreconditioned resid norm 2.259913355890e-05 true resid norm 2.185523824273e-04 ||r(i)||/||b|| 3.548796394378e-06\nLinear solve converged due to CONVERGED_RTOL iterations 2\nKSP Object: 1 MPI processes\ntype: fgmres\nrestart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement\nhappy breakdown tolerance 1e-30\nmaximum iterations=20, initial guess is zero\ntolerances:  relative=1e-06, absolute=1e-50, divergence=1e+100\nright preconditioning\nusing UNPRECONDITIONED norm type for convergence test\nPC Object: 1 MPI processes\ntype: bjacobi\nnumber of blocks = 1\nLocal solve is same for all blocks, in the following KSP and PC objects:\nKSP Object: (sub) 1 MPI processes\ntype: preonly\nmaximum iterations=10000, initial guess is zero\ntolerances:  relative=1e-05, absolute=1e-50, divergence=10000.\nleft preconditioning\nusing NONE norm type for convergence test\nPC Object: (sub_) 1 MPI processes\ntype: ilu\nout-of-place factorization\n4 levels of fill\ntolerance for zero pivot 2.22045e-14\nmatrix ordering: natural\nfactor fill ratio given 1., needed 20.6539\nFactored matrix follows:\nMat Object: 1 MPI processes\ntype: seqaij\nrows=6749, cols=6749\npackage used to perform factorization: petsc\ntotal: nonzeros=9791613, allocated nonzeros=9791613\ntotal number of mallocs used during MatSetValues calls=0\nusing I-node routines: found 4137 nodes, limit used is 5\nlinear system matrix = precond matrix:\nMat Object: () 1 MPI processes\ntype: seqaij\nrows=6749, cols=6749\ntotal: nonzeros=474081, allocated nonzeros=474081\ntotal number of mallocs used during MatSetValues calls=0\nnot using I-node routines\nlinear system matrix followed by preconditioner matrix:\nMat Object: 1 MPI processes\ntype: mffd\nrows=6749, cols=6749\nMatrix-free approximation:\nerr=1.49012e-08 (relative error in function evaluation)\nUsing wp compute h routine\nDoes not compute normU\nMat Object: () 1 MPI processes\ntype: seqaij\nrows=6749, cols=6749\ntotal: nonzeros=474081, allocated nonzeros=474081\ntotal number of mallocs used during MatSetValues calls=0\nnot using I-node routines\n1 Nonlinear |R| = [32m7.386756e-01[39m\n|residual|_2 of individual variables:\ntemperature:     0.738676\nbinder_fraction: 0.000231372\n0 Linear |R| = [32m7.386756e-01[39m\n0 KSP unpreconditioned resid norm 7.386756405972e-01 true resid norm 7.386756405972e-01 ||r(i)||/||b|| 1.000000000000e+00\nAfter this line the program exits with the following response on the command line:\napplication called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\n[unset]: write_line error; fd=-1 buf=:cmd=abort exitcode=1\n:\nsystem msg for write_line failure : Bad file descriptor\nI'm not sure how to go about debugging this issue, so any help would be greatly appreciated.\nThanks!\ndusty",
          "url": "https://github.com/idaholab/moose/discussions/16615",
          "updatedAt": "2022-07-01T01:56:03Z",
          "publishedAt": "2021-01-04T15:55:51Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hi Dusty\nCan you run this through a debugger, see moose-debugging ?\nThis will help locate the issue.\nBest,\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-260362",
                  "updatedAt": "2022-07-01T01:56:17Z",
                  "publishedAt": "2021-01-04T16:02:33Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "fdkong"
                  },
                  "bodyText": "Add a breakpoint on MPI_Abort using your favorite debugger",
                  "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-260372",
                  "updatedAt": "2022-07-01T01:56:22Z",
                  "publishedAt": "2021-01-04T16:05:30Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "dcolema4"
                          },
                          "bodyText": "So after adding breakpoints for MPI_Abort and exit I get:\nInferior 1 (process 20352) exited with code 0220\nHowever, no breakpoint seemed to be reached. I'm using gdb to debug but I'm also not very familiar with it's operation.",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-260682",
                          "updatedAt": "2022-07-01T01:56:22Z",
                          "publishedAt": "2021-01-04T17:55:54Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "can you type 'bt' in GDB to get the backtrace",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-260686",
                          "updatedAt": "2022-07-01T01:56:22Z",
                          "publishedAt": "2021-01-04T17:57:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dcolema4"
                          },
                          "bodyText": "'bt' results in 'No stack.'",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-260717",
                          "updatedAt": "2022-07-01T01:56:22Z",
                          "publishedAt": "2021-01-04T18:06:08Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "It really should break on MPI_Abort. How did you set the breakpoint? The backtrace would be really helpful",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-260878",
                          "updatedAt": "2023-02-16T16:53:38Z",
                          "publishedAt": "2021-01-04T19:05:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dcolema4"
                          },
                          "bodyText": "I'm running gdb on the debug version of the code. The following shows my gdb inputs:\n(gdb) b MPI_Abort\nFunction \"MPI_Abort\" not defined.\nMake breakpoint pending on future shared library load? (y or [n]) y\nBreakpoint 1 (MPI_Abort) pending.\n(gdb) r",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-260931",
                          "updatedAt": "2023-02-16T16:53:38Z",
                          "publishedAt": "2021-01-04T19:26:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "It's odd that MPI_Abort isn't found. Which MPI distribution are you using?",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-260981",
                          "updatedAt": "2023-02-16T16:53:38Z",
                          "publishedAt": "2021-01-04T19:45:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "Are you using moose-mpich (delivered via  conda)?",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-261047",
                          "updatedAt": "2023-02-16T16:53:39Z",
                          "publishedAt": "2021-01-04T20:07:54Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dcolema4"
                          },
                          "bodyText": "I installed MOOSE using the conda environment method so I'm assuming it is using moose-mpich",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-261168",
                          "updatedAt": "2023-02-16T16:53:34Z",
                          "publishedAt": "2021-01-04T20:59:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Is it called PMPI_Abort in that one?",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-261307",
                          "updatedAt": "2023-02-16T16:53:34Z",
                          "publishedAt": "2021-01-04T22:04:57Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dcolema4"
                          },
                          "bodyText": "With both MPI_Abort and PMPI_Abort I'm getting this output:\nPerforming automatic scaling calculation\n0 Nonlinear |R| = 6.158493e+01\n[Thread 0x7fffeb7b0700 (LWP 23304) exited]\n--Type  for more, q to quit, c to continue without paging--\n[Inferior 1 (process 23255) exited with code 0220]\n(gdb) bt\nNo stack.\n(gdb)",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-261322",
                          "updatedAt": "2023-02-16T16:53:34Z",
                          "publishedAt": "2021-01-04T22:12:49Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "dcolema4"
                  },
                  "bodyText": "gdb --args ../my-app/my-app-dbg -i debug_input.i\nReading symbols from ../my-app/my-app-dbg...\n(gdb) b MPI_Abort\nFunction \"MPI_Abort\" not defined.\nMake breakpoint pending on future shared library load? (y or [n]) y\nBreakpoint 1 (MPI_Abort) pending.\n(gdb) r\nStarting program: /home/colemand/projects/my-app/my-app-dbg -i debug_input.i\nLots of initialization here...\n...........\n0 Nonlinear |R| = 6.158493e+01\n[Thread 0x7fffeb7b0700 (LWP 23304) exited]\n--Type for more, q to quit, c to continue without paging--\n[Inferior 1 (process 23255) exited with code 0220]\n(gdb) bt\nNo stack.\n(gdb)",
                  "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-261341",
                  "updatedAt": "2022-07-01T01:56:22Z",
                  "publishedAt": "2021-01-04T22:25:48Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "dcolema4"
                          },
                          "bodyText": "When I use lldb I don't get the last line you have there. Instead I get:\n(lldb) b MPI_Abort\nBreakpoint 1: no locations (pending).\nWARNING:  Unable to resolve breakpoint to any actual locations.",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-261354",
                          "updatedAt": "2022-07-01T01:56:22Z",
                          "publishedAt": "2021-01-04T22:31:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "@milljm Do you have any thought on this? Did we use \"-g\" when compiling mpich?",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-263694",
                          "updatedAt": "2022-07-01T01:56:22Z",
                          "publishedAt": "2021-01-05T23:20:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "@dcolema4 appears to be running Linux (/home instead of /Users as it would be on the Mac). And is using LLVM compilers (lldb)... which we do not create.\nSo no... I have no idea how mpich was created on this machine. Conda will not give you LLVM compilers when installing moose-libmesh, moose-petsc, etc on Linux machines.\nThis would appear to be a custom build at first glance (unless your use of /home was a typo).",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-264910",
                          "updatedAt": "2022-07-01T01:56:22Z",
                          "publishedAt": "2021-01-06T14:54:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dcolema4"
                          },
                          "bodyText": "I'm using Ubuntu on WSL and originally went through the conda environment installation. Looking again I see there I may not have executed the hostname change on my Windows side. Perhaps that's the issue?",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-264991",
                          "updatedAt": "2022-08-02T15:21:11Z",
                          "publishedAt": "2021-01-06T15:23:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Looking again, I see you are also using gdb (GCC). That and you mentioning conda, probably means you are using conda :)\nI don't think -g is being thrown when building moose-mpich. But I am not 100% sure. I'll have to dig in and watch a build of that package to know for sure.\nI don't know what -g actually does...",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-265008",
                          "updatedAt": "2022-08-02T15:21:11Z",
                          "publishedAt": "2021-01-06T15:29:59Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "friedmud"
                  },
                  "bodyText": "My guess here is is that you have crossed up your MPI installations somehow (i.e. you are compiling/linking against one MPI and running with a different one).  Can you describe what your installation process was like?",
                  "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-265017",
                  "updatedAt": "2022-07-01T01:56:22Z",
                  "publishedAt": "2021-01-06T15:33:41Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "dcolema4"
                          },
                          "bodyText": "I followed the process listed here: https://mooseframework.inl.gov/getting_started/installation/conda.html\nHowever, I did miss the WSL info.",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-265029",
                          "updatedAt": "2022-07-01T01:56:22Z",
                          "publishedAt": "2021-01-06T15:38:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "friedmud"
                          },
                          "bodyText": "Did you already have some other MPI installed on your system?\nAny chance you could start \"clean\" and make another attempt at it?",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-265312",
                          "updatedAt": "2022-07-01T01:56:22Z",
                          "publishedAt": "2021-01-06T17:48:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dcolema4"
                          },
                          "bodyText": "Not that I'm aware of. I'm starting a clean install now.",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-265340",
                          "updatedAt": "2022-07-01T01:56:22Z",
                          "publishedAt": "2021-01-06T18:01:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dcolema4"
                          },
                          "bodyText": "After doing a clean install I'm trying to run the test shown by @fdkong above. It appears that the mpi library isn't being found.\n(moose) colemand@L2006040A:/projects/moose/test/tests/kernels/2d_diffusion$ gdb --args ../../../moose_test-dbg -i 2d_diffusion_test.i\nGNU gdb (Ubuntu 9.2-0ubuntu120.04) 9.2\nCopyright (C) 2020 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later http://gnu.org/licenses/gpl.html\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\nType \"show copying\" and \"show warranty\" for details.\nThis GDB was configured as \"x86_64-linux-gnu\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\nhttp://www.gnu.org/software/gdb/bugs/.\nFind the GDB manual and other documentation resources online at:\nhttp://www.gnu.org/software/gdb/documentation/.\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from ../../../moose_test-dbg...\n(gdb) b MPI_Abort\nFunction \"MPI_Abort\" not defined.\nMake breakpoint pending on future shared library load? (y or [n]) y\nBreakpoint 1 (MPI_Abort) pending.\n(gdb)",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-267481",
                          "updatedAt": "2022-08-02T15:21:15Z",
                          "publishedAt": "2021-01-07T17:17:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "friedmud"
                          },
                          "bodyText": "That's ok - the MPI_Abort may not be loaded yet at the beginning - are you still getting similar errors to what you were?",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-267553",
                          "updatedAt": "2022-08-02T15:21:18Z",
                          "publishedAt": "2021-01-07T17:58:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dcolema4"
                          },
                          "bodyText": "I am. When I run the debugger on my input file it gets to the end of the first nonlinear iteration to show:\n0 Nonlinear |R| = 6.158493e+01\n[Thread 0x7fffeb7b0700 (LWP 23304) exited]\n--Type for more, q to quit, c to continue without paging--\nThen no matter what I type it exits out showing:\n[Inferior 1 (process 23255) exited with code 0220]\n(gdb)\nI'm kind of at a loss for what to do.",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-267571",
                          "updatedAt": "2022-08-02T15:21:18Z",
                          "publishedAt": "2021-01-07T18:10:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "friedmud"
                          },
                          "bodyText": "That is absolutely bizarre.  To be able to make it that far and then go wrong is just odd.\nI literally can't think of anything that would cause that.\n@milljm I'm punting - any other ideas?",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-267826",
                          "updatedAt": "2022-08-02T15:21:18Z",
                          "publishedAt": "2021-01-07T20:49:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I am going to try and debug that same problem on my windows WSL... just gotta build in debug mode on my little 8 core machine :)\nI'll post when I am able/done. The only thing that comes to mind, is if the Windows host file is not configured to use MPI. But @dcolema4 mentioned they saw that step later on, so... I figure that was completed.",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-269385",
                          "updatedAt": "2022-08-02T15:21:19Z",
                          "publishedAt": "2021-01-08T15:18:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Here are my results:\nmilljm@DESKTOP-M0OU23R:~/projects/moose/test/tests/kernels/simple_diffusion$ gdb --args ../../../moose_test-dbg -i simple_diffusion.i\nGNU gdb (GDB) 9.2\nCopyright (C) 2020 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\nType \"show copying\" and \"show warranty\" for details.\nThis GDB was configured as \"x86_64-conda-linux-gnu\".\nType \"show configuration\" for configuration details.\nFor bug reporting instructions, please see:\n<http://www.gnu.org/software/gdb/bugs/>.\nFind the GDB manual and other documentation resources online at:\n    <http://www.gnu.org/software/gdb/documentation/>.\n\nFor help, type \"help\".\nType \"apropos word\" to search for commands related to \"word\"...\nReading symbols from ../../../moose_test-dbg...\n(gdb) b MPI_ABORT\nFunction \"MPI_ABORT\" not defined.\nMake breakpoint pending on future shared library load? (y or [n]) y\nBreakpoint 1 (MPI_ABORT) pending.\n(gdb) r\nStarting program: /home/milljm/projects/moose/test/moose_test-dbg -i simple_diffusion.i\n[Thread debugging using libthread_db enabled]\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n\n[New Thread 0x7f0875f20700 (LWP 373)]\n[Thread 0x7f0875f20700 (LWP 373) exited]\n[New Thread 0x7f0875f20700 (LWP 374)]\n[Thread 0x7f0875f20700 (LWP 374) exited]\n[New Thread 0x7f0875f20700 (LWP 375)]\n[Thread 0x7f0875f20700 (LWP 375) exited]\n[New Thread 0x7f0875f20700 (LWP 376)]\n[Thread 0x7f0875f20700 (LWP 376) exited]\n[New Thread 0x7f0875f20700 (LWP 377)]\n[Thread 0x7f0875f20700 (LWP 377) exited]\n[New Thread 0x7f0875f20700 (LWP 378)]\n[Thread 0x7f0875f20700 (LWP 378) exited]\n[New Thread 0x7f0875f20700 (LWP 379)]\n[Thread 0x7f0875f20700 (LWP 379) exited]\n[New Thread 0x7f0875f20700 (LWP 380)]\n[Thread 0x7f0875f20700 (LWP 380) exited]\n[New Thread 0x7f0875f20700 (LWP 381)]\n[Thread 0x7f0875f20700 (LWP 381) exited]\n[New Thread 0x7f0875f20700 (LWP 382)]\n[New Thread 0x7f0875f20700 (LWP 383)]\n[Thread 0x7f0875f20700 (LWP 382) exited]\n[Thread 0x7f0875f20700 (LWP 383) exited]\n[New Thread 0x7f0875f20700 (LWP 384)]\n[Thread 0x7f0875f20700 (LWP 384) exited]\n[New Thread 0x7f0875f20700 (LWP 385)]\n[Thread 0x7f0875f20700 (LWP 385) exited]\n[New Thread 0x7f0875f20700 (LWP 386)]\n[Thread 0x7f0875f20700 (LWP 386) exited]\n[New Thread 0x7f0875f20700 (LWP 387)]\n[Thread 0x7f0875f20700 (LWP 387) exited]\n[New Thread 0x7f0875f20700 (LWP 388)]\n[Thread 0x7f0875f20700 (LWP 388) exited]\n[New Thread 0x7f0875f20700 (LWP 389)]\n[Thread 0x7f0875f20700 (LWP 389) exited]\n[New Thread 0x7f0875f20700 (LWP 390)]\n[Thread 0x7f0875f20700 (LWP 390) exited]\n\nFramework Information:\nMOOSE Version:           git commit 9937196945 on 2020-08-05\nLibMesh Version:         4bf75a35d0f989335a2eb716912c3de4e7be20ea\nPETSc Version:           3.14.2\nSLEPc Version:           3.14.0\nCurrent Time:            Fri Jan  8 08:31:32 2021\nExecutable Timestamp:    Fri Jan  8 08:20:37 2021\n\nParallelism:\n  Num Processors:          1\n  Num Threads:             1\n\nMesh:\n  Parallel Type:           replicated\n  Mesh Dimension:          2\n  Spatial Dimension:       2\n  Nodes:\n    Total:                 121\n    Local:                 121\n  Elems:\n    Total:                 100\n    Local:                 100\n  Num Subdomains:          1\n  Num Partitions:          1\n\nNonlinear System:\n  Num DOFs:                121\n  Num Local DOFs:          121\n  Variables:               \"u\"\n  Finite Element Types:    \"LAGRANGE\"\n  Approximation Orders:    \"FIRST\"\n\nExecution Information:\n  Executioner:             Steady\n  Solver Mode:             Preconditioned JFNK\n  PETSc Preconditioner:    hypre boomeramg\n\n[New Thread 0x7f0875f20700 (LWP 391)]\n[Thread 0x7f0875f20700 (LWP 391) exited]\n[New Thread 0x7f0875f20700 (LWP 392)]\n[Thread 0x7f0875f20700 (LWP 392) exited]\n[New Thread 0x7f0875f20700 (LWP 393)]\n[Thread 0x7f0875f20700 (LWP 393) exited]\n[New Thread 0x7f0875f20700 (LWP 394)]\n[Thread 0x7f0875f20700 (LWP 394) exited]\n[New Thread 0x7f0875f20700 (LWP 395)]\n[Thread 0x7f0875f20700 (LWP 395) exited]\n[New Thread 0x7f0875f20700 (LWP 396)]\n[New Thread 0x7f0875f20700 (LWP 397)]\n[Thread 0x7f0875f20700 (LWP 396) exited]\n[Thread 0x7f0875f20700 (LWP 397) exited]\n[New Thread 0x7f0875f20700 (LWP 398)]\n[Thread 0x7f0875f20700 (LWP 398) exited]\n[New Thread 0x7f0875f20700 (LWP 399)]\n[Thread 0x7f0875f20700 (LWP 399) exited]\n[New Thread 0x7f0875f20700 (LWP 400)]\n[Thread 0x7f0875f20700 (LWP 400) exited]\n[New Thread 0x7f0875f20700 (LWP 401)]\n[Thread 0x7f0875f20700 (LWP 401) exited]\n[New Thread 0x7f0875f20700 (LWP 402)]\n[New Thread 0x7f0875f20700 (LWP 403)]\n[Thread 0x7f0875f20700 (LWP 402) exited]\n[Thread 0x7f0875f20700 (LWP 403) exited]\n[New Thread 0x7f0875f20700 (LWP 404)]\n[Thread 0x7f0875f20700 (LWP 404) exited]\n[New Thread 0x7f0875f20700 (LWP 405)]\n[Thread 0x7f0875f20700 (LWP 405) exited]\n[New Thread 0x7f0875f20700 (LWP 406)]\n[Thread 0x7f0875f20700 (LWP 406) exited]\n[New Thread 0x7f0875f20700 (LWP 407)]\n[Thread 0x7f0875f20700 (LWP 407) exited]\n[New Thread 0x7f0875f20700 (LWP 408)]\n[Thread 0x7f0875f20700 (LWP 408) exited]\n[New Thread 0x7f0875f20700 (LWP 409)]\n[Thread 0x7f0875f20700 (LWP 409) exited]\n[New Thread 0x7f0875f20700 (LWP 410)]\n[New Thread 0x7f0875f20700 (LWP 411)]\n[Thread 0x7f0875f20700 (LWP 410) exited]\n[Thread 0x7f0875f20700 (LWP 411) exited]\n[New Thread 0x7f0875f20700 (LWP 412)]\n[Thread 0x7f0875f20700 (LWP 412) exited]\n[New Thread 0x7f0875f20700 (LWP 413)]\n[Thread 0x7f0875f20700 (LWP 413) exited]\n[New Thread 0x7f0875f20700 (LWP 414)]\n[Thread 0x7f0875f20700 (LWP 414) exited]\n[New Thread 0x7f0875f20700 (LWP 415)]\n 0 Nonlinear |R| = 3.082207e+00\n[Thread 0x7f0875f20700 (LWP 415) exited]\n[New Thread 0x7f0875f20700 (LWP 416)]\n      0 Linear |R| = 3.082207e+00\n[Thread 0x7f0875f20700 (LWP 416) exited]\n[New Thread 0x7f0875f20700 (LWP 417)]\n      1 Linear |R| = 1.034260e-01\n[Thread 0x7f0875f20700 (LWP 417) exited]\n[New Thread 0x7f0875f20700 (LWP 418)]\n      2 Linear |R| = 2.616261e-03\n[Thread 0x7f0875f20700 (LWP 418) exited]\n[New Thread 0x7f0875f20700 (LWP 419)]\n[Thread 0x7f0875f20700 (LWP 419) exited]\n      3 Linear |R| = 9.194723e-05\n[New Thread 0x7f0875f20700 (LWP 420)]\n      4 Linear |R| = 1.366911e-06\n[Thread 0x7f0875f20700 (LWP 420) exited]\n  Linear solve converged due to CONVERGED_RTOL iterations 4\n[New Thread 0x7f0875f20700 (LWP 421)]\n[Thread 0x7f0875f20700 (LWP 421) exited]\n 1 Nonlinear |R| = 1.373820e-06\n[New Thread 0x7f0875f20700 (LWP 422)]\n[Thread 0x7f0875f20700 (LWP 422) exited]\n      0 Linear |R| = 1.373820e-06\n[New Thread 0x7f0875f20700 (LWP 423)]\n[Thread 0x7f0875f20700 (LWP 423) exited]\n      1 Linear |R| = 3.507042e-08\n[New Thread 0x7f0875f20700 (LWP 424)]\n[Thread 0x7f0875f20700 (LWP 424) exited]\n      2 Linear |R| = 4.049791e-10\n[New Thread 0x7f0875f20700 (LWP 425)]\n      3 Linear |R| = 1.351613e-11\n[Thread 0x7f0875f20700 (LWP 425) exited]\n  Linear solve converged due to CONVERGED_RTOL iterations 3\n[New Thread 0x7f0875f20700 (LWP 426)]\n[Thread 0x7f0875f20700 (LWP 426) exited]\n 2 Nonlinear |R| = 1.352021e-11\nNonlinear solve converged due to CONVERGED_FNORM_RELATIVE iterations 2\n Solve Converged!\n[New Thread 0x7f0875f20700 (LWP 427)]\n[Thread 0x7f0875f20700 (LWP 427) exited]\n[New Thread 0x7f0875f20700 (LWP 428)]\n[Thread 0x7f0875f20700 (LWP 428) exited]\n[New Thread 0x7f0875f20700 (LWP 429)]\n[Thread 0x7f0875f20700 (LWP 429) exited]\n[New Thread 0x7f0875f20700 (LWP 430)]\n[Thread 0x7f0875f20700 (LWP 430) exited]\n\n ----------------------------------------------------------------------------\n| Reference count information                                                |\n ----------------------------------------------------------------------------\n| N7libMesh10FEAbstractE reference count information:\n|  Creations:    17\n|  Destructions: 17\n| N7libMesh10Parameters5ValueE reference count information:\n|  Creations:    5762\n|  Destructions: 5762\n| N7libMesh12SparseMatrixIdEE reference count information:\n|  Creations:    6\n|  Destructions: 6\n| N7libMesh13NumericVectorIdEE reference count information:\n|  Creations:    63\n|  Destructions: 63\n| N7libMesh15EquationSystemsE reference count information:\n|  Creations:    1\n|  Destructions: 1\n| N7libMesh15GhostingFunctorE reference count information:\n|  Creations:    6\n|  Destructions: 6\n| N7libMesh15NonlinearSolverIdEE reference count information:\n|  Creations:    1\n|  Destructions: 1\n| N7libMesh4ElemE reference count information:\n|  Creations:    108\n|  Destructions: 108\n| N7libMesh4NodeE reference count information:\n|  Creations:    121\n|  Destructions: 121\n| N7libMesh5QBaseE reference count information:\n|  Creations:    20\n|  Destructions: 20\n| N7libMesh6DofMapE reference count information:\n|  Creations:    2\n|  Destructions: 2\n| N7libMesh6SystemE reference count information:\n|  Creations:    2\n|  Destructions: 2\n| N7libMesh9DofObjectE reference count information:\n|  Creations:    229\n|  Destructions: 229\n ----------------------------------------------------------------------------\n--Type <RET> for more, q to quit, c to continue without paging--c\n[Inferior 1 (process 369) exited normally]\n(gdb)\n\nI had to install gdb via Conda as well. That might make a difference if you happened to install Ubuntu's system debugger instead (honestly can't imagine why).\nconda install gdb\nI ended up grabbing the latest version (9.2).\nI suppose the larger issue, is that my solve converges and works. Can you perform an ldd on moose_test-dbg? I want to see what libraries it is linked to:\nmilljm@DESKTOP-M0OU23R:~/projects/moose/test$ ldd moose_test-dbg\n        linux-vdso.so.1 (0x00007fffc030c000)\n        libmoose_test-dbg.so.0 => /home/milljm/projects/moose/test/lib/libmoose_test-dbg.so.0 (0x00007f8a41287000)\n        libmoose-dbg.so.0 => /home/milljm/projects/moose/framework/libmoose-dbg.so.0 (0x00007f8a3c9c9000)\n        libhit-dbg.so.0 => /home/milljm/projects/moose/framework/contrib/hit/libhit-dbg.so.0 (0x00007f8a3c868000)\n        libmesh_dbg.so.0 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/libmesh_dbg.so.0 (0x00007f8a38c97000)\n        libtimpi_dbg.so.2 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/libtimpi_dbg.so.2 (0x00007f8a38c46000)\n        libgfortran.so.4 => /home/milljm/libs/miniconda3/envs/newmoose/lib/libgfortran.so.4 (0x00007f8a38b17000)\n        libgcc_s.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/lib/libgcc_s.so.1 (0x00007f8a38b03000)\n        libstdc++.so.6 => /home/milljm/libs/miniconda3/envs/newmoose/lib/libstdc++.so.6 (0x00007f8a3898f000)\n        libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f8a38838000)\n        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f8a38630000)\n        libmpi.so.12 => /home/milljm/libs/miniconda3/envs/newmoose/lib/libmpi.so.12 (0x00007f8a382e5000)\n        libpcre-dbg.so.0 => /home/milljm/projects/moose/framework/contrib/pcre/libpcre-dbg.so.0 (0x00007f8a382b1000)\n        libvtkCommonCore-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/libvtkCommonCore-6.3.so.1 (0x00007f8a37e59000)\n        libvtkCommonDataModel-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/libvtkCommonDataModel-6.3.so.1 (0x00007f8a37b42000)\n        libvtkImagingCore-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/libvtkImagingCore-6.3.so.1 (0x00007f8a378e4000)\n        libvtkIOImage-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/libvtkIOImage-6.3.so.1 (0x00007f8a37768000)\n        libvtkImagingMath-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/libvtkImagingMath-6.3.so.1 (0x00007f8a3771f000)\n        libvtkCommonExecutionModel-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/libvtkCommonExecutionModel-6.3.so.1 (0x00007f8a37689000)\n        libpetsc.so.3.14 => /home/milljm/libs/miniconda3/envs/newmoose/lib/libpetsc.so.3.14 (0x00007f8a35ef0000)\n        libHYPRE-2.19.0.so => /home/milljm/libs/miniconda3/envs/newmoose/lib/libHYPRE-2.19.0.so (0x00007f8a35a23000)\n        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f8a35a00000)\n        libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f8a359f0000)\n        /lib64/ld-linux-x86-64.so.2 (0x00007f8a4245a000)\n        libgomp.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/lib/libgomp.so.1 (0x00007f8a359c3000)\n        libnetcdf.so.13 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/libnetcdf.so.13 (0x00007f8a35877000)\n        libvtkIOXML-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../libmesh-vtk/lib/libvtkIOXML-6.3.so.1 (0x00007f8a357a6000)\n        libvtkIOParallelXML-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../libmesh-vtk/lib/libvtkIOParallelXML-6.3.so.1 (0x00007f8a3577e000)\n        libvtkParallelMPI-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../libmesh-vtk/lib/libvtkParallelMPI-6.3.so.1 (0x00007f8a35767000)\n        libvtkParallelCore-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../libmesh-vtk/lib/libvtkParallelCore-6.3.so.1 (0x00007f8a3570e000)\n        libz.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/libz.so.1 (0x00007f8a356f4000)\n        libslepc.so.3.14 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/libslepc.so.3.14 (0x00007f8a35337000)\n        libparmetis.so => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/libparmetis.so (0x00007f8a352e6000)\n        libmetis.so => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/libmetis.so (0x00007f8a35272000)\n        libquadmath.so.0 => /lib/x86_64-linux-gnu/libquadmath.so.0 (0x00007f8a35220000)\n        librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f8a35210000)\n        libvtksys-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/./libvtksys-6.3.so.1 (0x00007f8a351c0000)\n        libvtkCommonMisc-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/./libvtkCommonMisc-6.3.so.1 (0x00007f8a351a9000)\n        libvtkCommonSystem-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/./libvtkCommonSystem-6.3.so.1 (0x00007f8a35193000)\n        libvtkCommonTransforms-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/./libvtkCommonTransforms-6.3.so.1 (0x00007f8a3515f000)\n        libvtkCommonMath-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/./libvtkCommonMath-6.3.so.1 (0x00007f8a3512d000)\n        libvtkpng-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/./libvtkpng-6.3.so.1 (0x00007f8a350fa000)\n        libvtktiff-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/./libvtktiff-6.3.so.1 (0x00007f8a3508b000)\n        libvtkmetaio-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/./libvtkmetaio-6.3.so.1 (0x00007f8a34fe8000)\n        libvtkDICOMParser-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/./libvtkDICOMParser-6.3.so.1 (0x00007f8a34fc7000)\n        libvtkzlib-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/./libvtkzlib-6.3.so.1 (0x00007f8a34fa6000)\n        libvtkjpeg-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh-vtk/lib/./libvtkjpeg-6.3.so.1 (0x00007f8a34f70000)\n        libstrumpack.so => /home/milljm/libs/miniconda3/envs/newmoose/lib/./libstrumpack.so (0x00007f8a345c6000)\n        libsuperlu_dist.so.6 => /home/milljm/libs/miniconda3/envs/newmoose/lib/./libsuperlu_dist.so.6 (0x00007f8a344b9000)\n        libmpifort.so.12 => /home/milljm/libs/miniconda3/envs/newmoose/lib/./libmpifort.so.12 (0x00007f8a3447a000)\n        libmfhdf.so.0 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/./libmfhdf.so.0 (0x00007f8a34444000)\n        libdf.so.0 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/./libdf.so.0 (0x00007f8a34390000)\n        libhdf5_hl.so.100 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/./libhdf5_hl.so.100 (0x00007f8a3436a000)\n        libhdf5.so.103 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/./libhdf5.so.103 (0x00007f8a33fc9000)\n        libcurl.so.4 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/./libcurl.so.4 (0x00007f8a33f3a000)\n        libvtkIOXMLParser-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../libmesh-vtk/lib/./libvtkIOXMLParser-6.3.so.1 (0x00007f8a33f21000)\n        libvtkIOCore-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../libmesh-vtk/lib/./libvtkIOCore-6.3.so.1 (0x00007f8a33ea9000)\n        libvtkIOLegacy-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../libmesh-vtk/lib/./libvtkIOLegacy-6.3.so.1 (0x00007f8a33e04000)\n        libjpeg.so.9 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/././libjpeg.so.9 (0x00007f8a33dc2000)\n        libnghttp2.so.14 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/././libnghttp2.so.14 (0x00007f8a33d9a000)\n        libssh2.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/././libssh2.so.1 (0x00007f8a33d56000)\n        libssl.so.1.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/././libssl.so.1.1 (0x00007f8a33cc6000)\n        libcrypto.so.1.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/././libcrypto.so.1.1 (0x00007f8a339fa000)\n        libgssapi_krb5.so.2 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/././libgssapi_krb5.so.2 (0x00007f8a339a1000)\n        libkrb5.so.3 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/././libkrb5.so.3 (0x00007f8a338c8000)\n        libk5crypto.so.3 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/././libk5crypto.so.3 (0x00007f8a338a9000)\n        libcom_err.so.3 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/././libcom_err.so.3 (0x00007f8a3c862000)\n        libvtkexpat-6.3.so.1 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../libmesh-vtk/lib/././libvtkexpat-6.3.so.1 (0x00007f8a33872000)\n        libkrb5support.so.0 => /home/milljm/libs/miniconda3/envs/newmoose/libmesh/lib/../../lib/./././libkrb5support.so.0 (0x00007f8a33861000)\n        libresolv.so.2 => /lib/x86_64-linux-gnu/libresolv.so.2 (0x00007f8a33840000)",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-269446",
                          "updatedAt": "2022-08-02T15:21:19Z",
                          "publishedAt": "2021-01-08T15:40:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dcolema4"
                          },
                          "bodyText": "@milljm I get the same results as you when I run the 2d diffusion case. It is on my problem where the issue is showing up. Despite not being able to get the debugger to work, I was able to determine that the issue was coming from a vector material property. I still don't know why it caused the program to fail, but when I changed it to a scalar and updated my kernels to accept the scalar value everything worked fine.",
                          "url": "https://github.com/idaholab/moose/discussions/16615#discussioncomment-274646",
                          "updatedAt": "2022-08-02T15:21:26Z",
                          "publishedAt": "2021-01-11T15:26:23Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "CompositeElasticityTensor for crystal plasticity",
          "author": {
            "login": "Bala-1005"
          },
          "bodyText": "Hello everyone,\nI am working on coupling phase field and crystal plasticity. I have used ComputeElasticityTensorCP for describing the elasticity tensors for different phases in the system. I would like to combine them as in normal tensor mechanics using CompositeElasticityTensor.\nDoes crystal plasticity allow using CompositeElasticityTensor for its computations? Or should I create a custom material to achieve this functionality?\nThanks,\nBala",
          "url": "https://github.com/idaholab/moose/discussions/16638",
          "updatedAt": "2022-06-13T08:36:36Z",
          "publishedAt": "2021-01-08T01:22:46Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "ngrilli"
                  },
                  "bodyText": "Dear Bala @Bala-1005\nThe specific feature of ComputeElasticityTensorCP is the usage of the Euler angle reader.\nIf you want to use FiniteStrainCrystalPlasticity, you will always need those Euler angles.\nYou can start from another object to calculate the elasticity tensor, like CompositeElasticityTensor, and create your custom object in which\nyou add the Euler angle reader part of ComputeElasticityTensorCP.\nI did something similar here:\nhttps://github.com/ngrilli/c_pfor_am/blob/main/src/materials/ComputeElasticityTensorMelting.C\nHope this helps,\nBest Regards,\nNicol\u00f2 Grilli\nNational University of Singapore",
                  "url": "https://github.com/idaholab/moose/discussions/16638#discussioncomment-273477",
                  "updatedAt": "2022-06-13T08:36:38Z",
                  "publishedAt": "2021-01-11T03:23:35Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Bala-1005"
                          },
                          "bodyText": "Thank you Nicolo @ngrilli .I will take a look at your code and see if I can come up with something similar for compositeelasticity.\nBala",
                          "url": "https://github.com/idaholab/moose/discussions/16638#discussioncomment-273494",
                          "updatedAt": "2022-06-13T08:36:38Z",
                          "publishedAt": "2021-01-11T03:44:00Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "MeshGenerators during simulation?",
          "author": {
            "login": "aeslaughter"
          },
          "bodyText": "#16060 brings up a question. At what point should we invest time to enable the running a MeshGenerator after each timestep to allow dynamic mesh modification? For the problem in #16060 this would allow define/redefine the subdomains based on the level set? @friedmud @permcody @lindsayad @fdkong\nIf there is desire and funding I would love to implement that, I have always thought it would be useful for this reason. There is a version of this in Mastodon that uses Aux variables that could go away as well.",
          "url": "https://github.com/idaholab/moose/discussions/16635",
          "updatedAt": "2022-07-19T19:30:08Z",
          "publishedAt": "2021-01-07T21:35:30Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "fdkong"
                  },
                  "bodyText": "The mesh modification after each time step is already possible. Here is an example #16008\nAre you asking for regenerating a different mesh  after each time step?",
                  "url": "https://github.com/idaholab/moose/discussions/16635#discussioncomment-268014",
                  "updatedAt": "2022-07-19T19:27:37Z",
                  "publishedAt": "2021-01-07T22:43:56Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "I would like to modify the subdomains",
                          "url": "https://github.com/idaholab/moose/discussions/16635#discussioncomment-269956",
                          "updatedAt": "2022-07-19T19:27:37Z",
                          "publishedAt": "2021-01-08T19:03:38Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "YaqiWang"
                  },
                  "bodyText": "Will the entire mesh be regenerated or just applying some mesh generators on the existing mesh? This sounds to me should be done in a specific user object with meshChanged function. This may complicate our design a lot.",
                  "url": "https://github.com/idaholab/moose/discussions/16635#discussioncomment-268061",
                  "updatedAt": "2022-07-19T19:27:40Z",
                  "publishedAt": "2021-01-07T23:10:37Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "I think the phrase \"dynamic mesh modification\" is too general. There are mesh cutting, interface movement, element addition/deletion, dynamic remeshing, etc. How do you handle stateful properties in all scenarios in a unified way?",
                  "url": "https://github.com/idaholab/moose/discussions/16635#discussioncomment-268171",
                  "updatedAt": "2022-07-19T19:27:50Z",
                  "publishedAt": "2021-01-08T01:14:07Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "ngrilli"
                  },
                  "bodyText": "I am currently testing the approach in #16008\nto move elements from on subdomain to another.\nThis is useful for me to add elements for additive manufacturing simulations.\nThe material properties in the added elements are reinitialized properly, for instance plastic deformation\nYou can find some example tests here:\nhttps://github.com/ngrilli/c_pfor_am/tree/main/test/tests/ElementAddDelete\nI am currently having some negative Jacobian error when I run large simulations with lots of element deleted at each time step\nand I am investigating the issue. But I think it's not related to the method itself, it's my material model.\nBest Regards,\nNicol\u00f2 Grilli\nNational University of Singapore",
                  "url": "https://github.com/idaholab/moose/discussions/16635#discussioncomment-270948",
                  "updatedAt": "2022-07-19T19:27:52Z",
                  "publishedAt": "2021-01-09T11:25:15Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Call function in kernel or materials",
          "author": {
            "login": "kunokchang"
          },
          "bodyText": "I posted a question yesterday, but it seems to be lacking in explanation, so I will replenish it.\nI would like to call our custom function in kernel or materials.\nMaterials]\n[./bulk_free_energy]\ntype = DerivativeParsedMaterial\nf_name = fb\nargs = 'eta1'\nconstant_names = 'a'\nconstant_expressions = '2.057'\nfunction = 'a*((eta1)^2'\nderivative_order = 2\n[../]\nI need to add result of Fermi-Dirac function like a*((eta1)^2+F_{1/2}(eta1)\nWhich is given here\n[Functions]\n[./fermi]\ntype = PiecewiseLinear\ndata_file = fermi.csv\nformat = columns\n[../]\n[]\nfermi.csv is given by (part of it)\n-7.9899997711181641        2.1235612216047891E-004\n-7.9800000190734863        2.1449011174362248E-004\n-7.9699997901916504        2.1664564757151034E-004\n-7.9600000381469727        2.1882273951932466E-004\n-7.9499998092651367        2.2102181283188398E-004\n-7.9400000572204590        2.2324287758167501E-004\n-7.9299998283386230        2.2548636760039243E-004\n-7.9200000762939453        2.2775229316172307E-004\n-7.9099998474121094        2.3004109685755749E-004\n-7.9000000953674316        2.3235278916675431E-004\n-7.8899998664855957        2.3468782161821161E-004\nBut we are not clear how can we call result of Fermi-Dirac function with argument eta1.\nIf there is a better way to do this, we would really appreciate any comments.",
          "url": "https://github.com/idaholab/moose/discussions/16297",
          "updatedAt": "2022-06-14T11:54:58Z",
          "publishedAt": "2020-11-25T01:09:17Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "laagesen"
                  },
                  "bodyText": "Hi Kunok, I just commented on the other post, but let me amplify a little bit based on the extra detail in this post. Since you want the Fermi-Dirac distribution as a function of eta1, you can just add that term to the function you are defining in your DerivativeParsedMaterial, using the analytical form shown here:\nhttps://www.doitpoms.ac.uk/tlplib/semiconductors/fermi.php",
                  "url": "https://github.com/idaholab/moose/discussions/16297#discussioncomment-138689",
                  "updatedAt": "2022-06-14T12:12:49Z",
                  "publishedAt": "2020-11-25T15:19:52Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "kunokchang"
                          },
                          "bodyText": "Dear Laagesen\nSorry for confusion we cause. We want to calculate Fermi-Dirac integration, not distribution function.\nThe analytic form is given below and take integration is not a trivial job so we try to interpolate it.\nhttps://en.wikipedia.org/wiki/Complete_Fermi%E2%80%93Dirac_integral",
                          "url": "https://github.com/idaholab/moose/discussions/16297#discussioncomment-139097",
                          "updatedAt": "2022-06-14T12:13:15Z",
                          "publishedAt": "2020-11-26T00:46:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "kunokchang"
                          },
                          "bodyText": "We try to evaluate F_{1/2}(eta)",
                          "url": "https://github.com/idaholab/moose/discussions/16297#discussioncomment-139098",
                          "updatedAt": "2022-06-14T12:13:15Z",
                          "publishedAt": "2020-11-26T00:46:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "laagesen"
                          },
                          "bodyText": "I see-  so are you assuming F_{1/2}(eta) is not changing at each time step and using the same .csv file at each time step? Or are you wanting to output eta, interpolate it using the method that is generating the .csv file, then read it back in?",
                          "url": "https://github.com/idaholab/moose/discussions/16297#discussioncomment-140108",
                          "updatedAt": "2022-06-14T12:13:15Z",
                          "publishedAt": "2020-11-27T14:43:57Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "kunokchang"
                          },
                          "bodyText": "We assume F_{1/2}(eta) is not changing at each time step and using the same .csv file, however, eta is time-dependent term.",
                          "url": "https://github.com/idaholab/moose/discussions/16297#discussioncomment-140273",
                          "updatedAt": "2022-06-14T12:14:42Z",
                          "publishedAt": "2020-11-27T22:46:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "laagesen"
                          },
                          "bodyText": "Hm, I don't think there is a way to do what you are wanting to with the PiecewiseLinear function. PiecewiseLinear assumes the first part of the data in the csv file is either time (by default) or x,y, or z positions (with optional argument). See https://mooseframework.inl.gov/source/functions/PiecewiseLinear.html for more details on this. It's not set up to take problem variables as an argument.\nAn alternative that I think would work would be to use the PiecewiseLinearInterpolationMaterial. See here for details and examples. https://mooseframework.inl.gov/source/materials/PiecewiseLinearInterpolationMaterial.html\nYou would use this to create a material property that depends on eta1. Unfortunately it does not look like it has the capability to read in a csv file to get the x-y pairs, so you would have to specify them in the input file (although you could probably add the functionality to read csv files pretty easily using the code from the PiecewiseLinear function as an example). So PiecewiseLinearInterpolationMaterial would create a material property, say it's called F. You would then make that available to your existing DerivativeParsedMaterial called bulk_free_energy using material_property_names = F.\nIt does look like PiecewiseLinearInterpolationMaterial calculates derivatives, which I'm assuming you're going to need since most phase-field models need the derivative of the free energy. However just be aware that since you are using linear interpolation, all of your second derivatives are going to be zero; these are used to calculate Jacobians by the preconditioners, so convergence in your solve may be hurt.",
                          "url": "https://github.com/idaholab/moose/discussions/16297#discussioncomment-142897",
                          "updatedAt": "2022-06-14T12:14:42Z",
                          "publishedAt": "2020-11-30T17:09:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "kunokchang"
                          },
                          "bodyText": "Sorry for delayed response. I gave my grades at the end of the semester, and now I am back with this problem.\nThe answers you gave are very helpful in approaching the problem. I would like to express our sincere thanks.\nWe declare PiecewiseLinearInterpolationMaterial property as below and we want to call it in DerivativeParsedMaterial and is it possible to pass fermi_test(eta1/2.0) like this way? We are not clear how to pass argument of function.\n[./fermi_test]\ntype = PiecewiseLinearInterpolationMaterial\nproperty = fermi_test\nvariable = fermi\nx = '2.3235278916675431E-004 2.5678716657140193E-004 2.8379094144941774E-004 3.1363395344228466E-004 3.4661480184293897E-004 3.8306332338290775E-004 4.2334400591023457E-004 4.6785984730409537E-004 5.1705550309345436E-004 5.7142299341010746E-004 6.3150577678226284E-004 6.9790436428065117E-004 7.7128266457680090E-004 8.5237315288779500E-004 9.4198625416296500E-004 1.0410170287445100E-003 1.1504543904029938E-003 1.2713915208859713E-003 1.4050343362219744E-003 1.5527168865464539E-003 1.7159122952076673E-003 1.8962478534793204E-003 2.0955220648618544E-003 2.3157184525443913E-003 2.5590307589031741E-003 2.8278807548148058E-003 3.1249428476409127E-003 3.4531718412657711E-003 3.8158252930559911E-003 4.2165045470864424E-003 4.6591834873861418E-003 5.1482483588226420E-003 5.6885426181912955E-003 6.2854027000215241E-003 6.9447242909850462E-003 7.6730081043849289E-003 8.4774235261575424E-003 9.3658801128583824E-003 1.0347083644346118E-002 1.1430641727716809E-002 1.2627134940562407E-002 1.3948216455171785E-002 1.5406723459013859E-002 1.7016761909108134E-002 1.8793871039066580E-002 2.0755129368957407E-002 2.2919305158910545E-002 2.5307023402706061E-002 2.7940885872608706E-002 3.0845717761828609E-002 3.4048713809169372E-002 3.7579651543448035E-002 4.1471124785238581E-002 4.5758692787145720E-002 5.0481226394378478E-002 5.5681079412960861E-002  6.1404356606654949E-002  6.7701199554686134E-002  7.4625918988119933E-002  8.2237426265194619E-002  9.0599359529987766E-002  9.9780341016472748E-002 0.10985423431455218 0.12090012503482848 0.13300274604701343 0.14625237774326805 0.16074492035514423 0.17658192596875985 0.19387015748734349 0.21272181018402139 0.23325387611972009 0.25558775599966849 0.27984879160089887 0.30616507367908830 0.33466728239940641 0.36548725419244205 0.39875699587613084 0.43460766605346784 0.47316760840632061 0.51456206161858054 0.55891138443652488 0.60632883599122311 0.65692179481661872 0.71078722503159686 0.76801459136108396 0.82868157559417333 0.89285491195281386 0.96059203187739461 1.0319365775923359 1.1069236669593814 1.1855755056991080 1.2679037300406737 1.3539125176629796 1.4435937108406798 1.5369341623179518 1.6339106689096636 1.7344932376333677 1.8386489768331284 1.9463361365213048 2.0575128598853629 2.1721308396961607 2.2901389570235700 2.4114874479487196 2.5361205226177570 2.6639861303092491 2.7950281621238084 2.9291902642563219 3.0664201414726628 3.2066608072888370 3.3498612866938982 3.4959675214463499 3.6449264163040018 3.7966903498567839 3.9512072564175238 4.1084322668677187 4.2683174995857973 4.4308163880114391 4.5958884506057514 4.7634883782231263 4.9335786157998385 5.1061182183151681 5.2810675193328223 5.4583932138892495 5.6380565883668146 5.8200270067518005 6.0042699264614274 6.1907518921002884 6.3794459195002933 6.5703189478053332 6.7633461920765487 6.9584984147593936 7.1557472593657199 7.3550709491787369 7.5564410324326055 7.7598375481000117 7.9652355894842275 8.1726109369660396 8.3819460365873244 8.5932160735643013 8.8064049118334271 9.0214910110319586 9.2384533674000124 9.4572777810297612 9.6779423044161685 9.9004339103835033 10.124733776288622 10.350823490228317 10.578691569533502 10.808318301835646 11.039693111246226 11.272799257988478 11.507620323701010 11.744146994171068 11.982361330444245 12.222254797793555 12.463812384186419 12.707019323593313 12.951868106324753 13.198342188637293 13.446434662421453 '\ny = '-7.9 -7.8 -7.7 -7.6 -7.5 -7.4 -7.3 -7.2 -7.1 -7.0 -6.9 -6.8 -6.7 -6.6 -6.5 -6.4 -6.3 -6.2 -6.1 -6.0 -5.9 -5.8 -5.7 -5.6 -5.5 -5.4 -5.3 -5.2 -5.1 -5.0 -4.9 -4.8 -4.7 -4.6 -4.5 -4.4 -4.3 -4.2 -4.1 -4.0 -3.9 -3.8 -3.7 -3.6 -3.5 -3.4 -3.3 -3.2 -3.1 -3.0 -2.9 -2.8 -2.7 -2.6 -2.5 -2.4 -2.3 -2.2 -2.1 -2.0 -1.9 -1.8 -1.7 -1.6 -1.5 -1.4 -1.3 -1.2 -1.1 -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5.0 5.1 5.2 5.3 5.4 5.5 5.6 5.7 5.8 5.9 6.0 6.1 6.2 6.3 6.4 6.5 6.6 6.7 6.8 6.9 7.0 7.1 7.2 7.3 7.4 7.5 7.6 7.7 7.8 7.9 8.0'\n[../]\n[./bulk_free_energy]\ntype = DerivativeParsedMaterial\nf_name = fb\nargs = 'eta1 eta2 eta3 eta4 mu1 mu2 mu3 mu4'\nconstant_names = 'a aa temp temp_c T0_eta T0_mu b11 b13 c11 c13 bb11 bb13 cc11 cc13 h'\nconstant_expressions = '2.057 3.943 280 338 275 270 -0.623 0.121 0.331 4.189 1.368 -3.679 0.400 2.000 0.300'\nmaterial_property_names = 'fermi_test'\nfunction = 'a*(temp-T0_eta)/(2.temp_c)((eta1)^2+fermi_test'\nderivative_order = 2\n[../]",
                          "url": "https://github.com/idaholab/moose/discussions/16297#discussioncomment-246326",
                          "updatedAt": "2022-06-14T12:15:19Z",
                          "publishedAt": "2020-12-28T06:05:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "laagesen"
                          },
                          "bodyText": "From what you said earlier, you are wanting to interpolate F_{1/2}(eta)  - so you would need to specify variable = eta in the [./fermi_test] block (or whichever order parameter you want to interpolate as a function of, since it looks like there are multiple order parameters). Right now you are specifying variable = fermi and I'm not sure what that is. If the derivatives of fb are going to be used in one of your kernels, you will also want to specify material_property_names = fermi_test(eta1) in [./bulk_free_energy] (assuming eta1 is what you are interpolating based on) rather than just material_property_names = fermi_test- sorry I probably did not mention that before. This will make sure that the derivatives of fermi_test are accounted for when derivatives of fb are taken, as detailed here: https://mooseframework.inl.gov/source/materials/DerivativeParsedMaterial.html . It may also be helpful to output the property fermi_test to your exodus file by specifying outputs = exodus in the [./fermi_test] block so you can visualize it and make sure you are getting what you expect.",
                          "url": "https://github.com/idaholab/moose/discussions/16297#discussioncomment-251051",
                          "updatedAt": "2023-08-11T14:27:33Z",
                          "publishedAt": "2020-12-29T18:40:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "kunokchang"
                          },
                          "bodyText": "We really appreciate for your help. It has been a huge help in applying the MOOSE framework to our research.",
                          "url": "https://github.com/idaholab/moose/discussions/16297#discussioncomment-268266",
                          "updatedAt": "2023-08-11T14:27:33Z",
                          "publishedAt": "2021-01-08T02:51:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "laagesen"
                          },
                          "bodyText": "Great glad we could help!",
                          "url": "https://github.com/idaholab/moose/discussions/16297#discussioncomment-269768",
                          "updatedAt": "2023-08-11T14:27:33Z",
                          "publishedAt": "2021-01-08T17:36:16Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Multi-app for exchange between a set intersecting 2d planes and a 3d mesh using the same physics kernels",
          "author": {
            "login": "rpodgorney"
          },
          "bodyText": "MOOSE Users/team--I'm looking for someone to help me put together a demo simulation using a multi-app for heat (and maybe mass) exchange between a set intersecting 2d planes (tris--a 3d fracture network) and a a 3d mesh (tet--a porous media domain)?",
          "url": "https://github.com/idaholab/moose/discussions/16631",
          "updatedAt": "2021-03-17T15:14:22Z",
          "publishedAt": "2021-01-07T20:43:14Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "friedmud"
                  },
                  "bodyText": "Should be able to just do a MultiAppMeshFunctionTransfer to transfer both directions.\nBut - it's going to depend on how you want the lower dimensional domain to feed information up to the higher dimensional domain.",
                  "url": "https://github.com/idaholab/moose/discussions/16631#discussioncomment-267817",
                  "updatedAt": "2021-01-07T20:45:50Z",
                  "publishedAt": "2021-01-07T20:45:36Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "rpodgorney"
                          },
                          "bodyText": "thanks @friedmud, that's the kind of info I need---anyone @INL have an interest to play with this?  I have resources but no time!",
                          "url": "https://github.com/idaholab/moose/discussions/16631#discussioncomment-269396",
                          "updatedAt": "2021-01-08T15:22:13Z",
                          "publishedAt": "2021-01-08T15:22:13Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "Does @WilkAndy already have something like this?",
                  "url": "https://github.com/idaholab/moose/discussions/16631#discussioncomment-267887",
                  "updatedAt": "2021-01-07T21:21:40Z",
                  "publishedAt": "2021-01-07T21:21:26Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "rpodgorney"
                          },
                          "bodyText": "I don't know, hey @WilkAndy have you done this?",
                          "url": "https://github.com/idaholab/moose/discussions/16631#discussioncomment-269398",
                          "updatedAt": "2021-01-08T15:23:05Z",
                          "publishedAt": "2021-01-08T15:23:05Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}