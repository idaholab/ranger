{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMS0wMi0xOFQxMDo0Nzo0Mi0wNzowMM4AMTmc"
    },
    "edges": [
      {
        "node": {
          "title": "edge3 versus edge2",
          "author": {
            "login": "dealmeidavf"
          },
          "bodyText": "This input\n[Mesh]\n  [1d]\n   type = GeneratedMeshGenerator\n   dim = 1\n   nx = 4\n   xmin = -5\n   xmax = 5\n   elem_type = edge3\n  []\n[]\n\n[Variables]\n  [u]\n    order = second\n    family = lagrange\n  []\n[]\nwill not run unless elem_type = edge3. The error is:\n\n\nERROR: Bad ElemType = EDGE2 for SECOND order approximation!\n\n\nWhat is the difference between edge2 and  edge3.\nThanks.",
          "url": "https://github.com/idaholab/moose/discussions/17039",
          "updatedAt": "2023-02-10T14:24:25Z",
          "publishedAt": "2021-02-17T16:53:40Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "For second order variables you need a second order mesh.\nSpecify second_order = true in the [Mesh] block directly then you dont need to specify the elem_type for it to work I think\nThe difference is the number of dofs on the edge.",
                  "url": "https://github.com/idaholab/moose/discussions/17039#discussioncomment-379115",
                  "updatedAt": "2023-02-10T14:24:38Z",
                  "publishedAt": "2021-02-17T20:46:35Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "dealmeidavf"
                          },
                          "bodyText": "I tried but still got\n\nERROR: Bad ElemType = EDGE2 for SECOND order approximation!\n\nIt is ok. I understand the EDGE3 for quadratic Lagrange polynomials in 1D. The \"edge\" naming did not connect to 1D at first.\nThanks.",
                          "url": "https://github.com/idaholab/moose/discussions/17039#discussioncomment-389083",
                          "updatedAt": "2023-02-10T14:24:38Z",
                          "publishedAt": "2021-02-20T20:07:33Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Error compiling MOOSE and MASTODON on Falcon",
          "author": {
            "login": "samyogshr"
          },
          "bodyText": "Hi Moose group,\nI am using these commands to build MASTODON and MOOSE on Falcon but I get error while compiling. How can I resolve this? I have attached outcome of module list, if it helps.\nScripts used:\nmkdir ~/projects4\ncd ~/projects4\ngit clone https://github.com/idaholab/mastodon.git\ncd mastodon\ngit checkout master\ngit submodule update --init moose\ncd moose/scripts/\nJOBS=8 ./update_and_rebuild_libmesh.sh\ncd ~/projects4/mastodon\nmake -j 8\n./run_tests -j 8\nError I get when compiling:\n[/home/shresamy/projects5/mastodon/moose/framework/build/header_symlinks/Moose.h:236:2: error: #error PETSc has not been detected, please ensure your environment is set up properly then rerun the libmesh build script and try to compile MOOSE again.\n#error PETSc has not been detected, please ensure your environment is set up properly then rerun the libmesh build script and try to compile MOOSE again.\nfatal error: petscsys.h: No such file or directory\nModule list:\n\nThank you,\nSamyog",
          "url": "https://github.com/idaholab/moose/discussions/16793",
          "updatedAt": "2022-07-02T01:30:02Z",
          "publishedAt": "2021-01-22T20:33:29Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "@milljm You have any thoughts on this?",
                  "url": "https://github.com/idaholab/moose/discussions/16793#discussioncomment-355580",
                  "updatedAt": "2022-07-02T01:30:12Z",
                  "publishedAt": "2021-02-10T07:02:16Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "samyogshr"
                          },
                          "bodyText": "@aeslaughter Thank you but this has been resolved,",
                          "url": "https://github.com/idaholab/moose/discussions/16793#discussioncomment-356736",
                          "updatedAt": "2022-07-02T01:30:11Z",
                          "publishedAt": "2021-02-10T14:30:37Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Marking it as solved so it doesn't show up when looking for unsolved questions.\nPlease consider posting what the solution to your problem was (and tagging it as the accepted answer) to help others in the future.",
                          "url": "https://github.com/idaholab/moose/discussions/16793#discussioncomment-357121",
                          "updatedAt": "2022-07-02T01:30:11Z",
                          "publishedAt": "2021-02-10T16:12:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "cduartec"
                          },
                          "bodyText": "Hey @samyogshr. I am also getting the same error. Could you share how did you solve it? Thanks :)",
                          "url": "https://github.com/idaholab/moose/discussions/16793#discussioncomment-387135",
                          "updatedAt": "2022-07-02T01:30:11Z",
                          "publishedAt": "2021-02-19T22:23:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "samyogshr"
                          },
                          "bodyText": "<<The problem was with 'pbs' module, type 'module list' in the terminal to see the currently loaded modules, if 'pbs' is one of them, these steps might help.>>\nSteps:\n\nIn '.bash_profile', add the following 3 lines at the end.\n\nmodule purge\nmodule load use.moose PETSc\nconda deactivate\n<<I had activated 'conda' while trying to install, if you haven't you can avoid the third line>>\n\n\nSave, logout from HPC and log back in\n\n\nThen, follow these steps: (Note: I was specifically installing MASTODON)\n\n\nmkdir ~/projects4\ncd ~/projects4\ngit clone https://github.com/idaholab/mastodon.git\ncd mastodon\ngit checkout master\ngit submodule update --init moose\ncd moose/scripts/\nJOBS=8 ./update_and_rebuild_libmesh.sh\ncd ~/projects4/mastodon\nmake -j 8\n./run_tests -j 8\n\nIf most tests pass, problem solved.\n\nThank you @hoffwm for your help on this, credit to him.\nSamyog",
                          "url": "https://github.com/idaholab/moose/discussions/16793#discussioncomment-387414",
                          "updatedAt": "2022-07-02T01:30:31Z",
                          "publishedAt": "2021-02-20T01:13:57Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Relative computational expense: Material system vs. \"hard-coding\" + \"best practices\"",
          "author": {
            "login": "gka80"
          },
          "bodyText": "Hello-\nI had two questions regarding the Materials system that I was hoping someone could answer:\n\n\nWhat's \"best practice\" for using the Materials system? I have a tendency to break my materials into smaller \"submaterials\" or components within my input file (e.g. 'M = M1 + M2 + M3', where 'M1/2/3' represent the subcomponents and 'M' represents the final material that I would pass to a kernel; \"usually\" these materials are DerivativeParsedMaterial(s)). The benefit to this is that it makes input files easier to debug; however, I'm not sure if this is impacting performance (appreciably). For context, I have ~30 materials, 90% of which are DerivativeParsedMaterials.\n\n\n...Which leads me to my next question. Would there be any (appreciable) benefit to either: a) Condensing the number of materials used? I suspect that I could reduce the number of materials used by ~50%. Or b) Hard-coding the materials (and their derivatives) into custom material types?\n\n\nI'm not opposed to either; just trying to understand whether or not I'm abusing the Materials system by using a \"ton\" of DerivativeParsedMaterials (as opposed to calculating the derivatives by hand and placing them in their own material).\nThank you!\n-Garrett",
          "url": "https://github.com/idaholab/moose/discussions/17028",
          "updatedAt": "2022-10-11T13:51:04Z",
          "publishedAt": "2021-02-17T00:56:32Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "cpgr"
                  },
                  "bodyText": "Have you done any profiling to see if your materials are taking a significant proportion of the simulation time? If they aren't, it mightn't be worth spending any time refactoring your current design.",
                  "url": "https://github.com/idaholab/moose/discussions/17028#discussioncomment-376368",
                  "updatedAt": "2022-10-11T13:51:04Z",
                  "publishedAt": "2021-02-17T09:12:13Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "gka80"
                          },
                          "bodyText": "Thanks for the response.\nGood time to learn! I found some small inefficiencies with my code using Instruments: ~20% of the time for the function calls pertain to materials. I'm going to experiment by cleaning up my input file and combining some of the \"extra\" materials and perform more profiling to see if it is \"worthwhile;\" hopefully it'll help future users. I'll report back later.",
                          "url": "https://github.com/idaholab/moose/discussions/17028#discussioncomment-378521",
                          "updatedAt": "2023-05-31T17:09:05Z",
                          "publishedAt": "2021-02-17T17:34:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mangerij"
                          },
                          "bodyText": "Are you talking about the materials system with automatic differentiation? If so, I think it is definitely slower, possibly by a factor of 2 or 4",
                          "url": "https://github.com/idaholab/moose/discussions/17028#discussioncomment-380592",
                          "updatedAt": "2023-05-31T17:09:05Z",
                          "publishedAt": "2021-02-18T10:54:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dschwen"
                          },
                          "bodyText": "@mangerij where does that \"factor of 2 or 4\" come from? We apply arithmetic optimization and just in time compilation. In my experience these optimizations can create code that evaluates faster than hand coded materials.",
                          "url": "https://github.com/idaholab/moose/discussions/17028#discussioncomment-381405",
                          "updatedAt": "2023-05-31T17:09:05Z",
                          "publishedAt": "2021-02-18T15:48:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "gka80"
                          },
                          "bodyText": "@dschwen Is this the paper that should be referred to: https://doi.org/10.1016/j.commatsci.2017.02.017? Are there any instances where the optimizations potentially wouldn't be faster than hand-coded materials?\nAfter doing some more testing, it's unlikely that the performance gains that I would (potentially) experience outweigh the additional commitment to reworking (and testing) the code. I would, however, be interested to hear @mangerij 's experiences.",
                          "url": "https://github.com/idaholab/moose/discussions/17028#discussioncomment-381642",
                          "updatedAt": "2023-05-31T17:09:05Z",
                          "publishedAt": "2021-02-18T17:02:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dschwen"
                          },
                          "bodyText": "Yes, that's the paper. It really depends on what your expressions look like. In particular in very long expressions as we see them in thermodynamic free energies there are plenty of opportunities for term reordering, factoring out common factors, replacing float powers with integer powers (big deal) etc. Of course a skilled coder could make these optimization by hand, but the automated system is really good at making them, too. And once the expressions are compiled there is very little additional overhead (nothing form the derivative parsed material except copying variable values of the coupled variables into a fixed size vector to pass it to the JIT code). Any substantial overhead would come from moose calling an additional material class (iterating over the list of material objects, virtual function call overhead). Things that could start to add up if you have a lot of materials that each perform very little work compared to that overhead.",
                          "url": "https://github.com/idaholab/moose/discussions/17028#discussioncomment-381685",
                          "updatedAt": "2023-05-31T17:09:24Z",
                          "publishedAt": "2021-02-18T17:14:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mangerij"
                          },
                          "bodyText": "@dschwen just from what I've read in the mailing list... I believe I've seen a few posts regarding this.\nBut seeing the timings would make me a believer...",
                          "url": "https://github.com/idaholab/moose/discussions/17028#discussioncomment-381704",
                          "updatedAt": "2023-05-31T17:09:57Z",
                          "publishedAt": "2021-02-18T17:19:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mangerij"
                          },
                          "bodyText": "But now I can't find the post. I am pretty sure I saw someone saying AD is much more slow ... Oh well \ud83d\ude02",
                          "url": "https://github.com/idaholab/moose/discussions/17028#discussioncomment-381742",
                          "updatedAt": "2023-05-31T17:09:59Z",
                          "publishedAt": "2021-02-18T17:29:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mangerij"
                          },
                          "bodyText": "https://mooseframework.inl.gov/source/systems/NonlinearSystem.html#AD\n\"One can elect to sacrifice some computing speed and calculate Jacobians automatically using automatic differentiation (AD). MOOSE employs the DualNumber class from the MetaPhysicL package in order to enable AD.\"\n?",
                          "url": "https://github.com/idaholab/moose/discussions/17028#discussioncomment-382431",
                          "updatedAt": "2022-10-11T13:51:06Z",
                          "publishedAt": "2021-02-18T21:35:37Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "Just to be clear, are we talking about the Automatic Differentiation (https://mooseframework.inl.gov/automatic_differentiation/index.html) system for Jacobians, or the unfortunately similarly named Parsed Function-based Automatic Differentiation system (https://mooseframework.inl.gov/modules/phase_field/FunctionMaterials/AutomaticDifferentiation.html). Or are they one in the same? This often confuses me.",
                          "url": "https://github.com/idaholab/moose/discussions/17028#discussioncomment-382509",
                          "updatedAt": "2022-10-11T13:51:06Z",
                          "publishedAt": "2021-02-18T22:12:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "@jessecarterMOOSE  Two different systems.",
                          "url": "https://github.com/idaholab/moose/discussions/17028#discussioncomment-387062",
                          "updatedAt": "2022-10-11T13:51:06Z",
                          "publishedAt": "2021-02-19T21:47:00Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Performing FEA simulations one after another in a single session",
          "author": {
            "login": "abarun22"
          },
          "bodyText": "Dear all,\nI would like to run a multiple numer of in-elastic simulations one after the another in a single run of MOOSE with an option to revise the input stress/plastic strain hardening parameters in between the simulations. Essentially we would want to start the simlation with a rough estimate of the hardening parameters and revise them based on the state of the internal stresses in each of the converged FE simulation. We would like to continue this iterative behaviour until the input parameters are all optimized and that the stresses agree with its experimental counterpart. Is that something we could try to make it happen in MOOSE? At present it does it perfectly for one FEA simulation. And advise on how to run continuous simulations in a single session of MOOSE would be greatly apprecietaed?\nPlease see the attached input file i am using at present? Essentially i would be required to revise the contents of CSV file in the 'PiecewiseLinear' block  after each successful run of FEA simulation.\nKind regards,\nArun\nPieceWise_test.zip",
          "url": "https://github.com/idaholab/moose/discussions/16332",
          "updatedAt": "2022-06-10T14:38:19Z",
          "publishedAt": "2020-11-30T11:56:54Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "This is possible but will require you to write a little code using the stochastic tools module. You will need to create a custom Sampler object that will produce a new set of parameters to be modified at each timestep. @somu15 Could you add a comment about what you have done?\nhttps://mooseframework.inl.gov/modules/stochastic_tools/index.html",
                  "url": "https://github.com/idaholab/moose/discussions/16332#discussioncomment-142686",
                  "updatedAt": "2022-06-10T14:38:44Z",
                  "publishedAt": "2020-11-30T15:50:12Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "somu15"
                  },
                  "bodyText": "Yes, we can run sequential samples using MOOSE STM such that the next sample is influenced by the current one. There are three things you need to do:\n\nThere will be Main.i and sub.i files. Main.i controls your Monte Carlo simulations and sub.i is your model. In the Main.i file, set num_rows = 1 in your sampler object. You should also have an executioner block with num_steps equal to the number of samples you want to generate.\nIn the Main.i file again, you need to have the Vectorpostprocessors block with all the results your Sampler object needs to know to modify the next simulation.\nIn your Sampler class (i.e., NewSampler.C you create), use the vectorpostporcessor interface to access the results from the previous run and to modify the input sample accordingly for the next run.\n\nI hope the above will allow you get to started. If you are having trouble, I can put together some example input and .C files for your reference.",
                  "url": "https://github.com/idaholab/moose/discussions/16332#discussioncomment-143158",
                  "updatedAt": "2022-06-10T14:38:45Z",
                  "publishedAt": "2020-11-30T23:26:46Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "abarun22"
                  },
                  "bodyText": "Hi Som,\nSorry that it took a while to complete my testing activities before i could\ntake up this work. I will go through the stochastic modules to get a clear\nunderstanding of the technicalities needed to implement this iterative\nsimulation. Would be glad to hear from you on the example files which would\nhelp me to further my code developments.\nKind regards,\nArun\n\u2026\nOn Mon, Nov 30, 2020 at 11:27 PM Som Dhulipala ***@***.***> wrote:\n Yes, we can run sequential samples using MOOSE STM such that the next\n sample is influenced by the current one. There are three things you need to\n do:\n\n    - There will be Main.i and sub.i files. Main.i controls your Monte\n    Carlo simulations and sub.i is your model. In the Main.i file, set num_rows\n    = 1 in your sampler object. You should also have an executioner block\n    with num_steps equal to the number of samples you want to generate.\n    - In the Main.i file again, you need to have the Vectorpostprocessors\n    block with all the results your Sampler object needs to know to modify the\n    next simulation.\n    - In your Sampler class (i.e., NewSampler.C you create), use the\n    vectorpostporcessor interface to access the results from the previous run\n    and to modify the input sample accordingly for the next run.\n\n I hope the above will allow you get to started. If you are having trouble,\n I can put together some example input and .C files for your reference.\n\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n <#16332 (comment)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/AJSA2577VI2IEHTGHIWQ26TSSQS4HANCNFSM4UHO4OCQ>\n .",
                  "url": "https://github.com/idaholab/moose/discussions/16332#discussioncomment-281583",
                  "updatedAt": "2022-12-10T05:05:37Z",
                  "publishedAt": "2021-01-14T11:30:22Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "somu15"
                  },
                  "bodyText": "Hi Arun,\nWe have a pull request for a sequential sampler called Metropolis (a Markov Chain Monte Carlo sampler): #16398\nThis PR has a Metropolis class which does the sequential sampling and also has input file as part of the tests. Maybe you would like to take a look at this.\nLet me know if you have questions.\nSom",
                  "url": "https://github.com/idaholab/moose/discussions/16332#discussioncomment-281977",
                  "updatedAt": "2022-12-10T05:05:43Z",
                  "publishedAt": "2021-01-14T14:42:06Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "ngrilli"
                  },
                  "bodyText": "Dear @abarun22\nFor your optimization problem, you could couple MOOSE with the DAKOTA optimization software from Sandia.\nIt is quite easy to use,\nyou will need first to write a postprocessing script that calculates a residual (difference) between your simulation data\nand experimental data, then you prepare a DAKOTA input file with your favourite optimization procedure,\nthen you introduce the variable parameter in the MOOSE input file (adding the ${} symbol if I don't remember wrong),\nwhich are the ones you want to optimize.\nDAKOTA will launch for you the MOOSE simulations one after the other and will change the parameters in the MOOSE input file\nat each iteration.\nBest Regards,\nNicol\u00f2 Grilli\nNational University of Singapore",
                  "url": "https://github.com/idaholab/moose/discussions/16332#discussioncomment-283383",
                  "updatedAt": "2022-12-10T05:05:44Z",
                  "publishedAt": "2021-01-15T02:37:11Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "@lynnmunday Do you want to comment here. I believe this is exactly what you did for your inverse problem moose app.",
                  "url": "https://github.com/idaholab/moose/discussions/16332#discussioncomment-283511",
                  "updatedAt": "2022-12-10T05:05:45Z",
                  "publishedAt": "2021-01-15T04:35:58Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "lynnmunday"
                          },
                          "bodyText": "The inverse optimization app I'm working on would do this but it is not merged and has no documentation.  I think we will have this cleaned up and merged into moose by the end of March.",
                          "url": "https://github.com/idaholab/moose/discussions/16332#discussioncomment-387054",
                          "updatedAt": "2022-12-10T05:05:45Z",
                          "publishedAt": "2021-02-19T21:41:02Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "abarun22"
                  },
                  "bodyText": "Dear Som/ Nicol\u00f2,\nThanks for your suggestions. I tried executing an example model with the\nSTM and it seems like the sampling is performed essentially at each time\nstep, which will not work in our case. Ideally we would require a sample to\nbe composed of all the time steps, then perform the revision of parameters\nand then continue with the next sample (with all time steps) and then\nterminate this chain at a certain convergence criteria dictated by the\ninternal stress state.  I tried to mimic this behaviour by setting the\nidentifier 'execute_on' to 'final' but without much success. Also this\nmethod requires the specification of sample count (num_rows) prior to the\nanalysis which may not be possible, as it is determined at run-time based\non the chain convergence. I would welcome your suggestions as to whether\nanything could be done in this regard to achieve the intended behaviour.\nAttached here please find the input files and the execution log for this\ncomputation.\n\nAs  Nicol\u00f2 suggested, I did take a look at the DAKOTA optimization module\nand from the outset it seems like a possible approach to tackle such kind\nof optimization problems with ease. I will take a closer look at the\nfunctionalities, try example problems and see if we could implement a\nprototype that mimics the existing optimization approach handled through\nANSYS.\nKind regards,\nArun\n\u2026\nOn Fri, Jan 15, 2021 at 2:37 AM Nicol\u00f2 Grilli ***@***.***> wrote:\n Dear @abarun22 <https://github.com/abarun22>\n\n For your optimization problem, you could couple MOOSE with the DAKOTA\n optimization software from Sandia.\n It is quite easy to use,\n you will need first to write a postprocessing script that calculates a\n residual (difference) between your simulation data\n and experimental data, then you prepare a DAKOTA input file with your\n favourite optimization procedure,\n then you introduce the variable parameter in the MOOSE input file (adding\n the ${} symbol if I don't remember wrong),\n which are the ones you want to optimize.\n DAKOTA will launch for you the MOOSE simulations one after the other and\n will change the parameters in the MOOSE input file\n at each iteration.\n\n Best Regards,\n Nicol\u00f2 Grilli\n National University of Singapore\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#16332 (comment)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/AJSA255BXIRCU4JRZYSSDWDSZ6S6JANCNFSM4UHO4OCQ>\n .",
                  "url": "https://github.com/idaholab/moose/discussions/16332#discussioncomment-284340",
                  "updatedAt": "2022-12-10T05:05:45Z",
                  "publishedAt": "2021-01-15T12:16:03Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "The scenario should be possible with STM. The main file would use StochasticFullSolveMultiapp that run complete simulations. If the main uses Transient executioner the Sampler you create would build a new set of samples at each \"timestep\" (iteration) then run the complete sub-application simulations again, with new samples.",
                          "url": "https://github.com/idaholab/moose/discussions/16332#discussioncomment-291263",
                          "updatedAt": "2022-12-10T05:06:11Z",
                          "publishedAt": "2021-01-18T16:31:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "somu15"
                          },
                          "bodyText": "@abarun22 There is a Subset Simulation class on my branch which does exactly what Andew mentioned above. Here is the link to the class: https://github.com/somu15/moose/blob/Subset/modules/stochastic_tools/src/samplers/SubTest.C\nHere is a test input file: https://github.com/somu15/moose/blob/Subset/modules/stochastic_tools/test/tests/samplers/subtest/subsetsim.i\nWe are testing this class presently and we will submit a PR with an example soon.",
                          "url": "https://github.com/idaholab/moose/discussions/16332#discussioncomment-291401",
                          "updatedAt": "2022-12-10T05:06:10Z",
                          "publishedAt": "2021-01-18T16:59:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Dear Andrew/Somu/Nicolo,\nThanks for your suggestions. I further analysed whether STM approach can be tailored to run iterative type of FE simulations. The sampling approach that we use here is applied only in a pseudo sense, since we are not using the standard stochastic methods to perturb the parameters. Instead they are done through the results from the actual FEA and the experimental results. This mandates a two way communication between the main and sub apps, and thus the parameter 'num_rows' is always maintained one suggesting an execution of single sample, every time a parameter set is revised. I tried an example test case to establish connection between the main and sub app, but could not perform the other way. Unfortunately the two way coupled approach goes around an infinite loop without executing the results.\nAlteratively, i completely avoided the sampling method and introduced conventional multi-app approach with two similar models sitting on the main as well as sub apps domain. The models can be executed one after the other with the parameter set revised after the convergence checks. From my view, this seems to be the more neater way of running iterative simulations as against the STM method which seemed a bit tedious. I had a similar experience running this model as i failed to introduce a two way communication which is extremely vital in enabling a successful iterative simulation methodology. I have attached here the  input/output files used in these cases for your reference. The proposed methodology is explained in the attached slides. I would welcome your ideas/suggestions as whether these methods could be realised practically with minimal coding effort. Also please advise me on how a complete coupling (reverse connection) could be achieved for these methods. Please note that the parameter set is currently stored in a csv file and hence any further revision of this set can be re-written on the same file, thus saving efforts to store it on to special data structures. Also please note that the yield stress parameter ('yield_stress' found in STM/sub.i) needs to be modified after each revision of parameter set. I wonder whether a parameter substitution can be used here to enable a continuous usage of input file with right parameters.\nI am also open to DAKOTA-MOOSE interface to implement scheme of things, if it offers a convenient way of handling this coupled approach.\n20012021.zip",
                          "url": "https://github.com/idaholab/moose/discussions/16332#discussioncomment-321872",
                          "updatedAt": "2022-12-10T05:06:11Z",
                          "publishedAt": "2021-01-29T19:59:10Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "We just added the ability to dynamically change the samples and the resulting sup-application runs. So, you could create a Sampler that is informed by the output of any number of MultiApps for creating the next sample. But, if you are happy with the MultiApp approach you mentioned, then I wouldn't worry about.\n#16862",
                  "url": "https://github.com/idaholab/moose/discussions/16332#discussioncomment-355576",
                  "updatedAt": "2022-12-10T05:06:13Z",
                  "publishedAt": "2021-02-10T07:00:54Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Micromagnetics in Phase Field",
          "author": {
            "login": "alanchavez86"
          },
          "bodyText": "Hi all!\nI was wondering if micromagnetics have been implemented in any phase field models and was curious if anyone is interesting in sharing how they implemented it in their models.\nI'm looking into incorporating demagnetization energy (E_d) from dipole-dipole interactions which influences the behavior of the phase decomposition of FeCrCo.\nI would like to know if there is a simpler way to implement equation 6 from Toshiyuki Koyama 2008 Sci. Technol. Adv. Mater. 9 013006 (paper attached).\nThank you all for the help.\nAlan C.\nKoyama_2004.pdf",
          "url": "https://github.com/idaholab/moose/discussions/17074",
          "updatedAt": "2022-12-24T09:23:51Z",
          "publishedAt": "2021-02-19T02:21:14Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "mangerij"
                  },
                  "bodyText": "Hi Alan,\nMicromagnetics is implemented in Ferret: https://mangerij.github.io/ferret/\nWe use a Landau-Lifshitz-Bloch which handles finite temperature fluctuations and is used to constrain the magnetization vector to a fixed length. At 0K it reduces to the LLG problem. We have tested exchange, demag, and anisotropy couplings and all have reasonable agreement to other micromagnetic codes (i.e. muMax3 and Vampire).\nConcerning the 2004 Koyama paper: we have been approached already regarding this paper and the tentative conclusion is that their model is a bit incomplete and LLG is unnecessary.\n1.) The phase transitions you are investigating are at very high temperatures, therefore fluctuations of the magnetic order parameter would probably kill any domain ordering/magnetic precession you would get from solving the LLG equation.\n2.) The timescales on magnetic relaxation/precession are much much shorter than the spinodal decomposition process. Therefore, evolving the LLG+CH problem would take a very long time. The exchange length also is much much smaller than the CH interfacial width posing a problem to resolve magnetic topology.\n3.) The magnetic field H = -grad(\u03a6) can be calculated by just solving the Poisson equation with Dirichlet conditions on the opposing boundaries to generate an applied field. In the Poisson equation, you just include coupling to your concentration given magnetization saturation dependence on the concentration. This is possible and should be fairly straightforward. You are welcome to look at Ferret on how the Poisson equation is solved (but it is trivially a diffusion equation for \u03a6 and addition of a kernel that just does divM). But since you have M = M[c(r)], you need div(M[c(r)] which is a nasty expression. I would suggest using the AD system for this.\nSo my point is that the driving force for the 2D elongation of the phases along the field is because he includes a Zeeman term and also couples to the concentration via the demag field resulting from the Poisson equation and not from the other terms (exchange, anisotropy, magnetostriction, etc) from a \"micromagnetic\" model. I am speculating that you might get the same results ignoring demag completely and just including the Zeeman term in your CH equation - this might be a wise place to start.\nIn his later paper (see Koyama 2008 Sci. Technol. Adv. Mater. 9 013006), he includes an easier to follow equation set by the way.\nCant figure how to attach here...\nJohn",
                  "url": "https://github.com/idaholab/moose/discussions/17074#discussioncomment-383676",
                  "updatedAt": "2022-12-24T09:23:57Z",
                  "publishedAt": "2021-02-19T09:24:55Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "alanchavez86"
                          },
                          "bodyText": "John.\nThank you for the insight and information. I'll look into Koyama's later paper and try to follow that equation as well as working with Ferret. My mentor also provided me a document regarding Micromagnetics in Ferret so I'll be looking through that as well.\nThank you for your help.\nAlan",
                          "url": "https://github.com/idaholab/moose/discussions/17074#discussioncomment-386663",
                          "updatedAt": "2022-12-24T09:24:02Z",
                          "publishedAt": "2021-02-19T19:37:35Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Simulation freezes without any notification, HYPRE suspected",
          "author": {
            "login": "matthiasneuner"
          },
          "bodyText": "Dear MOOSE users,\nI am currently investigation a very strange issue, which is hard to reproduce (hence no minimum working example), and only occurs in MPI parallelized simulations. The key facts:\n\nA TensorMechanics like simulation involving multiple fields (displacement, rotations, etc.) using my own MOOSE app\nMonolithic SMP preconditioning with hypre boomeramg using following setup:\n\npetsc_options_iname = '     -pc_type\n                                -pc_hypre_type\n                                -ksp_type\n                                -ksp_gmres_restart\n                                -pc_hypre_boomeramg_relax_type_all\n                                -pc_hypre_boomeramg_strong_threshold\n                                -pc_hypre_boomeramg_agg_nl\n                                -pc_hypre_boomeramg_agg_num_paths\n                                -pc_hypre_boomeramg_max_levels\n                                -pc_hypre_boomeramg_coarsen_type\n                                -pc_hypre_boomeramg_interp_type\n                                -pc_hypre_boomeramg_P_max\n                                -pc_hypre_boomeramg_truncfactor' \n\n    petsc_options_value = '     hypre\n                                boomeramg\n                                gmres\n                                301\n                                Chebyshev\n                                0.65\n                                5 \n                                2\n                                25\n                                HMIS\n                                ext+i\n                                4\n                                0.4 '\n\nA simple example involving 130,000 DOFs occasionally freezes at the same time step, while most of the time the simulation works perfectly without any issue.\n\nThe freeze occurs without any warning or error message\nNo FPE is issued by libMESH\nThe freeze does not occur for threaded parallelization\nThe freeze does not occur when using a direct solver (e.g., strumpack)\n\nAttaching gdb to such a freezing simulation reveals that all MPI processes are stuck in HYPRE:\n0x00007f91dd7f3f28 in hypre_LINPACKcgpthy () from /home/matthias/anaconda3/envs/moose/lib/libHYPRE-2.19.0.so                                                                             \n(gdb) bt full                                                                                                                                                                            \n#0  0x00007f91dd7f3f28 in hypre_LINPACKcgpthy () from /home/matthias/anaconda3/envs/moose/lib/libHYPRE-2.19.0.so                                                                         \nNo symbol table info available.\n#1  0x00007f91dd7f4175 in hypre_LINPACKcgtql1 () from /home/matthias/anaconda3/envs/moose/lib/libHYPRE-2.19.0.so\nNo symbol table info available.\n#2  0x00007f91dd7f4b6d in hypre_ParCSRMaxEigEstimateCG () from /home/matthias/anaconda3/envs/moose/lib/libHYPRE-2.19.0.so\nNo symbol table info available.\n#3  0x00007f91dd7632d5 in hypre_BoomerAMGSetup () from /home/matthias/anaconda3/envs/moose/lib/libHYPRE-2.19.0.so\nNo symbol table info available.\n#4  0x00007f91de6c00b2 in PCSetUp_HYPRE () from /home/matthias/anaconda3/envs/moose/lib/libpetsc.so.3.14\nNo symbol table info available.\n#5  0x00007f91de6fb951 in PCSetUp () from /home/matthias/anaconda3/envs/moose/lib/libpetsc.so.3.14\nNo symbol table info available.\n#6  0x00007f91de7301b4 in KSPSetUp () from /home/matthias/anaconda3/envs/moose/lib/libpetsc.so.3.14\nNo symbol table info available.\n#7  0x00007f91de730e32 in KSPSolve_Private () from /home/matthias/anaconda3/envs/moose/lib/libpetsc.so.3.14\nNo symbol table info available.\n#8  0x00007f91de733443 in KSPSolve () from /home/matthias/anaconda3/envs/moose/lib/libpetsc.so.3.14\nNo symbol table info available.\n\nThis is valid for ALL MPI processes.\nCurrently, I have no clue what to do. Please, can anybody give me a hint what to do?\nThank you and best regards, Matthias",
          "url": "https://github.com/idaholab/moose/discussions/17058",
          "updatedAt": "2022-10-11T08:43:10Z",
          "publishedAt": "2021-02-18T16:09:47Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "@fdkong Can you help out here?",
                  "url": "https://github.com/idaholab/moose/discussions/17058#discussioncomment-381778",
                  "updatedAt": "2022-10-11T08:44:54Z",
                  "publishedAt": "2021-02-18T17:38:44Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "dschwen"
                  },
                  "bodyText": "I have not much to contribute here, other than, the hypre source code is a joy to read: https://github.com/hypre-space/hypre/blob/master/src/parcsr_ls/par_relax_more.c ... not!",
                  "url": "https://github.com/idaholab/moose/discussions/17058#discussioncomment-381788",
                  "updatedAt": "2022-10-11T08:43:42Z",
                  "publishedAt": "2021-02-18T17:40:13Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "Could you just be running out of memory?",
                          "url": "https://github.com/idaholab/moose/discussions/17058#discussioncomment-381794",
                          "updatedAt": "2022-10-11T08:43:42Z",
                          "publishedAt": "2021-02-18T17:41:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "matthiasneuner"
                          },
                          "bodyText": "Could you just be running out of memory?\n\nActually, I can exclude that. Other examples with 1 million++ DOFs,  'more nonlinearity' and a similar solver setup work very well. For the present example, the freeze occurs quite early, which means that most parts of the domain still behave linear elastic.",
                          "url": "https://github.com/idaholab/moose/discussions/17058#discussioncomment-381829",
                          "updatedAt": "2022-10-11T08:43:42Z",
                          "publishedAt": "2021-02-18T17:55:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "matthiasneuner"
                          },
                          "bodyText": "I have not much to contribute here, other than, the hypre source code is a joy to read: https://github.com/hypre-space/hypre/blob/master/src/parcsr_ls/par_relax_more.c ... not!\n\nMaybe a stupid question ... but does hypre_ParCSRMaxEigEstimateCG expect a symmetric matrix? Because my system is definitely nonsymmetric.",
                          "url": "https://github.com/idaholab/moose/discussions/17058#discussioncomment-381834",
                          "updatedAt": "2022-10-11T08:43:42Z",
                          "publishedAt": "2021-02-18T17:57:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "Yes, it could be an issue.\nCould you delete option -pc_hypre_boomeramg_relax_type_all Chebyshev? GS will be used by default, IIRC.  I do not have any issue with the default relax solvers for even larger problems( around 10 billions DoFs).\nYou might have a motivaiton to add -pc_hypre_boomeramg_relax_type_all Chebyshev ?",
                          "url": "https://github.com/idaholab/moose/discussions/17058#discussioncomment-382480",
                          "updatedAt": "2022-10-11T08:43:42Z",
                          "publishedAt": "2021-02-18T21:55:39Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "WilkAndy"
                  },
                  "bodyText": "I just want to point out that this is not necessarily a problem with hypre.   It could be that hypre+PETSc_snes+MPI happen to have forced your simulation into a \"bad place\" in parameter space, such that the Jacobian is very poor, or even has NaNs/bad_things in it.\nThis happens occasionally for me when trying to solve very stiff or otherwise difficult systems: the parallel linear solve is not accurate enough, which throws the system into a \"bad place\" from which it essentially never recovers.   Then i have to choose a stronger preconditioner or more accurate linear-solve.  I know it's unlikely to be your problem, but wanted to mention it, just in case.",
                  "url": "https://github.com/idaholab/moose/discussions/17058#discussioncomment-382421",
                  "updatedAt": "2022-10-11T08:43:59Z",
                  "publishedAt": "2021-02-18T21:27:29Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "matthiasneuner"
                  },
                  "bodyText": "Thank you for all the comments, despite the difficult nature of the problem!\nMy current understanding is  that the issue is related to the Eigenvalue estimation required by the Chebyshev relaxation:\nThe default algorithm hypre_ParCSRMaxEigEstimateCG for estimating the max. and mix. Eigenvalues uses CG and expects a symmetric matrix:\nhttps://github.com/hypre-space/hypre/blob/be18e595aed8dcd1af818eb563c8a962d96a162c/src/parcsr_ls/par_amg_setup.c#L3088\nIn principle, HYPRE would allow to use the algorithm hypre_ParCSRMaxEigEstimate  (estimating only the max. value) via hypre_ParAMGDataChebyEigEst, but the respective option is not exposed to us by PETSc, so we cannot use it.\nMy motivation for Chebyshev relaxation is that I observed it to be far superior to SOR/Jacobi (very good experiences in particular for Trilinos' ML). I have successfully applied it to other examples. Switching back to  SOR/Jacobi considerably decreases performance due to increasing gmres iterations, but I guess that I have no choice here :-) . Just for confirmation: No freezes occur for SOR/Jacobi.\nThank you all!",
                  "url": "https://github.com/idaholab/moose/discussions/17058#discussioncomment-384531",
                  "updatedAt": "2022-10-11T08:44:14Z",
                  "publishedAt": "2021-02-19T11:55:41Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "dschwen"
                  },
                  "bodyText": "but the respective option is not exposed to us by PETSc, so we cannot use it.\n\nThat shouldn't be hard to fix though, @fdkong , right?",
                  "url": "https://github.com/idaholab/moose/discussions/17058#discussioncomment-386026",
                  "updatedAt": "2022-10-11T08:44:39Z",
                  "publishedAt": "2021-02-19T16:05:26Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "It is very easy to fix, right. @matthiasneuner You are free to create an issue in PETSc, and tag me.",
                          "url": "https://github.com/idaholab/moose/discussions/17058#discussioncomment-386189",
                          "updatedAt": "2022-10-11T08:44:40Z",
                          "publishedAt": "2021-02-19T16:59:46Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to implement a function includes \"if and else\"",
          "author": {
            "login": "sSajjad90"
          },
          "bodyText": "Hi everyone,\nI am going to implement a \"ParsedMaterial\", which function includes three parts via time. In this regard, I need an \"if\" function with two conditions (an \"if\" and two else) like below, for example. But its syntax is not correct. Would you please let me know what the right way to write this command is?\n[./a1]\ntype = ParsedMaterial\nfunction = 'if(t<=15, act && 15<t<20, act*t,0)'\nmaterial_property_names = 'ac t'\nf_name = a1\n[../]\nI should mention that it works when there is a condition as follows, but I can't define it for the state with two conditions.\n[./a1]\ntype = ParsedMaterial\nfunction = 'if(t<15, ac*t ,0)'\nmaterial_property_names = 'ac t'\nf_name = a1\n[../]\nThanks in advance",
          "url": "https://github.com/idaholab/moose/discussions/17072",
          "updatedAt": "2021-02-19T16:27:51Z",
          "publishedAt": "2021-02-18T23:28:13Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "WilkAndy"
                  },
                  "bodyText": "I'm not sure i understand what you want, but how about\nif(t<=15, low_value, if(t<=20, medium_value, high_value))",
                  "url": "https://github.com/idaholab/moose/discussions/17072#discussioncomment-382690",
                  "updatedAt": "2021-02-18T23:47:56Z",
                  "publishedAt": "2021-02-18T23:47:40Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "sSajjad90"
                          },
                          "bodyText": "Many thanks, It works now.",
                          "url": "https://github.com/idaholab/moose/discussions/17072#discussioncomment-386103",
                          "updatedAt": "2021-02-19T16:26:10Z",
                          "publishedAt": "2021-02-19T16:26:10Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "FPDualReal in Fluid Properties UO",
          "author": {
            "login": "joe61vette"
          },
          "bodyText": "Hi:\nVery basic question.  Trying to code a UO for a fluid property that would use AD system.  In the header file I have:\nclass ConstantFluidProperties : public SinglePhaseFluidProperties\n...\nvirtual FPDualReal rho_from_p_T(const FPDualReal & pressure, const FPDualReal & temperature) const override;\nand in the corresponding C file:\nFPDualReal\nConstantFluidProperties::rho_from_p_T(const FPDualReal & pressure, const FPDualReal & temperature) const\n{\n...\n}\nI get a compile error:\nConstantFluidProperties.h:23:23: error: 'rho_from_p_T' marked 'override' but does not override any member functions\nvirtual FPDualReal rho_from_p_T(const FPDualReal & pressure, const FPDualReal & temperature) const override;\nIt seems like it should be overriding propfuncAD.  Is there something obvious that I am doing wrong?\nThanks,\nJoe K",
          "url": "https://github.com/idaholab/moose/discussions/17061",
          "updatedAt": "2022-10-24T22:16:37Z",
          "publishedAt": "2021-02-18T19:14:24Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hi Joe\nI think it's because propfuncAD is using DualReal for its arguments and return types. Going to check now.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/17061#discussioncomment-382173",
                  "updatedAt": "2022-10-24T22:16:38Z",
                  "publishedAt": "2021-02-18T19:43:26Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Also I think you can define your functions with non-AD Real types and propfuncAD will take care of defining the AD versions.",
                          "url": "https://github.com/idaholab/moose/discussions/17061#discussioncomment-382186",
                          "updatedAt": "2024-02-07T20:49:52Z",
                          "publishedAt": "2021-02-18T19:48:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I think defining with non-AD Real types and have the propfuncAD template do the conversion is the way to go here.\n@lindsayad is more comfortable than me with AD maybe he will have better advice",
                          "url": "https://github.com/idaholab/moose/discussions/17061#discussioncomment-382230",
                          "updatedAt": "2024-02-07T20:49:52Z",
                          "publishedAt": "2021-02-18T20:06:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "joe61vette"
                          },
                          "bodyText": "Thanks Guillaume:  I wondered about using DualReal but tried to follow BrineFluidProperties as a template.  It inherits from MultiComponentFluidProperties which does contain a new macro for propfuncAD to account for the extra independent variable.  But that macro (if I understand the coding) also uses DualReal but the the header and C files have:\nFPDualReal rho_from_p_T_X(const FPDualReal & pressure,\nconst FPDualReal & temperature,\nconst FPDualReal & xnacl) const;\nand\nFPDualReal\nBrineFluidProperties::rho_from_p_T_X(const FPDualReal & pressure,\nconst FPDualReal & temperature,\nconst FPDualReal & xnacl) const\nAlso, I think FPDualReal is just a specialization of DualReal:\ntypedef DualNumber<Real, DNDerivativeSize<5>> FPDualReal;\nJoe",
                          "url": "https://github.com/idaholab/moose/discussions/17061#discussioncomment-382360",
                          "updatedAt": "2024-02-07T20:49:52Z",
                          "publishedAt": "2021-02-18T21:00:08Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Yes the issue was not because of FPDualReal.\nThe routine in BrineFluidProperties you pasted is rho_from_p_T**_X**, which is not provided by propfuncAD I think, so it's not marked with override in the header. You could try just deleting the override keyword for your declaration, but then your routine could conflict with the one defined by propfuncAD.",
                          "url": "https://github.com/idaholab/moose/discussions/17061#discussioncomment-382391",
                          "updatedAt": "2024-02-07T20:49:52Z",
                          "publishedAt": "2021-02-18T21:15:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "cpgr"
                          },
                          "bodyText": "Hi Joe,\nI added that FPDualReal typedef to compute some tricky derivatives in BrineFluidProperties using AD. It was much faster to limit the number of derivatives (5 derivates was heaps faster than allowing 50). I think I also use it in WaterFluidProperties for the same reason. You might get some clues how they can be used in conjunction with propfunc macros looking there, eg \n  \n    \n      moose/modules/fluid_properties/src/userobjects/Water97FluidProperties.C\n    \n    \n         Line 842\n      in\n      6437ec8\n    \n  \n  \n    \n\n        \n          \n           FPDualReal",
                          "url": "https://github.com/idaholab/moose/discussions/17061#discussioncomment-382415",
                          "updatedAt": "2024-02-07T20:49:54Z",
                          "publishedAt": "2021-02-18T21:24:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "joe61vette"
                          },
                          "bodyText": "Thanks Chris:\nFrom Water97FluidProperties it seems that to use FPDualReal, one has to forget about inheriting from propfuncAD and instead write a new fn.  For example, there is:\nFPDualReal\nWater97FluidProperties::T_from_p_h_ad(const FPDualReal & pressure, const FPDualReal & enthalpy) const\nwhere the \"_ad\" has been added but is not in the macro.  Using FPDualReal seems to make a lot of sense but it appears that using it is outside of the normal guidance.  As this is a new app, I would like to try and follow the Moose guidelines as much as possible.\nIf I only use DualReal, all is well.  If I use FPDualReal and omit the \"override\" specifier, it compiles but I get the following run time error:\n*** ERROR ***\nThe following error occurred in the object \"const_props\", of type \"ConstantFluidProperties\".\nconst_props: virtual void SinglePhaseFluidProperties::rho_from_p_T(libMesh::Real, libMesh::Real, libMesh::Real &, libMesh::Real &, libMesh::Real &) const derivatives not implemented.\nIs it best to just go ahead and use DualReal (accepting performance penalty) but follow the fluid property system guidelines?\nJoe",
                          "url": "https://github.com/idaholab/moose/discussions/17061#discussioncomment-382499",
                          "updatedAt": "2024-02-07T20:50:31Z",
                          "publishedAt": "2021-02-18T22:05:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "cpgr"
                          },
                          "bodyText": "In your app, you probably want to call rho_from_p_T(DualReal p, DualReal T) to use the AD capability. But the way the propfunc macros work in the fluid properties classes is that you add non-AD versions, and then that macro automatically creates the AD version. This means that we can use those fluid properties with Real variables, and also DualReal variables, so we have a lot of flexibility.\nSo in your class, you want to add:\nReal rho_from_p_T(Real p, Real T)\n{\n  return rho = f(p, T) // for some function f\n}\n\nvoid rho_from_p_T(Real p, Real T, Real & rho, Real & rho_dp, Real & rho_dT)\n{\n  rho = f(p, T)\n  rho_dp = df_dp // whatever these derivatives are\n  rho_dT = df_dT\n}\n\nand then you get DualReal rho_from_p_T(DualReal & p, DualReal & T) for free without coding them! This is what the macros add for you.\nWhere I have used FPDualReal is in place of hand-coding the derivates in rho_from_p_T(Real p, Real T, Real & rho, Real & rho_dp, Real & rho_dT). Perhaps a better example than I gave earlier is \n  \n    \n      moose/modules/fluid_properties/src/userobjects/Water97FluidProperties.C\n    \n    \n        Lines 1683 to 1697\n      in\n      6437ec8\n    \n  \n  \n    \n\n        \n          \n           void \n        \n\n        \n          \n           Water97FluidProperties::T_from_p_h( \n        \n\n        \n          \n               Real pressure, Real enthalpy, Real & temperature, Real & dT_dp, Real & dT_dh) const \n        \n\n        \n          \n           { \n        \n\n        \n          \n             FPDualReal p = pressure; \n        \n\n        \n          \n             Moose::derivInsert(p.derivatives(), 0, 1.0); \n        \n\n        \n          \n             FPDualReal h = enthalpy; \n        \n\n        \n          \n             Moose::derivInsert(h.derivatives(), 1, 1.0); \n        \n\n        \n          \n            \n        \n\n        \n          \n             const FPDualReal T = T_from_p_h_ad(p, h); \n        \n\n        \n          \n            \n        \n\n        \n          \n             temperature = T.value(); \n        \n\n        \n          \n             dT_dp = T.derivatives()[0]; \n        \n\n        \n          \n             dT_dh = T.derivatives()[1]; \n        \n\n        \n          \n           } \n        \n    \n  \n\n\nSo I use the faster (less derivatives) AD form to compute the derivatives, and then get the full AD version through the macro automatically. Does that explain it?",
                          "url": "https://github.com/idaholab/moose/discussions/17061#discussioncomment-382603",
                          "updatedAt": "2024-02-07T20:50:58Z",
                          "publishedAt": "2021-02-18T22:54:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "joe61vette"
                          },
                          "bodyText": "THANKS!  I finally got it.  So, the \"approved\" way is to follow the normal\nReal rho_from_p_T(Real p, Real T)\ntype of coding and let the macro handle it.  I tested it and it does work for my case where rho is an AD Material Property and p & T are adCoupledValues.  One only implements the extra FPDualReal coding (as in your example) if the normal way is simply too slow.  Which for Water/Steam properties would be the case.\nI'll mark this as answered.  Thanks again,\nJoe",
                          "url": "https://github.com/idaholab/moose/discussions/17061#discussioncomment-382663",
                          "updatedAt": "2022-10-24T22:16:38Z",
                          "publishedAt": "2021-02-18T23:33:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "joe61vette"
                          },
                          "bodyText": "Well, I would have marked it as answered if I could figure out how.  Joe",
                          "url": "https://github.com/idaholab/moose/discussions/17061#discussioncomment-382687",
                          "updatedAt": "2022-10-24T22:16:43Z",
                          "publishedAt": "2021-02-18T23:46:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Unfortunately you can only mark the beginning of the thread as the answer.\nFWIW with the new sparse AD default configuration of MOOSE, DualReal should not be so slow any more because it keeps track of the number of derivative entries automatically and consequently dual number operations only happen over the sparsity entries (which can be a small number, less than 5 for example, in these fluid property contexts) and not over the entire underlying container size of 50.",
                          "url": "https://github.com/idaholab/moose/discussions/17061#discussioncomment-382700",
                          "updatedAt": "2022-10-24T22:16:43Z",
                          "publishedAt": "2021-02-18T23:59:11Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "joe61vette"
                  },
                  "bodyText": "Just want to thank everyone for the help.  It is great that I can simply use the old style property fns and have them magically return the value with derivatives.  Also, using the AD system as Chris did to get derivatives for properties that return them will save me time later.",
                  "url": "https://github.com/idaholab/moose/discussions/17061#discussioncomment-384696",
                  "updatedAt": "2022-10-24T22:16:44Z",
                  "publishedAt": "2021-02-19T12:40:55Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Issue associated with restart ArrayVariable",
          "author": {
            "login": "lingzou"
          },
          "bodyText": "Dear All, I am exploring the ArrayVariable capability, and encountered some issues associated with its restart capability. Here, ArrayVariable is something like:\n  [./v]\n    order = CONSTANT\n    family = MONOMIAL\n    components = 3\n    initial_condition = '12 13 14'\n  [../]\n\nPlease see attached two input files, step-0.i is a simple input file with 'u' a regular variable, and 'v' an ArrayVariable. In 'step-1.i', I try to restart the simulation using step-0 results by giving:\n[Mesh]\n  file = step-0_out.e\n[]\n\nThe error message is attached, which seems to be associated with the restart of the ArrayVariable.\nmap_find() error: key \"v\" not found in file ../src/systems/system.C on line 1405\nStack frames: 13\n0: 0   libmesh_opt.0.dylib                 0x0000000106b0d865 libMesh::print_trace(std::__1::basic_ostream<char, std::__1::char_traits<char> >&) + 1157\n1: 1   libmesh_opt.0.dylib                 0x0000000106b0a55e libMesh::MacroFunctions::report_error(char const*, int, char const*, char const*) + 270\n2: 2   libmesh_opt.0.dylib                 0x0000000107165b30 std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned short, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, unsigned short> > >::mapped_type const& libMesh::Utility::map_find<std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned short, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, unsigned short> > >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*)0>(std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned short, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, unsigned short> > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, char const*, int) + 688\n3: 3   libmesh_opt.0.dylib                 0x000000010716586c libMesh::System::variable_number(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const + 28\n4: 4   libmesh_opt.0.dylib                 0x0000000106def3fb libMesh::ExodusII_IO::copy_elemental_solution(libMesh::System&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, unsigned int) + 43\n5: 5   libmoose-opt.0.dylib                0x0000000105f2ca2d SystemBase::copyVars(libMesh::ExodusII_IO&) + 781\n6: 6   libmoose-opt.0.dylib                0x0000000105b524a3 FEProblemBase::initialSetup() + 2531\n7: 7   libmoose-opt.0.dylib                0x00000001058be604 Transient::init() + 1764\n8: 8   libmoose-opt.0.dylib                0x000000010627198b MooseApp::executeExecutioner() + 75\n9: 9   libmoose-opt.0.dylib                0x00000001062721b4 MooseApp::run() + 244\n10: 10  sam-opt                             0x0000000104606b7c main + 140\n11: 11  libdyld.dylib                       0x00007fff58a453d5 start + 1\n12: 12  ???                                 0x0000000000000003 0x0 + 3\n[0] ./include/libmesh/utility.h, line 154, compiled Dec 11 2020 at 10:49:02\napplication called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\n[unset]: write_line error; fd=-1 buf=:cmd=abort exitcode=1\n:\nsystem msg for write_line failure : Bad file descriptor\n\nI'd appreciate it if anyone could give me some suggestions on this issue.\nIf it is not hard to fix, maybe I can do it.\nPlease change step-0/1.txt to step-0/1.i\nstep-0.txt\nstep-1.txt\n-Ling",
          "url": "https://github.com/idaholab/moose/discussions/16915",
          "updatedAt": "2022-06-14T22:11:35Z",
          "publishedAt": "2021-02-07T20:31:07Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "YaqiWang"
                  },
                  "bodyText": "v is the array variable name in MOOSE but not a valid variable name in libMesh. Each component of a moose array variable is a valid libMesh variable with name being array_variable_name_# with number being the component index. So the correct syntax in step-1.txt should be either initial_from_file_var = 'v_0 v_1 v_2' (possibly not supported yet) or initial_from_file_var = v but with some internal name conventions. The later syntax could be better. It needs some code development.",
                  "url": "https://github.com/idaholab/moose/discussions/16915#discussioncomment-352034",
                  "updatedAt": "2022-06-14T22:11:43Z",
                  "publishedAt": "2021-02-09T07:20:57Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@amherm or @smharper might know about array variable restarts.",
                          "url": "https://github.com/idaholab/moose/discussions/16915#discussioncomment-352745",
                          "updatedAt": "2022-06-14T22:11:51Z",
                          "publishedAt": "2021-02-09T12:03:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lingzou"
                          },
                          "bodyText": "Thank you @YaqiWang! Yes, I got that there is some internal naming algorithm which causes the issue. I am scoping how much effort that would need to make it work, and if someone (or maybe I myself) can do it.",
                          "url": "https://github.com/idaholab/moose/discussions/16915#discussioncomment-353308",
                          "updatedAt": "2022-06-14T22:11:51Z",
                          "publishedAt": "2021-02-09T15:08:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "YaqiWang"
                          },
                          "bodyText": "Sorry, I should replied with this. I checked the code, should be fairly strait-forward to support this: in SystemBase::copyVars's if (hasVariable(vci._dest_name)), we will need to check fieldType() of the obtained moose variable (SystemBase::isArrayVariable could be better for this check). If it is array variable, we will need to loop through its components to copy one by one. Array variables have a member function componentName(unsigned int comp) to return the libMesh variable name of a component, which should be used in ExodusII_IO::copy_nodal_solution or ExodusII_IO::copy_elemental_solution.",
                          "url": "https://github.com/idaholab/moose/discussions/16915#discussioncomment-353583",
                          "updatedAt": "2022-06-14T22:11:51Z",
                          "publishedAt": "2021-02-09T16:14:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lingzou"
                          },
                          "bodyText": "Thank you @YaqiWang, I can probably give it a try to fix it.",
                          "url": "https://github.com/idaholab/moose/discussions/16915#discussioncomment-356849",
                          "updatedAt": "2022-06-16T18:10:28Z",
                          "publishedAt": "2021-02-10T14:57:54Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lingzou"
                          },
                          "bodyText": "I got it fixed, and will share the code back (moose MR) shortly.",
                          "url": "https://github.com/idaholab/moose/discussions/16915#discussioncomment-382122",
                          "updatedAt": "2022-06-16T18:10:28Z",
                          "publishedAt": "2021-02-18T19:23:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "YaqiWang"
                          },
                          "bodyText": "Thanks Ling.",
                          "url": "https://github.com/idaholab/moose/discussions/16915#discussioncomment-382130",
                          "updatedAt": "2022-06-16T18:10:28Z",
                          "publishedAt": "2021-02-18T19:25:44Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "CSV output on 1D problems",
          "author": {
            "login": "dealmeidavf"
          },
          "bodyText": "Hello,\nI don't seem to get a CSV output (1D test problem).\n[Outputs]\n  [csv]\n   type = CSV\n  []\n[]\nThanks for inputs on this. I mean: can CSV be used with steady-state, 1D problems where nodal values are listed either in columns and node positions listed on rows (or vice-versa).",
          "url": "https://github.com/idaholab/moose/discussions/17036",
          "updatedAt": "2023-09-15T01:44:35Z",
          "publishedAt": "2021-02-17T13:40:18Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "dealmeidavf"
                  },
                  "bodyText": "Since then I added this and it works:\n[VectorPostprocessors]\n  [1d]\n    type = LineValueSampler\n    variable = 'u'\n    start_point = '-5 0 0'\n    end_point = '5 0 0'\n    num_points = 11\n    sort_by = id\n  []\n[]\nAdapted from\n\n  \n    \n      moose/test/tests/outputs/csv_final_and_latest/final.i\n    \n    \n        Lines 49 to 60\n      in\n      130d29c\n    \n  \n  \n    \n\n        \n          \n           # Vector Postprocessor System \n        \n\n        \n          \n           [VectorPostprocessors] \n        \n\n        \n          \n             [./line_sample] \n        \n\n        \n          \n               type = LineValueSampler \n        \n\n        \n          \n               execute_on = 'timestep_end final' \n        \n\n        \n          \n               variable = 'u' \n        \n\n        \n          \n               start_point = '0 0.5 0' \n        \n\n        \n          \n               end_point = '1 0.5 0' \n        \n\n        \n          \n               num_points = 11 \n        \n\n        \n          \n               sort_by = id \n        \n\n        \n          \n             [../] \n        \n\n        \n          \n           []",
                  "url": "https://github.com/idaholab/moose/discussions/17036#discussioncomment-377753",
                  "updatedAt": "2023-09-15T01:44:35Z",
                  "publishedAt": "2021-02-17T14:28:43Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "Thanks for posting the solution.",
                          "url": "https://github.com/idaholab/moose/discussions/17036#discussioncomment-381806",
                          "updatedAt": "2023-09-15T01:44:44Z",
                          "publishedAt": "2021-02-18T17:47:42Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}