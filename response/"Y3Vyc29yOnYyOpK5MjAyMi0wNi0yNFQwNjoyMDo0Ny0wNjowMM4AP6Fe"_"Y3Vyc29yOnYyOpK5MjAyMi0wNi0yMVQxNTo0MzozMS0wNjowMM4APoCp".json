{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMi0wNi0yMVQxNTo0MzozMS0wNjowMM4APoCp"
    },
    "edges": [
      {
        "node": {
          "title": "Orientation of AbaqusUMATStress",
          "author": {
            "login": "jessecarterMOOSE"
          },
          "bodyText": "If my UMAT has an anisotropic material, is there a way to re-orient it and/or the inputs going into it?\ntagging @dschwen (sorry for another UMAT question)",
          "url": "https://github.com/idaholab/moose/discussions/21376",
          "updatedAt": "2022-07-21T23:44:04Z",
          "publishedAt": "2022-06-21T20:24:50Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "recuero"
                  },
                  "bodyText": "What is it in particular that you want to rotate/reorient?",
                  "url": "https://github.com/idaholab/moose/discussions/21376#discussioncomment-2997424",
                  "updatedAt": "2022-06-21T21:46:25Z",
                  "publishedAt": "2022-06-21T21:46:24Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "For my application I'd like to orient the material model a certain way. An example would be two grains that are the same material but are oriented differently in the global coordinate system. Ideally I'd use the same UMAT for both materials but MOOSE would handle the rotations somehow.",
                          "url": "https://github.com/idaholab/moose/discussions/21376#discussioncomment-2997506",
                          "updatedAt": "2022-06-21T22:08:22Z",
                          "publishedAt": "2022-06-21T22:08:22Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "recuero"
                          },
                          "bodyText": "Doing that will most likely require some development around UMAT. MOOSE does have grain tracking capabilities that could probably be leveraged for your use case.",
                          "url": "https://github.com/idaholab/moose/discussions/21376#discussioncomment-2997769",
                          "updatedAt": "2022-06-21T23:24:08Z",
                          "publishedAt": "2022-06-21T23:24:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "Are there any existing MOOSE capabilities that could be strung together currently to do what I need? Doesn't have to be grains (that was just an example).",
                          "url": "https://github.com/idaholab/moose/discussions/21376#discussioncomment-2997783",
                          "updatedAt": "2022-06-21T23:30:40Z",
                          "publishedAt": "2022-06-21T23:30:33Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "recuero"
                          },
                          "bodyText": "As far as I can see, code development will be required to allow the UMAT interface to correctly interpret local element/grain orientations. I assume you could potentially do what you want to do from the UMAT routines if you want to give it a try. But I understand that having it in the interface would be optimal.",
                          "url": "https://github.com/idaholab/moose/discussions/21376#discussioncomment-3014204",
                          "updatedAt": "2022-06-24T00:18:15Z",
                          "publishedAt": "2022-06-24T00:18:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "Does it make sense to apply a rotation to the stress/strain tensors before/after the call to the UMAT to represent some arbitrary orientation?\nUnfortunately I can't touch the UMAT source code. Abaqus allows the user to specify an orientation to material models in the input file, which like you say is pretty convenient.",
                          "url": "https://github.com/idaholab/moose/discussions/21376#discussioncomment-3014284",
                          "updatedAt": "2022-06-24T00:43:32Z",
                          "publishedAt": "2022-06-24T00:43:31Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "PorousFlow : Error in Kozeny Carman Equation",
          "author": {
            "login": "srinath-chakravarthy"
          },
          "bodyText": "For the kozeny carman equation with option poreperm = kozeny_carman_phi0, the implemented form results in a constant permeability = k0 * k_anisotropy.\nRationale :\n$k_{ij} = A * k_{ij}^0 \\frac{\\phi^n}{(1-\\phi)^m}$\n$ A = k0 * (1 - \\phi0)^m / \\phi0n$\nSubstituting for $A$ back into the first equation\n$k_{ij} = k_0 * k_{ij}^0$\nI checked the implementation and the relevant lines are seen below\nCalculation of factor A\n_A = _k0 * std::pow(1.0 - _phi0, _m) / std::pow(_phi0, _n);\nPermability calc\n _permeability_qp[_qp] = _k_anisotropy * _A * std::pow(_porosity_qp[_qp], _n) / std::pow(1.0 - _porosity_qp[_qp], _m);",
          "url": "https://github.com/idaholab/moose/discussions/21396",
          "updatedAt": "2022-07-21T23:43:47Z",
          "publishedAt": "2022-06-23T16:24:16Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "WilkAndy"
                  },
                  "bodyText": "Hang on, the _A depends on _phi0 (note the \"zero\"), while _permeability_qp[_qp] depends on _porosity_qp[_qp], which may not be _phi0.   I feel like i'm missing something obvious, sorry!",
                  "url": "https://github.com/idaholab/moose/discussions/21396#discussioncomment-3013335",
                  "updatedAt": "2022-06-23T20:34:03Z",
                  "publishedAt": "2022-06-23T20:33:57Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "srinath-chakravarthy"
                          },
                          "bodyText": "right if the porosity does not change, then _porosity[_qp] is equal to phi0 which means that the permeability will be just one. Also do you have a reference to this equation. Even if there is a porosity change, it seems strange to me, to be multiplying by the inverse of the power term.",
                          "url": "https://github.com/idaholab/moose/discussions/21396#discussioncomment-3013365",
                          "updatedAt": "2022-06-23T20:40:19Z",
                          "publishedAt": "2022-06-23T20:40:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "WilkAndy"
                          },
                          "bodyText": "@hsheldon - can you answer @srinath-chakravarthy 's question about the reference?   @srinath-chakravarthy - the equation seems reasonable to me: as porosity->0 only the numerator survives, and perm = por^n, but as porosity->1, the denominator starts to dominate, and perm = 1/(1-por)^m.  For n and m positive, this seems like the correct type of behaviour.",
                          "url": "https://github.com/idaholab/moose/discussions/21396#discussioncomment-3013391",
                          "updatedAt": "2022-06-23T20:44:46Z",
                          "publishedAt": "2022-06-23T20:44:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hsheldon"
                          },
                          "bodyText": "@srinath-chakravarthy it looks fine to me; when phi = phi0 you have k = k0 * anisotropy, but when phi != phi0 you have a different value of k (increasing with phi). As phi increases, the numerator increases and the denominator decreases so k increases. Regarding a reference, this equation is essentially the Kozeny-Carman equation; there is a nice explanation of the derivation in this paper (p. 140 in particular):\nOelkers, E.H., 1996, Physical and chemical properties of rocks and fluids for chemical mass\ntransport calculations, in Lichtner, P.C., Steefel, C.I. et al., Reactive transport in porous media, Reviews in Mineralogy, v. 34: Washington, Mineralogical Society of America, 131-192.",
                          "url": "https://github.com/idaholab/moose/discussions/21396#discussioncomment-3014157",
                          "updatedAt": "2022-06-24T00:02:31Z",
                          "publishedAt": "2022-06-24T00:02:30Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Compilation failing",
          "author": {
            "login": "j-bowhay"
          },
          "bodyText": "",
          "url": "https://github.com/idaholab/moose/discussions/21390",
          "updatedAt": "2022-06-23T12:48:27Z",
          "publishedAt": "2022-06-23T10:24:08Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "j-bowhay"
                  },
                  "bodyText": "Turns out by mamba install was broken, now fixed",
                  "url": "https://github.com/idaholab/moose/discussions/21390#discussioncomment-3009663",
                  "updatedAt": "2022-06-23T11:29:29Z",
                  "publishedAt": "2022-06-23T11:29:29Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Material with temperature question",
          "author": {
            "login": "wangzhaohao"
          },
          "bodyText": "Hello, everyone!. I want to set a expression about temperature, and i public ComputeThermalExpansionEigenstrainBaseTemp, the .C\nis this\n 45          auto temp = _temperature[_qp];\n  \n 46         Real thermal_expansion = 1e-6;\n 47         if ( temp> 0 && temp < 923 )\n 48             thermal_expansion = 9.828e-6 -6.39e-10 * temp + 1.33e-12 * temp * temp -1.757\n 49         else if ( temp>=923 && temp<= 3120 )\n 50             thermal_expansion = 1.1833e-5 -5.103e-9 * temp + 3.756e-12 * temp * temp -6.1\n 51         else\n 52             thermal_expansion = 1.08e-5;\n 53 \n 54         return thermal_expansion * (_temperature[_qp] - _stress_free_temperature[_qp]);\n\nI know thermal_expansion is not Real, but i cannot find a type to declare it. what should i do? Thanks for your help.",
          "url": "https://github.com/idaholab/moose/discussions/21331",
          "updatedAt": "2022-06-25T05:58:25Z",
          "publishedAt": "2022-06-17T03:43:58Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "wangzhaohao"
                  },
                  "bodyText": "I know this can use a Function part, to do this work, but i want to use this at one .C. i put the whole\ncode\n  template <bool is_ad>\n 40 ValueAndDerivative<is_ad>\n 41 ComputeMoxThermalExpansionEigenstrainTempl<is_ad>::computeThermalStrain()\n 42 {\n 43     /* for(unsigned int qp(0); qp < _qrule->n_points(); ++qp) */\n 44     /* { */   \n 45         Function thermal_expansion; \n 46         auto temp = _temperature[_qp];\n 47 \n 48         if ( temp> 0 && temp < 923 )\n 49             thermal_expansion = 9.828e-6 -6.39e-10 * temp + 1.33e-12 * temp * temp -1.757e-17 * pow(temp,3);\n 50         else if ( temp>=923 && temp<= 3120 )\n 51             thermal_expansion = 1.1833e-5 -5.103e-9 * temp + 3.756e-12 * temp * temp -6.125e-17 * pow(temp,3);\n 52         else\n 53             thermal_expansion = 1.08e-5;\n 54 \n 55         return thermal_expansion * (_temperature[_qp] - _stress_free_temperature[_qp]);\n 56     /* } */    \n 57 }",
                  "url": "https://github.com/idaholab/moose/discussions/21331#discussioncomment-2970222",
                  "updatedAt": "2022-06-17T09:22:51Z",
                  "publishedAt": "2022-06-17T09:22:48Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nFunction arent declared inline like this. Look at other materials like GenericFunctionMaterial to see how functions are called if you want to use a Function.\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/21331#discussioncomment-2972889",
                          "updatedAt": "2022-06-17T16:03:55Z",
                          "publishedAt": "2022-06-17T16:03:49Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "wangzhaohao"
                  },
                  "bodyText": "thanks! I got it.",
                  "url": "https://github.com/idaholab/moose/discussions/21331#discussioncomment-3008576",
                  "updatedAt": "2022-06-23T08:54:33Z",
                  "publishedAt": "2022-06-23T08:54:32Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Test suite runs on head vs fails on compute node",
          "author": {
            "login": "mntbighker"
          },
          "bodyText": "I'm an OpenHPC cluster manager trying to help my team install MOOSE. I'm to the point where the test suite runs with few failures (mostly lack of CPUs). But on the compute nodes pretty much every test is skipped, and ultimately MPI errors kill the run. I'm not having much success debugging this. OpenHPC has no MOOSE meta package. Most of the MOOSE dependencies are already part of OpenHPC, but so many of the packages are built with MOOSE compile dependencies missing that we pretty much built it per the instructions. PETSc being a prime example. Some examples of things I had to add to get MOOSE tests to work: libtirpc-devel, python36-devel, and...\npython -m pip install --upgrade pip\npython -m pip install -t /home/shared/petsc/python3.6 pandas\npython -m pip install -t /home/shared/petsc/python3.6 matplotlib\npython -m pip install -t /home/shared/petsc/python3.6 sympy\npython -m pip install -t /home/shared/petsc/python3.6 deepdiff\npython -m pip install -t /home/shared/petsc/python3.6 numpy\npython -m pip install -t /home/shared/petsc/python3.6 scipy\npython -m pip install -t /home/shared/petsc/python3.6 vtk\nMost of the python stuff has OpenHPC packages, that don't work with MOOSE either.\nI have re-run the libmesh upgrade script a few times as these things were added, in case that was necessary.",
          "url": "https://github.com/idaholab/moose/discussions/21241",
          "updatedAt": "2022-06-25T05:55:10Z",
          "publishedAt": "2022-06-09T00:45:19Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nCould you please give us the test log for a few of the errors?\nCan you run one of the failing test in an interactive job and paste the error here if the log isnt available?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2909668",
                  "updatedAt": "2022-06-09T00:57:42Z",
                  "publishedAt": "2022-06-09T00:57:41Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "mntbighker"
                  },
                  "bodyText": "vectorpostprocessors/work_balance.work_balance/distributed ..................... [MESH_MODE!=DISTRIBUTED] SKIP\n[c1.Thor2.local:37446] [[44689,0],0] ORTE_ERROR_LOG: Data unpack had inadequate space in file util/show_help.c at line 513\n[c1.Thor2.local:37446] [[44689,0],0] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file util/show_help.c at line 507\n[c1.Thor2.local:37446] [[44689,0],0] ORTE_ERROR_LOG: Data unpack would read past end of buffer in file util/show_help.c at line 501\n--------------------------------------------------------------------------\nIt looks like orte_init failed for some reason; your parallel process is\nlikely to abort.  There are many reasons that a parallel process can\nfail during orte_init; some of which are due to configuration or\nenvironment problems.  This failure appears to be an internal failure;\nhere's some additional information (which may only be relevant to an\nOpen MPI developer):\n\n  getting local rank failed\n  --> Returned value No permission (-17) instead of ORTE_SUCCESS\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nIt looks like orte_init failed for some reason; your parallel process is\nlikely to abort.  There are many reasons that a parallel process can\nfail during orte_init; some of which are due to configuration or\nenvironment problems.  This failure appears to be an internal failure;\nhere's some additional information (which may only be relevant to an\nOpen MPI developer):\n\n  orte_ess_init failed\n  --> Returned value No permission (-17) instead of ORTE_SUCCESS\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nIt looks like MPI_INIT failed for some reason; your parallel process is\nlikely to abort.  There are many reasons that a parallel process can\nfail during MPI_INIT; some of which are due to configuration or environment\nproblems.  This failure appears to be an internal failure; here's some\nadditional information (which may only be relevant to an Open MPI\ndeveloper):\n\n  ompi_mpi_init: ompi_rte_init failed\n  --> Returned \"No permission\" (-17) instead of \"Success\" (0)\n--------------------------------------------------------------------------\nmisc/check_error.steady_no_converge: Working Directory: /home/mwmoorcroft/moose/test/tests/misc/check_error\nmisc/check_error.steady_no_converge: Running command: /home/mwmoorcroft/moose/test/moose_test-opt -i steady_no_converge.i --error --error-unused --error-override --no-gdb-backtrace\nmisc/check_error.steady_no_converge: *** An error occurred in MPI_Init_thread\nmisc/check_error.steady_no_converge: *** on a NULL communicator\nmisc/check_error.steady_no_converge: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\nmisc/check_error.steady_no_converge: ***    and potentially your MPI job)\nmisc/check_error.steady_no_converge: [c1.Thor2.local:37573] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\nmisc/check_error.steady_no_converge: ################################################################################\nmisc/check_error.steady_no_converge:\nmisc/check_error.steady_no_converge: Unable to match the following pattern against the program's output:\nmisc/check_error.steady_no_converge:\nmisc/check_error.steady_no_converge: Aborting as solve did not converge\nmisc/check_error.steady_no_converge:\nmisc/check_error.steady_no_converge: ################################################################################\nmisc/check_error.steady_no_converge: Tester failed, reason: EXPECTED OUTPUT MISSING\nmisc/check_error.steady_no_converge:\nmisc/check_error.steady_no_converge ......................................... FAILED (EXPECTED OUTPUT MISSING)\nmisc/check_error.steady_no_converge: Working Directory: /home/mwmoorcroft/moose/test/tests/misc/check_error\nmisc/check_error.steady_no_converge: Running command: /home/mwmoorcroft/moose/test/moose_test-opt -i steady_no_converge.i --error --error-unused --error-override --no-gdb-backtrace\nmisc/check_error.steady_no_converge: *** An error occurred in MPI_Init_thread\nmisc/check_error.steady_no_converge: *** on a NULL communicator\nmisc/check_error.steady_no_converge: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\nmisc/check_error.steady_no_converge: ***    and potentially your MPI job)\nmisc/check_error.steady_no_converge: [c1.Thor2.local:37561] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\nmisc/check_error.steady_no_converge: ################################################################################\nmisc/check_error.steady_no_converge:\nmisc/check_error.steady_no_converge: Unable to match the following pattern against the program's output:\nmisc/check_error.steady_no_converge:\nmisc/check_error.steady_no_converge: Aborting as solve did not converge\nmisc/check_error.steady_no_converge:\nmisc/check_error.steady_no_converge: ################################################################################\nmisc/check_error.steady_no_converge: Tester failed, reason: EXPECTED OUTPUT MISSING\nmisc/check_error.steady_no_converge:\nmisc/check_error.steady_no_converge ......................................... FAILED (EXPECTED OUTPUT MISSING)\nmisc/check_error.missing_mesh_test: Working Directory: /home/mwmoorcroft/moose/test/tests/misc/check_error\nmisc/check_error.missing_mesh_test: Running command: /home/mwmoorcroft/moose/test/moose_test-opt -i missing_mesh_test.i --error --error-unused --error-override --no-gdb-backtrace\nmisc/check_error.missing_mesh_test: *** An error occurred in MPI_Init_thread\nmisc/check_error.missing_mesh_test: *** on a NULL communicator\nmisc/check_error.missing_mesh_test: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\nmisc/check_error.missing_mesh_test: ***    and potentially your MPI job)\nmisc/check_error.missing_mesh_test: [c1.Thor2.local:37576] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\nmisc/check_error.missing_mesh_test: ################################################################################\nmisc/check_error.missing_mesh_test:\nmisc/check_error.missing_mesh_test: Unable to match the following pattern against the program's output:\nmisc/check_error.missing_mesh_test:\nmisc/check_error.missing_mesh_test: Unable to open file \\S+\nmisc/check_error.missing_mesh_test:\nmisc/check_error.missing_mesh_test: ################################################################################\nmisc/check_error.missing_mesh_test: Tester failed, reason: EXPECTED ERROR MISSING\nmisc/check_error.missing_mesh_test:\nmisc/check_error.missing_mesh_test ........................................... FAILED (EXPECTED ERROR MISSING)\nmisc/check_error.function_file_test14: Working Directory: /home/mwmoorcroft/moose/test/tests/misc/check_error\nmisc/check_error.function_file_test14: Running command: /home/mwmoorcroft/moose/test/moose_test-opt -i function_file_test14.i --error --error-unused --error-override --no-gdb-backtrace\nmisc/check_error.function_file_test14: *** An error occurred in MPI_Init_thread\nmisc/check_error.function_file_test14: *** on a NULL communicator\nmisc/check_error.function_file_test14: *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\nmisc/check_error.function_file_test14: ***    and potentially your MPI job)\nmisc/check_error.function_file_test14: [c1.Thor2.local:37560] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\nmisc/check_error.function_file_test14: ################################################################################\nmisc/check_error.function_file_test14:\nmisc/check_error.function_file_test14: Unable to match the following pattern against the program's output:\nmisc/check_error.function_file_test14:\nmisc/check_error.function_file_test14: In \\S+: Read more than two rows of data from file '\\S+'.  Did you mean to use \"format = columns\" or set \"xy_in_file_only\" to false?\nmisc/check_error.function_file_test14:\nmisc/check_error.function_file_test14: ################################################################################\nmisc/check_error.function_file_test14: Tester failed, reason: EXPECTED ERROR MISSING\nmisc/check_error.function_file_test14:\nmisc/check_error.function_file_test14 ........................................ FAILED (EXPECTED ERROR MISSING)\nkernels/ad_max_dofs_per_elem_error.ad_max_dofs_per_elem_error ...................... [AD_MODE!=NONSPARSE] SKIP\nkernels/ad_max_dofs_per_elem_error.ad_max_dofs_per_elem_error ...................... [AD_MODE!=NONSPARSE] SKIP\nkernels/ad_max_dofs_per_elem_error.ad_max_dofs_per_elem_error ...................... [AD_MODE!=NONSPARSE] SKIP\nkernels/ad_max_dofs_per_elem_error.ad_max_dofs_per_elem_error ...................... [AD_MODE!=NONSPARSE] SKIP\nminimal_app.minimal ........................................................................ [NO DISPLAY] SKIP\nminimal_app.minimal ........................................................................ [NO DISPLAY] SKIP\nminimal_app.minimal ........................................................................ [NO DISPLAY] SKIP\nminimal_app.minimal ........................................................................ [NO DISPLAY] SKIP\n[c1.Thor2.local:37446] 3 more processes have sent help message help-orte-runtime / orte_init:startup:internal-failure\n[c1.Thor2.local:37446] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\n[c1.Thor2.local:37446] 3 more processes have sent help message help-mpi-runtime.txt / mpi_init:startup:internal-failure\nmisc/check_error.dynamic_check_name_block_mismatch_test .............................................. RUNNING\nmisc/check_error.missing_mesh_test .................................................................",
                  "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2915382",
                  "updatedAt": "2022-06-09T17:10:17Z",
                  "publishedAt": "2022-06-09T16:51:38Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "mntbighker"
                  },
                  "bodyText": "",
                  "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2915527",
                  "updatedAt": "2022-06-14T16:58:14Z",
                  "publishedAt": "2022-06-09T17:08:35Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "you can start interactive jobs on most clusters, for pbs it s qsub -i iirc.\nthen once you have a bash shell on a node you just run tests either with:\nspecifying a restriction on the test suite\n./run_tests --re \"pattern_matching_a_test\"\n\nor like a regular MOOSE input:\n./moose_test-opt -i input_file_for_that_test.i\n\nyou are getting MPI errors.\nCan you add to your job script (or run interactively)\nwhich mpicc\nwhich mpirun \nmodule list\n\nso we know which mpi is being used.\nAlso what does ldd <your_executable> return?",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2915662",
                          "updatedAt": "2022-06-09T17:25:11Z",
                          "publishedAt": "2022-06-09T17:25:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "[mwmoorcroft@thor2-head ~]$ cd moose/test/\n[mwmoorcroft@thor2-head test]$ module load moose-dev-gcc\n[mwmoorcroft@thor2-head test]$ salloc -N1 -n4 -t 240\nsalloc: Granted job allocation 1275\n[mwmoorcroft@c4 test]$ which mpicc\n/opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/bin/mpicc\n[mwmoorcroft@c4 test]$ which mpirun\n/opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/bin/mpirun\n[mwmoorcroft@c4 test]$ module list\n\nCurrently Loaded Modules:\n  1) autotools   3) gnu9/9.4.0    5) ucx/1.11.2         7) openmpi4/4.1.1   9) phdf5/1.10.8\n  2) prun/2.2    4) hwloc/2.5.0   6) libfabric/1.13.0   8) ohpc            10) moose-dev-gcc/4.0.1\n\n[mwmoorcroft@c4 test]$ python -m pip list\nDEPRECATION: The default format will switch to columns in the future. You can use --format=(legacy|columns) (or define a format=(legacy|columns) in your pip.conf under the [list] section) to disable this warning.\naiohttp (3.8.1)\naiosignal (1.2.0)\nasync-timeout (4.0.2)\nasynctest (0.13.0)\nattrs (21.4.0)\ncharset-normalizer (2.0.12)\ncycler (0.11.0)\ndeepdiff (5.7.0)\nfrozenlist (1.2.0)\nidna (3.3)\nidna-ssl (1.1.0)\nkiwisolver (1.3.1)\nmatplotlib (3.3.4)\nmpi4py (3.1.3)\nmpmath (1.2.1)\nmultidict (5.2.0)\nnumpy (1.19.5)\nordered-set (4.0.2)\npandas (1.1.5)\nPillow (8.4.0)\npip (9.0.3)\npyparsing (3.0.9)\npython-dateutil (2.8.2)\npytz (2022.1)\nPyYAML (3.12)\nscipy (1.5.4)\nsetuptools (39.2.0)\nsix (1.16.0)\nsympy (1.9)\ntyping-extensions (4.1.1)\nvtk (9.1.0)\nwslink (1.6.5)\nyarl (1.7.2)\n\n[mwmoorcroft@c4 test]$ ldd ../libmesh/installed/bin/meshtool-opt\n\tlinux-vdso.so.1 (0x00007ffe64b99000)\n\tlibmesh_opt.so.0 => /home/mwmoorcroft/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0 (0x00007f17efe08000)\n\tlibnetcdf.so.13 => /home/mwmoorcroft/moose/scripts/../libmesh/installed/lib/libnetcdf.so.13 (0x00007f17efaf9000)\n\tlibtimpi_opt.so.10 => /home/mwmoorcroft/moose/scripts/../libmesh/installed/lib/libtimpi_opt.so.10 (0x00007f17ef8da000)\n\tlibz.so.1 => /lib64/libz.so.1 (0x00007f17ef6c2000)\n\tlibslepc.so.3.16 => /home/shared/petsc/lib/libslepc.so.3.16 (0x00007f17ef104000)\n\tlibpetsc.so.3.16 => /home/shared/petsc/lib/libpetsc.so.3.16 (0x00007f17ed735000)\n\tlibHYPRE-2.23.0.so => /home/shared/petsc/lib/libHYPRE-2.23.0.so (0x00007f17ed10c000)\n\tlibstrumpack.so => /home/shared/petsc/lib/libstrumpack.so (0x00007f17ec5fe000)\n\tlibsuperlu_dist.so.7 => /home/shared/petsc/lib/libsuperlu_dist.so.7 (0x00007f17ec2ed000)\n\tlibhdf5_hl.so.100 => /opt/ohpc/pub/libs/gnu9/openmpi4/hdf5/1.10.8/lib/libhdf5_hl.so.100 (0x00007f17ec0ca000)\n\tlibhdf5.so.103 => /opt/ohpc/pub/libs/gnu9/openmpi4/hdf5/1.10.8/lib/libhdf5.so.103 (0x00007f17ebac4000)\n\tlibparmetis.so => /home/shared/petsc/lib/libparmetis.so (0x00007f17eb884000)\n\tlibmetis.so => /home/shared/petsc/lib/libmetis.so (0x00007f17eb621000)\n\tlibmpi_usempif08.so.40 => /opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/lib/libmpi_usempif08.so.40 (0x00007f17eb3e3000)\n\tlibmpi_usempi_ignore_tkr.so.40 => /opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/lib/libmpi_usempi_ignore_tkr.so.40 (0x00007f17eb1d8000)\n\tlibmpi_mpifh.so.40 => /opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/lib/libmpi_mpifh.so.40 (0x00007f17eaf6c000)\n\tlibmpi.so.40 => /opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/lib/libmpi.so.40 (0x00007f17eac40000)\n\tlibgfortran.so.5 => /opt/ohpc/pub/compiler/gcc/9.4.0/lib64/libgfortran.so.5 (0x00007f17ea7ad000)\n\tlibgcc_s.so.1 => /opt/ohpc/pub/compiler/gcc/9.4.0/lib64/libgcc_s.so.1 (0x00007f17ea595000)\n\tlibpthread.so.0 => /lib64/libpthread.so.0 (0x00007f17ea375000)\n\tlibrt.so.1 => /lib64/librt.so.1 (0x00007f17ea16d000)\n\tlibquadmath.so.0 => /opt/ohpc/pub/compiler/gcc/9.4.0/lib64/libquadmath.so.0 (0x00007f17e9f26000)\n\tlibstdc++.so.6 => /opt/ohpc/pub/compiler/gcc/9.4.0/lib64/libstdc++.so.6 (0x00007f17e9b46000)\n\n\tlibdl.so.2 => /lib64/libdl.so.2 (0x00007f17e9942000)\n\tlibtirpc.so.3 => /lib64/libtirpc.so.3 (0x00007f17e970f000)\n\tlibmpi_cxx.so.40 => /opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/lib/libmpi_cxx.so.40 (0x00007f17e94f2000)\n\tlibm.so.6 => /lib64/libm.so.6 (0x00007f17e9170000)\n\tlibgomp.so.1 => /opt/ohpc/pub/compiler/gcc/9.4.0/lib64/libgomp.so.1 (0x00007f17e8f3a000)\n\tlibc.so.6 => /lib64/libc.so.6 (0x00007f17e8b75000)\n\t/lib64/ld-linux-x86-64.so.2 (0x00007f17f159a000)\n\tlibopen-rte.so.40 => /opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/lib/libopen-rte.so.40 (0x00007f17e88bb000)\n\tlibopen-orted-mpir.so => /opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/lib/libopen-orted-mpir.so (0x00007f17e86b9000)\n\tlibopen-pal.so.40 => /opt/ohpc/pub/mpi/openmpi4-gnu9/4.1.1/lib/libopen-pal.so.40 (0x00007f17e83f0000)\n\tlibutil.so.1 => /lib64/libutil.so.1 (0x00007f17e81ec000)\n\tlibhwloc.so.15 => /opt/ohpc/pub/libs/hwloc/lib/libhwloc.so.15 (0x00007f17e7f94000)\n\tlibgssapi_krb5.so.2 => /lib64/libgssapi_krb5.so.2 (0x00007f17e7d3f000)\n\tlibkrb5.so.3 => /lib64/libkrb5.so.3 (0x00007f17e7a55000)\n\tlibk5crypto.so.3 => /lib64/libk5crypto.so.3 (0x00007f17e783e000)\n\tlibcom_err.so.2 => /lib64/libcom_err.so.2 (0x00007f17e763a000)\n\tlibxml2.so.2 => /lib64/libxml2.so.2 (0x00007f17e72d2000)\n\tlibkrb5support.so.0 => /lib64/libkrb5support.so.0 (0x00007f17e70c1000)\n\tlibkeyutils.so.1 => /lib64/libkeyutils.so.1 (0x00007f17e6ebd000)\n\tlibcrypto.so.1.1 => /lib64/libcrypto.so.1.1 (0x00007f17e69d4000)\n\tlibresolv.so.2 => /lib64/libresolv.so.2 (0x00007f17e67bd000)\n\tliblzma.so.5 => /lib64/liblzma.so.5 (0x00007f17e6596000)\n\tlibselinux.so.1 => /lib64/libselinux.so.1 (0x00007f17e636c000)\n\tlibpcre2-8.so.0 => /lib64/libpcre2-8.so.0 (0x00007f17e60e8000)",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2949218",
                          "updatedAt": "2022-06-14T17:54:03Z",
                          "publishedAt": "2022-06-14T17:10:35Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "What is this module? moose-dev-gcc/4.0.1\nIs it a module that includes the other ones?\nHas the petsc module been compiled with this mpi?\nFinally just a hunch, could you try compiling on the compute node? Like from scratch. just in case the compute node is too different from the head node\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2949598",
                          "updatedAt": "2022-06-14T18:05:38Z",
                          "publishedAt": "2022-06-14T18:05:37Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "[root@thor2-head ~]# cat /opt/ohpc/pub/moduledeps/gnu9-openmpi4/moose-dev-gcc/4.0.1\n#%Module1.0#####################################################################\n## MOOSE module\n\nset base_path   /home/shared\n\nproc ModulesHelp { } {\n\nputs stderr \" \"\nputs stderr \"This module loads moose gnu9\"\nputs stderr \"toolchain and the openmpi MPI stack.\"\nputs stderr \" \"\n\nputs stderr \"\\nVersion 4.0.1\\n\"\n\n}\nmodule-whatis \"Name: moose gnu9 compiler and openmpi MPI\"\nmodule-whatis \"Version: 4.0.1\"\n\nset     version                     4.0.1\n\nconflict petsc\nconflict openmc\ndepends-on phdf5\n\nsetenv CC       mpicc\nsetenv CXX      mpicxx\nsetenv F90      mpif90\nsetenv F77      mpif77\nsetenv FC       mpif90\nsetenv PETSC_ARCH\tarch-moose\nsetenv PETSC_DIR        $base_path/petsc\n# setenv PMIX_MCA_gds\thash\n\nprepend-path PYTHONPATH         /home/shared/petsc/python3.6\n\nYes, I compiled the shared petsc with the same mpi.\nI can look into compiling on a node. But I believe they are set up on OpenHPC with a lot of the \"build\" environment excluded from the node boot image. At present there is not even a make command on the nodes. As far as I knew the dev environment (build) is intended to be on the head. Then the binaries are run on the nodes. So all you need is the built shared libs.",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2949771",
                          "updatedAt": "2022-06-14T19:01:58Z",
                          "publishedAt": "2022-06-14T18:28:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@milljm I m not sure what is going on here, i dont see anything wrong",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2950037",
                          "updatedAt": "2022-06-14T19:02:12Z",
                          "publishedAt": "2022-06-14T19:02:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "You can probably drop PETSC_ARCH in the 4.0.1.lua file. As it looks like PETSc libs are located in\n/home/shared/petsc/lib/libpetsc.so.3.16\n\nand not in a $PETSC_DIR/$PETSC_ARCH path:\n/home/shared/petsc/arch-moose/lib/libpetsc.so.3.16\n\nCan you log into a compute node, and perform an ldd command on the moose_test-opt binary? If we're missing libraries that would be a good indicator. If we're not, does moose_test-opt run correctly enough to print a help page (as @GiudGiud asked)?\n<obtain a compute node>\n<module load>\ncd /home/mwmoorcroft/moose/test/\n./moose_test-opt -h\nAnd then if that works... try with MPI:\nmpiexec -n 2 ./moose_test-opt -h",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2950368",
                          "updatedAt": "2022-06-14T19:57:20Z",
                          "publishedAt": "2022-06-14T19:56:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "I made some mods to the node environment, and shockingly the recompile seems to be working so far. I'm not very far in yet.\nconfigure: Detected BOOST_ROOT; continuing with --with-boost=/opt/ohpc/pub/libs/gnu9/openmpi4/boost/1.76.0\nchecking for Boost headers version >= 1.47.0... /opt/ohpc/pub/libs/gnu9/openmpi4/boost/1.76.0/include\nchecking for Boost's header version... 1_76\nchecking for the toolset name used by Boost for mpicxx -std=gnu++17... configure: WARNING: could not figure out which toolset name to use for mpicxx -std=gnu++17\nchecking for the flags needed to use pthreads... conftest.cpp: In function 'int main()':\nconftest.cpp:34:24: warning: null argument where non-null required (argument 1) [-Wnonnull]\n34 |     pthread_attr_init(0); pthread_cleanup_push(0, 0);\n|                        ^\nconftest.cpp:35:27: warning: null argument where non-null required (argument 1) [-Wnonnull]\n35 |     pthread_create(0,0,0,0); pthread_cleanup_pop(0);\n|                           ^\nconftest.cpp:35:27: warning: null argument where non-null required (argument 3) [-Wnonnull]\nconftest.cpp:33:27: warning: 'th' is used uninitialized in this function [-Wuninitialized]\n33 | pthread_t th; pthread_join(th, 0);\n|               ~~~~~~~~~~~~^~~~~~~\n-pthread",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2950655",
                          "updatedAt": "2022-06-14T20:44:20Z",
                          "publishedAt": "2022-06-14T20:44:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "I removed arch-moose, but In order to compile on a node directly it appears I must follow this page...\nhttps://mooseframework.inl.gov/getting_started/installation/offline_installation.html\nThe nodes have no access to the internet.",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2950845",
                          "updatedAt": "2022-06-14T21:21:44Z",
                          "publishedAt": "2022-06-14T21:21:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I'd like to come back to the first issue of building on the head-node and then attempting running said built binaries on the compute node.\nI am more familiar with this method, as this is how our HPC clusters operate. This is in response to you creating a new issue: #21304. I don't think we can solve both at the same time, because you are operating from a shared location (that is my understanding anyway).\nSolving build issues on head-nodes is simpler, than solving build issues on what is supposed to be compute-only nodes (while I agree, building on compute nodes should work too).\nI am still curious what the following reports (after you build successfully on the head-node):\n<obtain a compute node>\n<module load>\ncd /home/mwmoorcroft/moose/test/\n./moose_test-opt -h\nAnd then if that works... try with MPI:\nmpiexec -n 2 ./moose_test-opt -h",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2957835",
                          "updatedAt": "2022-06-15T17:12:33Z",
                          "publishedAt": "2022-06-15T17:12:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "Ok, I appreciate the help. I'm going to complete the build on the head and I'll report back here when I'm ready to test again. I was under the impression that if you can't build moose on a compute node, you can't use moose on a compute node. It would be great if that's not true. I thought compiling was part of running moose. I'm the sysadmin, not the nuclear scientist.",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2958689",
                          "updatedAt": "2022-06-15T19:24:43Z",
                          "publishedAt": "2022-06-15T19:22:13Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "mntbighker"
                  },
                  "bodyText": "I'll check, but....\n[mwmoorcroft@c4 test]$ prun ./moose_test-opt -i ./tests/mesh/concentric_circle_mesh/concentric_circ\nle_mesh.i --mesh-only\n[prun] Master compute host = c4\n[prun] Resource manager = slurm\n[prun] Launch cmd = mpirun ./moose_test-opt -i ./tests/mesh/concentric_circle_mesh/concentric_circle_mesh.i --mesh-only (family=openmpi4)\nIn UnstructuredMesh::stitch_meshes:\nThis mesh has 53 nodes on boundary 1.\nOther mesh has 53 nodes on boundary 3.\nMinimum edge length on both surfaces is 0.002625.\nIn UnstructuredMesh::stitch_meshes:\nFound 53 matching nodes.\nIn UnstructuredMesh::stitch_meshes:\nThis mesh has 105 nodes on boundary 2.\nOther mesh has 105 nodes on boundary 4.\nMinimum edge length on both surfaces is 0.002625.\nIn UnstructuredMesh::stitch_meshes:\nFound 105 matching nodes.\nMesh Information:\nelem_dimensions()={2}\nspatial_dimension()=2\nn_nodes()=1225\nn_local_nodes()=228\nn_elem()=1212\nn_local_elem()=202\nn_active_elem()=1212\nn_subdomains()=9\nn_partitions()=6\nn_processors()=6\nn_threads()=1\nprocessor_id()=0\nis_prepared()=true\nis_replicated()=true\nMesh Bounding Box:\nMinimum: (x,y,z)=(-0.710315, -0.710315,        0)\nMaximum: (x,y,z)=(0.710315, 0.710315,        0)\nDelta:   (x,y,z)=( 1.42063,  1.42063,        0)\nMesh Element Type(s):\nQUAD4\nMesh Nodesets:\nNodeset 1, 7 nodes\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nBounding box delta: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nNodeset 2, 7 nodes\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nBounding box delta: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nNodeset 3, 7 nodes\nBounding box minimum: (x,y,z)=(0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nBounding box delta: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nNodeset 4, 7 nodes\nBounding box minimum: (x,y,z)=(-0.710315, 0.710315,        0)\nBounding box maximum: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nBounding box delta: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nMesh Sidesets:\nSideset 1 (left), 6 sides (EDGE2), 6 elems (QUAD4), 7 nodes\nSide volume: 1.42063\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(-0.710315, 0.710315,        0)\nBounding box delta: (x,y,z)=(2.22045e-16,  1.42063,        0)\nSideset 2 (bottom), 6 sides (EDGE2), 6 elems (QUAD4), 7 nodes\nSide volume: 1.42063\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(0.710315, -0.710315,        0)\nBounding box delta: (x,y,z)=( 1.42063, 3.33067e-16,        0)\nSideset 3 (right), 6 sides (EDGE2), 6 elems (QUAD4), 7 nodes\nSide volume: 1.42063\nBounding box minimum: (x,y,z)=(0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(0.710315, 0.710315,        0)\nBounding box delta: (x,y,z)=(2.22045e-16,  1.42063,        0)\nSideset 4 (top), 6 sides (EDGE2), 6 elems (QUAD4), 7 nodes\nSide volume: 1.42063\nBounding box minimum: (x,y,z)=(-0.710315, 0.710315,        0)\nBounding box maximum: (x,y,z)=(0.710315, 0.710315,        0)\nBounding box delta: (x,y,z)=( 1.42063, 1.11022e-16,        0)\nMesh Edgesets:\nNone\nMesh Subdomains:\nSubdomain 1: 276 elems (QUAD4, 276 active), 289 active nodes\nVolume: 0.201323\nBounding box minimum: (x,y,z)=( -0.2546,  -0.2546,        0)\nBounding box maximum: (x,y,z)=(  0.2546,   0.2546,        0)\nBounding box delta: (x,y,z)=(  0.5092,   0.5092,        0)\nSubdomain 2: 144 elems (QUAD4, 144 active), 161 active nodes\nVolume: 0.150984\nBounding box minimum: (x,y,z)=( -0.3368,  -0.3368,        0)\nBounding box maximum: (x,y,z)=(  0.3368,   0.3368,        0)\nBounding box delta: (x,y,z)=(  0.6736,   0.6736,        0)\nSubdomain 3: 96 elems (QUAD4, 96 active), 120 active nodes\nVolume: 0.0502081\nBounding box minimum: (x,y,z)=(   -0.36,    -0.36,        0)\nBounding box maximum: (x,y,z)=(    0.36,     0.36,        0)\nBounding box delta: (x,y,z)=(    0.72,     0.72,        0)\nSubdomain 4: 96 elems (QUAD4, 96 active), 120 active nodes\nVolume: 0.0502251\nBounding box minimum: (x,y,z)=( -0.3818,  -0.3818,        0)\nBounding box maximum: (x,y,z)=(  0.3818,   0.3818,        0)\nBounding box delta: (x,y,z)=(  0.7636,   0.7636,        0)\nSubdomain 5: 96 elems (QUAD4, 96 active), 101 active nodes\nVolume: 0.0252443\nBounding box minimum: (x,y,z)=( -0.3923,  -0.3923,        0)\nBounding box maximum: (x,y,z)=(  0.3923,   0.3923,        0)\nBounding box delta: (x,y,z)=(  0.7846,   0.7846,        0)\nSubdomain 6: 48 elems (QUAD4, 48 active), 72 active nodes\nVolume: 0.0251788\nBounding box minimum: (x,y,z)=( -0.4025,  -0.4025,        0)\nBounding box maximum: (x,y,z)=(  0.4025,   0.4025,        0)\nBounding box delta: (x,y,z)=(   0.805,    0.805,        0)\nSubdomain 7: 48 elems (QUAD4, 48 active), 72 active nodes\nVolume: 0.021476\nBounding box minimum: (x,y,z)=(  -0.411,   -0.411,        0)\nBounding box maximum: (x,y,z)=(   0.411,    0.411,        0)\nBounding box delta: (x,y,z)=(   0.822,    0.822,        0)\nSubdomain 8: 144 elems (QUAD4, 144 active), 162 active nodes\nVolume: 0.176113\nBounding box minimum: (x,y,z)=(  -0.475,   -0.475,        0)\nBounding box maximum: (x,y,z)=(   0.475,    0.475,        0)\nBounding box delta: (x,y,z)=(    0.95,     0.95,        0)\nSubdomain 9: 264 elems (QUAD4, 264 active), 288 active nodes\nVolume: 1.31744\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(0.710315, 0.710315,        0)\nBounding box delta: (x,y,z)=( 1.42063,  1.42063,        0)\nGlobal mesh volume = 2.01819\n[mwmoorcroft@c4 test]$ module unload pmix\n[mwmoorcroft@c4 test]$ prun ./moose_test-opt -i ./tests/mesh/concentric_circle_mesh/concentric_circ\nle_mesh.i --mesh-only\n[prun] Master compute host = c4\n[prun] Resource manager = slurm\n[prun] Launch cmd = mpirun ./moose_test-opt -i ./tests/mesh/concentric_circle_mesh/concentric_circle_mesh.i --mesh-only (family=openmpi4)\nIn UnstructuredMesh::stitch_meshes:\nThis mesh has 53 nodes on boundary 1.\nOther mesh has 53 nodes on boundary 3.\nMinimum edge length on both surfaces is 0.002625.\nIn UnstructuredMesh::stitch_meshes:\nFound 53 matching nodes.\nIn UnstructuredMesh::stitch_meshes:\nThis mesh has 105 nodes on boundary 2.\nOther mesh has 105 nodes on boundary 4.\nMinimum edge length on both surfaces is 0.002625.\nIn UnstructuredMesh::stitch_meshes:\nFound 105 matching nodes.\nMesh Information:\nelem_dimensions()={2}\nspatial_dimension()=2\nn_nodes()=1225\nn_local_nodes()=228\nn_elem()=1212\nn_local_elem()=202\nn_active_elem()=1212\nn_subdomains()=9\nn_partitions()=6\nn_processors()=6\nn_threads()=1\nprocessor_id()=0\nis_prepared()=true\nis_replicated()=true\nMesh Bounding Box:\nMinimum: (x,y,z)=(-0.710315, -0.710315,        0)\nMaximum: (x,y,z)=(0.710315, 0.710315,        0)\nDelta:   (x,y,z)=( 1.42063,  1.42063,        0)\nMesh Element Type(s):\nQUAD4\nMesh Nodesets:\nNodeset 1, 7 nodes\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nBounding box delta: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nNodeset 2, 7 nodes\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nBounding box delta: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nNodeset 3, 7 nodes\nBounding box minimum: (x,y,z)=(0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nBounding box delta: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nNodeset 4, 7 nodes\nBounding box minimum: (x,y,z)=(-0.710315, 0.710315,        0)\nBounding box maximum: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nBounding box delta: (x,y,z)=(-1.79769e+308, -1.79769e+308, -1.79769e+308)\nMesh Sidesets:\nSideset 1 (left), 6 sides (EDGE2), 6 elems (QUAD4), 7 nodes\nSide volume: 1.42063\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(-0.710315, 0.710315,        0)\nBounding box delta: (x,y,z)=(2.22045e-16,  1.42063,        0)\nSideset 2 (bottom), 6 sides (EDGE2), 6 elems (QUAD4), 7 nodes\nSide volume: 1.42063\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(0.710315, -0.710315,        0)\nBounding box delta: (x,y,z)=( 1.42063, 3.33067e-16,        0)\nSideset 3 (right), 6 sides (EDGE2), 6 elems (QUAD4), 7 nodes\nSide volume: 1.42063\nBounding box minimum: (x,y,z)=(0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(0.710315, 0.710315,        0)\nBounding box delta: (x,y,z)=(2.22045e-16,  1.42063,        0)\nSideset 4 (top), 6 sides (EDGE2), 6 elems (QUAD4), 7 nodes\nSide volume: 1.42063\nBounding box minimum: (x,y,z)=(-0.710315, 0.710315,        0)\nBounding box maximum: (x,y,z)=(0.710315, 0.710315,        0)\nBounding box delta: (x,y,z)=( 1.42063, 1.11022e-16,        0)\nMesh Edgesets:\nNone\nMesh Subdomains:\nSubdomain 1: 276 elems (QUAD4, 276 active), 289 active nodes\nVolume: 0.201323\nBounding box minimum: (x,y,z)=( -0.2546,  -0.2546,        0)\nBounding box maximum: (x,y,z)=(  0.2546,   0.2546,        0)\nBounding box delta: (x,y,z)=(  0.5092,   0.5092,        0)\nSubdomain 2: 144 elems (QUAD4, 144 active), 161 active nodes\nVolume: 0.150984\nBounding box minimum: (x,y,z)=( -0.3368,  -0.3368,        0)\nBounding box maximum: (x,y,z)=(  0.3368,   0.3368,        0)\nBounding box delta: (x,y,z)=(  0.6736,   0.6736,        0)\nSubdomain 3: 96 elems (QUAD4, 96 active), 120 active nodes\nVolume: 0.0502081\nBounding box minimum: (x,y,z)=(   -0.36,    -0.36,        0)\nBounding box maximum: (x,y,z)=(    0.36,     0.36,        0)\nBounding box delta: (x,y,z)=(    0.72,     0.72,        0)\nSubdomain 4: 96 elems (QUAD4, 96 active), 120 active nodes\nVolume: 0.0502251\nBounding box minimum: (x,y,z)=( -0.3818,  -0.3818,        0)\nBounding box maximum: (x,y,z)=(  0.3818,   0.3818,        0)\nBounding box delta: (x,y,z)=(  0.7636,   0.7636,        0)\nSubdomain 5: 96 elems (QUAD4, 96 active), 101 active nodes\nVolume: 0.0252443\nBounding box minimum: (x,y,z)=( -0.3923,  -0.3923,        0)\nBounding box maximum: (x,y,z)=(  0.3923,   0.3923,        0)\nBounding box delta: (x,y,z)=(  0.7846,   0.7846,        0)\nSubdomain 6: 48 elems (QUAD4, 48 active), 72 active nodes\nVolume: 0.0251788\nBounding box minimum: (x,y,z)=( -0.4025,  -0.4025,        0)\nBounding box maximum: (x,y,z)=(  0.4025,   0.4025,        0)\nBounding box delta: (x,y,z)=(   0.805,    0.805,        0)\nSubdomain 7: 48 elems (QUAD4, 48 active), 72 active nodes\nVolume: 0.021476\nBounding box minimum: (x,y,z)=(  -0.411,   -0.411,        0)\nBounding box maximum: (x,y,z)=(   0.411,    0.411,        0)\nBounding box delta: (x,y,z)=(   0.822,    0.822,        0)\nSubdomain 8: 144 elems (QUAD4, 144 active), 162 active nodes\nVolume: 0.176113\nBounding box minimum: (x,y,z)=(  -0.475,   -0.475,        0)\nBounding box maximum: (x,y,z)=(   0.475,    0.475,        0)\nBounding box delta: (x,y,z)=(    0.95,     0.95,        0)\nSubdomain 9: 264 elems (QUAD4, 264 active), 288 active nodes\nVolume: 1.31744\nBounding box minimum: (x,y,z)=(-0.710315, -0.710315,        0)\nBounding box maximum: (x,y,z)=(0.710315, 0.710315,        0)\nBounding box delta: (x,y,z)=( 1.42063,  1.42063,        0)\nGlobal mesh volume = 2.01819",
                  "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2960335",
                  "updatedAt": "2022-06-16T01:40:10Z",
                  "publishedAt": "2022-06-16T01:40:09Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Some tests are designed to only work in serial, while other are designed to only work in parallel. It would seem you need to prefix every moose based app command with prun. You can do so by exporting the following influential environment variable:\nexport MOOSE_MPI_COMMAND=prun\n./run_test -j 16\nThere are two tests which require a minimum of 16 cores:\ncd moose/test/tests\ngit grep \"min_parallel\"\n<trimmed>\npartitioners/hierarchical_grid_partitioner/tests:    min_parallel = 16\nfvkernels/fv_adapt/tests:    min_parallel = 16\n<trimmed>\nIf you do not provide -j 16, these two tests will be skipped with \"insufficient slots\"\nBut anyway... lets see if running with prun changes the game at all first!",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2964037",
                          "updatedAt": "2022-06-16T12:54:18Z",
                          "publishedAt": "2022-06-16T12:54:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I can't really go into anything about Python being our issue. The TestHarness is dying very early. Probably because the TestHarness is breaking some rules while on a node, and being killed externally (not running with prun I suspect).\nWhen the TestHarness is killed prematurely you will commonly see a stack trace caused by Python threading:\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 355, in apply_async\n    raise ValueError(\"Pool not running\")\nValueError: Pool not running",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2964058",
                          "updatedAt": "2022-06-16T12:59:25Z",
                          "publishedAt": "2022-06-16T12:56:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "With the prun env variable it dies pretty quickly. The prun command is a wrapper script that runs mpiexec if I recall.\n[mwmoorcroft@c1 test]$ prun\nThis OpenHPC utility is used to launch parallel (MPI) applications\nwithin a supported resource manager.\nUsage: prun  executable [arguments]\nwhere available options are as follows:\n-h        generate help message and exit\n-v        enable verbose output\nmisc/check_error.dot_integrity_check: Tester failed, reason: EXPECTED ERROR MISSING\nmisc/check_error.dot_integrity_check:\nmisc/check_error.dot_integrity_check ......................................... FAILED (EXPECTED ERROR MISSING)\nmisc/check_error.multi_precond_test: Working Directory: /home/mwmoorcroft/moose/test/tests/misc/check_error\nmisc/check_error.multi_precond_test: Running command: prun -n 1 /home/mwmoorcroft/moose/test/moose_test-opt -i multi_precond_test.i --error --error-unused --error-override --no-gdb-backtrace\nmisc/check_error.multi_precond_test: /opt/ohpc/pub/utils/prun/2.2/prun: illegal option -- n\nmisc/check_error.multi_precond_test: Invalid option: -",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2965587",
                          "updatedAt": "2022-06-16T16:42:27Z",
                          "publishedAt": "2022-06-16T16:41:22Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "MOOSE_MPI_COMMAND=mpirun\nand\n#> ./run_script -j 6 \n\nSeems to be giving the best result.\nAt this point it appears to me we are back to python modules that exist on head that are missing on nodes. But it's big progress.",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2965667",
                          "updatedAt": "2022-06-16T16:55:13Z",
                          "publishedAt": "2022-06-16T16:55:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "Not surprisingly, still lots of these...\nusability.command-line/empty: Working Directory: /home/mwmoorcroft/moose/test/tests/usability\nusability.command-line/empty: Running command: /home/mwmoorcroft/moose/test/moose_test-opt\nusability.command-line/empty: [c1.Thor2.local:158018] OPAL ERROR: Unreachable in file pmix3x_client.c at line 112\nusability.command-line/empty: --------------------------------------------------------------------------\nusability.command-line/empty: The application appears to have been direct launched using \"srun\",\nusability.command-line/empty: but OMPI was not built with SLURM's PMI support and therefore cannot\nusability.command-line/empty: execute. There are several options for building PMI support under\nusability.command-line/empty: SLURM, depending upon the SLURM version you are using:\nusability.command-line/empty:\nusability.command-line/empty:   version 16.05 or later: you can use SLURM's PMIx support. This\nusability.command-line/empty:   requires that you configure and build SLURM --with-pmix.\nusability.command-line/empty:\nusability.command-line/empty:   Versions earlier than 16.05: you must use either SLURM's PMI-1 or\nusability.command-line/empty:   PMI-2 support. SLURM builds PMI-1 by default, or you can manually\nusability.command-line/empty:   install PMI-2. You must then build Open MPI using --with-pmi pointing\nusability.command-line/empty:   to the SLURM PMI library location.\nusability.command-line/empty:\nusability.command-line/empty: Please configure as appropriate and try again.",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2965691",
                          "updatedAt": "2022-06-16T17:53:00Z",
                          "publishedAt": "2022-06-16T16:58:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "It's still dying with python errors without getting the end statement with skipped, failed, etc.\nindicators/laplacian_jump_indicator.group/test_biharmonic ................................................. OK\nrunWorker Exception: Traceback (most recent call last):\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/RunParallel.py\", line 44, in run\n    job.run()\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Job.py\", line 227, in run\n    self.__tester.run(self.timer, self.options)\n  File \"/home/mwmoorcroft/moose/python/TestHarness/testers/bench.py\", line 163, in run\n    t.run(timer, timeout=p['max_time'])\n  File \"/home/mwmoorcroft/moose/python/TestHarness/testers/bench.py\", line 98, in run\n    raise RuntimeError('command {} returned nonzero exit code'.format(cmd))\nRuntimeError: command ['/home/mwmoorcroft/moose/test/moose_test-opt', '-i', '/home/mwmoorcroft/moose/test/tests/kernels/simple_diffusion/simple_diffusion.i', '--check-input', 'Mesh/nx=100', 'Mesh/ny=100', 'Outputs/console=false', 'Outputs/exodus=false', 'Outputs/csv=false', '--no-gdb-backtrace'] returned nonzero exit code\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 412, in runJob\n    self.run(job) # Hand execution over to derived scheduler\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/RunParallel.py\", line 74, in run\n    tester.setStatus(tester.error, 'PYTHON EXCEPTION')\nAttributeError: 'SpeedTest' object has no attribute 'error'\n\nrunWorker Exception: Traceback (most recent call last):\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 438, in runJob\n    self.queueJobs(jobs, j_lock)\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 260, in queueJobs\n    self.run_pool.apply_async(self.runJob, (job, jobs, j_lock))\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 355, in apply_async\n    raise ValueError(\"Pool not running\")\nValueError: Pool not running\n\nrunWorker Exception: Traceback (most recent call last):\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 438, in runJob\n    self.queueJobs(jobs, j_lock)\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 260, in queueJobs\n    self.run_pool.apply_async(self.runJob, (job, jobs, j_lock))\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 355, in apply_async\n    raise ValueError(\"Pool not running\")\nValueError: Pool not running\n\nrunWorker Exception: Traceback (most recent call last):\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 438, in runJob\n    self.queueJobs(jobs, j_lock)\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 255, in queueJobs\n    self.status_pool.apply_async(self.jobStatus, (job, jobs, j_lock))\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 355, in apply_async\n    raise ValueError(\"Pool not running\")\nValueError: Pool not running\nrunWorker Exception: Traceback (most recent call last):\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 438, in runJob\n    self.queueJobs(jobs, j_lock)\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 260, in queueJobs\n    self.run_pool.apply_async(self.runJob, (job, jobs, j_lock))\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 355, in apply_async\n    raise ValueError(\"Pool not running\")\nValueError: Pool not running\n\n\nrunWorker Exception: Traceback (most recent call last):\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 438, in runJob\n    self.queueJobs(jobs, j_lock)\n  File \"/home/mwmoorcroft/moose/python/TestHarness/schedulers/Scheduler.py\", line 255, in queueJobs\n    self.status_pool.apply_async(self.jobStatus, (job, jobs, j_lock))\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 355, in apply_async\n    raise ValueError(\"Pool not running\")\nValueError: Pool not running",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2965700",
                          "updatedAt": "2022-06-16T17:48:21Z",
                          "publishedAt": "2022-06-16T17:00:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "What I will say is it's not leaving a dozen running threads behind like it was before when it dies. I just ran on 2 nodes with 12 cores allocated and the tests seem to still be running.",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2965709",
                          "updatedAt": "2022-06-16T18:02:20Z",
                          "publishedAt": "2022-06-16T17:02:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "something is killing the TestHarness not by the doing of the TestHarness itself. When you see these errors. e.g:\n    raise ValueError(\"Pool not running\")\nValueError: Pool not running\n\nThe TestHarness was sent a kill signal by an external program.\nIt means we are not executing moose_test-opt correctly while on a node. The test where it failed at was a benchmark test. I believe the benchmark tests do not honor the MOOSE_MPI_COMMAND. Can you try this:\nexport MOOSE_MPI_COMMAND=mpirun\n./run_tests -j 6 -i tests\nThis basically tells the TestHarness to only run specification files with the name 'tests'.",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2966105",
                          "updatedAt": "2022-06-16T18:13:20Z",
                          "publishedAt": "2022-06-16T18:13:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "The following is just for informational purposes....\nAs for prun failures, it is because we assume all MPI commands accepts -n and then a number... If it comes down to it, we can HAK the TestHarness to drop this, just to see if that works on your HPC system.\nThe file/line responsible for the additional -n argument is here: \n  \n    \n      moose/python/TestHarness/testers/RunApp.py\n    \n    \n         Line 207\n      in\n      74571c4\n    \n  \n  \n    \n\n        \n          \n           command = self.mpi_command + ' -n ' + str(ncpus) + ' ' + command \n        \n    \n  \n\n\nFor your purposes, you would want to alter that to:\n            command = self.mpi_command + ' ' + command\nHowever, if my understanding of prun is correct, running prun throws all available resources (cores) at one binary. Not something we want! The TestHarness manages resource allocation for you. e.g: we can't have two programs doing the same things (the TestHarness would try to run multiple pruns).",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2966197",
                          "updatedAt": "2022-06-16T18:26:19Z",
                          "publishedAt": "2022-06-16T18:26:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mntbighker"
                          },
                          "bodyText": "It's running. For some reason some of the python modules I manually installed are not detected, so they get skipped. One example is vtk. A lot of the ones I installed are detected though.\nEverything seems pretty good just using MOOSE_MPI_COMMAND=mpirun. Probably don't need to bother about prun since it's just a convenience wrapper.\nThe tests are STILL running. :-)",
                          "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2966207",
                          "updatedAt": "2022-06-16T19:10:17Z",
                          "publishedAt": "2022-06-16T18:27:30Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "mntbighker"
                  },
                  "bodyText": "For the record there has been a bug in OpenHPC (maybe it's slurm) that prevented pmix from working correctly.\nApparently they STILL have not fixed pmix.\n[]openhpc/ohpc#1320\nThey pushed the milestone from OHPC 2.5 to OHPC 2.6.  :-(  The only answer is to rebuild openmpi yourself. I have have done that, but then patches don't arrive with dnf.\nedit: I'm told OpenMPI PMIx may get fixed after July.",
                  "url": "https://github.com/idaholab/moose/discussions/21241#discussioncomment-2960339",
                  "updatedAt": "2022-06-22T18:12:43Z",
                  "publishedAt": "2022-06-16T01:41:01Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Anisotropic elasticity problem",
          "author": {
            "login": "avtarsinghh1991"
          },
          "bodyText": "Hello\nI am trying to incorporate anisotropic material properties with euler angles for solving simple elasticity problem\nFollowing is the input file.\n[GlobalParams]\n  displacements = 'disp_x disp_y'\n[]\n\n[Mesh]\n  [generated]\n    type = GeneratedMeshGenerator\n    dim = 2\n    nx = 10\n    ny = 10\n    xmax = 1\n    ymax = 1\n  []\n  [pin]\n    type = ExtraNodesetGenerator\n    input = generated\n    new_boundary = pin\n    coord = '0 0 0'\n  []\n[]\n\n[Modules/TensorMechanics/Master]\n  [all]\n    add_variables = true\n    strain = FINITE\n    generate_output = 'stress_xx stress_yy vonmises_stress'\n  []\n[]\n\n#\n# Added boundary/loading conditions\n# https://mooseframework.inl.gov/modules/tensor_mechanics/tutorials/introduction/step02.html\n#\n[BCs]\n  [left_x]\n    type = DirichletBC\n    variable = disp_x\n    boundary = left\n    value = 0\n  []\n  [bottom_y]\n    type = DirichletBC\n    variable = disp_y\n    boundary = bottom\n    value = 0\n  []\n  [right_x]\n    type = FunctionDirichletBC\n    variable = disp_y\n    boundary = top\n    function = '0.001*t'\n  []\n  [pin_x]\n    type = DirichletBC\n    variable = disp_x\n    boundary = pin\n    value = 0\n  []\n  [pin_y]\n    type = DirichletBC\n    variable = disp_y\n    boundary = pin\n    value = 0\n  []\n[]\n\n[Materials]\n  [./elasticity_matrix]\n    type = ComputeElasticityTensor\n    base_name = 'rotation_matrix'\n    fill_method = symmetric9\n    C_ijkl = '1.684e9 0.176e9 0.176e9 1.684e9 0.176e9 1.684e9 0.754e9 0.754e9 0.754e9'\n    # rotation matrix for rotating a vector 30 degrees about the z-axis\n    rotation_matrix = '0.8660254 -0.5       0.\n                       0.5        0.8660254 0\n                       0          0         1'\n  [../]\n  [./elasticity_euler]\n    type = ComputeElasticityTensor\n    base_name = 'euler'\n    fill_method = symmetric9\n    C_ijkl = '1.684e9 0.176e9 0.176e9 1.684e9 0.176e9 1.684e9 0.754e9 0.754e9 0.754e9'\n    euler_angle_1 = -30.  # same as above but opposite direction because _transpose_ gets built from these angles\n    euler_angle_2 = 0.\n    euler_angle_3 = 0.\n  [../]\n\n  [stress]\n    type = ComputeFiniteStrainElasticStress\n  []\n[]\n\n\n# consider all off-diagonal Jacobians for preconditioning\n[Preconditioning]\n  [SMP]\n    type = SMP\n    full = true\n  []\n[]\n\n[Executioner]\n  type = Transient\n  # we chose a direct solver here\n  petsc_options_iname = '-pc_type'\n  petsc_options_value = 'lu'\n  end_time = 100\n  dt = 1\n[]\n\nWhen I am running it. I am getting the following error\n\n*** ERROR ***\nThe following error occurred in the object \"MOOSE Problem\", of type \"FEProblem\".\n\nMaterial property 'elasticity_tensor', requested by 'stress' is not defined on block 0\nMaterial property 'elasticity_tensor', requested by 'stress_face' is not defined on block 0\nMaterial property 'elasticity_tensor', requested by 'stress_neighbor' is not defined on block 0\n\nFurther, i have my own abaqus mesh file with few grains. How can I read the euler angles and youngs modulus for different grains from the file. Please suggest some reference example.\nPlease help. Thank you in advance.",
          "url": "https://github.com/idaholab/moose/discussions/19576",
          "updatedAt": "2022-06-22T16:38:56Z",
          "publishedAt": "2021-12-07T23:58:14Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "jiangwen84"
                  },
                  "bodyText": "you should not define two ComputeElasticityTensor\nIt seems that you want to model polycrystalline materials. If so, please take a look the crystal plasticity models in MOOSE. see https://mooseframework.inl.gov/source/materials/crystal_plasticity/ComputeMultipleCrystalPlasticityStress.html",
                  "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-1769881",
                  "updatedAt": "2022-06-13T08:09:01Z",
                  "publishedAt": "2021-12-08T02:14:06Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Thanks for your reply. Actually, I am trying to rotate the elastic tensor depending on the crystallographic plane.\nC' = RCR^T\nin which R is the rotation matrix.",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-1770071",
                          "updatedAt": "2022-06-13T08:09:02Z",
                          "publishedAt": "2021-12-08T03:07:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "You can keep just one ComputeElasticityTensor and then remove the base_name.",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-1770174",
                          "updatedAt": "2022-06-13T08:09:11Z",
                          "publishedAt": "2021-12-08T03:43:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Yes, I did the same. I even did a few runs to make sure the orientation is fine. It's working now. Thank you very much.",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-1770178",
                          "updatedAt": "2022-06-13T08:09:11Z",
                          "publishedAt": "2021-12-08T03:43:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Thanks. Just a follow-up question. I also want to give different Euler angles for different grains. and I want to read those angles from the file. I have my own mesh file with different volume tags for different grains. Can you please suggest some example problems?\nMost of the CP model examples are single-crystal only.",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-1770193",
                          "updatedAt": "2022-06-19T00:50:28Z",
                          "publishedAt": "2021-12-08T03:51:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "You can use ElementPropertyReadFile with read_type option to read in the Euler angles for each block.\nHere is one example\nmoose/modules/tensor_mechanics/test/tests/crystal_plasticity/user_object_based/prop_block_read.i",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-1770293",
                          "updatedAt": "2022-06-19T00:50:28Z",
                          "publishedAt": "2021-12-08T04:30:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "avtarsinghh1991"
                          },
                          "bodyText": "Thank you. I implemented it and its working.\nI am just wondering is that only works with \"ComputeElasticityTensorCP\" or any material type. Because I can only able to run it for \"ComputeElasticityTensorCP\".\nBelow is the input file.\n[GlobalParams]\ndisplacements = 'disp_x disp_y'\n[]\n[Mesh]\n[generated]\ntype = GeneratedMeshGenerator\ndim = 2\nnx = 10\nny = 10\nxmax = 1\nymax = 1\n[]\n[pin]\ntype = ExtraNodesetGenerator\ninput = generated\nnew_boundary = pin\ncoord = '0 0 0'\n[]\n[]\n[UserObjects]\n[./euler_angle_read]\ntype = ElementPropertyReadFile\nprop_file_name = 'input_file.txt'\nnprop = 3\nread_type = block\nnblock= 1\n[../]\n[]\n[Modules/TensorMechanics/Master]\n[all]\nadd_variables = true\nstrain = FINITE\ngenerate_output = 'stress_xx stress_yy vonmises_stress'\n[]\n[]\n[BCs]\n[left_x]\ntype = DirichletBC\nvariable = disp_x\nboundary = left\nvalue = 0\n[]\n[bottom_y]\ntype = DirichletBC\nvariable = disp_y\nboundary = bottom\nvalue = 0\n[]\n[right_x]\ntype = FunctionDirichletBC\nvariable = disp_x\nboundary = right\nfunction = '0.001t'\n[]\n[top_y]\ntype = FunctionDirichletBC\nvariable = disp_y\nboundary = top\nfunction = '0.001t'\n[]\n[pin_x]\ntype = DirichletBC\nvariable = disp_x\nboundary = pin\nvalue = 0\n[]\n[pin_y]\ntype = DirichletBC\nvariable = disp_y\nboundary = pin\nvalue = 0\n[]\n[]\n[Materials]\n[./elasticity_tensor_with_Euler]\ntype = ComputeElasticityTensorCP\nfill_method = axisymmetric9\nC_ijkl = '1.984e9 0.196e9 0.176e9 1.684e9 0.176e9 1.584e9 0.754e9 0.754e9 0.754e9'\nread_prop_user_object = euler_angle_read\n[../]\n[stress]\ntype = ComputeFiniteStrainElasticStress\n[]\n[]\n[Preconditioning]\n[SMP]\ntype = SMP\nfull = true\n[]\n[]\n[Executioner]\ntype = Transient\npetsc_options_iname = '-pc_type'\npetsc_options_value = 'lu'\nend_time = 100\ndt = 1\n[]\n[Outputs]\nexodus = true\ninterval =10\n[]\nIf I am putting material type = \"ComputeElasticityTensor\" instead of \"ComputeElasticityTensorCP\". It says \"unused parameter 'Materials/elasticity_tensor_with_Euler/read_prop_user_object'\"\nOr Can I use \"ElementPropertyReadFile\" for providing other properties to different materials.\nThanks in advance.",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-1775393",
                          "updatedAt": "2022-06-19T00:50:28Z",
                          "publishedAt": "2021-12-08T21:08:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "For elasticity tensors, only ComputeElasticityTensorCP is implemented to take Euler angles from ElementPropertyReadFile UO.",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-1775963",
                          "updatedAt": "2022-06-19T00:51:03Z",
                          "publishedAt": "2021-12-08T23:51:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "latmarat"
                          },
                          "bodyText": "@jiangwen84,\nwhen assigning euler angles to grains as \"blocks\", how do you set the correspondence between moose mesh elements and block ids?\nAlso, do you know how moose mesh elements are ordered along x,y,z (i.e., in which order the element labels are increasing)?\nThanks.\n@asingh-mit, I am also trying to run an elastic simulation with a polycrystal. Would you mind sharing your input file that worked for your elastic polycrystal?\nThank you all!",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-2978523",
                          "updatedAt": "2022-06-19T00:57:13Z",
                          "publishedAt": "2022-06-19T00:57:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "@latmarat You can either use MOOSE's SubdomainBoundingBoxGenerator or use external meshing tools, e.g. Cubit or Gmsh to assign block ids.",
                          "url": "https://github.com/idaholab/moose/discussions/19576#discussioncomment-3003954",
                          "updatedAt": "2022-06-22T16:38:56Z",
                          "publishedAt": "2022-06-22T16:38:55Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Question about random seed with ConservedLangevinNoise",
          "author": {
            "login": "mangerij"
          },
          "bodyText": "I have four calculations that I am running simultaneously on a cluster with 32 processors each (parallel across the nodes).\nEach of the calculations has the same random seed given to the variables (set in the UserObject as instructed).\nCan I trust that the noise that is given to my four calculations is the same because I have the same number of processors?",
          "url": "https://github.com/idaholab/moose/discussions/21379",
          "updatedAt": "2022-06-25T05:54:12Z",
          "publishedAt": "2022-06-22T09:06:00Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "The noise should be the consistent even with different parallel setting:\n\nMOOSE currently distributes a high-quality efficient Pseudo Random Number Generator package (mtwist) that is stable across different machine architectures. This random number generator is tied into MOOSE's random number generator system that can generate consistent spatial random number fields as parallel discretization changes (e.g. the number of threads/processors does not impact generated fields). The random number interface is very straightforward to use.\n\nSee https://mooseframework.inl.gov/source/interfaces/RandomInterface.html",
                  "url": "https://github.com/idaholab/moose/discussions/21379#discussioncomment-3003369",
                  "updatedAt": "2022-06-22T15:30:15Z",
                  "publishedAt": "2022-06-22T15:30:13Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "You can always confirm this by either\n\noutputting the generated field for visualization\noutputting some metrics about the sampled field using postprocessors",
                          "url": "https://github.com/idaholab/moose/discussions/21379#discussioncomment-3003383",
                          "updatedAt": "2022-06-22T15:32:28Z",
                          "publishedAt": "2022-06-22T15:32:28Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How does Materials object call member functions in VectorPostprocessor object?",
          "author": {
            "login": "PengWei97"
          },
          "bodyText": "Dear MOOSE experts,\nRecently I wanted to construct a grain boundary energy as a function of grain size in the material object GBEvolution. By searching the code, I found only the case where the material class is coupled with the userobject, like: _grain_tracker(getUserObject<GrainTracker>(\"grain_tracker\")), but not the case where the VectorPostprocessor is called. In addition, the FeatureVolumeVectorPostprocessor object calculates the volume of each feature, and creates the pubilc function getFeatureVolume so that we can get the volume of each feature based on the feature id.\nSo my question is, how to call FeatureVolumeVectorPostprocessor.getFeatureVolume in material object GBEvolution to get feature volume.\nAny suggestions or recommendations to fix the problem would be greatly appreciated.\nThank you\nWei",
          "url": "https://github.com/idaholab/moose/discussions/21369",
          "updatedAt": "2022-06-22T15:21:22Z",
          "publishedAt": "2022-06-21T15:54:42Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nIf this is a standard vectorpostprocessor you should  _fe_problem.getVectorPostprocessorValueByName(_vpp_name, vec_name);\nto retrieve the values of the VPP. You can bind this to a reference in the constructor so that you only call this once.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21369#discussioncomment-2996602",
                  "updatedAt": "2022-06-21T19:13:30Z",
                  "publishedAt": "2022-06-21T19:13:30Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "PengWei97"
                          },
                          "bodyText": "Ok, Thank you for your very kind reply.\nThis function getVectorPostprocessorValueByName can achieve my requirements, with the following code,\n  const auto & k1 = _fe_problem.getVectorPostprocessorValueByName(_vector_postprocessor_name, _vector_name); // _fe_problem. VectorPostprocessorValue \n  std::cout << \"the size of k1 is \" << k1.size() <<std::endl;\n  for(unsigned int i = 0; i < k1.size(); i++)\n      std::cout << \"the value of k1 is \" << k1[i] <<std::endl;  \nBut I'm not quite sure why _fe_problem is added? Please can you give me guidance on this.\nSimilarly, if you do not use _fe_problem, you can also get the desired result, and the result is as follows.\n\nwei",
                          "url": "https://github.com/idaholab/moose/discussions/21369#discussioncomment-3002355",
                          "updatedAt": "2022-06-22T13:38:54Z",
                          "publishedAt": "2022-06-22T13:38:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "it depends on which object has the VPPInterface to use this routine. The problem object does. I guess your material also does.\nEither way is fine",
                          "url": "https://github.com/idaholab/moose/discussions/21369#discussioncomment-3003151",
                          "updatedAt": "2022-06-22T15:05:41Z",
                          "publishedAt": "2022-06-22T15:05:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "PengWei97"
                          },
                          "bodyText": "good, the problem is solved, thank you very much!",
                          "url": "https://github.com/idaholab/moose/discussions/21369#discussioncomment-3003287",
                          "updatedAt": "2022-06-22T15:21:20Z",
                          "publishedAt": "2022-06-22T15:21:18Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Peacock's problem",
          "author": {
            "login": "tmewhy"
          },
          "bodyText": "Hi all,\nI installed moose on ubuntu20.04 offline using the official document solution. After I finished the installation, I used a simple case for testing. When opening peacock, the following reminder appeared:\n\nQCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 111, 111, 111, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 180, 180, 180, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 111, 111, 111, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 111, 111, 111, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 180, 180, 180, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 111, 111, 111, 255'\n\nWhen I use peacock to open the output file (.e), peacock crashes with the following error:\n\nQCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 111, 111, 111, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 180, 180, 180, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 111, 111, 111, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 111, 111, 111, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 180, 180, 180, 255'   QCssParser::parseColorValue: Specified color without alpha value but alpha given: 'rgb 111, 111, 111, 255\nSegmentation fault (core dumped)\n\nMy guess is that qt or vtk has not been installed well, but I have reinstalled them several times and it still has no effect. Thanks for the help!",
          "url": "https://github.com/idaholab/moose/discussions/18048",
          "updatedAt": "2022-06-22T15:03:40Z",
          "publishedAt": "2021-06-10T02:35:55Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "milljm"
                  },
                  "bodyText": "It's impressive you were able to get Peacock to launch at all using the Offline instructions :) I am not seeing anything about Peacock and its myriad amount of dependencies one needs to install in order to make it work.\nHow did you manage (just curious)? Did you build each from source, or use a package manager?\nIndeed this feels like a python binding to one of the VTK or Qt libraries. Probably VTK. Since Peacock at the very least opens without more than a few notices. If it were Qt and it's stack, I would recon Peacock wouldn't load up at all.\nWe can use 'chigger' to test pieces of your install. Try the following simple test:\nexport MOOSE_DIR=/path/to/moose\nexport PYTHONPATH=/path/to/moose/python:$PYTHONPATH\n# where /path/to/moose is your absolute path to moose\n\ncd /path/to/moose/python/chigger/tests/simple\n./simple.py\nThe simple.py script should run for a bit, and produce a file: simple.png (if all goes well). It probably won't in your case, and I am hoping for a bit more information other than Seg fault. Regardless, this moose/python/chigger/tests/ location is littered with tests we can run to try and zero in on the issue. @aeslaughter would know best (tagging if he would like chime in).",
                  "url": "https://github.com/idaholab/moose/discussions/18048#discussioncomment-851864",
                  "updatedAt": "2022-06-22T03:58:58Z",
                  "publishedAt": "2021-06-10T13:35:40Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "tmewhy"
                          },
                          "bodyText": "Thanks, I used \"apt-offline\" and \"pip download\" to obtain the relevant installation package on an online computer, and then install it on an offline computer. It did take some time : ) Although it has passed the make -j 6 test, I am not sure that I installed it correctly.\nI ran simple.pyand got the following prompt:\n\nAborted (core dumped)",
                          "url": "https://github.com/idaholab/moose/discussions/18048#discussioncomment-856208",
                          "updatedAt": "2022-06-22T03:58:58Z",
                          "publishedAt": "2021-06-11T01:47:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Is this still an issue? Thinking of closing, we dont have too many resources on peacock these days.\nTo get a backtrace on a seg fault in python, should look at https://docs.python.org/3/library/faulthandler.html",
                          "url": "https://github.com/idaholab/moose/discussions/18048#discussioncomment-1829264",
                          "updatedAt": "2022-06-22T03:59:26Z",
                          "publishedAt": "2021-12-16T23:04:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jianqixi"
                          },
                          "bodyText": "Did you solve the problem? I also met the same issue of \"Aborted (core dumped)\" when call peacock",
                          "url": "https://github.com/idaholab/moose/discussions/18048#discussioncomment-2998793",
                          "updatedAt": "2022-06-22T04:00:43Z",
                          "publishedAt": "2022-06-22T04:00:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "@jianqixi, what OS are you operating from?\nThe following has nothing to do with your current situation. But I feel I must mention:\nI am currently removing support for Peacock in our next large update to our Conda packages: #21324\nI am opting instead to create a separate package (lets say: moose-peacock) which will most likely need to be installed into a separate environment e.g. something like this:\n(this doesn't work/exist yet, this is just for demonstration purposes)\nmamba create -n moose moose-libmesh moose-tools    # For compiling moose-based application\nmamba create -n peacock moose-peacock              # Strictly for running Peacock\nYou would then activate which ever environment you needed at the time.\nThe reason for separating them is due to the increasingly difficult-to-solve constraints the dependency chain required for Peacock operation V's the dependency chain required for the compilers/libraries necessary to build/run MOOSE based application.",
                          "url": "https://github.com/idaholab/moose/discussions/18048#discussioncomment-3002444",
                          "updatedAt": "2022-06-22T21:47:08Z",
                          "publishedAt": "2022-06-22T13:47:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Posting this more for myself;\nPeacock (on MacOS) requires these Conda packages:\nmamba create -n peacock pyqt vtk matplotlib pandas python=3.x  # (where x is whatever\n                                                                  version of python that\n                                                                  your moose environment\n                                                                  is using)",
                          "url": "https://github.com/idaholab/moose/discussions/18048#discussioncomment-3003130",
                          "updatedAt": "2022-06-22T15:03:41Z",
                          "publishedAt": "2022-06-22T15:03:40Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Output at each fixed point iteration",
          "author": {
            "login": "BoZeng1997"
          },
          "bodyText": "Hi,\nI want to know if there is a way to output exodus file at each picard iteration within a timestep? Or where can I find an example of creating custom flag for picard iteration?\nBo",
          "url": "https://github.com/idaholab/moose/discussions/21117",
          "updatedAt": "2022-06-25T05:53:36Z",
          "publishedAt": "2022-05-24T18:43:28Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nwe currently do not have this feature.\nif this is just for debugging I d recommend setting the max number of fixed point iterations at all the desired times (running the code multiple times) then setting the accept_on_max_fixed_point_iteration parameter of the executioner to true\nguillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21117#discussioncomment-2814653",
                  "updatedAt": "2022-05-24T19:26:57Z",
                  "publishedAt": "2022-05-24T19:26:56Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "BoZeng1997"
                          },
                          "bodyText": "Not for debugging. I want to see how picard iteration find its path. The cost to run it till the critical step is high and the iteration in that step is of hundreds. Is there a way I can probably create an object to achieve this?",
                          "url": "https://github.com/idaholab/moose/discussions/21117#discussioncomment-2814685",
                          "updatedAt": "2022-05-24T19:32:43Z",
                          "publishedAt": "2022-05-24T19:32:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "If you\u2019re open to coding it s not that difficult to create an execute_on flag to output on fixed point iterations.\n@lindsayad has something like this in one of his branches I think",
                          "url": "https://github.com/idaholab/moose/discussions/21117#discussioncomment-2814695",
                          "updatedAt": "2022-05-24T19:35:19Z",
                          "publishedAt": "2022-05-24T19:35:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "BoZeng1997"
                          },
                          "bodyText": "is Alex Lindsay open to inquire here? If so, shall I wait till he replies? I will be grateful if I can follow an example.",
                          "url": "https://github.com/idaholab/moose/discussions/21117#discussioncomment-2814725",
                          "updatedAt": "2022-05-24T19:41:10Z",
                          "publishedAt": "2022-05-24T19:41:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "He s busy right now I believe but he ll get back to us when he has time",
                          "url": "https://github.com/idaholab/moose/discussions/21117#discussioncomment-2814799",
                          "updatedAt": "2022-05-24T19:56:26Z",
                          "publishedAt": "2022-05-24T19:56:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "This is what I have for an example: lindsayad/moose@7d0e9a3228cd221",
                          "url": "https://github.com/idaholab/moose/discussions/21117#discussioncomment-2815809",
                          "updatedAt": "2022-05-24T23:41:45Z",
                          "publishedAt": "2022-05-24T23:41:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "BoZeng1997"
                          },
                          "bodyText": "Hi @lindsayad . Is there a way to pack each iteration within a step into one exodus file in sequence? I did not find the way to append iteration step in exodus output.",
                          "url": "https://github.com/idaholab/moose/discussions/21117#discussioncomment-2934483",
                          "updatedAt": "2022-06-12T22:44:30Z",
                          "publishedAt": "2022-06-12T22:44:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Probably not for Picard iterations. What would you even want to write the timestep as?",
                          "url": "https://github.com/idaholab/moose/discussions/21117#discussioncomment-2997413",
                          "updatedAt": "2022-06-21T21:43:32Z",
                          "publishedAt": "2022-06-21T21:43:31Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}