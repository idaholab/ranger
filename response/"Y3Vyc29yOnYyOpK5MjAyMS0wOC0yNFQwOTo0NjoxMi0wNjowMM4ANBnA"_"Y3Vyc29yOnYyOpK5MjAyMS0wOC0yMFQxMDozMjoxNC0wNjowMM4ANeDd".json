{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMS0wOC0yMFQxMDozMjoxNC0wNjowMM4ANeDd"
    },
    "edges": [
      {
        "node": {
          "title": "AMR and Presplit Mesh",
          "author": {
            "login": "makeclean"
          },
          "bodyText": "Maybe I'm missing something obvious, but why cant one use Adaptivity & Distributed mesh?\n*** ERROR ***\nThe following error occurred in the object \"mesh\", of type \"FileMesh\".\n\nCannot use StatefulMaterials + Adaptivity with DistributedMesh!\nConsider specifying parallel_type = 'replicated' in your input file\nto prevent it from being run with DistributedMesh.\n\nLarge problems basically require pre-splitting and then use of the split mesh, right?",
          "url": "https://github.com/idaholab/moose/discussions/17367",
          "updatedAt": "2023-03-15T02:18:42Z",
          "publishedAt": "2021-03-18T00:04:30Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Large problems will indeed require distributed meshes.\nLooks related to #4532\n@permcody or @friedmud will know more.",
                  "url": "https://github.com/idaholab/moose/discussions/17367#discussioncomment-610671",
                  "updatedAt": "2023-03-15T02:18:52Z",
                  "publishedAt": "2021-04-14T15:26:42Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "As I have been using this system more lately, pre-splitting is only useful if the mesh is so big that it can't fit on a single node. This is only for very large problems.\nIt's also a good optimization to reduce IO on slower filesystems.",
                          "url": "https://github.com/idaholab/moose/discussions/17367#discussioncomment-1225593",
                          "updatedAt": "2023-03-15T02:18:52Z",
                          "publishedAt": "2021-08-24T05:02:23Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "makeclean"
                  },
                  "bodyText": "Yes, AMR would be very useful in these situations too though right?\n\u2026\n________________________________\nFrom: Guillaume Giudicelli ***@***.***>\nSent: Tuesday, August 24, 2021 6:02:34 AM\nTo: idaholab/moose ***@***.***>\nCc: Davis, Andrew ***@***.***>; Author ***@***.***>\nSubject: Re: [idaholab/moose] AMR and Presplit Mesh (#17367)\n\n\nAs I have been using this system more lately, pre-splitting is only useful if the mesh is so big that it can't fit on a single node. This is only for very large problems.\nIt's also a good optimization to reduce IO on slower filesystems.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<#17367 (reply in thread)>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AASTUST6Q2HQESP77LQC47DT6MRWVANCNFSM4ZLSK4BQ>.\nTriage notifications on the go with GitHub Mobile for iOS<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android<https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>.",
                  "url": "https://github.com/idaholab/moose/discussions/17367#discussioncomment-1225886",
                  "updatedAt": "2023-03-15T02:18:52Z",
                  "publishedAt": "2021-08-24T06:58:24Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Well I imagine there's a good reason for your use of stateful materials so that can't change.\nIf I were to choose one of the 3 to remove to make it work, it would be AMR. The mesh can be pre-refined.\nCombining the 3 is currently not supported and no one is currently working on allowing this. If you have someone work on this, we will be glad to help them enable it.",
                          "url": "https://github.com/idaholab/moose/discussions/17367#discussioncomment-1227983",
                          "updatedAt": "2023-03-15T02:20:13Z",
                          "publishedAt": "2021-08-24T15:04:49Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "PorousFlow - Barometric pp",
          "author": {
            "login": "MatiasAllay"
          },
          "bodyText": "Hi MOOSE,\nI was testing the barometric_fully_confined.i and I got a question. Why when I increase the bulk modulus (lower compressibility)  the porepressure goes down? I would expect the opposite behavior since a harder material should make the fluid bear all the pressure change than a soft one. I'm confused.\nCheers!\nMatias",
          "url": "https://github.com/idaholab/moose/discussions/18678",
          "updatedAt": "2022-07-26T13:29:58Z",
          "publishedAt": "2021-08-23T10:38:40Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "WilkAndy"
                  },
                  "bodyText": "When K is increased the Skempton coefficient, B, decreases, so the porepressure responds less to external pressures (see the Doan reference for the derivation).  What is happening is that the stiff matrix is bearing most of the load, and the fluid doesn't have to bear much at all.  Remember this \"fully confined\" situation is rather contrived - the aquitards sitting above and below the model don't let the fluid \"see\" the external pressure explicitly, only through the mechanical deformations.",
                  "url": "https://github.com/idaholab/moose/discussions/18678#discussioncomment-1224679",
                  "updatedAt": "2022-07-26T13:31:03Z",
                  "publishedAt": "2021-08-23T22:28:09Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "MatiasAllay"
                          },
                          "bodyText": "That answers my question. Thanks Andy",
                          "url": "https://github.com/idaholab/moose/discussions/18678#discussioncomment-1227169",
                          "updatedAt": "2022-07-26T13:31:03Z",
                          "publishedAt": "2021-08-24T12:10:17Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Implementing a Reynolds- and Prandtl-dependent Heat Transfer Coefficient",
          "author": {
            "login": "JKHINL"
          },
          "bodyText": "Hello,\nI have a geometry consisting of prismatic blocks with holes cut out through them, such that fluid can flow through the holes. These blocks are heated and have a temperature of T_solid, which is defined as a global variable. Likewise, the fluid temperature, T_fluid, is also declared as a global variable, and is heated via convection with the blocks.\nFrom hand calculations and equations of state, I know that the flow is laminar, but based on experimental data, I can't assume a uniform heat flux or surface temperature condition (which would make things much easier). I have a specific correlation that I would like to apply that is a function of the Reynolds and Prandtl numbers of the fluid for material property alpha (which is proportional to the heat transfer coefficient).\nI have had success implementing the Reynolds and Prandtl numbers as AuxVariables and AuxKernels, and can see these two variables in the postprocessor; however, when I try to implement these two into the GenericFunctionMaterial or ParsedMaterial -type Material blocks, Pronghorn informs me that it cannot recognize these two AuxVariables.\nIs there a recommended way that I would be able to implement these two fluid properties into a material block? Alternatively, I do have the mass flux rho*v as a global variable, but I would need to be able to extract the absolute viscosity mu, and I am not quite sure how to do that from the FluidProperties block. Thanks!",
          "url": "https://github.com/idaholab/moose/discussions/18677",
          "updatedAt": "2022-09-08T14:33:32Z",
          "publishedAt": "2021-08-23T03:35:53Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nUsing a material property for alpha is the correct approach for this.\nGenericFunctionMaterial will not work will variables. The ParsedMaterial should work. If you paste that (or part of that) material definition and the variable definitions here we may be able to spot the problem.\nHowever, I would recommend you simply add a new material for your correlation. Besides, we would certainly welcome the contribution in Pronghorn.\nAnother note is that the Reynolds and Prandtl numbers are already defined as material properties in PronghornFluidProps, so you could just add this material to have access to these. Note that if you are using automatic differentiation, auxiliary variables will not propagate the derivatives, so your Jacobian wont be as good with a material taking the AuxVariables as arguments.\nDo you have source code access on Pronghorn btw? If so we can point to you to specific examples of other materials.\nBest,\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/18677#discussioncomment-1220296",
                  "updatedAt": "2022-09-08T14:33:32Z",
                  "publishedAt": "2021-08-23T03:47:17Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "JKHINL"
                          },
                          "bodyText": "Hi Guillaume,\nI believe that I do have access to the source code, but I haven't made any contributions to it during my internship. Creating a separate material block specifically for this correlation that accepts the channel diameter, D, channel length, L, and fluid properties (Re, Pr, and fluid thermal conductivity, k) would certainly enrich my time here.\nHere is my block of code so far (D and L would have to be hardcoded in this format, and k_fluid estimated to a single value):\n  [Material_Properties_Greencast_Inner_Bypass]\n    type = ParsedMaterial\n    args = 'Re Pr'\n    function = '${fparse 0.065*k_fluid*Re*Pr*(D/L)/(1+0.04*(Re*Pr*(D/L))^(2/3))}'\n    material_property_names = 'alpha'\n    block = '16'\n  []\n\nWhen it comes to .C files, I understand how to request user input for D and L. My biggest source of confusion is how the code knows how and where it could pull Re, Pr, and k_fluid.",
                          "url": "https://github.com/idaholab/moose/discussions/18677#discussioncomment-1220885",
                          "updatedAt": "2023-05-25T18:28:59Z",
                          "publishedAt": "2021-08-23T07:22:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "The ParsedMaterial does not make use of the fparse system. It s a different parser.\nPlease look at the example in ParsedMaterial and follow this syntax instead. The list of parameters should help too\nmaterial_property_names is for the names of the material properties used in the function NOT for the name of the. material property you are declaring\nIf you make a new .C file instead of using the ParsedMaterial, then you can add variable parameters as\n\nparams.addCoupledVar(\"var_name\", \"pronghorn default name\", \"var description\")\nBut we would rather you used material properties,\nso params.addParam(\"mat prop name\", \"pronghorn namespace default name\", \"description\").\n\nPlease see for example the KTADragCoefficient.C material for an example of retrieving material properties.\nAnd DragCoefficients.C for an example of declaring new material properties.",
                          "url": "https://github.com/idaholab/moose/discussions/18677#discussioncomment-1222894",
                          "updatedAt": "2022-09-08T14:33:36Z",
                          "publishedAt": "2021-08-23T15:00:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "JKHINL"
                          },
                          "bodyText": "I am first trying to get ParsedMaterial to work because I ran into issues with compiling Pronghorn with HPC when I made some changes to which modules I accessed earlier this summer. If I have time before Thursday, I'll try to write my own .C and .h files.\nI've been able to resolve some of the problems when it comes to ParsedMaterial. Here is the block I have so far:\n[Material_Properties_Greencast_Inner_Bypass]\ntype = ParsedMaterial\nf_name = SiederTateHTC\nmaterial_property_names = 'Re Pr k_fluid'\nfunction = '(k_fluid/0.01875^2) * (3.66+(0.065 * Re * Pr * (0.01875/2.0))/(1+0.04 * (Re * Pr * (0.01875/2.0))^(2/3)))'\noutput_properties = alpha\nblock = '16'\nI get the following error in the log.out file:\n*** ERROR ***\nThe requested regular material property Re is declared as an AD property. Either retrieve it as an AD property with getADMaterialProperty or declare it as a regular property with declareProperty\ufffd[39m\nI am not quite sure what this exactly means, or where I can go to do what it tells me to do.",
                          "url": "https://github.com/idaholab/moose/discussions/18677#discussioncomment-1224676",
                          "updatedAt": "2022-09-08T14:33:44Z",
                          "publishedAt": "2021-08-23T22:25:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "So there are two kinds of materials, the one using automatic differentiation (AD) to get a great Jacobian and the ones that are not (and have to have the derivatives implemented manually).\nAre you using PronghornFluidProps? This is using AD. Your parsed material is not.  You need to switch from ParsedMaterial to ADParsedMaterial",
                          "url": "https://github.com/idaholab/moose/discussions/18677#discussioncomment-1224727",
                          "updatedAt": "2022-09-08T14:33:44Z",
                          "publishedAt": "2021-08-23T22:42:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "JKHINL"
                          },
                          "bodyText": "Works like a charm! Thank you very much!\nMy only remaining question pertains to the Reynolds number. When I look at it via the postprocessor, I see that it is substantially smaller than what I had handcalculated. The obvious reason would be that the porosity is factored in.\nWhen I use Re for the ParsedMaterial module, should I correct it so that it does not take porosity into account? I am looking at the equations solved in Pronghorn:\n(https://hpcsc.hpc.inl.gov/ssl/PRONGHORN/site/input_manual/kernels.html)\nand I do not see an indication where alpha relies on the adjusted velocity (the velocity that results when implementing porous physics.)",
                          "url": "https://github.com/idaholab/moose/discussions/18677#discussioncomment-1224810",
                          "updatedAt": "2022-09-08T14:33:44Z",
                          "publishedAt": "2021-08-23T23:13:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "you can select between the Reynolds and the superficial reynolds number. Have a look at PronghornFluidProps.C to see more about this.",
                          "url": "https://github.com/idaholab/moose/discussions/18677#discussioncomment-1224827",
                          "updatedAt": "2022-09-08T14:34:14Z",
                          "publishedAt": "2021-08-23T23:20:15Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Unable to use Neumann BCs in 3D",
          "author": {
            "login": "richmondodufisan"
          },
          "bodyText": "Hi all, I'm having trouble applying Neumann BCs on 3D meshes in MOOSE. I initially thought the problem was with the meshes I used previously, which were created by gmsh, but I tried this other mesh made with cubit (both as .inp files). FunctionDirichletBCs work fine (for BOTH meshes), but Neumann conditions give the following error:\n*** ERROR ***\n/home/richmondodufisan/projects/farm/Concrete/test_input2_3D.i:152: (BCs/load/boundary):\n    the following side set ids do not exist on the mesh: 3\n\n    MOOSE distinguishes between \"node sets\" and \"side sets\" depending on whether\n    you are using \"Nodal\" or \"Integrated\" BCs respectively. Node sets corresponding\n    to your side sets are constructed for you by default.\n\n    Try setting \"Mesh/construct_side_list_from_node_list=true\" if you see this error.\n    Note: If you are running with adaptivity you should prefer using side sets.\n\nEven after adding that extra argument to the Mesh block, I still get the same error. Are there any leads for the possible cause of the error? They're both sent to MOOSE as .inp files.\nI also tried exporting the gmsh mesh as a .vtu mesh. With that one, I got a different error:\n*** ERROR ***\nERROR: negative Jacobian -5.88862e-07 at point (x,y,z)=( 0.34306, 0.326145, -0.000642792) in element 41303\n\nThen I tried using the --mesh-only argument to run the file that I saw in a different discussion. The .inp meshes worked fine and showed the Mesh Information, but the .vtu mesh gave an error:\n*** ERROR ***\nError: Exodus requires all elements with a given subdomain ID to be the same type.\nCan't write both TET4 and QUAD4 in the same block!\n\nWhich is unexpected, because all elements are Quad elements. Again, I want to stress that all meshes worked completely fine with Dirichlet conditions. Also, the 2D meshes I create with gmsh work with Dirichlet and Neumann conditions. My issue is only with 3D meshes. Here are the mesh files used:\nmesh_files.zip\nIn case it's necessary the gmsh mesh script is saved as a .geo_unrolled file there too.",
          "url": "https://github.com/idaholab/moose/discussions/18668",
          "updatedAt": "2022-07-01T03:12:39Z",
          "publishedAt": "2021-08-20T22:04:51Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThe side set error can happen for a number of reasons, usually mispelling between the 'boundary=' field and the boundary name in the mesh. Does the sideset appear when you open the mesh in paraview?\nThe Jacobian error is likely because you have an inverted or very flat element. I had a look at your mesh in Paraview and some elements have rather high quality metrics.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/18668#discussioncomment-1216942",
                  "updatedAt": "2022-07-01T03:12:56Z",
                  "publishedAt": "2021-08-21T17:34:29Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "Hi,\nThe \"boundary=boundary_name\" isn't misspelled, the error only comes after changing \"Dirichlet\" to \"Neumann\" in the \"type=\" field (with Dirichlet it runs fine).\nWe managed to figure out the problem- we needed to slightly modify the problem to run with MOOSE. When we defined the nodeset as two rows of nodes, for some reason MOOSE had trouble converting to sidesets. But when we used 4 rows then it worked fine. This is with the .inp files created by cubit.",
                          "url": "https://github.com/idaholab/moose/discussions/18668#discussioncomment-1224006",
                          "updatedAt": "2022-07-01T03:12:56Z",
                          "publishedAt": "2021-08-23T18:55:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "ok that's odd. Could you please paste the 2 row and 4 row setup here? It's probably a parsing issue of some sort",
                          "url": "https://github.com/idaholab/moose/discussions/18668#discussioncomment-1224190",
                          "updatedAt": "2022-07-01T03:12:57Z",
                          "publishedAt": "2021-08-23T19:40:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "beam.zip\nThese are the two meshes",
                          "url": "https://github.com/idaholab/moose/discussions/18668#discussioncomment-1224436",
                          "updatedAt": "2022-07-01T03:13:08Z",
                          "publishedAt": "2021-08-23T21:02:05Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Output inteval flag seems not work if adaptivity mesh is turned on",
          "author": {
            "login": "ZhigangPu"
          },
          "bodyText": "Hi all, I am new to MOOSE and I have trouble dealing with output when I turn on the adaptivity mesh refining. My disk capacity is limited so I don't want MOOSE to output results every timestep. It's fine when I don't use adaptivity mesh refining(AMR), there is only one exodus output file and 'inteval' flag works. But when I turn on AMR, MOOSE will output a exodus file every step, whose suffix is '.e-sxxx'(xxx is timestep number). I wonder if there is some flags that can turn off this. For you better reference, below is my configuration for AMR and output.\nThanks in advance!\n\n[./Adaptivity]\ninitial_adaptivity = 6 # Number of times mesh is adapted to initial condition\nrefine_fraction = 0.7 # Fraction of high error that will be refined\ncoarsen_fraction = 0.1 # Fraction of low error that will coarsened\nmax_h_level = 7 # Max number of refinements used, starting from initial mesh (before uniform refinement)\nweight_names = 'c   eta'\nweight_values = '1  1'\ninteval = 5\n[../]\n[Outputs]\n[./exodus]\ntype = Exodus\ninteval = 3\nexecute_on = 'TIMESTEP_END'\n[../]\n[./my_checkpoint]\ntype=Checkpoint\nnum_files = 4\ninterval = 30\n[../]\n[]",
          "url": "https://github.com/idaholab/moose/discussions/18670",
          "updatedAt": "2021-08-23T04:14:28Z",
          "publishedAt": "2021-08-21T02:38:23Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\ninterval is misspelled in the Exodus block. Could that be the problem?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/18670#discussioncomment-1215150",
                  "updatedAt": "2021-08-21T06:16:07Z",
                  "publishedAt": "2021-08-21T06:15:57Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "ZhigangPu"
                          },
                          "bodyText": "Hello Guillaume\nThanks for pointing out the typo. I've changed the typo and re-run, but things are the same.\nThanks anyway!\nzhigang",
                          "url": "https://github.com/idaholab/moose/discussions/18670#discussioncomment-1215749",
                          "updatedAt": "2021-08-21T12:46:03Z",
                          "publishedAt": "2021-08-21T12:46:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "did you catch the typo in the [Adaptivity] block as well? Not sure if you were trying to limit adaptivity to limit output here\nIf interval does not work for your case, I dont see any other parameters that would reduce the number outputs in a similar way.\nI'd have to look in the code to see why interval doesnt work",
                          "url": "https://github.com/idaholab/moose/discussions/18670#discussioncomment-1216909",
                          "updatedAt": "2021-08-21T17:16:33Z",
                          "publishedAt": "2021-08-21T17:16:33Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ZhigangPu"
                          },
                          "bodyText": "Oh, I think I get the point.\nI thought the suffix of the output exodus file stands for the step. For example, 'xxx.e-s002' stands for time step 002, but actually it is not so. It only means that this is a second output file, the real time step it represents may be 3, 10, 20, whatever depending on my interval setting. But to be honest, I think it is a little bit misleading. I think I will try to find ways to change the naming rule.\nAnd I don't need to add inteval flags in the [Adaptivity] block, inteval in [Output] block is enough.\nThanks!\nzhigang",
                          "url": "https://github.com/idaholab/moose/discussions/18670#discussioncomment-1220175",
                          "updatedAt": "2021-08-23T03:03:42Z",
                          "publishedAt": "2021-08-23T02:59:11Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to simply iterate over nodes/elements to compute a source term used in a Kernel?",
          "author": {
            "login": "lgab13"
          },
          "bodyText": "Hi Moose experts,\nHow iterate over the elements/nodes of the mesh to compute, at the beginning or at the end of each time step, a  source term that depends on one or more fields and that is used in a Kernel?\nPlease consider the following (absurd) example\nfor(unsigned_int n=0; n<nnodes;n++){\n\n   MySourceTerm[n] = sqrt(Temperature[n]) ;   \n\n}\n\nWhat is the best way to do it (AuxKernels, Kernels, UserObject,...) ?\nDo you have any examples?\nIt is likely that this loop should be performed at a higher level but I did not find these explanations in the documentation.\nIs there a document describing a generic \"solving tree\" with the name of the methods and the order in which they are called at runtime:\npreprocessing()?\ninitialize() ?\nexecute() ?\npostprocessing()?\nfinalize()?\nBest regards,\nlg",
          "url": "https://github.com/idaholab/moose/discussions/18675",
          "updatedAt": "2022-10-20T18:38:09Z",
          "publishedAt": "2021-08-22T00:38:49Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nI would add an AuxVariable source, to the simulation, and a new AuxKernel that computes the custom source.\nIf the expression of source is simple, like sqrt(T), you can use a ParsedAux for this purpose\nFor specifying beginning/end of time step for computing the new source, you can use the execute_on = TIMESTEP_BEGIN / END parameter of the AuxKernel.\nThe Kernel that uses the source in the equation can then take it as a CoupledVar parameter. Note that if this kernel is a simple constant * variable term there are already pre-implemented kernels for this such as CoupledForce\nBest,\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/18675#discussioncomment-1220097",
                  "updatedAt": "2022-11-04T14:22:36Z",
                  "publishedAt": "2021-08-23T02:21:50Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Problem in installing atom",
          "author": {
            "login": "aaelmeli"
          },
          "bodyText": "Hi\nI downloaded atom for windows from the link provided in MOOS's link. I installed it using the downloaded file and I could open it. However, I do not know how to do this command select _Atom->Install Shell Commands_ shown in the link above.\nAnd when I do atom in the terminal, I am getting this message\n\n(moose) aaelmeli@CCEE-DT-284:~/projects$ atom\n'\\wsl$\\Ubuntu-20.04\\home\\aaelmeli\\projects'\nCMD.EXE was started with the above path as the current directory.\nUNC paths are not supported.  Defaulting to Windows directory.\n\nso, can you please help me to fix this?\nthank you.\nAbdo.",
          "url": "https://github.com/idaholab/moose/discussions/18653",
          "updatedAt": "2022-06-23T15:13:24Z",
          "publishedAt": "2021-08-20T06:55:24Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "dschwen"
                  },
                  "bodyText": "Launch Atom from the Windows Start menu",
                  "url": "https://github.com/idaholab/moose/discussions/18653#discussioncomment-1214821",
                  "updatedAt": "2022-06-23T15:13:43Z",
                  "publishedAt": "2021-08-21T02:33:50Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aaelmeli"
                          },
                          "bodyText": "Thank you, dschwen, for the reply.\nIn this case, I think I have already done that. However, I am still getting the above error message (in the original post) when trying to launch it from the terminal using atom . I think it has to be something with the path.",
                          "url": "https://github.com/idaholab/moose/discussions/18653#discussioncomment-1216514",
                          "updatedAt": "2022-06-23T15:13:45Z",
                          "publishedAt": "2021-08-21T14:43:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "I remember in windows you can open a folder, then right click, and the context menu will have an option to let you open the current directory in atom. Can you try that?\nThe tutorial on the moose website is specific to MacOS I believe, while it is similar in Linux. But not for windows.",
                          "url": "https://github.com/idaholab/moose/discussions/18653#discussioncomment-1216732",
                          "updatedAt": "2022-06-23T15:13:45Z",
                          "publishedAt": "2021-08-21T15:58:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dschwen"
                          },
                          "bodyText": "In this case, I think I have already done that.\n\nGood\n\nHowever, I am still getting the above error message (in the original post) when trying to launch it from the terminal using atom.\n\nThen don't do that. :-D",
                          "url": "https://github.com/idaholab/moose/discussions/18653#discussioncomment-1217156",
                          "updatedAt": "2022-06-23T15:13:52Z",
                          "publishedAt": "2021-08-21T19:23:59Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Solving Peridynamics Problem with Retained FE Mesh",
          "author": {
            "login": "ppandit95"
          },
          "bodyText": "Dear MOOSE Community\nI was trying to run a test application with retain_fe_mesh option as \"true\" but I was not able to solve the problem and ran into error as -\n*** ERROR ***\nThe following error occurred in the object \"mesh\", of type \"PeridynamicsMesh\".\nQuerying node ID exceeds the available PD node IDs!\nBut was able to vizualise the mesh with '--mesh-only' option while executing . So my querry iswhather can one solve the problem with retaining FE mesh or not .\nRegards\nPushkar",
          "url": "https://github.com/idaholab/moose/discussions/18582",
          "updatedAt": "2022-07-14T21:07:25Z",
          "publishedAt": "2021-08-11T16:54:39Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@dschwen not sure who to tag on peridynamics",
                  "url": "https://github.com/idaholab/moose/discussions/18582#discussioncomment-1214166",
                  "updatedAt": "2022-07-14T21:08:36Z",
                  "publishedAt": "2021-08-20T20:12:04Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "actually, @hchen139",
                          "url": "https://github.com/idaholab/moose/discussions/18582#discussioncomment-1214171",
                          "updatedAt": "2022-07-14T21:08:41Z",
                          "publishedAt": "2021-08-20T20:14:42Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "hchen139"
                  },
                  "bodyText": "The retained FE mesh is for the purpose of developing other capabilities in the future. For now, we don't really use the retained FE mesh for any PD problems. You will need to set the \"retain_fe_mesh\" option to false in order to use the current PD capabilities.\nBTW, why do you need to retain the FE mesh in your PD simulation?",
                  "url": "https://github.com/idaholab/moose/discussions/18582#discussioncomment-1214264",
                  "updatedAt": "2022-07-14T21:08:43Z",
                  "publishedAt": "2021-08-20T20:53:43Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "ppandit95"
                  },
                  "bodyText": "Thanks alot for the explanation, ohh regarding retained_fe_mesh  I was just\nplaying around in order to understand how peridynamics module really works.\n\u2026\nOn Sat, Aug 21, 2021 at 2:23 AM Hailong Chen ***@***.***> wrote:\n The retained FE mesh is for the purpose of developing other capabilities\n in the future. For now, we don't really use the retained FE mesh for any PD\n problems. You will need to set the \"retain_fe_mesh\" option to false in\n order to use the current PD capabilities.\n\n BTW, why do you need to retain the FE mesh in your PD simulation?\n\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n <#18582 (comment)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/ALP7KHSAJFSLC6SVE3WUBVDT526GFANCNFSM5B67MALQ>\n .\n Triage notifications on the go with GitHub Mobile for iOS\n <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n or Android\n <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>\n .",
                  "url": "https://github.com/idaholab/moose/discussions/18582#discussioncomment-1215042",
                  "updatedAt": "2022-07-14T21:08:47Z",
                  "publishedAt": "2021-08-21T04:51:21Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Issue with the efficiency of Distributed mesh with mesh adaptivity",
          "author": {
            "login": "simopier"
          },
          "bodyText": "When performing large 3D phase field simulations, the computational cost can quickly become prohibitive. For that reason, I have been trying to leverage some of the features offered by MOOSE to decrease these costs. Two of them being the use of Distributed mesh over Replicated mesh, and the use of mesh adaptivity.\nHowever, I do not get good performances when I try to combined the two, especially when my phase field simulation uses elasticity. After discussing this with @roystgnr, @amjokisaari, and @jiangwen84, I figured I would provide examples of the types of system I am trying to simulate, and document the performance issues I was observing.\nI am performing phase field simulation with elasticity, and below are the different combinations of options that I have used:\n\nUsing Distributed mesh or Replicated mesh\nUsing mesh adaptivity (2 levels) or not.\nWhen using Distributed mesh, I also tried using the part_package = ptscotch option.\n\nI have recorded the active time for each of these 6 simulations using the postprocessor\n[./activetime]  \n    type = PerfGraphData  \n    data_type = TOTAL  \n    section_name = Root  \n[../]\n\nThe results are provided in the table below. I made sure to perform large enough simulations to have a significant amount of nonlinear DOFs (> 2M) for Distributed mesh to be relevant. Note that when using mesh adaptivity (MA), I simulated a larger domain to still have a large amount of DOFs. Results show that without mesh adaptivity (left figure), the active time does decrease when using the distributed mesh rather than the regular mesh. However, the trend is reversed when using mesh adaptivity (right figure). With mesh adaptivity, using Distributed mesh is more time consuming that using Replicated mesh. I am not sure what causes this, but it limits the advantage of Distributed mesh.\n\n\n\nWithout mesh adaptivity\nWith mesh adaptivity (larger domain)\n\n\n\n\n\n\n\n\n\nAttached with this post are the input files, and example of the .pbs file that I have been using to run these on HPC (falcon), and the output files. Note that I have run these simulations with the -snes_view -log_view options, as well as with\n[./pgraph]\n  type = PerfGraphOutput\n  execute_on = 'initial final'  # Default is \"final\"\n  level = 3                     # Default is 1\n  heaviest_branch = true        # Default is false\n  heaviest_sections = 7         # Default is 0\n []\n\nto provide performance data. Let me know I need to provide more information or run additional tests.\nMoose_discussion_simopier_distributed_mesh.zip",
          "url": "https://github.com/idaholab/moose/discussions/18163",
          "updatedAt": "2022-09-20T10:03:32Z",
          "publishedAt": "2021-06-24T16:10:31Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "lynnmunday"
                  },
                  "bodyText": "Why does distributed run faster than replicated?  I thought the only advantage distributed had over replicated was a reduction in memory.",
                  "url": "https://github.com/idaholab/moose/discussions/18163#discussioncomment-917571",
                  "updatedAt": "2022-09-20T10:04:08Z",
                  "publishedAt": "2021-06-24T20:52:47Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "roystgnr"
                          },
                          "bodyText": "That's a very good question.  DistributedMesh setup can theoretically be faster, since you're not setting up a bunch of remote elements on each processor that you'll never use, but after that point the only advantage in CPU speed ought to be when the mesh is modified, which shouldn't affect the non-adaptive case.",
                          "url": "https://github.com/idaholab/moose/discussions/18163#discussioncomment-921705",
                          "updatedAt": "2022-09-20T10:04:08Z",
                          "publishedAt": "2021-06-25T16:57:46Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "fdkong"
                  },
                  "bodyText": "Why does distributed run faster than replicated? I thought the only advantage distributed had over replicated was a reduction in memory.\n\nI believe the time reduction mainly came from a distributed mesh generator. The distributed mesh generator is way faster than the serial generator since it does not need to move data around.\n[gmg]\n  type = DistributedRectilinearMeshGenerator\n  dim = 3\n  nx = 64\n  ny = 64\n  nz = 64\n  xmin = -20\n  xmax = 20\n  ymin = -20\n  ymax = 20\n  zmin = -20\n  zmax = 20\n[]",
                  "url": "https://github.com/idaholab/moose/discussions/18163#discussioncomment-933976",
                  "updatedAt": "2022-09-20T10:04:12Z",
                  "publishedAt": "2021-06-28T17:46:34Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "fdkong"
                  },
                  "bodyText": "It would be helpful if someone can share a graph from gperftools for the distributed mesh with the MA.  https://mooseframework.inl.gov/application_development/profiling.html\nFrom @simopier's input and description, I feel like this might be related to \"point ghosting functor\" on which we have trouble when doing uniform refinement.  However, a confirmation is still needed to avoid doing too much unrelated optimization.",
                  "url": "https://github.com/idaholab/moose/discussions/18163#discussioncomment-934004",
                  "updatedAt": "2022-09-20T10:04:24Z",
                  "publishedAt": "2021-06-28T17:58:13Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "simopier"
                          },
                          "bodyText": "I would be happy to do that, but these simulations are quite expensive to run and require HPC. Unfortunately, I have not been able to install/use gperftools on HPC.\nAny recommendations/insight for that?\nThank you!",
                          "url": "https://github.com/idaholab/moose/discussions/18163#discussioncomment-935650",
                          "updatedAt": "2023-03-23T20:45:46Z",
                          "publishedAt": "2021-06-28T23:48:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "If you are on sawtooth, you could do the following\nmodule load use.moose\nmodule  load gperftools/2.9-gcc-8.5.0",
                          "url": "https://github.com/idaholab/moose/discussions/18163#discussioncomment-938960",
                          "updatedAt": "2023-03-23T20:45:47Z",
                          "publishedAt": "2021-06-29T15:40:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "simopier"
                          },
                          "bodyText": "I performed the exact same simulations on Sawtooth with gperftools, but I am getting very different results then on Falcon, as you can see in the table below:\n\n\n\nWithout mesh adaptivity\nWith mesh adaptivity (larger domain)\n\n\n\n\n\n\n\n\n\nNow, the relative performances of the simulations are the opposite of what I was getting on falcon. Distributed mesh slows down the simulations when I am not using mesh adaptivity, but speed things up when I do use mesh adaptivity. I attach the .prof files of the runs on Sawtooth. I am quite confused by these results, as they seem to depend on the machine on which I perform the simulations.\nLet me know if you need additional information, or if you have any insight.\nThank you.\nhttps://inlbox.box.com/s/v4a8vp2iplre9255nhnpa49ef1u82sql",
                          "url": "https://github.com/idaholab/moose/discussions/18163#discussioncomment-971673",
                          "updatedAt": "2023-03-23T20:45:47Z",
                          "publishedAt": "2021-07-06T19:03:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "How many MPI proceses are you running on each? did you bind the processes? (--bind-to option, not usually required, but can explain some performance differences if processes migrate from one socket to the next)",
                          "url": "https://github.com/idaholab/moose/discussions/18163#discussioncomment-971760",
                          "updatedAt": "2023-03-23T20:45:47Z",
                          "publishedAt": "2021-07-06T19:27:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "simopier"
                          },
                          "bodyText": "For both Falcon and Sawtooth, I used #PBS -l select=2:ncpus=36:mpiprocs=36.\nI have never used the --bind-to option. I saw here that it can be used as --bind-to-none (Default), --bind-to-core, or as --bind-to-socket. Which option would you advise me to use?",
                          "url": "https://github.com/idaholab/moose/discussions/18163#discussioncomment-974758",
                          "updatedAt": "2023-03-23T20:45:47Z",
                          "publishedAt": "2021-07-07T12:59:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Why select=2? If in one run all the processes ended on one node and one other it was split, then that would make the communication costs very different. To know where the processes ended up, you can call hostname inside mpirun\nI used to bind to socket with OpenMOC, to avoid inter-socket migration. Binding to cores is fine too, as long as only one process gets bound to each core.",
                          "url": "https://github.com/idaholab/moose/discussions/18163#discussioncomment-974867",
                          "updatedAt": "2023-03-23T20:45:54Z",
                          "publishedAt": "2021-07-07T13:23:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "Why select=2?\n\n@simopier is trying to take two nodes. The nodes are not shared by anyone because he selected all 36 cores. That being said, the PBS script should not be the issue.\nWe pushed too far here. Let us back a bit.\nQuestions for @simopier\n\n\nCould you generate a picture of the call graph with .prof files? Nobody except you can create that picture. The .prof files depend on your executable and libs.  Again, instructions are here https://mooseframework.inl.gov/application_development/profiling.html\n\n\nIn the second picture (with mesh adaptivity), there are three lines. Were all three simulations generated using the same environment? Especially whether or not the gperf module was loaded for all three simulations. If the module were loaded only for one simulation, the simulation results would be very different. TMalloc shipped with the gperf can significantly speed up a simulation for specific problems.\n\n\nIt might be worthwhile to unload the gperf module and regenerate the second picture. We are trying to figure out whether or not the gperf module changed the whole story.",
                          "url": "https://github.com/idaholab/moose/discussions/18163#discussioncomment-975284",
                          "updatedAt": "2023-03-23T20:45:56Z",
                          "publishedAt": "2021-07-07T14:48:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "simopier"
                          },
                          "bodyText": "Could you generate a picture of the call graph with .prof files? Nobody except you can create that picture. The .prof files depend on your executable and libs. Again, instructions are here https://mooseframework.inl.gov/application_development/profiling.html\n\nI am not sure what I am doing wrong, but I cannot manage to create that picture.\nIn my folder on Sawtooth, I have a list of files named run12_*.prof with * going from 0 to num_processor - 1.\nI loaded module load use.moose gperftools, and then I have tried running:\npprof run12.prof\n(pprof) png > run12.png\n\nbut I get the following error:\nUsing remote profile at run12.prof.\nUse of uninitialized value $line in substitution (s///) at /apps/moose/stack/gperftools-2.9-gcc-8.5.0/bin/pprof line 3326.\nhttp://run12.prof/pprof/symbol doesn't exist\n/var/spool/pbs/mom_priv/jobs/5202194.sawtoothpbs.SC: line 29: syntax error near unexpected token `png'\n/var/spool/pbs/mom_priv/jobs/5202194.sawtoothpbs.SC: line 29: `(pprof) png > run12.png'\n\nWhen I try:\npprof run12_1.prof\n(pprof) png > run12_1.png\n\nI get:\nUsing local file run12_1.prof.\nDid not specify profile file\n\nfollowed by the help message.\nWhat am I missing?\n\n\nIn the second picture (with mesh adaptivity), there are three lines. Were all three simulations generated using the same environment? Especially whether or not the gperf module was loaded for all three simulations. If the module were loaded only for one simulation, the simulation results would be very different. TMalloc shipped with the gperf can significantly speed up a simulation for specific problems.\nIt might be worthwhile to unload the gperf module and regenerate the second picture. We are trying to figure out whether or not the gperf module changed the whole story.\n\nI used gperf for all 6 simulations on sawtooth. Once I figure out how to generate the gperf pictures, I will perform these simulations on sawtooth with and without gperf to check if the gperf module changed the whole story",
                          "url": "https://github.com/idaholab/moose/discussions/18163#discussioncomment-1001213",
                          "updatedAt": "2023-03-23T20:46:10Z",
                          "publishedAt": "2021-07-13T22:32:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "pprof might want the executable as an argument as well.\nUsage:\n/apps/moose/stack/gperftools-2.9-gcc-8.5.0/bin/pprof [options] <program> <profiles>\n   <profiles> is a space separated list of profile names.\n\nPlease let us know how this goes",
                          "url": "https://github.com/idaholab/moose/discussions/18163#discussioncomment-1048160",
                          "updatedAt": "2023-03-23T20:46:19Z",
                          "publishedAt": "2021-07-25T20:49:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "any news on this?",
                          "url": "https://github.com/idaholab/moose/discussions/18163#discussioncomment-1122157",
                          "updatedAt": "2023-03-23T20:46:19Z",
                          "publishedAt": "2021-08-02T20:58:43Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "fdkong"
                  },
                  "bodyText": "I also converted  this discussion  to a moose ticket in case nontrivial work is required to resolve the issue.\n#18191",
                  "url": "https://github.com/idaholab/moose/discussions/18163#discussioncomment-934021",
                  "updatedAt": "2022-09-20T10:04:39Z",
                  "publishedAt": "2021-06-28T18:04:26Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to Refine mesh in Moose",
          "author": {
            "login": "Ali1990dashti"
          },
          "bodyText": "Dear community,\nI have a mesh file which has some volumes and their related physical groups. I want to refine my mesh in one of the physical groups. I have seen that in moose we can define a box and then refine mesh in that box. But, is it possible to refine the mesh in the volume using its name or block number rather than a box with coordinates? I tried the following code to refine in the box but it was not successful for me (I want to for example split each tetrahedron to three ones in one of my physical groups (the Reservoir group)):\n[Adaptivity]\n max_h_level = 3\n cycles_per_step = 1\n marker = box\n [./Markers]\n   [./box]\n     type = BoxMarker\n     bottom_left = '1800. 1200. -1950.'\n     top_right = '2500. 800. -1500.'\n     inside = REFINE\n     outside =  DO_NOTHING\n   [../]\n [../]\n[]\n\nI have seen explanations in this page but still I could not solve my issue. I have also uploaded my mesh file.\nI do appreciate any consideration in advance.\nmesh.zip",
          "url": "https://github.com/idaholab/moose/discussions/18655",
          "updatedAt": "2022-06-14T21:47:31Z",
          "publishedAt": "2021-08-20T12:59:28Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "The UniformMarker, as the name suggests, can perform a uniform refinement within a subdomain by setting the \"block\" parameter.\nhttps://mooseframework.inl.gov/source/markers/UniformMarker.html",
                  "url": "https://github.com/idaholab/moose/discussions/18655#discussioncomment-1211997",
                  "updatedAt": "2022-06-14T21:47:45Z",
                  "publishedAt": "2021-08-20T14:30:49Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "Actually, all markers have the \"block\" parameter so they should be able to be restricted to a domain.",
                          "url": "https://github.com/idaholab/moose/discussions/18655#discussioncomment-1212004",
                          "updatedAt": "2022-06-14T21:47:41Z",
                          "publishedAt": "2021-08-20T14:32:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Ali1990dashti"
                          },
                          "bodyText": "Dear @aeslaughter, Thanks for devoting time to my issue. I tried the following method you proposed but it did not converge. My problem is a steady state one and I just want to refine mesh in one block and then solve the steady state problem. It firstly converges in solving the problem and then goes for adaptavity but in this step it does not converge. it says : Linear solve did not converge due to DIVERGED_BREAKDOWN iterations 30.",
                          "url": "https://github.com/idaholab/moose/discussions/18655#discussioncomment-1212146",
                          "updatedAt": "2022-06-14T21:47:41Z",
                          "publishedAt": "2021-08-20T15:03:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Can you cut the simulation there and look at the mesh? Did the refinement work as expected?",
                          "url": "https://github.com/idaholab/moose/discussions/18655#discussioncomment-1212328",
                          "updatedAt": "2022-06-14T21:47:41Z",
                          "publishedAt": "2021-08-20T15:28:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Ali1990dashti"
                          },
                          "bodyText": "When I use the --mesh-only functionality, it gives me the mesh but it is not refined. I do not know why it firstly runs the simulation and then starts to adapt the mesh. I expected that firstly it should refine the mesh and then do all the numerical calculations.",
                          "url": "https://github.com/idaholab/moose/discussions/18655#discussioncomment-1212373",
                          "updatedAt": "2022-07-05T06:42:09Z",
                          "publishedAt": "2021-08-20T15:34:54Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Adaptivity does not run with --mesh-only. You need to look at the output of the mesh  after the simulation.\nSince the simulation fails after adaptivity:\n\ntry to relax the tolerances enough so that you still get a 'simulation converged' and regular output\nOR\nchange the output execute_on flags, that you still get the output right after adaptivity",
                          "url": "https://github.com/idaholab/moose/discussions/18655#discussioncomment-1213020",
                          "updatedAt": "2022-07-05T06:42:09Z",
                          "publishedAt": "2021-08-20T16:00:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "I am not able to get the adaptivity working like I would expect. I am creating an issue with an input file and will see if anyone knows what is missing or if there is a problem.",
                          "url": "https://github.com/idaholab/moose/discussions/18655#discussioncomment-1213150",
                          "updatedAt": "2022-07-05T06:42:09Z",
                          "publishedAt": "2021-08-20T16:16:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Ali1990dashti"
                          },
                          "bodyText": "I exported the incomplete simulation using flags in the output block of my input file. In paraview I can see the selected block (I called it const ), but its mesh size has not changed. I set my parameters as following:\n[Adaptivity]\n  [./Markers]\n    [./const]\n      type =  UniformMarker\n      mark = REFINE\n      block = 'Reservoir'\n    [../]\n  [../]\n  marker = const\n  steps = 2\n[]\n\nIt also creates a file with this prefix which is new for me: .e-s002. But still it does not converge.",
                          "url": "https://github.com/idaholab/moose/discussions/18655#discussioncomment-1213154",
                          "updatedAt": "2022-07-05T06:42:10Z",
                          "publishedAt": "2021-08-20T16:16:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Sounds good.\nOtherwise, @socratesgorilla is working on a block refinement using mesh generators. This will be much easier to use than adaptivity in my opinion. It's coming real soon, PR next week probably",
                          "url": "https://github.com/idaholab/moose/discussions/18655#discussioncomment-1213204",
                          "updatedAt": "2022-07-05T06:42:11Z",
                          "publishedAt": "2021-08-20T16:25:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "I figured out the correct syntax, see #18657.",
                          "url": "https://github.com/idaholab/moose/discussions/18655#discussioncomment-1213231",
                          "updatedAt": "2022-07-05T06:42:11Z",
                          "publishedAt": "2021-08-20T16:30:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "aeslaughter"
                          },
                          "bodyText": "It also creates a file with this prefix which is new for me: .e-s002. But still it does not converge.\n\nEvery time the mesh changes a new Exodus file is created.",
                          "url": "https://github.com/idaholab/moose/discussions/18655#discussioncomment-1213234",
                          "updatedAt": "2022-07-05T06:42:11Z",
                          "publishedAt": "2021-08-20T16:32:14Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}