{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMy0wMi0yMFQwMTo1MDoxMC0wNzowMM4ASk5Z"
    },
    "edges": [
      {
        "node": {
          "title": "Cannot find flags to link with the Boost filesystem library (libboost-filesystem)",
          "author": {
            "login": "Dundric"
          },
          "bodyText": "Hi everyone,\nI recently had to reinstall Moose from scratch using the GCC/MPICH manual installation. My original installation a while back ran through perfectly, but now I am encountering the following error while compiling libmesh:\nconfigure: error: cannot find flags to link with the Boost filesystem library (libboost-filesystem)\nconfigure: error: ../../../contrib/metaphysicl/configure failed for contrib/metaphysicl\nRunning make -j 1...\nmake: *** No targets specified and no makefile found.  Stop.\nI am installing Moose on a remote linux ssh server running Ubuntu 20.04.5 x86_64, and my mpicc is mpicxx. I have tried all the solutions online and none of them have worked; I am very new to working with moose and dealing with compilation issues so any help would be really appreciated. Thankyou.",
          "url": "https://github.com/idaholab/moose/discussions/23327",
          "updatedAt": "2023-02-22T05:18:35Z",
          "publishedAt": "2023-02-02T18:44:34Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Is this a fresh repository?\nCan you please paste the output of moose/scripts/diagnostic",
                  "url": "https://github.com/idaholab/moose/discussions/23327#discussioncomment-4855105",
                  "updatedAt": "2023-02-02T19:32:45Z",
                  "publishedAt": "2023-02-02T19:32:44Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Dundric"
                          },
                          "bodyText": "I believe it is a fresh repository, but I easily could be wrong. Here is the diagnostic output:\n./diagnostics.sh\nThu 02 Feb 2023 01:41:17 PM CST\nNo LSB modules are available.\n\nSystem Arch: Distributor ID: Ubuntu Description: Ubuntu 20.04.5 LTS Release: 20.04 Codename: focal\n\nMOOSE Package Version: Custom Build\n\nCPU Count: 12\n\nMemory Free: 131116.684 MB\n\nVariable `which $CC` check:\n/usr/bin/mpicc\n\n$CC --version:\ngcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\nCopyright (C) 2019 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nMPICC:\nwhich mpicc:\n        /usr/bin/mpicc\nmpicc -show:\n        gcc -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi\n\nCOMPILER gcc:\ngcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\nCopyright (C) 2019 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nPython:\n        /home/pmont2/packages/miniconda/bin/python\n        Python 3.9.16\n\nMODULES NOT AVAILABLE\n\nPETSC_DIR not set\n\nENVIRONMENT:\nSHELL=/bin/bash\nCONDA_EXE=/home/pmont2/packages/miniconda/bin/conda\n_CE_M=\nFPATH=/home/pmont2/packages/mpich-4.0.2/include:\nPACKAGES_DIR=/home/pmont2/packages\nPWD=/home/pmont2/projects/moose/scripts\nLOGNAME=pmont2\nXDG_SESSION_TYPE=tty\nCONDA_PREFIX=/home/pmont2/packages/miniconda\nMANPATH=/home/pmont2/packages/mpich-4.0.2/share/man:\nCXX=mpicxx\nMOTD_SHOWN=pam\nHOME=/home/pmont2\nLANG=en_US.UTF-8\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:\nCONDA_PROMPT_MODIFIER=(base) \nSSH_CONNECTION=10.193.40.151 51911 128.174.244.48 22\nLESSCLOSE=/usr/bin/lesspipe %s %s\nXDG_SESSION_CLASS=user\nTERM=xterm-color\n_CE_CONDA=\nCPLUS_INCLUDE_PATH=/home/pmont2/packages/mpich-4.0.2/include:\nLESSOPEN=| /usr/bin/lesspipe %s\nUSER=pmont2\nCONDA_SHLVL=1\nF90=mpif90\nSHLVL=1\nPROJ_LIB=/home/pmont2/packages/miniconda/share/proj\nXDG_SESSION_ID=67\nCONDA_PYTHON_EXE=/home/pmont2/packages/miniconda/bin/python\nLD_LIBRARY_PATH=/home/pmont2/packages/gcc-10.4.0/lib64:/home/pmont2/packages/gcc-10.4.0/lib:/home/pmont2/packages/gcc-10.4.0/lib/gcc/x86_64-pc-linux-gnu/10.4.0:/home/pmont2/packages/gcc-10.4.0/libexec/gcc/x86_64-pc-linux-gnu/10.4.0:/home/pmont2/packages/mpich-4.0.2/lib:\nXDG_RUNTIME_DIR=/run/user/1011\nSSH_CLIENT=10.193.40.151 51911 22\nCONDA_DEFAULT_ENV=base\nFC=mpif90\nXDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop\nPATH=/home/pmont2/packages/gcc-10.4.0/bin:/home/pmont2/packages/mpich-4.0.2/bin:/home/pmont2/packages/miniconda/bin:/home/pmont2/packages/miniconda/bin:/home/pmont2/packages/miniconda/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\nCC=mpicc\nDBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1011/bus\nC_INCLUDE_PATH=/home/pmont2/packages/mpich-4.0.2/include:\nSSH_TTY=/dev/pts/1\nOLDPWD=/home/pmont2/projects/moose\n_=/usr/bin/env",
                          "url": "https://github.com/idaholab/moose/discussions/23327#discussioncomment-4855209",
                          "updatedAt": "2023-02-06T14:03:03Z",
                          "publishedAt": "2023-02-02T19:47:02Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "So we have two mpi here, one from the system and one from conda.\nYou probably need to hide the conda environment before building\ngcc -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi\nLD_LIBRARY_PATH=/home/pmont2/packages/gcc-10.4.0/lib64:/home/pmont2/packages/gcc-10.4.0/lib:/home/pmont2/packages/gcc-10.4.0/lib/gcc/x86_64-pc-linux-gnu/10.4.0:/home/pmont2/packages/gcc-10.4.0/libexec/gcc/x86_64-pc-linux-gnu/10.4.0:/home/pmont2/packages/mpich-4.0.2/lib:",
                  "url": "https://github.com/idaholab/moose/discussions/23327#discussioncomment-4855235",
                  "updatedAt": "2023-02-02T19:50:09Z",
                  "publishedAt": "2023-02-02T19:50:08Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Dundric"
                          },
                          "bodyText": "I tried hiding the conda environment before building, but received the same error. I did several different things: I removed all the paths in the enviornment before compiling libmesh, I deleted the environment before compiling libmesh, and I deleted all non-essential files from my linux profile and did all the things above again, but I got the same error. If you have any further advice on what I may be able to do I would appreciate it.",
                          "url": "https://github.com/idaholab/moose/discussions/23327#discussioncomment-4865250",
                          "updatedAt": "2023-02-03T19:14:00Z",
                          "publishedAt": "2023-02-03T19:13:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "ok then I think you ll want to look at the missing package route. I dont see cmake in your diagnostics script output. It s required for this manual installation route.",
                          "url": "https://github.com/idaholab/moose/discussions/23327#discussioncomment-4865705",
                          "updatedAt": "2023-02-03T20:18:37Z",
                          "publishedAt": "2023-02-03T20:18:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Dundric"
                          },
                          "bodyText": "I have cmake installed through conda. I don't know if that effects anything; I am not able to install it using sudo as that is disabled.\nwhich cmake = /home/pmont2/miniconda/bin/cmake",
                          "url": "https://github.com/idaholab/moose/discussions/23327#discussioncomment-4865968",
                          "updatedAt": "2023-02-03T20:55:33Z",
                          "publishedAt": "2023-02-03T20:55:33Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "if that conda is bringing mpi too then that's a problem\nIs this a computing cluster? Are there modules you could use?",
                          "url": "https://github.com/idaholab/moose/discussions/23327#discussioncomment-4865975",
                          "updatedAt": "2023-02-03T20:56:54Z",
                          "publishedAt": "2023-02-03T20:56:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "It's still odd that cmake was not found by the diagnostics script. Was the conda environment not activated when you run it?\nCan you try from a fresh conda environment with only the packages mentioned in the manual GCC/MPICH build page?",
                          "url": "https://github.com/idaholab/moose/discussions/23327#discussioncomment-4868013",
                          "updatedAt": "2023-02-04T06:48:25Z",
                          "publishedAt": "2023-02-04T06:48:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "There is also the issue where GCC version 10.x is available before the system version MPICH was built upon:\nPATH=/home/pmont2/packages/gcc-10.4.0/bin: <trimmed>\n\nDoes this directory exist? If so, I'd imagine a which gcc would point here. Which might start confusing some contrib builds in PETSc that do not require an MPI wrapper...",
                          "url": "https://github.com/idaholab/moose/discussions/23327#discussioncomment-4882796",
                          "updatedAt": "2023-02-06T14:07:08Z",
                          "publishedAt": "2023-02-06T14:07:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Dundric"
                          },
                          "bodyText": "Sorry for the late response. This is not a computer cluster it is a Synology Diskstation DS620slim cloud storage computer. I tried compiling on both the base environment and the peacock environment through the newer GCC MPICH tutorial and I still got the same error when I got to the part where I compiled libmesh. To give you some more information on the system as a whole, The GCC and MPICH have already been installed by the system maintainer, and I personally do not have access to commands like sudo. My which GCC points to /usr/bin/gcc and which mpicc /usr/bin/mpicc, while my which python points to /home/pmont2/mambaforge3/envs/peacock/bin/python and my which cmake is /home/pmont2/mambaforge3/envs/peacock/bin/cmake. There are other people running moose on this system who seem to be having minimal problems, so I do not know what my problem is. I have asked them for help, but they can't seem to figure it out.The system cmake is not a high enough version to compile petsc so that is why I have to conda install the latest version of Cmake. Thankyou for all for the help so far, and if you have any other advice it would be extraordinarily helpful.",
                          "url": "https://github.com/idaholab/moose/discussions/23327#discussioncomment-4929056",
                          "updatedAt": "2023-02-10T07:26:59Z",
                          "publishedAt": "2023-02-10T07:26:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "The diagnostics script in moose/scripts should give a full picture of the environment. Could you please paste or attach its output?",
                          "url": "https://github.com/idaholab/moose/discussions/23327#discussioncomment-4941438",
                          "updatedAt": "2023-02-11T05:09:02Z",
                          "publishedAt": "2023-02-11T05:09:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Dundric"
                          },
                          "bodyText": "Here is the diagnostics outpute:\nSat 11 Feb 2023 11:19:33 PM CST\nNo LSB modules are available.\n\nSystem Arch: Distributor ID: Ubuntu Description: Ubuntu 20.04.5 LTS Release: 20.04 Codename: focal\n\nMOOSE Package Version: Custom Build\n\nCPU Count: 12\n\nMemory Free: 110910.449 MB\n\nVariable `which $CC` check:\n/usr/bin/mpicc\n\n$CC --version:\ngcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\nCopyright (C) 2019 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nMPICC:\nwhich mpicc:\n        /usr/bin/mpicc\nmpicc -show:\n        gcc -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi\n\nCOMPILER gcc:\ngcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\nCopyright (C) 2019 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\nPython:\n        /home/pmont2/mambaforge3/envs/peacock/bin/python\n        Python 3.10.8\n\nMODULES NOT AVAILABLE\n\nPETSC_DIR not set\n\nENVIRONMENT:\nSHELL=/bin/bash\nCONDA_EXE=/home/pmont2/mambaforge3/bin/conda\n_CE_M=\nPACKAGES_DIR=/home/pmont2/packages\nPWD=/home/pmont2/projects/moose/scripts\nGSETTINGS_SCHEMA_DIR=/home/pmont2/mambaforge3/envs/peacock/share/glib-2.0/schemas\nLOGNAME=pmont2\nXDG_SESSION_TYPE=tty\nCONDA_PREFIX=/home/pmont2/mambaforge3/envs/peacock\nGSETTINGS_SCHEMA_DIR_CONDA_BACKUP=\nCXX=mpicxx\nMOTD_SHOWN=pam\nHOME=/home/pmont2\nLANG=en_US.UTF-8\nLS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:\nCONDA_PROMPT_MODIFIER=(peacock) \nSSH_CONNECTION=10.193.40.151 64800 128.174.244.48 22\nLESSCLOSE=/usr/bin/lesspipe %s %s\nXDG_SESSION_CLASS=user\nTERM=xterm-color\n_CE_CONDA=\nLESSOPEN=| /usr/bin/lesspipe %s\nUSER=pmont2\nCONDA_SHLVL=2\nF90=mpif90\nSHLVL=1\nPROJ_LIB=/home/pmont2/mambaforge3/envs/peacock/share/proj\nXDG_SESSION_ID=145\nCONDA_PYTHON_EXE=/home/pmont2/mambaforge3/bin/python\nXDG_RUNTIME_DIR=/run/user/1011\nSSH_CLIENT=10.193.40.151 64800 22\nCONDA_DEFAULT_ENV=peacock\nFC=mpif90\nXDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop\nPATH=/home/pmont2/packages/miniconda/bin:/home/pmont2/mambaforge3/envs/peacock/bin:/home/pmont2/mambaforge3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\nCC=mpicc\nDBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1011/bus\nSSH_TTY=/dev/pts/2\nCONDA_PREFIX_1=/home/pmont2/mambaforge3\nPROJ_NETWORK=ON\nOLDPWD=/home/pmont2/projects/moose\n_=/usr/bin/env",
                          "url": "https://github.com/idaholab/moose/discussions/23327#discussioncomment-4947293",
                          "updatedAt": "2023-02-13T13:46:47Z",
                          "publishedAt": "2023-02-12T05:25:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I see a peacock environment is active.\nWhat do you have installed in that environment? What does mamba list report?\nCan you try to install without an active mamba environment?",
                          "url": "https://github.com/idaholab/moose/discussions/23327#discussioncomment-4952247",
                          "updatedAt": "2023-02-13T01:22:16Z",
                          "publishedAt": "2023-02-13T01:22:15Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "Dundric"
                  },
                  "bodyText": "Ok I figured out how to solve the issue. So after I started a fresh install of Moose using the GCC/MPICH method, I cloned the repository and compiled petsc. Before I compiled libmesh, I ran the command: \"mamba install boost\" and that solved the issue. There seemed to have been something from the  boost/libboost packages that was missing from my system, but now it compiles and I passed all the tests. Thankyou very much Guillaume and Jason for the help.",
                  "url": "https://github.com/idaholab/moose/discussions/23327#discussioncomment-5072998",
                  "updatedAt": "2023-02-22T05:18:33Z",
                  "publishedAt": "2023-02-22T05:18:32Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Numerical oscillation?",
          "author": {
            "login": "Joseph-0123"
          },
          "bodyText": "Dear all,\nFor the advection-diffusion equation,\nI find two versions (compressible and incompressible versions) of advection terms in the advection-diffusion equation.\nNow I am trying to test these two cases in MOOSE. The compressible one produces stable results, however, the compressible results are unstable.\nDo you have any comments or ideas about this? How to fix this? Thanks.\nJ",
          "url": "https://github.com/idaholab/moose/discussions/23475",
          "updatedAt": "2023-02-21T20:01:17Z",
          "publishedAt": "2023-02-18T01:50:40Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThere are a few other differences here\n\neq 1 has a source term R which equation 2 does not have\neq 1 can have a spatial dependent diffusion coefficient, in eq 2 it s out of the laplacian\neq 2 has a multiplier on the derivative, which would make the 2 equations very different\n\nWe'd need more details to conclude, notably which finite element family and order you are using and which other equations you are solving. Compressible flow is more complex for sure.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/23475#discussioncomment-5019142",
                  "updatedAt": "2023-02-18T15:37:08Z",
                  "publishedAt": "2023-02-18T15:37:08Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "Hello\nThere are a few other differences here\n\neq 1 has a source term R which equation 2 does not have\neq 1 can have a spatial dependent diffusion coefficient, in eq 2 it s out of the laplacian\neq 2 has a multiplier on the derivative, which would make the 2 equations very different\n\nWe'd need more details to conclude, notably which finite element family and order you are using and which other equations you are solving. Compressible flow is more complex for sure.\nGuillaume\n\nDear Guillaume,\nplease have a look at the updated version of my problem. All the equations are the same except for the different weak forms. The main variable is \u2018C'.\n[Variables]\n  [C]\n    order = FIRST\n    family = LAGRANGE\n  []\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/23475#discussioncomment-5019508",
                          "updatedAt": "2023-02-19T02:50:29Z",
                          "publishedAt": "2023-02-18T16:52:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "you also performed an integration by parts to go from one formulation to the other.\nDid you change the boundary condition appropriately?",
                          "url": "https://github.com/idaholab/moose/discussions/23475#discussioncomment-5025412",
                          "updatedAt": "2023-02-19T15:04:24Z",
                          "publishedAt": "2023-02-19T15:04:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "you also performed an integration by parts to go from one formulation to the other. Did you change the boundary condition appropriately?\n\nThe compressible version has stable results, but the incompressible version with DirichletBC and NeumannBC all produce unstable results.\nDirichletBC:\n  [./left_C]\n    type = DirichletBC\n    variable = C\n    boundary = 'left'\n    value = 1\n  [../]\n\nor\nNeumannBC\n  [./left_C]\n    type = NeumannBC\n    variable = C\n    boundary = 'left'\n    value = 0.1 // or 0.1; or 0.01\n  [../]",
                          "url": "https://github.com/idaholab/moose/discussions/23475#discussioncomment-5025545",
                          "updatedAt": "2023-02-19T15:52:10Z",
                          "publishedAt": "2023-02-19T15:30:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You need to do more than that. Have a look at the Navier Stokes module finite element treatment of the pressure gradient.\nI think the sign should have flipped at the very least.\nIs this XY or RZ?",
                          "url": "https://github.com/idaholab/moose/discussions/23475#discussioncomment-5025928",
                          "updatedAt": "2023-02-19T15:54:28Z",
                          "publishedAt": "2023-02-19T15:54:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "You need to do more than that. Have a look at the Navier Stokes module finite element treatment of the pressure gradient.\nI think the sign should have flipped at the very least.\nIs this XY or RZ?\n\nThis is XY.",
                          "url": "https://github.com/idaholab/moose/discussions/23475#discussioncomment-5025972",
                          "updatedAt": "2023-02-19T16:03:06Z",
                          "publishedAt": "2023-02-19T16:03:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "You need to do more than that. Have a look at the Navier Stokes module finite element treatment of the pressure gradient.\nI think the sign should have flipped at the very least.\nIs this XY or RZ?\n\nThanks a lot, Guillaume. It works after I flip the sign.",
                          "url": "https://github.com/idaholab/moose/discussions/23475#discussioncomment-5026024",
                          "updatedAt": "2023-02-19T16:09:05Z",
                          "publishedAt": "2023-02-19T16:09:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "You need to do more than that. Have a look at the Navier Stokes module finite element treatment of the pressure gradient.\nI think the sign should have flipped at the very least.\nIs this XY or RZ?\n\nHello Guillaume.\nAfter I performed an integration by parts on the advection term, it will also produce a boundary term? How to set or deal with this boundary term? Should I just ignore it and use the default in MOOSE?",
                          "url": "https://github.com/idaholab/moose/discussions/23475#discussioncomment-5026534",
                          "updatedAt": "2023-02-21T20:01:38Z",
                          "publishedAt": "2023-02-19T17:56:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You ll need to create a BC object for it",
                          "url": "https://github.com/idaholab/moose/discussions/23475#discussioncomment-5026616",
                          "updatedAt": "2023-02-19T18:19:04Z",
                          "publishedAt": "2023-02-19T18:19:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Joseph-0123"
                          },
                          "bodyText": "You ll need to create a BC object for it\n\n\n\nCan I just use NeumannBC for the variable C shown in the upper figures instead of creating a BC object for the advection term of variable C?\n\n\nIn addition, if you look at this open access app, they use this Object BC but do not give any value of the variable in their input file?  Does this mean that this Object BC will not work?\n\n\nOr does this Object BC corresponds to the extra term derived from integration by parts? Is it compulsory to use this defined  Object BC  in the input file?",
                          "url": "https://github.com/idaholab/moose/discussions/23475#discussioncomment-5026665",
                          "updatedAt": "2023-02-19T19:25:50Z",
                          "publishedAt": "2023-02-19T18:29:05Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "right/wrong way to read from file in parallel?",
          "author": {
            "login": "jessecarterMOOSE"
          },
          "bodyText": "I have a large data file that feeds my AuxKernel (temperature field data, for example). It's a large model and I want to run in parallel. Is there a way I can read the file once and then send all that data to all my CPU's? Right now I've got the file-reading code in initialSetup of my AuxKernel, and it appears to be happening on all CPU's when I run in parallel, which doesn't seem very efficient and creates a bottleneck.",
          "url": "https://github.com/idaholab/moose/discussions/23459",
          "updatedAt": "2023-06-24T19:40:20Z",
          "publishedAt": "2023-02-17T13:37:27Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThis is possible.\nIn your intialSetup(), check for processor_id() == 0 before reading.\nThen once you have read on rank 0, use the MPI communicator to broadcast. It's usually a class attribute named _comm.\nSee the doxygen for that class\nhttps://mooseframework.inl.gov/docs/doxygen/timpi/classlibMesh_1_1Parallel_1_1Communicator.html\nYou ll like want something like \"broadcast\" for a rank 0 to all others communication.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/23459#discussioncomment-5011144",
                  "updatedAt": "2023-02-17T14:46:16Z",
                  "publishedAt": "2023-02-17T14:46:15Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "note that depending on your system, distributing the file and reading only the part that matters to each rank can be a faster solution. It's likely that a hybrid solution, with file reading in some rank then communication to groups of other ranks is the fastest.",
                          "url": "https://github.com/idaholab/moose/discussions/23459#discussioncomment-5011158",
                          "updatedAt": "2023-02-17T14:47:14Z",
                          "publishedAt": "2023-02-17T14:47:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "Thanks, I'll look into that. What kinds of data are supported? Can I just distribute basic types like floats and strings or can I make a std::map and broadcast that?",
                          "url": "https://github.com/idaholab/moose/discussions/23459#discussioncomment-5011785",
                          "updatedAt": "2023-02-17T15:43:59Z",
                          "publishedAt": "2023-02-17T15:43:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I see a map_broadcast so you can try maps.\nI m not sure where the full list of instanciations of these templated communication routines are.\n@lindsayad ?",
                          "url": "https://github.com/idaholab/moose/discussions/23459#discussioncomment-5011866",
                          "updatedAt": "2023-02-17T15:49:57Z",
                          "publishedAt": "2023-02-17T15:49:56Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "Are there any (simple) examples of this? If process 0 broadcasts some data, how do other processes receive it?",
                          "url": "https://github.com/idaholab/moose/discussions/23459#discussioncomment-5012207",
                          "updatedAt": "2023-02-17T16:21:00Z",
                          "publishedAt": "2023-02-17T16:21:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "There's a broadcast of some tuple in MultiAppTransfer.C to communicate the multiapp coord transform",
                          "url": "https://github.com/idaholab/moose/discussions/23459#discussioncomment-5012807",
                          "updatedAt": "2023-02-17T17:26:13Z",
                          "publishedAt": "2023-02-17T17:26:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "looks like map_broadcast doesn't take a std::map but instead a Map type. Any idea how one converts to/from that? Also the function takes a few other arguments which I can't figure out.",
                          "url": "https://github.com/idaholab/moose/discussions/23459#discussioncomment-5013124",
                          "updatedAt": "2023-02-17T18:07:46Z",
                          "publishedAt": "2023-02-17T18:07:45Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "Took a guess at the other arguments (0 and true) and the compiler complains that the map_broadcast function is private. I do see a few overloaded versions of the function in the doxygen, but I can't tell the difference. They all take a Map, unsigned int, and bool.",
                          "url": "https://github.com/idaholab/moose/discussions/23459#discussioncomment-5013269",
                          "updatedAt": "2023-02-17T18:23:24Z",
                          "publishedAt": "2023-02-17T18:23:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "you call broadcast and under the hood it links to map_broadcast.\nThe broadcast call should be very simple\n_comm.broadcast(the thing)",
                          "url": "https://github.com/idaholab/moose/discussions/23459#discussioncomment-5013313",
                          "updatedAt": "2023-02-17T18:28:32Z",
                          "publishedAt": "2023-02-17T18:28:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "oh, thanks for pointing that out",
                          "url": "https://github.com/idaholab/moose/discussions/23459#discussioncomment-5013381",
                          "updatedAt": "2023-02-17T18:37:18Z",
                          "publishedAt": "2023-02-17T18:37:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jessecarterMOOSE"
                          },
                          "bodyText": "ok, new error. The compiler doesn't seem to like my map, which is std::map<std::vector<Real>, const Function *>. (changing the map's return type to a Real does compile though, so we're getting close)",
                          "url": "https://github.com/idaholab/moose/discussions/23459#discussioncomment-5013493",
                          "updatedAt": "2023-02-17T18:52:40Z",
                          "publishedAt": "2023-02-17T18:52:16Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Error with ADComputeFiniteStrain",
          "author": {
            "login": "rh201"
          },
          "bodyText": "Hi MOOSEer,\nWhen I use the module [ADComputeFiniteStrain], the following error comes up. I checked the mesh and don't think the elements are heavily distorted, the strain level is low actually. Any suggestions/comments to avoid this error? Thanks.\nCannot take square root of a number less than or equal to zero in the calculation of C1 for the Rashid approximation for the rotation tensor. This zero or negative number may occur when elements become heavily distorted. Cannot take square root of a number less than or equal to zero in the calculation of C3_test for the Rashid approximation for the rotation tensor. This zero or negative number may occur when elements become heavily distorted.",
          "url": "https://github.com/idaholab/moose/discussions/23495",
          "updatedAt": "2023-06-24T20:01:52Z",
          "publishedAt": "2023-02-21T13:58:37Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nDid the solve finish after this?\nYou need to examine the output after this warning to see distortion. There is a FAILED execution flag for output you can use\nThe solution for this is usually to take smaller time steps, or if it's happening near a Dirichlet boundary condition, to not pre-set the dirichlet BCs.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/23495#discussioncomment-5064975",
                  "updatedAt": "2023-02-21T14:05:26Z",
                  "publishedAt": "2023-02-21T14:05:26Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "rh201"
                          },
                          "bodyText": "The solve stops calculation until reach the time limit, and the distortion is not big at all. And it's not happening near a Dirichlet B.C., I'll try with a smaller dt.\nHere is the error output message.\nerr.log",
                          "url": "https://github.com/idaholab/moose/discussions/23495#discussioncomment-5065059",
                          "updatedAt": "2023-02-21T14:14:36Z",
                          "publishedAt": "2023-02-21T14:14:35Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "does it make the first time step?\nThe elements seem very inverted from the libmesh output in the log",
                          "url": "https://github.com/idaholab/moose/discussions/23495#discussioncomment-5065105",
                          "updatedAt": "2023-02-21T14:17:40Z",
                          "publishedAt": "2023-02-21T14:17:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "rh201"
                          },
                          "bodyText": "No, it occurs in the middle fo the calculation.\nThe mesh type is tetrahedron, does it matter?",
                          "url": "https://github.com/idaholab/moose/discussions/23495#discussioncomment-5065230",
                          "updatedAt": "2023-02-21T14:29:27Z",
                          "publishedAt": "2023-02-21T14:29:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Tets should be fine.",
                          "url": "https://github.com/idaholab/moose/discussions/23495#discussioncomment-5065455",
                          "updatedAt": "2023-02-21T14:48:56Z",
                          "publishedAt": "2023-02-21T14:48:56Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Scalar product of two vectors in kernel",
          "author": {
            "login": "Minjiang-Zhu"
          },
          "bodyText": "I wonder how to make scalar product of two vectors when writing a kernel, like u=v.*w, while all of u, v, w are vectors, or specifically, ADRealVectorValue. Is there a related documentation explaining this, or other related operations?",
          "url": "https://github.com/idaholab/moose/discussions/23492",
          "updatedAt": "2023-06-24T19:42:04Z",
          "publishedAt": "2023-02-20T23:52:13Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "You can have a look at the doxygen for the base class of RealVectorValue\nhttps://mooseframework.inl.gov/docs/doxygen/libmesh/classlibMesh_1_1VectorValue.html",
                  "url": "https://github.com/idaholab/moose/discussions/23492#discussioncomment-5058213",
                  "updatedAt": "2023-02-20T23:56:24Z",
                  "publishedAt": "2023-02-20T23:56:23Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "and if you cant find that there, this is a very simple for loop",
                          "url": "https://github.com/idaholab/moose/discussions/23492#discussioncomment-5059403",
                          "updatedAt": "2023-02-21T03:18:24Z",
                          "publishedAt": "2023-02-21T03:18:23Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "What you described is not a scalar product btw.",
                  "url": "https://github.com/idaholab/moose/discussions/23492#discussioncomment-5058413",
                  "updatedAt": "2023-02-21T00:37:58Z",
                  "publishedAt": "2023-02-21T00:37:57Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "VectorVariable Gradient at a Point",
          "author": {
            "login": "maxnezdyur"
          },
          "bodyText": "Is there a way to get a VectorVariable gradient at a point? Just looking for a function that I can call within the VectorNodalBC class.",
          "url": "https://github.com/idaholab/moose/discussions/23478",
          "updatedAt": "2023-02-21T03:35:47Z",
          "publishedAt": "2023-02-18T18:22:33Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "ngrilli"
                  },
                  "bodyText": "@maxnezdyur  I have done something similar here:\nhttps://github.com/ngrilli/c_pfor_am/blob/main/src/auxkernels/ArrayDirectionalDerivative.C\nIn my case it is a directional derivative, but you can simply use the gradient,\nthe variable that express the gradient is:\n_grad_variable_qp\nTrust this helps,\nNicol\u00f2",
                  "url": "https://github.com/idaholab/moose/discussions/23478#discussioncomment-5021481",
                  "updatedAt": "2023-02-19T00:31:03Z",
                  "publishedAt": "2023-02-19T00:31:02Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "are you thinking of coupledGradient from the coupleable interface ?",
                  "url": "https://github.com/idaholab/moose/discussions/23478#discussioncomment-5021761",
                  "updatedAt": "2023-02-19T02:37:50Z",
                  "publishedAt": "2023-02-19T02:37:49Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "So when using the VectorNodalBC class, I need to find the gradient at a location within the element. Do you know how coupledGradient works when you are on a node? I may be overthinking this and it would work. I finally found this in system.h, it has a few functions for point_gradient, but I don't know if it works for vector variables.",
                          "url": "https://github.com/idaholab/moose/discussions/23478#discussioncomment-5021980",
                          "updatedAt": "2023-02-19T03:36:07Z",
                          "publishedAt": "2023-02-19T03:36:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "so coupledVectorGradient then for vector variables most likely.\nand this is a nodal vector variable right? a Lagrange_vec ?",
                          "url": "https://github.com/idaholab/moose/discussions/23478#discussioncomment-5025355",
                          "updatedAt": "2023-02-19T14:51:50Z",
                          "publishedAt": "2023-02-19T14:51:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "Yes this is Lagrange_vec.\nSo coupledVectorGradient calls var->gradSln() which calls _element_data->gradSln(Moose::Current). I may be misunderstanding this but when using the VectorNodalBC, which element gradient will it be giving? A node may be connected to multiple elements with different gradients.",
                          "url": "https://github.com/idaholab/moose/discussions/23478#discussioncomment-5025425",
                          "updatedAt": "2023-02-19T15:06:04Z",
                          "publishedAt": "2023-02-19T15:06:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "yep this doesnt look right.\nThe coupleable interface has nodal value routines. (coupledNodalValue for example)\nI think you may need to build the gradient ones because I dont see them",
                          "url": "https://github.com/idaholab/moose/discussions/23478#discussioncomment-5025449",
                          "updatedAt": "2023-02-19T15:09:59Z",
                          "publishedAt": "2023-02-19T15:09:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "I probably will try to build version of point_gradient that can work with Lagrange_vec.",
                          "url": "https://github.com/idaholab/moose/discussions/23478#discussioncomment-5025460",
                          "updatedAt": "2023-02-19T15:12:08Z",
                          "publishedAt": "2023-02-19T15:12:08Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "For a Lagrange variable, the derivative is not continuous on nodes is it?\nThis is typically why these routines are missing. For other higher order types we could have been better defined",
                          "url": "https://github.com/idaholab/moose/discussions/23478#discussioncomment-5025465",
                          "updatedAt": "2023-02-19T15:12:58Z",
                          "publishedAt": "2023-02-19T15:12:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "maxnezdyur"
                          },
                          "bodyText": "Yes I agree. That's why I am trying to find the gradient at a point that I know is within an element so it should be better defined.",
                          "url": "https://github.com/idaholab/moose/discussions/23478#discussioncomment-5025478",
                          "updatedAt": "2023-02-19T15:15:06Z",
                          "publishedAt": "2023-02-19T15:15:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "I do think that point_gradient is the best (maybe only) API to use here. If it doesn't work yet for vector finite element types ... actually yea as I'm typing this I realize that it most certainly won't work. Let me think a little bit more",
                          "url": "https://github.com/idaholab/moose/discussions/23478#discussioncomment-5056755",
                          "updatedAt": "2023-02-20T19:26:15Z",
                          "publishedAt": "2023-02-20T19:26:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Ok here is some example code\nauto fe = FEGenericBase<RealVectorValue>::build(elem->dim(), _var.feType());\nconst auto & dphi = fe->get_dphi();\nconst std::vector<Point> my_points = {point};\nfe->reinit(elem, &my_points);\nstd::vector<dof_id_type> dof_indices;\nconst auto & dof_map = _sys.dofMap();\ndof_map.dof_indices(elem, dof_indices, _var.number());\nmooseAssert(dof_indices.size() == dphi.size(), \"These must match\");\nRealTensorValue grad = 0;\nfor (const auto i : index_range(dof_indices))\n  grad += _sys.solution()(dof_indices[i]) * dphi[i][0];\n@roystgnr might have some other suggestions. There are definitely options out of FEMContext but a lot of those look pretty heavy (at least in terms of line count ... maybe not in terms of performance)",
                          "url": "https://github.com/idaholab/moose/discussions/23478#discussioncomment-5057038",
                          "updatedAt": "2023-02-20T20:06:10Z",
                          "publishedAt": "2023-02-20T20:06:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "roystgnr"
                          },
                          "bodyText": "FEMContext is a good way to go if you already have one in use for a quadrature loop; I don't think it offers anything over the lower-level APIs if you just need a one-off.",
                          "url": "https://github.com/idaholab/moose/discussions/23478#discussioncomment-5057994",
                          "updatedAt": "2023-02-20T22:56:56Z",
                          "publishedAt": "2023-02-20T22:56:55Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "PETSC ERROR: Out of memory",
          "author": {
            "login": "galibubc"
          },
          "bodyText": "Hello,\nI am facing out of memory problem after 62 seconds of simulation though i think i asked for maximum memory. Please give your suggestions on it.\nPreconditioning and Executioner used -\n[Preconditioning]                \n  [./SMP]                        \n    type = SMP                   \n    full = true                  \n    petsc_options_iname = '-pc_type -ksp_grmres_restart -sub_ksp_type -sub_pc_type -pc_asm_overlap'\n    petsc_options_value = 'asm      121                  preonly       lu           4'\n  [../]\n[]\n\n[Executioner]\n  type = Transient              \n  scheme = bdf2                 \n  verbose = True                \n  solve_type ='PJFNK'           \n                                                       \n  l_max_its = 50\n  l_tol = 1e-5\n  nl_max_its = 50\n  nl_rel_tol = 1e-5\n  nl_abs_tol = 1e-6\n  petsc_options_iname = '-pc_type -ksp_gmres_restart -sub_ksp_type -sub_pc_type -pc_asm_overlap'\n  petsc_options_value = 'asm      31                  preonly      lu          4'\n                        \n  dt=0.01               \n  end_time = 400       \n  line_search = 'none'\n[]\n\nMy job submission allocation request is :\n#!/bin/bash\n#PBS -l walltime=168:00:00,select=4:ncpus=32:mpiprocs=32:mem=180gb\n#PBS -N Zn_MOOSE\n#PBS -A st-mponga1-1\n#PBS -o output.txt\n#PBS -e error.txt\n\n################################################################################\n\nmodule load gcc/9.4.0            \nmodule load cmake/3.20.3\nmodule load git/2.31.1\nmodule load python/3.8.10\nmodule load openmpi/4.1.1-cuda11-3\n\nconda activate moose\n\ncd $PBS_O_WORKDIR\n\nmpiexec -n 128 /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt -i input.i>log.txt\n\nError for this job:\nCommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\nTo initialize your shell, run\n\n    $ conda init <SHELL_NAME>\n\nCurrently supported shells are:\n  - bash\n  - fish\n  - tcsh\n  - xonsh\n  - zsh\n  - powershell\n\nSee 'conda init --help' for more information and options.\n\nIMPORTANT: You may need to close and restart your shell after running 'conda init'.\n\n\n[52]PETSC ERROR: [54]PETSC ERROR: [63]PETSC ERROR: [32]PETSC ERROR: [33]PETSC ERROR: [35]PETSC ERROR: [36]PETSC ERROR: [37]PETSC ERROR: [39]PETSC ERROR: [43]PETSC ERROR: [44]PETSC ERROR: [45]PETSC ERROR: [46]PETSC ERROR: [107]PETSC ERROR: [108]PETSC ERROR: [116]PETSC ERROR: [117]PETSC ERROR: [118]PETSC ERROR: [119]PETSC ERROR: [121]PETSC ERROR: [122]PETSC ERROR: [124]PETSC ERROR: [126]PETSC ERROR: [127]PETSC ERROR: [96]PETSC ERROR: [98]PETSC ERROR: [100]PETSC ERROR: [101]PETSC ERROR: [97]PETSC ERROR: [102]PETSC ERROR: --------------------- Error Message --------------------------------------------------------------\n[45]PETSC ERROR: Out of memory. This could be due to allocating\n[45]PETSC ERROR: too large an object or bleeding by not properly\n[45]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[54]PETSC ERROR: Out of memory. This could be due to allocating\n[54]PETSC ERROR: too large an object or bleeding by not properly\n[54]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[63]PETSC ERROR: Out of memory. This could be due to allocating\n[63]PETSC ERROR: too large an object or bleeding by not properly\n[63]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[32]PETSC ERROR: Out of memory. This could be due to allocating\n[32]PETSC ERROR: too large an object or bleeding by not properly\n[32]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[33]PETSC ERROR: Out of memory. This could be due to allocating\n[33]PETSC ERROR: too large an object or bleeding by not properly\n[33]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[35]PETSC ERROR: Out of memory. This could be due to allocating\n[35]PETSC ERROR: too large an object or bleeding by not properly\n[35]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[36]PETSC ERROR: Out of memory. This could be due to allocating\n[36]PETSC ERROR: too large an object or bleeding by not properly\n[36]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[37]PETSC ERROR: Out of memory. This could be due to allocating\n[37]PETSC ERROR: too large an object or bleeding by not properly\n[37]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[39]PETSC ERROR: Out of memory. This could be due to allocating\n[39]PETSC ERROR: too large an object or bleeding by not properly\n[39]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[43]PETSC ERROR: Out of memory. This could be due to allocating\n[43]PETSC ERROR: too large an object or bleeding by not properly\n[43]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[44]PETSC ERROR: Out of memory. This could be due to allocating\n[44]PETSC ERROR: too large an object or bleeding by not properly\n[44]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[52]PETSC ERROR: Out of memory. This could be due to allocating\n[52]PETSC ERROR: too large an object or bleeding by not properly\n[52]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[46]PETSC ERROR: Out of memory. This could be due to allocating\n[46]PETSC ERROR: too large an object or bleeding by not properly\n[46]PETSC ERROR: destroying unneeded objects.\n[3]PETSC ERROR: [4]PETSC ERROR: [5]PETSC ERROR: [6]PETSC ERROR: [7]PETSC ERROR: [11]PETSC ERROR: [12]PETSC ERROR: [13]PETSC ERROR: [14]PETSC ERROR: [17]PETSC ERROR: [19]PETSC ERROR: [20]PETSC ERROR: [22]PETSC ERROR: [23]PETSC ERROR: [25]PETSC ERROR: [28]PETSC ERROR: [30]PETSC ERROR: [79]PETSC ERROR: [80]PETSC ERROR: [81]PETSC ERROR: [82]PETSC ERROR: [85]PETSC ERROR: [92]PETSC ERROR: [93]PETSC ERROR: [94]PETSC ERROR: [95]PETSC ERROR: [64]PETSC ERROR: [67]PETSC ERROR: [68]PETSC ERROR: [69]PETSC ERROR: [70]PETSC ERROR: [71]PETSC ERROR: [73]PETSC ERROR: [74]PETSC ERROR: [75]PETSC ERROR: [76]PETSC ERROR: [77]PETSC ERROR: --------------------- Error Message --------------------------------------------------------------\n[126]PETSC ERROR: Out of memory. This could be due to allocating\n[126]PETSC ERROR: too large an object or bleeding by not properly\n[126]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[107]PETSC ERROR: Out of memory. This could be due to allocating\n[107]PETSC ERROR: too large an object or bleeding by not properly\n[107]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[108]PETSC ERROR: Out of memory. This could be due to allocating\n[108]PETSC ERROR: too large an object or bleeding by not properly\n[108]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[116]PETSC ERROR: Out of memory. This could be due to allocating\n[116]PETSC ERROR: too large an object or bleeding by not properly\n[116]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[117]PETSC ERROR: Out of memory. This could be due to allocating\n[117]PETSC ERROR: too large an object or bleeding by not properly\n[117]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[118]PETSC ERROR: Out of memory. This could be due to allocating\n[118]PETSC ERROR: too large an object or bleeding by not properly\n[118]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[119]PETSC ERROR: Out of memory. This could be due to allocating\n[119]PETSC ERROR: too large an object or bleeding by not properly\n[119]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[121]PETSC ERROR: Out of memory. This could be due to allocating\n[121]PETSC ERROR: too large an object or bleeding by not properly\n[121]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[122]PETSC ERROR: Out of memory. This could be due to allocating\n[122]PETSC ERROR: too large an object or bleeding by not properly\n[122]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[124]PETSC ERROR: Out of memory. This could be due to allocating\n[124]PETSC ERROR: too large an object or bleeding by not properly\n[124]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[102]PETSC ERROR: Out of memory. This could be due to allocating\n[102]PETSC ERROR: too large an object or bleeding by not properly\n[102]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[127]PETSC ERROR: Out of memory. This could be due to allocating\n[127]PETSC ERROR: too large an object or bleeding by not properly\n[127]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[101]PETSC ERROR: Out of memory. This could be due to allocating\n[101]PETSC ERROR: too large an object or bleeding by not properly\n[101]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[96]PETSC ERROR: Out of memory. This could be due to allocating\n[96]PETSC ERROR: too large an object or bleeding by not properly\n[96]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[98]PETSC ERROR: Out of memory. This could be due to allocating\n[98]PETSC ERROR: too large an object or bleeding by not properly\n[98]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[100]PETSC ERROR: Out of memory. This could be due to allocating\n[100]PETSC ERROR: too large an object or bleeding by not properly\n[100]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[97]PETSC ERROR: Out of memory. This could be due to allocating\n[97]PETSC ERROR: too large an object or bleeding by not properly\n[97]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[85]PETSC ERROR: Out of memory. This could be due to allocating\n[85]PETSC ERROR: too large an object or bleeding by not properly\n[85]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[92]PETSC ERROR: Out of memory. This could be due to allocating\n[92]PETSC ERROR: too large an object or bleeding by not properly\n[92]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[93]PETSC ERROR: Out of memory. This could be due to allocating\n[93]PETSC ERROR: too large an object or bleeding by not properly\n[93]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[77]PETSC ERROR: Out of memory. This could be due to allocating\n[77]PETSC ERROR: too large an object or bleeding by not properly\n[77]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[95]PETSC ERROR: Out of memory. This could be due to allocating\n[95]PETSC ERROR: too large an object or bleeding by not properly\n[95]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[64]PETSC ERROR: Out of memory. This could be due to allocating\n[64]PETSC ERROR: too large an object or bleeding by not properly\n[64]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[67]PETSC ERROR: Out of memory. This could be due to allocating\n[67]PETSC ERROR: too large an object or bleeding by not properly\n[67]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[68]PETSC ERROR: Out of memory. This could be due to allocating\n[68]PETSC ERROR: too large an object or bleeding by not properly\n[68]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[70]PETSC ERROR: Out of memory. This could be due to allocating\n[70]PETSC ERROR: too large an object or bleeding by not properly\n[70]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[71]PETSC ERROR: Out of memory. This could be due to allocating\n[71]PETSC ERROR: too large an object or bleeding by not properly\n[71]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[73]PETSC ERROR: Out of memory. This could be due to allocating\n[73]PETSC ERROR: too large an object or bleeding by not properly\n[73]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[74]PETSC ERROR: Out of memory. This could be due to allocating\n[74]PETSC ERROR: too large an object or bleeding by not properly\n[74]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[75]PETSC ERROR: Out of memory. This could be due to allocating\n[75]PETSC ERROR: too large an object or bleeding by not properly\n[75]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[76]PETSC ERROR: Out of memory. This could be due to allocating\n[76]PETSC ERROR: too large an object or bleeding by not properly\n[76]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[94]PETSC ERROR: Out of memory. This could be due to allocating\n[94]PETSC ERROR: too large an object or bleeding by not properly\n[94]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[79]PETSC ERROR: Out of memory. This could be due to allocating\n[79]PETSC ERROR: too large an object or bleeding by not properly\n[79]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[80]PETSC ERROR: Out of memory. This could be due to allocating\n[80]PETSC ERROR: too large an object or bleeding by not properly\n[80]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[81]PETSC ERROR: Out of memory. This could be due to allocating\n[81]PETSC ERROR: too large an object or bleeding by not properly\n[81]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[82]PETSC ERROR: Out of memory. This could be due to allocating\n[82]PETSC ERROR: too large an object or bleeding by not properly\n[82]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[69]PETSC ERROR: Out of memory. This could be due to allocating\n[69]PETSC ERROR: too large an object or bleeding by not properly\n[69]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[17]PETSC ERROR: Out of memory. This could be due to allocating\n[17]PETSC ERROR: too large an object or bleeding by not properly\n[17]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[19]PETSC ERROR: Out of memory. This could be due to allocating\n[19]PETSC ERROR: too large an object or bleeding by not properly\n[19]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[20]PETSC ERROR: Out of memory. This could be due to allocating\n[20]PETSC ERROR: too large an object or bleeding by not properly\n[20]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[22]PETSC ERROR: Out of memory. This could be due to allocating\n[22]PETSC ERROR: too large an object or bleeding by not properly\n[22]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[23]PETSC ERROR: Out of memory. This could be due to allocating\n[23]PETSC ERROR: too large an object or bleeding by not properly\n[23]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[25]PETSC ERROR: Out of memory. This could be due to allocating\n[25]PETSC ERROR: too large an object or bleeding by not properly\n[25]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[28]PETSC ERROR: Out of memory. This could be due to allocating\n[28]PETSC ERROR: too large an object or bleeding by not properly\n[28]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[30]PETSC ERROR: Out of memory. This could be due to allocating\n[30]PETSC ERROR: too large an object or bleeding by not properly\n[30]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[3]PETSC ERROR: Out of memory. This could be due to allocating\n[3]PETSC ERROR: too large an object or bleeding by not properly\n[3]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[4]PETSC ERROR: Out of memory. This could be due to allocating\n[4]PETSC ERROR: too large an object or bleeding by not properly\n[4]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[5]PETSC ERROR: Out of memory. This could be due to allocating\n[5]PETSC ERROR: too large an object or bleeding by not properly\n[5]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[6]PETSC ERROR: Out of memory. This could be due to allocating\n[6]PETSC ERROR: too large an object or bleeding by not properly\n[6]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[7]PETSC ERROR: Out of memory. This could be due to allocating\n[7]PETSC ERROR: too large an object or bleeding by not properly\n[7]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[11]PETSC ERROR: Out of memory. This could be due to allocating\n[11]PETSC ERROR: too large an object or bleeding by not properly\n[11]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[12]PETSC ERROR: Out of memory. This could be due to allocating\n[12]PETSC ERROR: too large an object or bleeding by not properly\n[12]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[13]PETSC ERROR: Out of memory. This could be due to allocating\n[13]PETSC ERROR: too large an object or bleeding by not properly\n[13]PETSC ERROR: destroying unneeded objects.\n--------------------- Error Message --------------------------------------------------------------\n[14]PETSC ERROR: Out of memory. This could be due to allocating\n[14]PETSC ERROR: too large an object or bleeding by not properly\n[14]PETSC ERROR: destroying unneeded objects.\n[45]PETSC ERROR: Memory allocated 0 Memory used by process 291250176\n[45]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[45]PETSC ERROR: [46]PETSC ERROR: Memory allocated 0 Memory used by process 289386496\n[46]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[46]PETSC ERROR: [124]PETSC ERROR: Memory allocated 0 Memory used by process 284884992\n[124]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[124]PETSC ERROR: [54]PETSC ERROR: Memory allocated 0 Memory used by process 292646912\n[54]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[54]PETSC ERROR: [117]PETSC ERROR: Memory allocated 0 Memory used by process 284549120\n[117]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[117]PETSC ERROR: [32]PETSC ERROR: Memory allocated 0 Memory used by process 294809600\n[32]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[32]PETSC ERROR: [33]PETSC ERROR: Memory allocated 0 Memory used by process 291696640\n[33]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[33]PETSC ERROR: [119]PETSC ERROR: Memory allocated 0 Memory used by process 288264192\n[119]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[119]PETSC ERROR: [36]PETSC ERROR: Memory allocated 0 Memory used by process 291717120\n[36]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[36]PETSC ERROR: [122]PETSC ERROR: Memory allocated 0 Memory used by process 282456064\n[122]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[122]PETSC ERROR: [52]PETSC ERROR: Memory allocated 0 Memory used by process 295411712\n[52]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[52]PETSC ERROR: [97]PETSC ERROR: Memory allocated 0 Memory used by process 286879744\n[97]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[97]PETSC ERROR: [63]PETSC ERROR: Memory allocated 0 Memory used by process 289193984\n[63]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[63]PETSC ERROR: [127]PETSC ERROR: Memory allocated 0 Memory used by process 287559680\n[127]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[127]PETSC ERROR: [35]PETSC ERROR: Memory allocated 0 Memory used by process 291241984\n[35]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[35]PETSC ERROR: [102]PETSC ERROR: Memory allocated 0 Memory used by process 283561984\n[102]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[102]PETSC ERROR: [37]PETSC ERROR: Memory allocated 0 Memory used by process 292257792\n[37]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[37]PETSC ERROR: [101]PETSC ERROR: Memory allocated 0 Memory used by process 284704768\n[101]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[101]PETSC ERROR: [39]PETSC ERROR: Memory allocated 0 Memory used by process 291028992\n[39]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[39]PETSC ERROR: [96]PETSC ERROR: Memory allocated 0 Memory used by process 286806016\n[96]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[96]PETSC ERROR: [43]PETSC ERROR: Memory allocated 0 Memory used by process 293695488\n[43]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[43]PETSC ERROR: [98]PETSC ERROR: Memory allocated 0 Memory used by process 287076352\n[98]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[98]PETSC ERROR: [44]PETSC ERROR: Memory allocated 0 Memory used by process 294776832\n[44]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[44]PETSC ERROR: [100]PETSC ERROR: Memory allocated 0 Memory used by process 282927104\n[100]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[100]PETSC ERROR: [126]PETSC ERROR: Memory allocated 0 Memory used by process 285835264\n[126]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[126]PETSC ERROR: [107]PETSC ERROR: Memory allocated 0 Memory used by process 283938816\n[107]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[107]PETSC ERROR: [108]PETSC ERROR: Memory allocated 0 Memory used by process 286384128\n[108]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[108]PETSC ERROR: [116]PETSC ERROR: Memory allocated 0 Memory used by process 287711232\n[116]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[116]PETSC ERROR: [121]PETSC ERROR: Memory allocated 0 Memory used by process 286539776\n[121]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[121]PETSC ERROR: [118]PETSC ERROR: Memory allocated 0 Memory used by process 284815360\n[118]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[118]PETSC ERROR: [11]PETSC ERROR: Memory allocated 0 Memory used by process 290631680\n[11]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[11]PETSC ERROR: Memory requested 18446744073709447168\n[11]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[11]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[11]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[11]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[11]PETSC ERROR: [12]PETSC ERROR: Memory allocated 0 Memory used by process 293015552\n[12]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[12]PETSC ERROR: Memory requested 18446744073709451264\n[12]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[12]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[12]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[12]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[12]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[13]PETSC ERROR: Memory allocated 0 Memory used by process 290598912\n[13]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[13]PETSC ERROR: Memory requested 18446744073709434880\n[13]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[13]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[13]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[13]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[13]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[28]PETSC ERROR: Memory allocated 0 Memory used by process 293396480\n[28]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[28]PETSC ERROR: Memory requested 18446744073709418496\n[28]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[28]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[28]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[28]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[28]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[17]PETSC ERROR: Memory allocated 0 Memory used by process 289746944\n[17]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[17]PETSC ERROR: Memory requested 18446744073709436928\n[17]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[17]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[17]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[17]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[17]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[19]PETSC ERROR: Memory allocated 0 Memory used by process 290349056\n[19]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[19]PETSC ERROR: Memory requested 18446744073709453312\n[19]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[19]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[19]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[19]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[19]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[20]PETSC ERROR: Memory allocated 0 Memory used by process 292847616\n[20]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[20]PETSC ERROR: Memory requested 18446744073709447168\n[20]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[20]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[20]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[20]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[20]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[22]PETSC ERROR: Memory allocated 0 Memory used by process 287326208\n[22]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[22]PETSC ERROR: Memory requested 18446744072720066560\n[22]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[22]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[22]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[22]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[22]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[23]PETSC ERROR: Memory allocated 0 Memory used by process 289542144\n[23]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[23]PETSC ERROR: Memory requested 18446744069371140096\n[23]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[23]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[23]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[23]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[23]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[25]PETSC ERROR: Memory allocated 0 Memory used by process 287985664\n[25]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[25]PETSC ERROR: Memory requested 18446744072811667456\n[25]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[25]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[25]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[25]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[25]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[14]PETSC ERROR: Memory allocated 0 Memory used by process 294146048\n[14]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[14]PETSC ERROR: Memory requested 18446744073709461504\n[14]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[14]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[14]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[14]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[14]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[30]PETSC ERROR: Memory allocated 0 Memory used by process 292311040\n[30]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[30]PETSC ERROR: Memory requested 18446744069381713920\n[30]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[30]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[30]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[30]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[30]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[3]PETSC ERROR: Memory allocated 0 Memory used by process 292286464\n[3]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[3]PETSC ERROR: Memory requested 18446744072897898496\n[3]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[3]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[3]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[3]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[3]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[4]PETSC ERROR: Memory allocated 0 Memory used by process 290336768\n[4]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[4]PETSC ERROR: Memory requested 18446744072805648384\n[4]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[4]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[4]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[4]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[4]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[5]PETSC ERROR: Memory allocated 0 Memory used by process 292540416\n[5]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[5]PETSC ERROR: Memory requested 18446744073709402112\n[5]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[5]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[5]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[5]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[5]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[6]PETSC ERROR: Memory allocated 0 Memory used by process 289923072\n[6]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[6]PETSC ERROR: Memory requested 18446744073709430784\n[6]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[6]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[6]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[6]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[6]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[7]PETSC ERROR: Memory allocated 0 Memory used by process 293433344\n[7]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[7]PETSC ERROR: Memory requested 18446744073709422592\n[7]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[7]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[7]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se025 by galibubc Sun Feb 12 11:11:54 2023\n[7]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[7]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n#1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[81]PETSC ERROR: Memory allocated 0 Memory used by process 285691904\n[81]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[81]PETSC ERROR: Memory requested 18446744073709453312\n[81]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[81]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[81]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[81]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[81]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[93]PETSC ERROR: Memory allocated 0 Memory used by process 281915392\n[93]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[93]PETSC ERROR: Memory requested 18446744073709428736\n[93]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[93]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[93]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[93]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[93]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[85]PETSC ERROR: Memory allocated 0 Memory used by process 286908416\n[85]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[85]PETSC ERROR: Memory requested 18446744073709459456\n[85]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[85]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[85]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[85]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[85]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[92]PETSC ERROR: Memory allocated 0 Memory used by process 282001408\n[92]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[92]PETSC ERROR: Memory requested 18446744073709430784\n[92]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[92]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[92]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[92]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[92]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[69]PETSC ERROR: Memory allocated 0 Memory used by process 285347840\n[69]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[69]PETSC ERROR: Memory requested 18446744073709445120\n[69]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[69]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[69]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[69]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[69]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[77]PETSC ERROR: Memory allocated 0 Memory used by process 283537408\n[77]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[77]PETSC ERROR: Memory requested 18446744073709449216\n[77]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[77]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[77]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[77]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[77]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[95]PETSC ERROR: Memory allocated 0 Memory used by process 285888512\n[95]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[95]PETSC ERROR: Memory requested 18446744073709551616\n[95]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[95]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[95]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[95]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[95]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[64]PETSC ERROR: Memory allocated 0 Memory used by process 290496512\n[64]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[64]PETSC ERROR: Memory requested 18446744073709408256\n[64]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[64]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[64]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[64]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[64]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[67]PETSC ERROR: Memory allocated 0 Memory used by process 283787264\n[67]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[67]PETSC ERROR: Memory requested 18446744073709459456\n[67]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[67]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[67]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[67]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[67]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[82]PETSC ERROR: Memory allocated 0 Memory used by process 287887360\n[82]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[82]PETSC ERROR: Memory requested 18446744072744970240\n[82]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[82]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[82]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[82]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[82]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[68]PETSC ERROR: Memory allocated 0 Memory used by process 288690176\n[68]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[68]PETSC ERROR: Memory requested 18446744072669489152\n[68]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[68]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[68]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[68]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[68]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[70]PETSC ERROR: Memory allocated 0 Memory used by process 286961664\n[70]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[70]PETSC ERROR: Memory requested 18446744073709445120\n[70]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[70]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[70]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[70]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[70]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[71]PETSC ERROR: Memory allocated 0 Memory used by process 285470720\n[71]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[71]PETSC ERROR: Memory requested 18446744073709465600\n[71]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[71]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[71]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[71]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[71]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[73]PETSC ERROR: Memory allocated 0 Memory used by process 284262400\n[73]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[73]PETSC ERROR: Memory requested 18446744073709447168\n[73]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[73]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[73]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[73]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[73]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[74]PETSC ERROR: Memory allocated 0 Memory used by process 285093888\n[74]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[74]PETSC ERROR: Memory requested 18446744072773650432\n[74]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[74]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[74]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[74]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[74]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[75]PETSC ERROR: Memory allocated 0 Memory used by process 285880320\n[75]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[75]PETSC ERROR: Memory requested 18446744073709443072\n[75]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[75]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[75]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[75]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[75]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[76]PETSC ERROR: Memory allocated 0 Memory used by process 284860416\n[76]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[76]PETSC ERROR: Memory requested 18446744072766652416\n[76]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[76]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[76]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[76]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[76]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[94]PETSC ERROR: Memory allocated 0 Memory used by process 284688384\n[94]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[94]PETSC ERROR: Memory requested 18446744073709441024\n[94]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[94]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[94]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[94]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[94]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[79]PETSC ERROR: Memory allocated 0 Memory used by process 282791936\n[79]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[79]PETSC ERROR: Memory requested 18446744073709455360\n[79]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[79]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[79]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[79]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[79]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[80]PETSC ERROR: Memory allocated 0 Memory used by process 287817728\n[80]PETSC ERROR: Try running with -malloc_dump or -malloc_view for info.\n[80]PETSC ERROR: Memory requested 18446744073709443072\n[80]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[80]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[80]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se147 by galibubc Sun Feb 12 11:11:54 2023\n[80]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[80]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744072872298496\n[44]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[44]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[44]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se031 by galibubc Sun Feb 12 11:11:54 2023\n[44]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[44]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709445120\n[119]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[119]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[119]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[119]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[119]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744072871331840\n[46]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[46]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[46]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se031 by galibubc Sun Feb 12 11:11:54 2023\n[46]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[46]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744072874123264\n[52]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[52]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[52]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se031 by galibubc Sun Feb 12 11:11:54 2023\n[52]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[52]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709443072\n[118]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[118]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[118]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[118]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[118]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744072782778368\n[54]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[54]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[54]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se031 by galibubc Sun Feb 12 11:11:54 2023\n[54]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[54]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709438976\n[127]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[127]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[127]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[127]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[127]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709430784\n[32]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[32]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[32]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se031 by galibubc Sun Feb 12 11:11:54 2023\n[32]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[32]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744072771522560\n[97]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[97]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[97]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[97]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[97]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709451264\n[63]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[63]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[63]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se031 by galibubc Sun Feb 12 11:11:54 2023\n[63]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[63]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709445120\n[33]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[33]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[33]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se031 by galibubc Sun Feb 12 11:11:54 2023\n[33]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[33]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709447168\n[35]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[35]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[35]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se031 by galibubc Sun Feb 12 11:11:54 2023\n[35]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[35]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709455360\n[102]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[102]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[102]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[102]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[102]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709453312\n[37]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[37]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[37]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se031 by galibubc Sun Feb 12 11:11:54 2023\n[37]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[37]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709457408\n[101]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[101]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[101]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[101]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[101]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709445120\n[39]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[39]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[39]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se031 by galibubc Sun Feb 12 11:11:54 2023\n[39]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[39]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709418496\n[96]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[96]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[96]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[96]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[96]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709445120\n[36]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[36]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[36]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se031 by galibubc Sun Feb 12 11:11:54 2023\n[36]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[36]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744072864335872\n[122]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[122]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[122]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[122]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[122]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744072850481152\n[45]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[45]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[45]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se031 by galibubc Sun Feb 12 11:11:54 2023\n[45]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[45]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709447168\n[126]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[126]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[126]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[126]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[126]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709459456\n[43]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[43]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[43]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se031 by galibubc Sun Feb 12 11:11:54 2023\n[43]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[43]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744072800354304\n[107]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[107]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[107]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[107]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[107]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709441024\n[108]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[108]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[108]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[108]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[108]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709447168\n[121]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[121]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[121]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[121]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[121]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744065119617024\n[117]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[117]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[117]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[117]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[117]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709449216\n[116]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[116]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[116]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[116]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[116]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709447168\n[98]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[98]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[98]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[98]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[98]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744073709459456\n[100]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[100]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[100]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[100]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[100]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\nMemory requested 18446744072705482752\n[124]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[124]PETSC ERROR: Petsc Release Version 3.12.1, unknown \n[124]PETSC ERROR: /home/galibubc/scratch/musanna/MOOSE/project/dendrite__zn/dendrite__zn-opt on a arch-moose named se187 by galibubc Sun Feb 12 11:11:54 2023\n[124]PETSC ERROR: Configure options --download-hypre=1 --with-debugging=no --with-shared-libraries=1 --download-fblaslapack=1 --download-metis=1 --download-ptscotch=1 --download-parmetis=1 --download-superlu_dist=1 --download-mumps=1 --download-scalapack=1 --download-slepc=git://https://gitlab.com/slepc/slepc.git --download-slepc-commit= 59ff81b --with-mpi=1 --with-cxx-dialect=C++11 --with-fortran-bindings=0 --with-sowing=0\n[124]PETSC ERROR: #1 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[12]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[12]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[12]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[12]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[12]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[12]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[12]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[13]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[13]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[13]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[13]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[13]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[13]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[13]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[28]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[28]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[28]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[28]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[28]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[28]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[28]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[20]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[20]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[20]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[20]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[20]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[20]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[20]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[23]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[23]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[23]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 165 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[23]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[23]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[23]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[23]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[11]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[11]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[11]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[11]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[11]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[11]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[11]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[14]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[14]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[14]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[14]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[14]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[14]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[14]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[30]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[30]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[30]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 165 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[30]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[30]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[30]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[30]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[3]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[3]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[3]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[3]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[3]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[3]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[3]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[5]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[5]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[5]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[5]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[5]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[5]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[5]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[25]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[25]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[25]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[25]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[25]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[25]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[25]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[4]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[4]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[4]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[4]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[4]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[4]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[4]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[6]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[6]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[6]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[6]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[6]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[6]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[6]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[7]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[7]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[7]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[7]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[7]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[7]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[7]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[17]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[17]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[17]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[17]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[17]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[17]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[17]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[19]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[19]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[19]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[19]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[19]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[19]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[19]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[22]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[22]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[22]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[22]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[22]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[22]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[22]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[74]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[74]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[75]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[75]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[76]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[76]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[94]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[94]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[79]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[79]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[92]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[92]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[81]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[81]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[93]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[93]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[85]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[85]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[80]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[80]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[69]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[69]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[77]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[77]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[64]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[64]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[67]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[67]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[82]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[82]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[68]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[68]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[70]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[70]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[71]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[71]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[73]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[73]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[95]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[95]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[36]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[36]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[36]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[36]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[36]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[36]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[36]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[45]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[45]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[45]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[45]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[45]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[45]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[45]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[43]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[43]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[43]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[43]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[43]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[43]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[43]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[44]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[44]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[44]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[44]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[44]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[44]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[44]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[46]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[46]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[46]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[46]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[46]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[46]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[46]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[52]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[52]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[52]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[52]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[52]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[52]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[52]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[54]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[54]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[54]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[54]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[54]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[54]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[54]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[32]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[32]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[32]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[32]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[32]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[32]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[32]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[63]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[63]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[63]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[63]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[63]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[63]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[63]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[33]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[33]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[33]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[33]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[33]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[33]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[33]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[35]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[35]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[35]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[35]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[35]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[35]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[35]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[37]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[37]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[37]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[37]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[37]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[37]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[37]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[39]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[39]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[39]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[39]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[39]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[39]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[39]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n[96]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[96]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[98]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[98]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[122]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[122]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[126]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[126]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[107]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[107]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[100]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[100]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[108]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[108]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[121]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[121]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[117]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[117]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[119]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[119]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[124]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[124]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[118]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[118]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[127]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[127]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[97]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[97]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[102]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[102]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[101]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[101]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[116]PETSC ERROR: #2 PetscSegBufferAlloc_Private() line 31 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[116]PETSC ERROR: #3 PetscSegBufferGet() line 96 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/segbuffer.c\n[92]PETSC ERROR: [81]PETSC ERROR: [101]PETSC ERROR: [119]PETSC ERROR: [93]PETSC ERROR: [95]PETSC ERROR: [96]PETSC ERROR: [80]PETSC ERROR: [98]PETSC ERROR: [77]PETSC ERROR: [122]PETSC ERROR: [64]PETSC ERROR: [107]PETSC ERROR: [100]PETSC ERROR: [70]PETSC ERROR: [121]PETSC ERROR: [71]PETSC ERROR: [117]PETSC ERROR: [85]PETSC ERROR: [116]PETSC ERROR: [124]PETSC ERROR: [74]PETSC ERROR: [76]PETSC ERROR: [126]PETSC ERROR: [79]PETSC ERROR: [108]PETSC ERROR: [75]PETSC ERROR: [118]PETSC ERROR: [94]PETSC ERROR: [127]PETSC ERROR: [69]PETSC ERROR: [97]PETSC ERROR: [73]PETSC ERROR: [102]PETSC ERROR: [67]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[101]PETSC ERROR: [82]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[122]PETSC ERROR: [68]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[126]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[126]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[67]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[100]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[100]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[121]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[121]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[79]PETSC ERROR: #4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[75]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[75]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[102]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[102]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[81]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[81]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[118]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[118]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[68]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[68]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[97]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[97]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[69]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[69]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[96]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[96]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[77]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[77]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[98]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[98]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[82]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[82]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[124]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[124]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[64]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[64]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[101]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[70]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[70]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[107]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[107]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[71]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[71]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[122]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[85]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[85]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[67]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[108]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[108]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[74]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[74]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[127]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[127]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[79]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 165 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[117]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[117]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[76]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[76]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[93]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[93]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[119]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[119]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[92]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[92]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[116]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[116]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[95]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[95]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[94]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[94]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[80]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[80]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n#4 VecAssemblyRecv_MPI_Private() line 179 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[73]PETSC ERROR: #5 PetscCommBuildTwoSidedFReq_Ibarrier() line 415 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[73]PETSC ERROR: #6 PetscCommBuildTwoSidedFReq() line 574 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/sys/utils/mpits.c\n[76]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[93]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[117]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[73]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[121]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[95]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[81]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[77]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[96]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[64]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[98]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[70]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[101]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[71]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[107]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[85]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[122]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[67]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[100]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[79]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[116]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[75]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[119]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[92]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[127]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[68]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[97]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[69]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[124]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[80]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[126]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[102]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[74]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[94]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[108]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[82]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[118]PETSC ERROR: #7 VecAssemblyBegin_MPI_BTS() line 260 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/impls/mpi/pbvec.c\n[94]PETSC ERROR: [121]PETSC ERROR: [64]PETSC ERROR: [96]PETSC ERROR: [70]PETSC ERROR: [80]PETSC ERROR: [97]PETSC ERROR: [67]PETSC ERROR: [98]PETSC ERROR: [101]PETSC ERROR: [73]PETSC ERROR: [122]PETSC ERROR: [92]PETSC ERROR: [118]PETSC ERROR: [74]PETSC ERROR: [116]PETSC ERROR: [68]PETSC ERROR: [108]PETSC ERROR: [95]PETSC ERROR: [119]PETSC ERROR: [82]PETSC ERROR: [117]PETSC ERROR: [77]PETSC ERROR: [124]PETSC ERROR: [69]PETSC ERROR: [126]PETSC ERROR: [81]PETSC ERROR: [102]PETSC ERROR: [71]PETSC ERROR: [100]PETSC ERROR: [85]PETSC ERROR: [127]PETSC ERROR: [93]PETSC ERROR: [107]PETSC ERROR: [79]PETSC ERROR: [75]PETSC ERROR: [76]PETSC ERROR: #8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n#8 VecAssemblyBegin() line 133 in /scratch/st-mponga1-1/musanna/MOOSE/project/moose/petsc/src/vec/vec/interface/vector.c\n--------------------------------------------------------------------------\nMPI_ABORT was invoked on rank 82 in communicator MPI_COMM_WORLD\nwith errorcode 1.\n\nNOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\nYou may or may not see output from other processes, depending on\nexactly when Open MPI kills them.\n--------------------------------------------------------------------------\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] PMIX ERROR: UNREACHABLE in file server/pmix_server.c at line 2198\n[se025:225767] 66 more processes have sent help message help-mpi-api.txt / mpi-abort\n[se025:225767] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages",
          "url": "https://github.com/idaholab/moose/discussions/23479",
          "updatedAt": "2023-04-07T16:31:49Z",
          "publishedAt": "2023-02-19T05:16:37Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nPETSc reports a very high number for the desired allocation. Could you please give us more details about your simulation?\nHow many elements, how many DOFs, have you run a smaller version before?\nIs there anything else in the log.txt file?\nalso why are you activating a conda environment here? Is MOOSE installed with conda or with the modules you load?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/23479#discussioncomment-5025345",
                  "updatedAt": "2023-02-19T14:50:45Z",
                  "publishedAt": "2023-02-19T14:50:44Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "galibubc"
                          },
                          "bodyText": "Hello,\nI am adding the input file and log file here. The size of the output.e file is also too big ---> 150G for input_out.e\nlog.txt\nInput.txt\nYes MOOSE is installed with conda using the following process:\n####################################################################\n################# Uninstall Conda MOOSE Environment ################\n####################################################################\nconda deactivate \nconda env remove -n moose\nconda config --remove channels idaholab\n\n\n\n\n##################################################################\n############# Python 3.x Development libraries ###################\n##################################################################\n\n\nconda config --add channels https://conda.software.inl.gov/public\n      conda config --show channels  #####Command to display active conda channels\nconda create --name moose -q -y\nconda activate moose \nconda install moose-tools\nconda deactivate\nconda activate moose\n\n\n\n#############################################################\n################## loading modules ##########################\n#############################################################\n\nmodule load gcc/9.4.0            \nmodule load cmake/3.20.3\nmodule load git/2.31.1\nmodule load python/3.8.10\nmodule load openmpi/4.1.1-cuda11-3\n\n################################################################\n######################## Create MOOSE Profile ##################\n################################################################\n\nvim .bashrc\n\nexport CC=mpicc\nexport CXX=mpicxx\nexport F90=mpif90\nexport F77=mpif77\nexport FC=mpif90\n\n\nsource .bashrc\n\n#########################################################################\n#################### Cloning MOOSE from github ##########################\n#########################################################################\n\nmkdir projects\ncd projects\ngit clone https://github.com/idaholab/moose.git\ncd moose\ngit checkout master   ##### for current version\ngit branch -a         ##### to check the previous version\ngit checkout remotes/origin/2020-01-24-release       \n\n###########################################################\n#################### Compile PETSc  #######################\n###########################################################\n\nunset PETSC_DIR PETSC_ARCH\n./scripts/update_and_rebuild_petsc.sh\n\n###############################################################\n###################### Compile libMesh ########################\n###############################################################\nsource .bashrc  #in home folder\n./scripts/update_and_rebuild_libmesh.sh\n\n\n################################################################\n#################### Compile and Test MOOSE ####################\n################################################################\n\ncd ~/projects/moose/test\nmake -j 32\n./run_tests -j 32",
                          "url": "https://github.com/idaholab/moose/discussions/23479#discussioncomment-5026725",
                          "updatedAt": "2023-02-19T18:43:27Z",
                          "publishedAt": "2023-02-19T18:43:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "This all looks fine. Maybe the convergence criteria are a little loose but that's it.\nDid the job run out of time / got killed by the scheduler by any chance?\nDo you have enough RAM for these 150 Gb exodus output?",
                          "url": "https://github.com/idaholab/moose/discussions/23479#discussioncomment-5038995",
                          "updatedAt": "2023-02-20T06:31:29Z",
                          "publishedAt": "2023-02-20T06:31:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "galibubc"
                          },
                          "bodyText": "Hello,\nI didn't understand \"how to tighten the convergence criteria.\" Can you please give some suggestion. Also, how might this cause petse memory error?\nNo the job was scheduled for 168 hours but run less that  68 hours. I don't explicitly asked for RAM in my scheduler. Is it the cause that the output file size is too big? Any way to reduce the output file size?\nThanks",
                          "url": "https://github.com/idaholab/moose/discussions/23479#discussioncomment-5044737",
                          "updatedAt": "2023-02-20T06:44:51Z",
                          "publishedAt": "2023-02-20T06:44:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "To tighten the convergence criteria, you can make the numbers in nl_abs_tol, nl_rel_tol smaller\nIt's unlikely to cause a petsc error, though it's happened before that petsc reports running out of memory when it s really diverging\nYou can reduce the output file size by reducing the number of fields output (use show=' ' in Exodus block)\nIt's unlikely to be the cause\nYou also can fix:\n\npetsc options are passed twice, once in Preconditioning once in Executioner. You only need that once\nyou have 2 exodus outputs, one called other with a large interval, and another one every time step",
                          "url": "https://github.com/idaholab/moose/discussions/23479#discussioncomment-5054883",
                          "updatedAt": "2023-02-20T15:41:28Z",
                          "publishedAt": "2023-02-20T15:41:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "galibubc"
                          },
                          "bodyText": "Thanks. Can you please explain how to use show in Exodus block to specify which field output i want?\nFor some reason, block arrays i defined is not showing in paraview? How can i get these field output?",
                          "url": "https://github.com/idaholab/moose/discussions/23479#discussioncomment-5057598",
                          "updatedAt": "2023-02-20T21:46:12Z",
                          "publishedAt": "2023-02-20T21:46:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "This isnt right. They should show up. Maybe it didnt \"Apply\" the settings on the middle left property.",
                          "url": "https://github.com/idaholab/moose/discussions/23479#discussioncomment-5057749",
                          "updatedAt": "2023-02-20T22:10:26Z",
                          "publishedAt": "2023-02-20T22:10:25Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Generate Mesh with different material blocks",
          "author": {
            "login": "mfitzka"
          },
          "bodyText": "Hi all,\nWhich internal mesh generator would be the most suitable for me?\nI am looking to generate simple meshes at first with two different materials, which would be similar to a cylindrical pipe with steel on the outside and another material inside.\nThis would be a first tests before I go on to more \"advanced\" geometries.\nIs it possible to do this with something like GeneratedMesh directly in MOOSE or should I use something else like GMSH?\nThanks!",
          "url": "https://github.com/idaholab/moose/discussions/23481",
          "updatedAt": "2023-02-20T17:37:35Z",
          "publishedAt": "2023-02-19T18:17:18Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nMaterial blocks are subdomains or blocks in MOOSE.\nMany generators allow for this feature. I think GeneratedMeshGenerator allows it, if not CartesianMeshGenerator is a good option\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/23481#discussioncomment-5026618",
                  "updatedAt": "2023-02-19T18:19:55Z",
                  "publishedAt": "2023-02-19T18:19:54Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "mfitzka"
                  },
                  "bodyText": "The CartesianMeshGenerator worked to define the materials to subdomains.\nHowever, now I am running into a new problem:\nERROR\nMaterial properties must be retrieved during object construction. This is a code problem.\nDoes this also have to do with the generated mesh and the assignment of materials and its properties during the generation of the mesh?",
                  "url": "https://github.com/idaholab/moose/discussions/23481#discussioncomment-5026787",
                  "updatedAt": "2023-02-19T18:57:33Z",
                  "publishedAt": "2023-02-19T18:57:32Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "No this seems independent. Do you mind sharing your input file here?\nAre you using a moose module or an app?",
                          "url": "https://github.com/idaholab/moose/discussions/23481#discussioncomment-5026962",
                          "updatedAt": "2023-02-19T19:42:13Z",
                          "publishedAt": "2023-02-19T19:42:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mfitzka"
                          },
                          "bodyText": "The simulation uses quite alot of custom built kernels, but I can also include them in the repository.\nhttps://github.com/mfitzka/phoenixpublic\nThe input file is called SimpleTestCartesian.i",
                          "url": "https://github.com/idaholab/moose/discussions/23481#discussioncomment-5026987",
                          "updatedAt": "2023-02-19T19:49:33Z",
                          "publishedAt": "2023-02-19T19:49:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "ok then in one of the custom-made objects (or kernels) you must have something like getMaterialProperty outside of the constructor. This will not work, you need to modify the code to make it retrieve a reference to the material property during the construction of the kernel.\nAn example:\nhttps://github.com/idaholab/moose/blob/next/framework/src/kernels/ADMatReaction.C\nhttps://github.com/idaholab/moose/blob/next/framework/include/kernels/ADMatReaction.h\nlook at the _reaction_rate material property",
                          "url": "https://github.com/idaholab/moose/discussions/23481#discussioncomment-5028863",
                          "updatedAt": "2023-02-20T01:46:39Z",
                          "publishedAt": "2023-02-20T01:46:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mfitzka"
                          },
                          "bodyText": "I added the custom kernels used in the application in the repository.\nAt first glance, I cant seem to find the problem, as some of them are original MOOSE code and in all of the others the getMaterialProperty command should be inside the constructor.\nAm I overlooking something?",
                          "url": "https://github.com/idaholab/moose/discussions/23481#discussioncomment-5051628",
                          "updatedAt": "2023-02-20T10:30:32Z",
                          "publishedAt": "2023-02-20T10:30:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I dont see any kernels in the repo.\nIs it this one?\nhttps://github.com/mfitzka/phoenixpublic",
                          "url": "https://github.com/idaholab/moose/discussions/23481#discussioncomment-5054837",
                          "updatedAt": "2023-02-20T15:36:24Z",
                          "publishedAt": "2023-02-20T15:36:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mfitzka"
                          },
                          "bodyText": "Sorry for the confusion, I forgot to confirm the uploads.\nAll of them can now be found in the folder \"kernels\".",
                          "url": "https://github.com/idaholab/moose/discussions/23481#discussioncomment-5055239",
                          "updatedAt": "2023-02-20T16:13:14Z",
                          "publishedAt": "2023-02-20T16:13:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "This kernel for example\nhttps://github.com/mfitzka/phoenixpublic/blob/main/kernels/CoupledCureTimeDerivative.C\nThis should be in the constructor not the initialSetup\n    _density = &getMaterialProperty<Real>(\"density\");\n    _Hr = &getMaterialProperty<Real>(\"Hr\");",
                          "url": "https://github.com/idaholab/moose/discussions/23481#discussioncomment-5055294",
                          "updatedAt": "2023-02-20T16:19:38Z",
                          "publishedAt": "2023-02-20T16:19:37Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mfitzka"
                          },
                          "bodyText": "Simply moving it to the constructor did not work, as i got several errors.\n(error expected \"{\" before \"_density\")\nI guess I must also change the syntax to match the others, but with for example\n_density(getMaterialProperty(\"density\")), I also get errors.\nAre there some tutorials regarding coding this, as I am completely new to C++?",
                          "url": "https://github.com/idaholab/moose/discussions/23481#discussioncomment-5055904",
                          "updatedAt": "2023-02-20T17:14:50Z",
                          "publishedAt": "2023-02-20T17:14:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "just do this:\n    CoupledCureTimeDerivative::CoupledCureTimeDerivative(const InputParameters & parameters) :\n        Kernel(parameters),\n    _density(&getMaterialProperty<Real>(\"density\")),\n    _Hr(&getMaterialProperty<Real>(\"Hr\")),\n\t_v_dot(coupledDot(\"v\")),\n\t _dv_dot(coupledDotDu(\"v\")),\n        _v_var(coupled(\"v\"))\n    {}",
                          "url": "https://github.com/idaholab/moose/discussions/23481#discussioncomment-5055929",
                          "updatedAt": "2023-02-20T17:38:23Z",
                          "publishedAt": "2023-02-20T17:18:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mfitzka"
                          },
                          "bodyText": "I tried this, but then I get the error:\nmultiple initializations given for \"CoupledCureTimeDerivative::_density\"\nand the same for _Hr.\nShould I remove the _density(NULL), and _Hr(NULL), lines?",
                          "url": "https://github.com/idaholab/moose/discussions/23481#discussioncomment-5055963",
                          "updatedAt": "2023-02-20T17:24:07Z",
                          "publishedAt": "2023-02-20T17:24:07Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "problem with number of processors used for simulations",
          "author": {
            "login": "Oops-Qiao"
          },
          "bodyText": "Hello,\nI have a quick question regarding the following code to run my simulations.\nwhen I use  mpiexec -n 2  ~/projects/fly-opt -i test.i to run my test, terminal reports an error like below\nHowever, when I use mpiexec  ~/projects/fly-opt -i test.i to run my test, it works well.\nI want to use as many processors as I can to speed up the simulations. How to understand this? Thanks very much.\nBest regards,\nJeery",
          "url": "https://github.com/idaholab/moose/discussions/23482",
          "updatedAt": "2023-06-24T19:43:39Z",
          "publishedAt": "2023-02-20T14:28:50Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "Oops-Qiao"
                  },
                  "bodyText": "@GiudGiud Perhaps you have seen this before? :)",
                  "url": "https://github.com/idaholab/moose/discussions/23482#discussioncomment-5054097",
                  "updatedAt": "2023-02-20T14:29:37Z",
                  "publishedAt": "2023-02-20T14:29:37Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThis has been answered before on the forum indeed.\nPreconditioning defaults are different in parallel and in serial. The serial one is more effective but does not scale to parallel. You ll need to work on getting the appropriate preconditioning. Some info here:\nhttps://mooseframework.inl.gov/source/executioners/Steady.html\nPlease stop posting screenshots. It's better to copy paste text inside triple backquotes.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/23482#discussioncomment-5054913",
                  "updatedAt": "2023-02-20T15:45:54Z",
                  "publishedAt": "2023-02-20T15:44:27Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Oops-Qiao"
                          },
                          "bodyText": "Hello\nThis has been answered before on the forum indeed. Preconditioning defaults are different in parallel and in serial. The serial one is more effective but does not scale to parallel. You ll need to work on getting the appropriate preconditioning. Some info here: https://mooseframework.inl.gov/source/executioners/Steady.html\nPlease stop posting screenshots. It's better to copy paste text inside triple backquotes.\nGuillaume\n\nThanks a lot Guillaume. I am sorry and the screenshot is deleted. No more next time.\nJerry",
                          "url": "https://github.com/idaholab/moose/discussions/23482#discussioncomment-5054966",
                          "updatedAt": "2023-02-20T15:49:29Z",
                          "publishedAt": "2023-02-20T15:49:29Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Computing effective elastic moduli",
          "author": {
            "login": "clariu25"
          },
          "bodyText": "Hi,\nI'm trying to retrieve the bulk and shear moduli from a simple cube (homogeneous). The idea is to later add porosity and evaluate its effective moduli, therefore, I can't extract the values from the elasticity tensor, as that remains constant. For now I'm only applying some load at the top (ZZ direction). I'm using the Young modulus and Poisson ratio as inputs for the ComputeIsotropicElasticityTensor.\nBy evaluating the stress/strain relations I should be able to retrieve the same values for the elastic moduli that I used as inputs, as the material hasn't changed, but that is not case. Non of the calculated moduli coincide with the input ones.\nResults (Modulus - expected value - output value - calculation):\n\nK - 36GPa  - 30GPa     -   hydrostatic_stress/volumetric_stress\nG - 72MPa -144MPa   -  stress_xz/strain_xz (exactly twice as the input file value)\nE - 22MPa  - 18MPa   -  stress_zz/strain_zz\nPoisson - 0.499 - 0.01  - strain_yy/strain_zz\n\nHas anyone encounter something similar? Or is my assumption of the non-changed material properties wrong?\nThank you in advance for your replies.\nRegards,\nClara",
          "url": "https://github.com/idaholab/moose/discussions/23460",
          "updatedAt": "2023-02-20T08:50:10Z",
          "publishedAt": "2023-02-17T15:08:58Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "Can you elaborate on how you retrieve the moduli \"by evaluating the stress/strain relations\"?",
                  "url": "https://github.com/idaholab/moose/discussions/23460#discussioncomment-5014100",
                  "updatedAt": "2023-02-17T20:15:33Z",
                  "publishedAt": "2023-02-17T20:15:33Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "clariu25"
                          },
                          "bodyText": "The relations that allow to calculate the elastic moduli. The equations that I wrote next to the results.",
                          "url": "https://github.com/idaholab/moose/discussions/23460#discussioncomment-5050498",
                          "updatedAt": "2023-02-20T08:50:11Z",
                          "publishedAt": "2023-02-20T08:50:10Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}