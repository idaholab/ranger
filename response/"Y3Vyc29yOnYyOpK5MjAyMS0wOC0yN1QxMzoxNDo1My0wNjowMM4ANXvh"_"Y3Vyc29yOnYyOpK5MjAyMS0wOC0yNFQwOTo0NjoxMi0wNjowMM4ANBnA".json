{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMS0wOC0yNFQwOTo0NjoxMi0wNjowMM4ANBnA"
    },
    "edges": [
      {
        "node": {
          "title": "Issue w/ Pressure Diffusion Example Output",
          "author": null,
          "bodyText": "I'm currently trying to work through the application development tutorial from the MOOSE framework website on my device with Ubuntu OS. Specifically, I'm trying to run the pressure diffusion example. I have used both the code displayed on the website and the code provided in the MOOSE workshop slides, but neither of them display a result resembling the one displayed on the MOOSE website:\n\nInstead, Peacock is displaying this after execution:\n\nHas anyone else experienced a similar issue, or know what I can try to fix it? I can provide the input file if necessary.\nThanks!\nEdit: This error does not exist when I perform the simulation using the darcy_thermo_mech directory within moose/tutorials that comes with installation. This error was a result from attempting to perform the same simulation using my own developed application, so I assume there is an issue in the way I setup my application originally.",
          "url": "https://github.com/idaholab/moose/discussions/17983",
          "updatedAt": "2024-07-30T13:45:23Z",
          "publishedAt": "2021-06-01T20:05:59Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nAre you talking about the darcy_thermo_mech example?\nWhich step of the workshop slides?\nBest,\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/17983#discussioncomment-814259",
                  "updatedAt": "2024-07-30T13:45:23Z",
                  "publishedAt": "2021-06-02T06:28:26Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": null,
                          "bodyText": "Yes, I'm speaking of that one; which is on Step 1: Geometry and Diffusion in the workshop slides (slide 7). Sorry for not specifying earlier!",
                          "url": "https://github.com/idaholab/moose/discussions/17983#discussioncomment-815876",
                          "updatedAt": "2024-07-30T13:45:30Z",
                          "publishedAt": "2021-06-02T13:22:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "no worries.\nI tried it just now in tutorials/darcy_thermo_mech/step01_diffusion/problems and it looks just like in the top picture in peacock.\nCan you paste your input? I ll look if it differs from the code in the repository",
                          "url": "https://github.com/idaholab/moose/discussions/17983#discussioncomment-817495",
                          "updatedAt": "2024-07-30T13:45:30Z",
                          "publishedAt": "2021-06-02T18:56:20Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "crswong888"
                  },
                  "bodyText": "@speyres, you probably just need to rescale the contours to render it like the image in the tutorial. Double check the contour limits you used when rendering this image (I think there are settings \"min\" and \"max\" in Peacock, set these to the values for the BCs at the right (0 Pa) and left (4000 Pa) of the pipe, respectively).",
                  "url": "https://github.com/idaholab/moose/discussions/17983#discussioncomment-1245749",
                  "updatedAt": "2024-07-30T13:45:30Z",
                  "publishedAt": "2021-08-27T18:51:37Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to well compute the gradient of a 'Material' object?",
          "author": {
            "login": "lgab13"
          },
          "bodyText": "Hi moose experts\uff0c\nI created my application but as I'm a beginner with MOOSE, I miss a lot of basic things.\nI defined a Real property and  I would like to define its gradient in order to use it in one of my PDE. How can I do this easily?\nPlease consider the following basic example:\nMyProperty::MyProperty(const InputParameters & parameters)\n: Material(parameters),\n  _Rmax(getParam<Real>(\"Rmax\")),\n  _profile(declareProperty<Real>(\"MyProfile\"))\n{\n}\n\nvoid\nMyProperty::computeQpProperties()\n{\n  const auto rx = _q_point[_qp](0);\n  const auto ry = _q_point[_qp](1);\n  const auto rz = _q_point[_qp](2);\n  const auto sqr_norm = rx*rx+ry*ry+rz*rz;\n  _profile[_qp] = __Rmax*_Rmax - sqr_norm;\n}\nHere-above, I simply defined f(rx,ry,rz) and now, my goal is to work with  ' grad f ' in the weak form of a diffusive flux of my PDE.\nIs the following approach correct and if not, how do it?\nMyProperty::MyProperty(const InputParameters & parameters)\n: Material(parameters),\n  _Rmax(getParam<Real>(\"Rmax\")),\n  _profile(declareProperty<Real>(\"MyProfile\")),\n  _grad_profile(declareProperty<RealGradient>(\"Mygradient\"))\n{\n}\n\nvoid\nMyProperty::computeQpProperties()\n{\n  const auto rx = _q_point[_qp](0);\n  const auto ry = _q_point[_qp](1);\n  const auto rz = _q_point[_qp](2);\n  const auto sqr_norm = rx*rx+ry*ry+rz*rz;\n  _profile[_qp] = _Rmax*_Rmax - sqr_norm;\n  _grad_profile[_qp][0] = - 2. * rx ;\n  _grad_profile[_qp][1] = - 2. * ry ;\n  _grad_profile[_qp][2] = - 2. * rz ;\n}\nIn this example, the function clearly depends on the spatial coordinates. But, if the property only defines a constant per quadrature points, how can we built the gradient of this property in order to use it in the weak form of a diffusive flux ? When the gradient of the function f is defined as the pressure gradient in the NS equations, that's good for me. But, in case of diffusive flux, I didn't find examples. Maybe I have to populate an AuxVariable or to use an UserObject?\nbest regards\nlg",
          "url": "https://github.com/idaholab/moose/discussions/18674",
          "updatedAt": "2022-10-04T19:45:13Z",
          "publishedAt": "2021-08-21T23:57:16Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nPlease consult #18415 for more information on this topic.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/18674#discussioncomment-1220088",
                  "updatedAt": "2022-10-04T19:45:54Z",
                  "publishedAt": "2021-08-23T02:16:46Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "lgab13"
                          },
                          "bodyText": "Thank you  Guillaume. This is what I was looking for.\nbest regards,\nLG",
                          "url": "https://github.com/idaholab/moose/discussions/18674#discussioncomment-1242055",
                          "updatedAt": "2022-10-04T19:45:55Z",
                          "publishedAt": "2021-08-27T00:09:00Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Executing moose's executable (on WSL) through Matlab.",
          "author": {
            "login": "aaelmeli"
          },
          "bodyText": "Hi\nI am using moose as the engine to solve the forward problem required for optimization problems. I am using Matlab's optimization toolbox for the optimization process (such as BFGS). Now, I am trying to call moose's executable from Matlab. I am using moose on WSL.\nSo, is there a way to call moose's executable from Matlab? for instance,\nsomething like this system('cmd /C myprogram.exe');\nThank you.",
          "url": "https://github.com/idaholab/moose/discussions/18707",
          "updatedAt": "2022-12-14T12:40:34Z",
          "publishedAt": "2021-08-26T18:23:46Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "dschwen"
                  },
                  "bodyText": "You need to use the wsl wrapper command. https://docs.microsoft.com/en-us/windows/wsl/wsl-config",
                  "url": "https://github.com/idaholab/moose/discussions/18707#discussioncomment-1241904",
                  "updatedAt": "2022-12-14T12:40:34Z",
                  "publishedAt": "2021-08-26T23:12:25Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to reset sub-app data after each time step of main app?",
          "author": {
            "login": "abarun22"
          },
          "bodyText": "Dear all,\nI am currently using multi-app feature in MOOSE to construct a multi-scale model that simulates the structural response of a component with reference to underlying micro-structural behaviour. The sub-app essentially contains an RVE modelled with strain periodicity approach. The workflow of this set-up requires that the stresses from region of interest in the main model is passed to the RVE, constraining the periodic boundaries and the resulting displacement (fluctuating) field is passed on to the main model enabling the determination of overall mechanical response.\nI am able to visualise this particular workflow under a multi-apps framework, transferring the required data back and forth between the models. While tyring out an example model, i observed that the sub-model seems to resume the analysis from the state attained at previous time step of main model, negating my requirement of a fresh start after each time step. I see that a sub-app reset can be performed at a specified time step of main app with the 'reset_apps' option (i guess that work only with 'TransientMultiApp'). Could any body advise me if this can be done at the end of each time step, so that the sub-app execution starts with a clean slate that ensures correct set of results? Attached here are the input files used and the log of execution.\nKind regards,\nArun\nExecution_log.txt\nmacro.txt\nsub_gs_uniaxial.txt",
          "url": "https://github.com/idaholab/moose/discussions/18549",
          "updatedAt": "2022-06-01T01:52:15Z",
          "publishedAt": "2021-08-06T16:19:28Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nSeems like what you want should be doable. While waiting for someone else to pitch in, could you please try:\n\nadding an initial condition in your subapp, see if it is obeyed at the next solve\nuse the no_backup_and_restore = true parameter of the FullSolveMultiapp. It s meant for fixed point iterations, but those systems are intertwined a bit\nuse the reset_apps and reset_time parameters. This is not just for TransientMultiapp, the times specified are the time steps for the main application. This should work but it's a little extreme.\n\nFinally, do you really need a FullSolveMultiApp with a single time step and a reset? It seems like this should either be a steady state solve in a FullSolveMultiapp OR a transient solve in a TransientMultiApp. Just an impression though\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1140940",
                  "updatedAt": "2022-06-01T01:52:15Z",
                  "publishedAt": "2021-08-06T20:38:07Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "abarun22"
                  },
                  "bodyText": "Hi  Guillaume,\nThe inclusion of  an initial condition and the option 'no_backup_and_restore\n= true' does not seem to affect the response. In principle I would\nessentially want to solve a transient problem and so the usage of\n'TransientMultiApp' would be more optimal for the computation. Inclusion of\n'reset_apps' and 'reset_time' parameters in 'TransientMultiApp' does not\nreset the sub-app data after every time step of the main model.\nI've got my custom transfers (from and to the main model) to be performed\nbefore and after execution of the sub-model.  The quantities that are\nsupposed to be transferred across are the stresses and displacements, which\nare computed separately from the main and sub model respectively. Going by\nthese, I do not want any of the default transfers that are happening in\nthis workflow.\n\nI really wonder what would be the best way to start the sub-app computation\nin a clean state when the custom transfers are not implemented.\nKind regards,\nArun\n\u2026\nOn Fri, Aug 6, 2021 at 9:38 PM Guillaume Giudicelli < ***@***.***> wrote:\n Hello\n\n Seems like what you want should be doable. While waiting for someone else\n to pitch in, could you please try:\n\n    - adding an initial condition in your subapp, see if it is obeyed at\n    the next solve\n    - use the no_backup_and_restore = true parameter of the\n    FullSolveMultiapp. It s meant for fixed point iterations, but those systems\n    are intertwined a bit\n    - use the reset_apps and reset_time parameters. This is not just for\n    TransientMultiapp, the times specified are the time steps for the main\n    application. This should work but it's a little extreme.\n\n Finally, do you really need a FullSolveMultiApp with *a single time step*\n and a reset? It seems like this should either be a steady state solve in a\n FullSolveMultiapp OR a transient solve in a TransientMultiApp. Just an\n impression though\n\n Guillaume\n\n \u2014\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n <#18549 (comment)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/AJSA252K3CQFWWCJWD227GLT3RB3VANCNFSM5BWGVUEA>\n .\n Triage notifications on the go with GitHub Mobile for iOS\n <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>\n or Android\n <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>\n .",
                  "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1150381",
                  "updatedAt": "2022-06-01T01:52:36Z",
                  "publishedAt": "2021-08-09T16:40:15Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Thanks for trying these out.\nIt doesnt reset even if you specify every time step in the list of time steps?",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1152396",
                          "updatedAt": "2022-06-01T01:52:44Z",
                          "publishedAt": "2021-08-10T05:26:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "I'm afraid we may not be able to provide a list of time steps to 'reset_time' option. And another problem i am facing now is that i could not re-produce the attainment of sub-app reset after the specified main app time step through this option. May be i am missing some thing. But nevertheless the objective now is that the reset is required to be functional on all time steps.",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1160078",
                          "updatedAt": "2022-06-01T01:52:44Z",
                          "publishedAt": "2021-08-11T18:13:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You mean you can only provide a single value?\nThis would be a fairly easy modification to the framework to allow an option for all timesteps, or to be able to specify a list of timesteps.\nWe would assist you in merging this capability in the framework if you developed it. If you are affiliated with a US university or national laboratory or other DOE subcontractors we could also just develop that for you.",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1166023",
                          "updatedAt": "2022-06-01T01:52:44Z",
                          "publishedAt": "2021-08-13T00:03:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Yes, at present it can only take a single time value. I did tweak the code a little bit to allow for the reset to be happening for all the timesteps. Pasted here is the modified code block for all 'target_time' exceeding the specified reset time, without any reference to the reset history.\n// First, see if any Apps need to be Reset\n//  if (!_reset_happened && target_time + 1e-14 >= _reset_time)\nif (target_time + 1e-14 >= _reset_time)\n{\n_reset_happened = true;\nfor (auto & app : _reset_apps)\nresetApp(app);\n}\nThis works ok for me at present. Now the question to be answered is whether you want the single reset or the bulk reset to be retained in the code. As you said the list (of reset_time) would be much better option to implement at this stage, given that it is more generic. I guess the modification in 'MultiApp.C' would be seen in all the related derived classes. I am happy to do the modification if you are ok with it.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1167514",
                          "updatedAt": "2022-07-08T07:22:24Z",
                          "publishedAt": "2021-08-13T10:01:33Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "We would be very interested in supporting the list of reset_time.\nThe single reset time was likely developed to support another application, so we cannot switch the behavior from single to all like this though.",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1168674",
                          "updatedAt": "2022-07-08T07:22:25Z",
                          "publishedAt": "2021-08-13T14:47:29Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "abarun22"
                  },
                  "bodyText": "Hi Guillaume,\nHere is the modified code block for enabling the variable '_reset_time' as a list of times from the main app.\nvoid\nMultiApp::preTransfer(Real /dt/, Real target_time)\n{\nif ((_reset_time.size() == 0))\nmooseError(\"Please specify the list comprising times.\");\n// First, see if any Apps need to be Reset\nfor (unsigned int i=0; i<_reset_time.size();i++){\nif (target_time == _reset_time[i])\n{\nfor (auto & app : _reset_apps)\nresetApp(app);\n}\n}\nI've tested that only with TransientMultiApp, while other objects are yet to be tested. Please let me know if this can be approved through a PR?\nKind regards,\nArun",
                  "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1199774",
                  "updatedAt": "2022-06-01T01:53:14Z",
                  "publishedAt": "2021-08-18T09:54:54Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I think this\nif ((_reset_time.size() == 0)) mooseError(\"Please specify the list comprising times.\");\nmakes reset times mandatorym which we dont necessarily want.\nIs there any logic with reset_times than makes sure that we hit all of them exactly? Or if the time is not reached exactly then the reset is skipped? What did you see in your testing?",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1201500",
                          "updatedAt": "2022-06-24T20:21:37Z",
                          "publishedAt": "2021-08-18T14:52:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Yes the chosen logic will compare each element in the list to the current time 'target_time' making sure that all of those in reset_time are addressed. In the other case, where the specified times are not reached during the course of computation (for example due to time step cut by auto time stepping scheme) we may not be able to reset the sub-app following that time step. These things are subjective and cannot be known at the time of preparing the input commands. I welcome your suggestions in that regard. Attached here are the example input files i used earlier for testing this feature.\n18082021.zip",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1201759",
                          "updatedAt": "2022-06-24T20:21:42Z",
                          "publishedAt": "2021-08-18T15:33:22Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Ok my question was more the second point. This is really not good but it's also not because of your change, it seems to be an existing problem with the reset_time parameter.\nFor comparing the target time and the reset times, you cannot use ==, this will be vulnerable to floating point errors. Use the\nabsoluteFuzzyEqual() routine.\nOtherwise, please make the PR at your convenience",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1201821",
                          "updatedAt": "2022-06-24T20:21:42Z",
                          "publishedAt": "2021-08-18T15:46:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "ok actually the existing code behaves differently\n  // First, see if any Apps need to be Reset\n  if (!_reset_happened && target_time + 1e-14 >= _reset_time)\n  {\n    _reset_happened = true;\n    for (auto & app : _reset_apps)\n      resetApp(app);\n  }\n\nyou need to mimick that behavior (reset after time has passed) rather than try to hit the times exactly",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1201915",
                          "updatedAt": "2022-09-12T09:17:10Z",
                          "publishedAt": "2021-08-18T16:04:20Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "friedmud"
                  },
                  "bodyText": "Before going futher here... I would like to know more about your application.  Are you doing a steady state solve in the sub-app?  If so, it shouldn't matter what solution was there before.  Are you trying to do a transient run in the sub-app at every timestep?\nSome more information would be useful.",
                  "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1201901",
                  "updatedAt": "2022-06-01T01:53:21Z",
                  "publishedAt": "2021-08-18T16:01:07Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "friedmud"
                          },
                          "bodyText": "@abarun22 I really don't think you need to be doing anything with resetting apps.  That would be a very rare use-case.  Can you please explain the physical system you're trying to solve?",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1202357",
                          "updatedAt": "2022-06-01T01:53:20Z",
                          "publishedAt": "2021-08-18T17:39:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Dear Derek/Guillaume,\nThanks for your comments. Attached please find a presentation that describes the physics i am trying to solve. In my opinion, i would prefer a sub-app reset as that would allow me to start the sub-model computation from a clean state, thus enabling the custom transfers i am planning to implement, taking shape. The sub-app essentially mimics a steady state analysis though.\nCPFEM_descr_19Aug2021.pptx\nRegarding the reset_time modification, i still feel that hitting the right time would be the correct approach, as the current approach uses the precision value in comparing the times, which in our case is by-passed by the usage of absoluteFuzzyEqual(). Here is that code.\n void\nMultiApp::preTransfer(Real /*dt*/, Real target_time)\n{\n  if ((_reset_time.size() == 0))\n    mooseError(\"Please specify the list comprising times.\");\n\n   // First, see if any Apps need to be Reset\n//  if (!_reset_happened && target_time + 1e-14 >= _reset_time)\n   for (unsigned int i=0; i<_reset_time.size();i++){\n     if (MooseUtils::absoluteFuzzyEqual(target_time, _reset_time[i]))\n  {\n//    _reset_happened = true;\n    for (auto & app : _reset_apps)\n      resetApp(app);\n  }\n}\n\nOtherwise this code block does the right job, still resetting the required times.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1205585",
                          "updatedAt": "2022-06-01T01:53:21Z",
                          "publishedAt": "2021-08-19T10:23:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "When using a FullSolveMultiapp, you usually do NOT want to start from a clean state, because the result is usually the same whether you start from a clean state or from a fresh case, it just takes longer to get to the same place.\nIn your current configuration of FullSolveMultiApp + Transient with 1 timestep, you are getting a different result if you re-initialize, which was why I was helping you with the reset.\nIf your subapp is mimicking a steady state analysis, why are you not using a Steady executioner to try to get to steady state?\nFor a steady subapp, definitely no need to reset.\nThis code block does not reset if you go over the reset_time (without hitting it exactly or even having a timestep near it). So it doesnt work for what people have used it for and I think it may fail the test suite.",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1208548",
                          "updatedAt": "2022-06-01T01:53:21Z",
                          "publishedAt": "2021-08-19T19:41:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Yes i think FullSolveMultiapp runs the sub-model independently from the main model and so we will not be required to reset the sub-app as we did for the TransientMultiApp. However FullSolveMultiapp produces a segmentation violation (at the end of second time step of main app), even with the original case of a single value reset_time and so i could not check the state of sub-app computation in the subsequent time steps.  From my perspective i will intend to use FullSolveMultiapp given the computation runs OK without seg faults. As it stands, i shall continue using TransientMultiApp with Transient runs in both apps and that gives acceptable results with manual imposition of list of reset times. Now that i know the sub-app starts off in a clean manner, i shall then think about writing my own transfer functions to push and pull the data between the apps.\nAs per your suggestion i've also modified the code further to perform a reset even at odd times decided by the time stepping scheme. Here is the new block of code.\n\n   for (unsigned int i=0; i<_reset_time.size();i++){\n      if (target_time + 1e-14 >= _reset_time[i])\n  {\n    for (auto & app : _reset_apps)\n      resetApp(app);\n      break;\n  }\n\nAlso find attached the new set of input files relevant to my current work, which will form the building block for further developments.\n20082021.zip\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1212285",
                          "updatedAt": "2022-07-08T07:23:27Z",
                          "publishedAt": "2021-08-20T15:19:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hi Arun\nThis new piece of code will continuously reset the apps. You need to keep track of when each reset happened, using a vector of boolean. The original code there should guide you into what this system should be able to do.\nA seg fault is very concerning. Could you please compile in debug mode, METHOD=dbg make -j #cores then get us a backtrace for this error, either by running with start-in-debugger, or with gdb --ex run --ex bt --args executable-dbg -i input_file.i\nAlso there are a number of existing transfers. Does none of them work for your case? What are you trying to transfer?",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1212368",
                          "updatedAt": "2022-07-08T07:24:31Z",
                          "publishedAt": "2021-08-20T15:32:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Hi Guillaume,\nHere is the new segment of code that does it correctly. This will reset not only the times given in the list, but also the others which arise in-between them. Essentially we check if the target_time falls in between the consecutive reset_time's in the list and reset those target times. Please take a look at it and let me know if that's ok with you.\n\n   // Sort the 'reset_time' list in increasing order\n   std::sort(_reset_time.begin(), _reset_time.end());\n\n   // Reset apps for the times provided in the list\n   if (_reset_time.size()>1){\n      for (unsigned int i=0; i<_reset_time.size()-1;i++){\n        if (target_time + 1e-14 >= _reset_time[i] && target_time < _reset_time[i+1]){\n          for (auto & app : _reset_apps)\n          resetApp(app);\n        }\n      }\n  }\n\n  // Reset apps for the following cases:\n  // 1) list contains a single entry\n  // 2) Final entry in the list which was not reset previously\n\n  if ((_reset_time.size()==1 && target_time==_reset_time[0]) || target_time == _reset_time[_reset_time.size()-1]){\n        for (auto & app : _reset_apps)\n          resetApp(app);\n  }\n\nI am yet to look in to the seg fault issues with FullSolveMultiapp. Will look in to that and let you know the status soon. Regarding the transfer, all of the available schemes seems to couple certain field variables. None of them allows me to transfer just the data that is required to be visible to the other app. I am interested to transfer the following:\n\noriginal/deformed coordinates of main model - From main app to sub app\nStress tensor of the heavily stresses element - From main app to sub app\nUpdated displacements of the sub-model to be linked with the equivalent displacements in main model - From sub-app to main app\nI presume that the data from the first two cases be packaged in to a user object, which can in some way be transferred to the sub-app. The sub-app in turn may need a receiver class to fetch the data and make it ready for the sub-app space. The displacements can be coupled through a standard scheme such as 'MultiAppNearestNodeTransfer' if i am correct. Eager to know your suggestions in this regard.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1223356",
                          "updatedAt": "2022-07-08T07:24:34Z",
                          "publishedAt": "2021-08-23T16:17:56Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hi Arun\nThis still does not do the trick. You don't keep track of which reset has happened, so you ll keep resetting indefinitely.\nWe also do NOT need to separate the case with more than one reset from the case with a single reset time.\nYou need to look at the original code and understand why that boolean, keeping track of when the sub app reset was performed, was there in the first place.\nOk I'd like to see that backtrace for the seg fault.\n@dschwen is there any pre-made transfers/user objects for transfering stress tensors to a sub app? Or should they move the data to an aux-variable then use a regular transfer?\nAre you going to use 3. to try to deform the main app mesh? Or to compute some quantity in the main app?\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1223797",
                          "updatedAt": "2022-07-08T07:24:41Z",
                          "publishedAt": "2021-08-23T18:05:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "Hi Guillaume,\nI guess this time we may be ok. The code below might do the trick.\n\n   // Sort the 'reset_time' list in increasing order\n   std::sort(_reset_time.begin(), _reset_time.end());\n\n  // First, see if any Apps need to be Reset\n     for (unsigned int i=0; i<_reset_time.size();i++){\n        if (!_reset_happened[i] && (target_time + 1e-14 >= _reset_time[i]))\n       {\n        _reset_happened[i] = true;\n        for (auto & app : _reset_apps)\n           resetApp(app);\n       }\n     }\n\nI tried this before, but was overshadowed by the odd behaviour where the time step changes drastically for each length of list we provide in the input file. I wonder what is that causing the dependency of time step cut on the reset process. Strictly speaking the time step should not be influenced by the number of elements provided in the reset_time list.\nI will give you my seg fault results possibly in my next post.\nYes, i calculate the updated displacements of the main app mesh inside the sub app and transfer that on to the main app with an intention to overwrite the old displacements, that will then feed on to the next time step.\nKind regards,\nArun",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1228493",
                          "updatedAt": "2022-07-08T07:24:41Z",
                          "publishedAt": "2021-08-24T16:42:54Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Yup this looks good. We also would not want to reset the app multiple times if two reset_times have been hit\nSo let's have another, single, boolean to track that\nUmm this might be difficult. disp_x is not an AuxVariable, it s a main variable right? In which case you can't modify it like this",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1228823",
                          "updatedAt": "2022-07-08T07:24:41Z",
                          "publishedAt": "2021-08-24T17:39:54Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "abarun22"
                          },
                          "bodyText": "With this approach, the boolean array is enough to track the status of each reset_time in the list, whether the reset is accomplished or not. I do not see any reason why an app should be reset multiple times here. Could you please brief me a bit more here and help me understand the problem deeply?\nYes i am interested in overwriting disp_x which is the main variable. I do not know why this can't be modified.",
                          "url": "https://github.com/idaholab/moose/discussions/18549#discussioncomment-1232027",
                          "updatedAt": "2022-07-08T07:24:43Z",
                          "publishedAt": "2021-08-25T09:34:03Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Weird MPI errors with distrubuted mesh",
          "author": {
            "login": "heinono1"
          },
          "bodyText": "Hi. We are running a module on top of moose and use distributed meshes in order to avoid choking memory on the nodes. This has worked very well. Then the cluster was down for maintenance and after that we get weird errors - the code or build have not been touched. It may be that the errors and the maintenance are coincidental - I don't know. Anyhow, we do the usual - first run to distribute the mesh for N tasks,\nsrun /lcrc/project/Meso/projects/hedgehog_kks/hedgehog_kks-opt -i KKS_CALPHAD_LHS_3D.i --split-mesh 144 --split-file foo.cpr\nand then run the job\nsrun -n 144 /lcrc/project/Meso/projects/hedgehog_kks/hedgehog_kks-opt -i KKS_CALPHAD_LHS_3D.i --use-split --split-file foo.cpr\nWe use checkpointing and restarts. The errors are as below. Clearly weird stuff going on with MPI. The technical support people (who are quite good) are sure this has to do with the specific app and is something they cannot address directly. Any insight would be greatly appreciated.\nHere is an example of the error:\n[MPID_nem_tmi_pending_ssend_dequeue]: ERROR: can not find matching ssend. context=0, rank=65, tag=0\n[MPID_nem_tmi_pending_ssend_dequeue]: ERROR: can not find matching ssend. context=0, rank=70, tag=0\n[MPID_nem_tmi_pending_ssend_dequeue]: pending ssend list: (context=0, rank=0, tag=400008f5, sreq=0x4266be0) (context=0, rank=1, tag=400008f5, sreq=0x2b4436aafe80) (context=0, rank=4, tag=400008f5, sreq=0x2b4436aae800) (context=0, rank=5, tag=400008f5, sreq=0x2b4436ab5400) (context=0, rank=13, tag=400008f5, sreq=0x423b6e0) (context=0, rank=22, tag=400008f5, sreq=0x425a160) (context=0, rank=36, tag=400008f5, sreq=0x42338e0) (context=0, rank=48, tag=400008f5, sreq=0x2b4436abbb80) (context=0, rank=56, tag=400008f5, sreq=0x422bae0) (context=0, rank=63, tag=400008f5, sreq=0x42686e0) (context=0, rank=64, tag=400008f5, sreq=0x42302e0) (context=0, rank=65, tag=400008f5, sreq=0x425fb60) (context=0, rank=113, tag=400008f5, sreq=0x4239760) (context=0, rank=114, tag=400008f5, sreq=0x43652c0) (context=0, rank=118, tag=400008f5, sreq=0x4228960) (context=0, rank=130, tag=400008f5, sreq=0x43796c0)\n[MPID_nem_tmi_pending_ssend_dequeue]: pending ssend list: (context=0, rank=0, tag=400008f5, sreq=0x3649c40) (context=0, rank=11, tag=400008f5, sreq=0x3634f40) (context=0, rank=15, tag=400008f5, sreq=0x3608cc0) (context=0, rank=57, tag=400008f5, sreq=0x3644b40) (context=0, rank=59, tag=400008f5, sreq=0x3631dc0) (context=0, rank=66, tag=400008f5, sreq=0x2b20f0d0e580) (context=0, rank=68, tag=400008f5, sreq=0x2b20f0d2a300) (context=0, rank=70, tag=400008f5, sreq=0x360d940) (context=0, rank=77, tag=400008f5, sreq=0x2b20f0d22e00) (context=0, rank=80, tag=400008f5, sreq=0x3632fc0) (context=0, rank=88, tag=400008f5, sreq=0x2b20f0d1ea80) (context=0, rank=99, tag=400008f5, sreq=0x2b20f0d20a00) (context=0, rank=101, tag=400008f5, sreq=0x2b20f0d16800)\nMPID_nem_tmi_handle_ssend_ack: &pad1=0x2b4436b14300, &ssend_ack_recv_buffer=0x2b4436b14308, &pad2=0x2b4436b14318\nMPID_nem_tmi_handle_ssend_ack: &pad1=0x2b20f0d7c300, &ssend_ack_recv_buffer=0x2b20f0d7c308, &pad2=0x2b20f0d7c318\nMPID_nem_tmi_handle_ssend_ack: pad1=0, pad2=0 (both should be 0)\nMPID_nem_tmi_handle_ssend_ack: pad1=0, pad2=0 (both should be 0)\nsrun: error: bdw-0605: task 124: Segmentation fault (core dumped)\nsrun: error: bdw-0531: task 98: Segmentation fault (core dumped)",
          "url": "https://github.com/idaholab/moose/discussions/18603",
          "updatedAt": "2022-07-07T16:34:50Z",
          "publishedAt": "2021-08-13T16:07:30Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nDid you re-compile after the cluster got updated? Did they change anything about the MPI distribution during maintenance?\nCould we please get more information about the MOOSE version and the MPI distribution used?\nBest,\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/18603#discussioncomment-1187773",
                  "updatedAt": "2022-08-21T07:06:22Z",
                  "publishedAt": "2021-08-16T00:40:09Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "heinono1"
                  },
                  "bodyText": "Hi,\n\nI don\u2019t think the MPI dist got updated (other mpi codes run without problems afaik). They did not do any software updates \u2013 they just had a problem with the file server that they sorted out.\n\u2026\n-Olle\n\nOlle Heinonen\nSr. Materials Scientist\n***@***.***\n\n\nFrom: Guillaume Giudicelli ***@***.***>\nReply-To: idaholab/moose ***@***.***>\nDate: Sunday, August 15, 2021 at 7:40 PM\nTo: idaholab/moose ***@***.***>\nCc: \"Heinonen, Olle G.\" ***@***.***>, Author ***@***.***>\nSubject: Re: [idaholab/moose] Weird MPI errors with distrubuted mesh (#18603)\n\n\nHello\n\nDid you re-compile after the cluster got updated? Did they change anything about the MPI distribution?\n\nBest,\nGuillaume\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<#18603 (comment)>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AEKZEF3JMHL25OPKM2QWL4TT5BM7LANCNFSM5CD35ZFA>.\nTriage notifications on the go with GitHub Mobile for iOS<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android<https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>.",
                  "url": "https://github.com/idaholab/moose/discussions/18603#discussioncomment-1187825",
                  "updatedAt": "2022-08-21T07:06:29Z",
                  "publishedAt": "2021-08-16T01:00:01Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Does it work with less processes than 144?",
                          "url": "https://github.com/idaholab/moose/discussions/18603#discussioncomment-1187857",
                          "updatedAt": "2022-08-21T07:06:32Z",
                          "publishedAt": "2021-08-16T01:18:08Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "heinono1"
                          },
                          "bodyText": "I haven't checked on fewer than 144 processes.\nmoose-environment package version (https://github.com/idaholab/package_builder)\nidaholab/package_builder#217\nrepo-hash:7af64c8a0de933a801d096655b79fb957ecc10b1\nIntel compilers (17.0.4) and mpi wrappers, intel mkl",
                          "url": "https://github.com/idaholab/moose/discussions/18603#discussioncomment-1189989",
                          "updatedAt": "2022-08-21T07:06:32Z",
                          "publishedAt": "2021-08-16T13:21:17Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "heinono1"
                  },
                  "bodyText": "I don\u2019t think a bad node is the problem \u2013 we have run several times and gotten the same error. Because the cluster has some 1000 nodes, every time I queue up a job requesting 4 nodes I am likely to get 4 different nodes.\n\u2026\n-Olle\n\nOlle Heinonen\nSr. Materials Scientist\nArgonne National Laboratory\n***@***.***\n\n\nFrom: Guillaume Giudicelli ***@***.***>\nDate: Sunday, August 15, 2021 at 8:18 PM\nTo: idaholab/moose ***@***.***>\nCc: Heinonen, Olle G. ***@***.***>, Author ***@***.***>\nSubject: Re: [idaholab/moose] Weird MPI errors with distrubuted mesh (#18603)\n\nCould it be that there is a bad node? Eg, does it work with less processes than 144?\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<#18603 (reply in thread)>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AEKZEF6UL4PVXOIISK4N4TDT5BRNVANCNFSM5CD35ZFA>.\nTriage notifications on the go with GitHub Mobile for iOS<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android<https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>.",
                  "url": "https://github.com/idaholab/moose/discussions/18603#discussioncomment-1190005",
                  "updatedAt": "2022-08-21T07:06:33Z",
                  "publishedAt": "2021-08-16T13:24:25Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Ok if recompiling does not help and if trying smaller configurations is not possible, we should try to get a backtrace for the error.\nCould you please recompile in debug mode with METHOD=dbg make -j #procs\nthen run inside a debugger with\nsrun -n 144 gdb --ex run --ex bt --args /lcrc/project/Meso/projects/hedgehog_kks/hedgehog_kks-dbg -i KKS_CALPHAD_LHS_3D.i --use-split --split-file foo.cpr",
                  "url": "https://github.com/idaholab/moose/discussions/18603#discussioncomment-1190320",
                  "updatedAt": "2022-08-21T07:06:33Z",
                  "publishedAt": "2021-08-16T14:24:40Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Any news of this? Has the problem been identified?",
                  "url": "https://github.com/idaholab/moose/discussions/18603#discussioncomment-1228751",
                  "updatedAt": "2022-08-21T07:06:33Z",
                  "publishedAt": "2021-08-24T17:26:04Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "heinono1"
                  },
                  "bodyText": "It appears that something got mucked up in the mpi handling of distributed meshes. I have to rebuild moose\u2026.\n\u2026\n-Olle\n\nOlle Heinonen\nSr. Materials Scientist\nArgonne National Laboratory\n***@***.***\n\n\nFrom: Guillaume Giudicelli ***@***.***>\nDate: Tuesday, August 24, 2021 at 12:26 PM\nTo: idaholab/moose ***@***.***>\nCc: Heinonen, Olle G. ***@***.***>, Author ***@***.***>\nSubject: Re: [idaholab/moose] Weird MPI errors with distrubuted mesh (#18603)\n\nAny news of this? Has the problem been identified?\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<#18603 (comment)>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AEKZEF2YVEUI4A57DOB4QRLT6PI3PANCNFSM5CD35ZFA>.\nTriage notifications on the go with GitHub Mobile for iOS<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android<https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>.",
                  "url": "https://github.com/idaholab/moose/discussions/18603#discussioncomment-1228766",
                  "updatedAt": "2022-08-21T07:06:33Z",
                  "publishedAt": "2021-08-24T17:28:08Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "heinono1"
                  },
                  "bodyText": "Hi,\n\nAfter battling conflicting versions of anaconda and miniconda, I did get the conda environment cleaned up and proceeded with the minconda3 install of moose on our large cluster (Ubuntu).  The build went without problem but the tests crash. A sample below. All crash errors (and pretty much everything crashes) seem to be related to problems in the actual codes or input files used for testing. What is going on???\n\n\n(moose) ***@***.*** test]$ ./run_tests -j 4\n\noutputs/vtk.solution/diff_parallel_mesh ................................. [#6149, MESH_MODE!=DISTRIBUTED] SKIP\n\noutputs/png.image_tests/wedge ............................................................ [LIBPNG!=TRUE] SKIP\n\noutputs/png.image_tests/square_domain .................................................... [LIBPNG!=TRUE] SKIP\n\noutputs/png.image_tests/adv_diff_reaction ................................................ [LIBPNG!=TRUE] SKIP\n\noutputs/nemesis.nemesis_scalar_distributed ..................................... [MESH_MODE!=DISTRIBUTED] SKIP\n\noutputs/nemesis.nemesis_elemental_distributed .................................. [MESH_MODE!=DISTRIBUTED] SKIP\n\noutputs/format.tecplot_bin_test_override ............................................... [TECPLOT!=FALSE] SKIP\n\ncontrols/error.tid_warehouse_error ........................................................ [METHOD!=DBG] SKIP\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: Working Directory: /gpfs/fs1/home/heinonen/projects/moose/test/tests/time_integrators/explicit_ssp_runge_kutta\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: Running command: /gpfs/fs1/home/heinonen/projects/moose/test/moose_test-opt -i explicit_ssp_runge_kutta.i --error --error-unused --error-override --no-gdb-backtrace\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: /gpfs/fs1/home/heinonen/projects/moose/test/moose_test-opt: symbol lookup error: /home/heinonen/miniconda3/envs/moose/lib/./libmpifort.so.12: undefined symbol: MPII_F_TRUE\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: /gpfs/fs1/home/heinonen/projects/moose/test/moose_test-opt: symbol lookup error: /home/heinonen/miniconda3/envs/moose/lib/./libmpifort.so.12: undefined symbol: MPII_F_TRUE\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order:\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order:\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: Exit Code: 127\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: ################################################################################\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: Tester failed, reason: CRASH\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order:\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order ..................................... FAILED (CRASH)\n\n\nOlle Heinonen\nSr. Materials Scientist\nArgonne National Laboratory\n***@***.***\n\n\nFrom: Heinonen, Olle G. ***@***.***>\nDate: Tuesday, August 24, 2021 at 12:28 PM\nTo: idaholab/moose ***@***.***>, idaholab/moose ***@***.***>\nCc: Author ***@***.***>\nSubject: Re: [idaholab/moose] Weird MPI errors with distrubuted mesh (#18603)\nIt appears that something got mucked up in the mpi handling of distributed meshes. I have to rebuild moose\u2026.\n\u2026\n-Olle\n\nOlle Heinonen\nSr. Materials Scientist\nArgonne National Laboratory\n***@***.***\n\n\nFrom: Guillaume Giudicelli ***@***.***>\nDate: Tuesday, August 24, 2021 at 12:26 PM\nTo: idaholab/moose ***@***.***>\nCc: Heinonen, Olle G. ***@***.***>, Author ***@***.***>\nSubject: Re: [idaholab/moose] Weird MPI errors with distrubuted mesh (#18603)\n\nAny news of this? Has the problem been identified?\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<#18603 (comment)>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AEKZEF2YVEUI4A57DOB4QRLT6PI3PANCNFSM5CD35ZFA>.\nTriage notifications on the go with GitHub Mobile for iOS<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android<https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>.",
                  "url": "https://github.com/idaholab/moose/discussions/18603#discussioncomment-1233208",
                  "updatedAt": "2022-08-21T07:06:33Z",
                  "publishedAt": "2021-08-25T13:25:44Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "heinono1"
                  },
                  "bodyText": "The main culprit seems to be that MPII_F_TRUE is undefined. Where is that supposed to be defined an what is the symbol for?\n\u2026\n-Olle\n\nOlle Heinonen\nSr. Materials Scientist\nArgonne National Laboratory\n***@***.***\n\n\nFrom: Heinonen, Olle G. ***@***.***>\nDate: Wednesday, August 25, 2021 at 8:25 AM\nTo: idaholab/moose ***@***.***>, idaholab/moose ***@***.***>\nCc: Author ***@***.***>\nSubject: Re: [idaholab/moose] Weird MPI errors with distrubuted mesh (#18603)\nHi,\n\nAfter battling conflicting versions of anaconda and miniconda, I did get the conda environment cleaned up and proceeded with the minconda3 install of moose on our large cluster (Ubuntu).  The build went without problem but the tests crash. A sample below. All crash errors (and pretty much everything crashes) seem to be related to problems in the actual codes or input files used for testing. What is going on???\n\n\n(moose) ***@***.*** test]$ ./run_tests -j 4\n\noutputs/vtk.solution/diff_parallel_mesh ................................. [#6149, MESH_MODE!=DISTRIBUTED] SKIP\n\noutputs/png.image_tests/wedge ............................................................ [LIBPNG!=TRUE] SKIP\n\noutputs/png.image_tests/square_domain .................................................... [LIBPNG!=TRUE] SKIP\n\noutputs/png.image_tests/adv_diff_reaction ................................................ [LIBPNG!=TRUE] SKIP\n\noutputs/nemesis.nemesis_scalar_distributed ..................................... [MESH_MODE!=DISTRIBUTED] SKIP\n\noutputs/nemesis.nemesis_elemental_distributed .................................. [MESH_MODE!=DISTRIBUTED] SKIP\n\noutputs/format.tecplot_bin_test_override ............................................... [TECPLOT!=FALSE] SKIP\n\ncontrols/error.tid_warehouse_error ........................................................ [METHOD!=DBG] SKIP\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: Working Directory: /gpfs/fs1/home/heinonen/projects/moose/test/tests/time_integrators/explicit_ssp_runge_kutta\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: Running command: /gpfs/fs1/home/heinonen/projects/moose/test/moose_test-opt -i explicit_ssp_runge_kutta.i --error --error-unused --error-override --no-gdb-backtrace\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: /gpfs/fs1/home/heinonen/projects/moose/test/moose_test-opt: symbol lookup error: /home/heinonen/miniconda3/envs/moose/lib/./libmpifort.so.12: undefined symbol: MPII_F_TRUE\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: /gpfs/fs1/home/heinonen/projects/moose/test/moose_test-opt: symbol lookup error: /home/heinonen/miniconda3/envs/moose/lib/./libmpifort.so.12: undefined symbol: MPII_F_TRUE\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order:\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order:\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: Exit Code: 127\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: ################################################################################\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: Tester failed, reason: CRASH\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order:\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order ..................................... FAILED (CRASH)\n\n\nOlle Heinonen\nSr. Materials Scientist\nArgonne National Laboratory\n***@***.***\n\n\nFrom: Heinonen, Olle G. ***@***.***>\nDate: Tuesday, August 24, 2021 at 12:28 PM\nTo: idaholab/moose ***@***.***>, idaholab/moose ***@***.***>\nCc: Author ***@***.***>\nSubject: Re: [idaholab/moose] Weird MPI errors with distrubuted mesh (#18603)\nIt appears that something got mucked up in the mpi handling of distributed meshes. I have to rebuild moose\u2026.\n-Olle\n\nOlle Heinonen\nSr. Materials Scientist\nArgonne National Laboratory\n***@***.***\n\n\nFrom: Guillaume Giudicelli ***@***.***>\nDate: Tuesday, August 24, 2021 at 12:26 PM\nTo: idaholab/moose ***@***.***>\nCc: Heinonen, Olle G. ***@***.***>, Author ***@***.***>\nSubject: Re: [idaholab/moose] Weird MPI errors with distrubuted mesh (#18603)\n\nAny news of this? Has the problem been identified?\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<#18603 (comment)>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AEKZEF2YVEUI4A57DOB4QRLT6PI3PANCNFSM5CD35ZFA>.\nTriage notifications on the go with GitHub Mobile for iOS<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android<https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>.",
                  "url": "https://github.com/idaholab/moose/discussions/18603#discussioncomment-1233214",
                  "updatedAt": "2022-08-21T07:06:33Z",
                  "publishedAt": "2021-08-25T13:27:10Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "This seems like the mpi detected by the test suite and the mpi used to compile dont match",
                          "url": "https://github.com/idaholab/moose/discussions/18603#discussioncomment-1234049",
                          "updatedAt": "2022-08-21T07:06:33Z",
                          "publishedAt": "2021-08-25T15:59:40Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "heinono1"
                  },
                  "bodyText": "OK, I had to purge the Lmod StdEnv and the tests seem to pass (at least they are still running without crashing).\n\nI guess one thing I am curious about is performance comparisons between the gfortran and gcc (and their mpi wrappers) and the Intel compilers and wrappers.\n\u2026\n-Olle\n\nOlle Heinonen\nSr. Materials Scientist\nArgonne National Laboratory\n***@***.***\n\n\nFrom: Heinonen, Olle G. ***@***.***>\nDate: Wednesday, August 25, 2021 at 8:26 AM\nTo: idaholab/moose ***@***.***>, idaholab/moose ***@***.***>\nCc: Author ***@***.***>\nSubject: Re: [idaholab/moose] Weird MPI errors with distrubuted mesh (#18603)\nThe main culprit seems to be that MPII_F_TRUE is undefined. Where is that supposed to be defined an what is the symbol for?\n-Olle\n\nOlle Heinonen\nSr. Materials Scientist\nArgonne National Laboratory\n***@***.***\n\n\nFrom: Heinonen, Olle G. ***@***.***>\nDate: Wednesday, August 25, 2021 at 8:25 AM\nTo: idaholab/moose ***@***.***>, idaholab/moose ***@***.***>\nCc: Author ***@***.***>\nSubject: Re: [idaholab/moose] Weird MPI errors with distrubuted mesh (#18603)\nHi,\n\nAfter battling conflicting versions of anaconda and miniconda, I did get the conda environment cleaned up and proceeded with the minconda3 install of moose on our large cluster (Ubuntu).  The build went without problem but the tests crash. A sample below. All crash errors (and pretty much everything crashes) seem to be related to problems in the actual codes or input files used for testing. What is going on???\n\n\n(moose) ***@***.*** test]$ ./run_tests -j 4\n\noutputs/vtk.solution/diff_parallel_mesh ................................. [#6149, MESH_MODE!=DISTRIBUTED] SKIP\n\noutputs/png.image_tests/wedge ............................................................ [LIBPNG!=TRUE] SKIP\n\noutputs/png.image_tests/square_domain .................................................... [LIBPNG!=TRUE] SKIP\n\noutputs/png.image_tests/adv_diff_reaction ................................................ [LIBPNG!=TRUE] SKIP\n\noutputs/nemesis.nemesis_scalar_distributed ..................................... [MESH_MODE!=DISTRIBUTED] SKIP\n\noutputs/nemesis.nemesis_elemental_distributed .................................. [MESH_MODE!=DISTRIBUTED] SKIP\n\noutputs/format.tecplot_bin_test_override ............................................... [TECPLOT!=FALSE] SKIP\n\ncontrols/error.tid_warehouse_error ........................................................ [METHOD!=DBG] SKIP\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: Working Directory: /gpfs/fs1/home/heinonen/projects/moose/test/tests/time_integrators/explicit_ssp_runge_kutta\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: Running command: /gpfs/fs1/home/heinonen/projects/moose/test/moose_test-opt -i explicit_ssp_runge_kutta.i --error --error-unused --error-override --no-gdb-backtrace\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: /gpfs/fs1/home/heinonen/projects/moose/test/moose_test-opt: symbol lookup error: /home/heinonen/miniconda3/envs/moose/lib/./libmpifort.so.12: undefined symbol: MPII_F_TRUE\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: /gpfs/fs1/home/heinonen/projects/moose/test/moose_test-opt: symbol lookup error: /home/heinonen/miniconda3/envs/moose/lib/./libmpifort.so.12: undefined symbol: MPII_F_TRUE\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order:\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order:\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: Exit Code: 127\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: ################################################################################\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order: Tester failed, reason: CRASH\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order:\n\ntime_integrators/explicit_ssp_runge_kutta.all/first_order ..................................... FAILED (CRASH)\n\n\nOlle Heinonen\nSr. Materials Scientist\nArgonne National Laboratory\n***@***.***\n\n\nFrom: Heinonen, Olle G. ***@***.***>\nDate: Tuesday, August 24, 2021 at 12:28 PM\nTo: idaholab/moose ***@***.***>, idaholab/moose ***@***.***>\nCc: Author ***@***.***>\nSubject: Re: [idaholab/moose] Weird MPI errors with distrubuted mesh (#18603)\nIt appears that something got mucked up in the mpi handling of distributed meshes. I have to rebuild moose\u2026.\n-Olle\n\nOlle Heinonen\nSr. Materials Scientist\nArgonne National Laboratory\n***@***.***\n\n\nFrom: Guillaume Giudicelli ***@***.***>\nDate: Tuesday, August 24, 2021 at 12:26 PM\nTo: idaholab/moose ***@***.***>\nCc: Heinonen, Olle G. ***@***.***>, Author ***@***.***>\nSubject: Re: [idaholab/moose] Weird MPI errors with distrubuted mesh (#18603)\n\nAny news of this? Has the problem been identified?\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<#18603 (comment)>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AEKZEF2YVEUI4A57DOB4QRLT6PI3PANCNFSM5CD35ZFA>.\nTriage notifications on the go with GitHub Mobile for iOS<https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675> or Android<https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>.",
                  "url": "https://github.com/idaholab/moose/discussions/18603#discussioncomment-1233542",
                  "updatedAt": "2022-08-21T07:06:33Z",
                  "publishedAt": "2021-08-25T14:22:20Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "It should be fairly similar. In my experience, threading is harder to get good performance with intel compilers, while they are slightly faster in serial.",
                          "url": "https://github.com/idaholab/moose/discussions/18603#discussioncomment-1234052",
                          "updatedAt": "2022-08-21T07:06:33Z",
                          "publishedAt": "2021-08-25T16:00:41Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Some question about the possibility of the combination of grain_growth and  mechanical modules",
          "author": {
            "login": "zengfy-hust"
          },
          "bodyText": "Hello Moose workers:\nI recent use the dream3d to as the  input file for grain growth ,the reasults seemed well . however ,I encountered some problem which may seem stupid ,hop you can help me with it :\nFirst ,I want to use python scrpit to deal with the output file of moose in the format of .csv  .to calculate the grain diameter of each grain ,I set the output options as followed :\noutputs.txt\nhowever ,the grain volumes stays unchangeded in the outputs .could you help me with this ?\nsecond ,I wander if grain_growth can combined with mecahnical ,which would be better fit for my paper .was is possible now ?\nps . In grain_growth.i the euler angle was read but i wander what use was it for in the phase field calculation? I searched the introduction file of moose ,but nothing was found about it .can you specified the use of it  ?\nthank you so much",
          "url": "https://github.com/idaholab/moose/discussions/18171",
          "updatedAt": "2022-09-19T11:18:20Z",
          "publishedAt": "2021-06-25T10:44:30Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "zengfy-hust"
                  },
                  "bodyText": "test.zip\nFYI\uff0cI provide the csv file of my simulation ,but the grain volume did not change at all ,I just wonder why .",
                  "url": "https://github.com/idaholab/moose/discussions/18171#discussioncomment-920669",
                  "updatedAt": "2022-10-20T01:37:30Z",
                  "publishedAt": "2021-06-25T12:54:55Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@dschwen @laagesen",
                          "url": "https://github.com/idaholab/moose/discussions/18171#discussioncomment-1228882",
                          "updatedAt": "2022-10-20T01:37:50Z",
                          "publishedAt": "2021-08-24T17:53:08Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "amjokisaari"
                          },
                          "bodyText": "Please provide individual files instead of a .zip file.  We (edit) should not open it for computer security reasons.  Please provide a full copy of your input file.\nGrain growth can be combined with mechanics.  The elastic energy driving force can be incorporated into the Allen-Cahn equation.  Depending on your exact specific needs, the implementation may already exist in the phase field module.\nIn grain_growth.i, the Euler angles are read to provide each grain with a crystallographic rotation with respect to the reference frame of the simulation.  This allows the elastic stiffness tensor and any eigenstrains or other directional properties to be rotated along with the grain.",
                          "url": "https://github.com/idaholab/moose/discussions/18171#discussioncomment-1230188",
                          "updatedAt": "2022-10-20T01:37:50Z",
                          "publishedAt": "2021-08-24T23:18:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dschwen"
                          },
                          "bodyText": "We need to see your input as Andrea said (by the way I can open the zip just fine). Take a look at FeatureVolumeVectorPostprocessor. That can give you locations and volumes of grains.",
                          "url": "https://github.com/idaholab/moose/discussions/18171#discussioncomment-1230242",
                          "updatedAt": "2022-10-20T01:37:52Z",
                          "publishedAt": "2021-08-24T23:25:33Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "amjokisaari"
                          },
                          "bodyText": "Risky click, @dschwen.  No way am I opening some random zip file on the internet!",
                          "url": "https://github.com/idaholab/moose/discussions/18171#discussioncomment-1230267",
                          "updatedAt": "2022-10-20T01:37:52Z",
                          "publishedAt": "2021-08-24T23:35:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "zengfy-hust"
                          },
                          "bodyText": "Thank you for your help .I successfully obtained the grain volume . @dschwen ,by he way ,I want to know your timetable about fixing the module moose/modules/combined/examples/phase_field-mechanics/EBSD_reconstruction_grain_growth_mech.i ,which is exactly  the module I want to use .",
                          "url": "https://github.com/idaholab/moose/discussions/18171#discussioncomment-1231809",
                          "updatedAt": "2022-10-20T01:37:53Z",
                          "publishedAt": "2021-08-25T08:45:07Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "poly_grain_growth_2D_eldrforce can not use the data from EBSD(TSL)",
          "author": {
            "login": "zengfy-hust"
          },
          "bodyText": "Hello Moose:\nI recently made a  test using on the basis of poly_grain_growth_2D_eldrforce.i ,trying to use it to explain my experiment data .\nThe abnormal grain growth in aluminium alloy .the method I used was shown as following :\nFirst ,I exported grain file from EBSD(TSL) data and defined the grain number and Euler angle \uff08in radians).\nsecond ,I use Excel to change the data into MOOSE format and renamed it as GrainFile.tex  .\nthird\uff0c I change the input file in poly_grain_growth_2D_eldrforce to GrainFile.tex and reload the module\nhowever ,the module Stopped due to the error\n*** ERROR ***\nThe following error occurred in the object \"voronoi\", of type \"PolycrystalVoronoi\".\nCoupled polycrystal variables differ in periodicity\n\nI dou't know how to fix it ,Any suggestion will be welcome .\nThank you .\nin addition ,can ICs of poly_grain_growth_2D_eldrforce change to Reading EBSD data ? can you tell me any module can help explain Abnormal grain growth apart from this module ?\nGrainFile.tex.txt\npoly_grain_growth_2D_eldrforce.i.txt",
          "url": "https://github.com/idaholab/moose/discussions/18615",
          "updatedAt": "2022-09-19T11:18:14Z",
          "publishedAt": "2021-08-17T09:11:32Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "zengfy-hust"
                  },
                  "bodyText": "@dschwen",
                  "url": "https://github.com/idaholab/moose/discussions/18615#discussioncomment-1204810",
                  "updatedAt": "2022-09-19T11:19:18Z",
                  "publishedAt": "2021-08-19T07:50:59Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@laagesen if one of you has time",
                          "url": "https://github.com/idaholab/moose/discussions/18615#discussioncomment-1228889",
                          "updatedAt": "2022-09-19T11:19:18Z",
                          "publishedAt": "2021-08-24T17:54:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dschwen"
                          },
                          "bodyText": "All grain order parameters will have to have periodic BCs applied or none. You seem to have a mix (some periodic, some not).",
                          "url": "https://github.com/idaholab/moose/discussions/18615#discussioncomment-1230249",
                          "updatedAt": "2022-09-19T11:19:18Z",
                          "publishedAt": "2021-08-24T23:27:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "zengfy-hust"
                          },
                          "bodyText": "Could you be more specific  about the problem? In fact ,I change the BCs several time ,but the ERROR message kept unchanged .\n*** ERROR ***\nThe following error occurred in the object \"voronoi\", of type \"PolycrystalVoronoi\".\nCoupled polycrystal variables differ in periodicity\n\nHow could I  change the voroni or BCs to make them correspond to each other ? Thank you so much!@dschwen",
                          "url": "https://github.com/idaholab/moose/discussions/18615#discussioncomment-1231492",
                          "updatedAt": "2022-09-19T11:19:19Z",
                          "publishedAt": "2021-08-25T07:32:34Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Linearize displaced mesh with MultiApp applications",
          "author": {
            "login": "rtaylo45"
          },
          "bodyText": "Hello,\nI have a situation where im using MultiApps to transfer variables from an app that uses a displaced mesh to model a flow loop to an app i am developing which looks to linearize the mesh and use periodic boundary conditions to simulate the \"loop\" part. Bellow i have a simple figure which only show part of the loop, but visualizes the issue. the black circles represent the mesh nodes.\n\nI am having trouble trying to figure out how to transfer variables from the main app to the sub app. I assume MultiAppNearestNodeTransfer finds the nearest node by x, y, z coordinates and im not sure how to do this with positions in MultiAppMeshFunctionTransfer. In the figure each app has the same number of mesh nodes however, in my case i would like to use more nodes in my sub app. As i mentioned before, the main app uses a displaced mesh, but my sub app would not. I would also like the flexibility to have my app be the main app and the other app to be the sub app. Can anyone give me some guidance in solving this problem?\nThanks,\nZack",
          "url": "https://github.com/idaholab/moose/discussions/18416",
          "updatedAt": "2022-07-14T04:07:19Z",
          "publishedAt": "2021-07-23T17:13:32Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hi Zack\nIs this rotation the only transformation you care about or is there going to be arbitrary transforms in space?\nDo the initial meshes match exactly at the beginning of the simulation ?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1043258",
                  "updatedAt": "2022-07-14T04:09:30Z",
                  "publishedAt": "2021-07-23T17:33:38Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "rtaylo45"
                          },
                          "bodyText": "In the picture I am only showing the elbow example but in reality the main apps mesh is a loop. So it has another elbow which completes the loop.   So to answer your questions it would be arbitrary transforms in space. I would need to conserve the total length of the loop.  They utilize a displaced mesh and connect the variables within each sub mesh using boundary conditions and constraints, to simulate a loop.\nBoth meshes would be static and would not match exactly at the beginning of the simulations. The main app would have mesh coordinates which visually represent a loop. The submesh would only have a single dimension, with periodic BCs at each end.\nOn other note is that the main apps mesh condaints sub meshes which could include solid areas for wall conduction and fluid areas for flow. My subapp is only modeling physics which are in the fluid domain.\nZack",
                          "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1043316",
                          "updatedAt": "2022-07-14T04:09:31Z",
                          "publishedAt": "2021-07-23T17:56:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "This all seems difficult for mesh to mesh transfer. Like you would need to code your own transfer.\nAn approximate transfer could use vectorpostprocessors, which are easier to arbitrarily distribute in space then match one by one. I ll let others comment on potential solutions.\nIf you could make the subapp mesh match instead, it would be much easier. Are you doing this 1D periodic subapp mesh to lower computational cost?",
                          "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1043800",
                          "updatedAt": "2022-07-14T04:09:31Z",
                          "publishedAt": "2021-07-23T20:41:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "rtaylo45"
                          },
                          "bodyText": "Ok. If the main app uses a displaced mesh, how would i go about generating their mesh for my sub app to use? I tried using --mesh-only but it seems the the displaced information is not being saved in the in.e file. Because the mesh is displaced could i used Dirichlet boundary conditions on the nonlinear variables in my sub app to \"connect\" the mesh displacements?",
                          "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1043893",
                          "updatedAt": "2022-08-18T01:15:42Z",
                          "publishedAt": "2021-07-23T21:20:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "If you can write the mesh in a sub app as the transform of the mesh in the main app, you could use a modified https://mooseframework.inl.gov/source/meshgenerators/TransformGenerator.html , which implements the transform of your choice. Then you could do the transfer on the same mesh.\nI don't know about the displaced mesh way. I asked the rest of the team for input.",
                          "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1059637",
                          "updatedAt": "2022-08-27T08:51:29Z",
                          "publishedAt": "2021-07-27T01:27:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Did you set use_displaced in the exodus output block?\nFor the transfer problem, I've reached out and the recommendation is to a NearestPointLayeredAverage uo with a MultiAppUserObjectTransfer.\nhttps://mooseframework.inl.gov/source/userobject/NearestPointLayeredAverage.html\nThis will allow to match points between the two different meshes",
                          "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1063407",
                          "updatedAt": "2022-08-27T08:51:29Z",
                          "publishedAt": "2021-07-27T14:53:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "rtaylo45"
                          },
                          "bodyText": "I did set use_displaced in the exodus block and when i run the input file with --mesh-only and view the in.e file, the mesh is not the same. I attached the generated mesh using --mesh-only and the main apps solution if i were to run the input file.\nfiles.zip",
                          "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1083104",
                          "updatedAt": "2022-08-27T08:51:28Z",
                          "publishedAt": "2021-07-28T15:39:40Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "Are you doing this just to get the periodic boundary condition? If so, it might be better to work on creating the correct Periodic Boundary condition. If this is the case, we should be able to connect the boundaries in the elbow.\nIf you indeed need to do the transfer, based on your figure it is a one-to-one transfer, so the MultiAppCopyTransfer would just work. But, I am also unsure of how to handle the displacements correctly. @fdkong do you know how to transfer displacements? Is it possible to create a Transfer, using the MultiAppFieldTransfer with the displaced problem supplied to the transfer method to get the desired result.",
                  "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1063431",
                  "updatedAt": "2022-08-18T01:15:42Z",
                  "publishedAt": "2021-07-27T14:56:07Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "rtaylo45"
                          },
                          "bodyText": "@aeslaughter @GiudGiud So I kinda look at solving this problem in two ways.\n\nUse the main apps displaced mesh with one-to-one transfer so that all of the nodal variables line up so that i can do variable transfer easy. I could possibly use already developed moose boundary conditions such as ADMatchedValueBC to \"connect\" the variables on the displaced mesh boundaries?\nGenerate a mesh that is used for my subapp that is different than the main apps mesh. Then i would need to make sure that i can properly transfer the variables from the main apps mesh to my subapp.\n\nI was suggesting using periodic boundary conditions at the bottom and top boundaries on the linear subapps mesh to simulate the loop effect of the main apps problem. Im still new to moose, so im not sure of the best way to solve this problem and not sure what transfer tools i can use to do this. In any case, im not sure which method, either 1 or 2, would be the best way to solve this problem. If i go with option 1, would i need to generate a mesh file for my subapp to read? or could I simply just use MultiApps to transfer the main apps mesh to the sub app and could I do either of those with a displaced mesh? If i go option 2, i feel like it would be a lot of work to transfer the variables from the main apps mesh onto the subsapps generated mesh. I would probably need to write my own transfer class? It looks like i might be able to do what @GiudGiud said and use NearestPointLayeredAverage to transfer the variables from the main app to the subapp when they have different meshes. Is this true?",
                          "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1083170",
                          "updatedAt": "2022-08-18T01:15:42Z",
                          "publishedAt": "2021-07-28T15:56:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "OK, there are two questions here (not one question)\n\nPeriodic boundary conditions in subapp.  We already have this capability. You could use something like this\n\n\n[BCs]\n  [./Periodic]\n    [./all]\n      variable = u \n      auto_direction = 'x'\n    [../]\n  [../]\n[]\n\n\nTransfer between main-app and subapp. You might try to make the mesh nodes match up first and then use MultiAppCopyTransfer. Once this works, we can then consider other transfer options. There are plenty of transfers.",
                          "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1084725",
                          "updatedAt": "2022-08-18T01:15:42Z",
                          "publishedAt": "2021-07-28T23:25:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "rtaylo45"
                          },
                          "bodyText": "Yes i am aware of periodic boundary conditions in moose/subapps. My question is not about using periodic boundary conditions. Part of my question is how to transfer the displaced mesh from the main app to the sub app. I tried using the --mesh-only flag when running the executable but it does not seem to properly create a mesh that i can use. I thing this is because the main app is using a displaced mesh.",
                          "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1085080",
                          "updatedAt": "2023-01-04T17:16:21Z",
                          "publishedAt": "2021-07-29T02:24:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "mesh only cuts the simulation very early, possibly before the transfers occur. I ll ask for transferring a displaced mesh",
                          "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1122220",
                          "updatedAt": "2023-01-04T17:16:21Z",
                          "publishedAt": "2021-08-02T21:15:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Ok could you try using the clone_master_mesh parameter of the multiapp?\nThis should allow you to use the displaced mesh in the subapp",
                          "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1122232",
                          "updatedAt": "2023-01-04T17:16:21Z",
                          "publishedAt": "2021-08-02T21:17:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "rtaylo45"
                          },
                          "bodyText": "So this is how the the multiapp block is set up in my main app.\n[MultiApps]\n  [mole]\n    type = FullSolveMultiApp\n    app_type = moleApp\n    input_files = 'mole_sub.i'\n    execute_on = 'FINAL'\n    use_displaced_mesh = true\n    clone_master_mesh = true\n  []\n[]\n\nAs it still is not transfer the displaced mesh. This is the example output.",
                          "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1131992",
                          "updatedAt": "2023-01-04T17:16:21Z",
                          "publishedAt": "2021-08-04T20:59:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@fdkong do you know about displaced mesh transfer?",
                          "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1164351",
                          "updatedAt": "2023-01-04T17:16:22Z",
                          "publishedAt": "2021-08-12T15:45:22Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "travismui"
                  },
                  "bodyText": "@rtaylo45 In case someone else from the SAM team hasn't dropped you a message, we've added an --output_displaced_mesh runtime flag in SAM to output the displaced mesh and quit, similar to the expected behavior from --mesh-only. This update has been pushed to SAM's devel branch so hopefully this helps with this specific issue, let us know if you run into any problems.",
                  "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1160770",
                  "updatedAt": "2023-01-04T17:16:22Z",
                  "publishedAt": "2021-08-11T20:56:57Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hi Travis, this seems like something that would be helpful for every application all across the framework if you could consider pushing it upstream?",
                          "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1164358",
                          "updatedAt": "2023-01-04T17:16:22Z",
                          "publishedAt": "2021-08-12T15:46:37Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "travismui"
                          },
                          "bodyText": "Hi Guillaume\u2013 I did not work on this update in SAM, but from what I can tell our modification was to add an action (within SAM) that points to the displaced mesh for the mesh output function. Taking a look at the framework, I believe the only real difference lies at this line in the MeshOnlyAction, for which our action points to  _action_warehouse.displacedMesh() rather than   _action_warehouse.mesh().",
                          "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1164581",
                          "updatedAt": "2023-01-04T17:16:22Z",
                          "publishedAt": "2021-08-12T16:28:49Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@rtaylo45 none of what we proposed worked out right?\n@fdkong is going be working on a coordinate mapping system in FY22 so that we can map main-app coordinates to any sub app coordinates. This should solve your case and @vincentlaboure.\nThe relevant issue in MOOSE is #12293",
                  "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1228830",
                  "updatedAt": "2023-01-04T17:16:22Z",
                  "publishedAt": "2021-08-24T17:42:49Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "rtaylo45"
                          },
                          "bodyText": "No, none of it did. But we are going a different route to solving the problem. We are going to compile the two codes together, this ended up making the most sense because of some other SAM related reasons. When the coordinate mapping system becomes available, it would be nice to go back and revisit this problem. But for now we are going to compile the codes together.  Thanks for your help on this issue.",
                          "url": "https://github.com/idaholab/moose/discussions/18416#discussioncomment-1228867",
                          "updatedAt": "2023-01-04T17:16:22Z",
                          "publishedAt": "2021-08-24T17:49:24Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "AuxKernel local element node numbering _i",
          "author": {
            "login": "KhaledNabilSharafeldin"
          },
          "bodyText": "Hello,\nI am trying to get the equivalent of _i  index as in kernels but for AuxKernel. _current_node->id() return the global id not the current node id within the current element.\nI am retrieving a variable using coupledDofValues so I need the index of the current local node id for it. Does using coupledValue return the current node for the current element and remove the need for _i indexing?\nall variables are nodal.\nThanks in advance!",
          "url": "https://github.com/idaholab/moose/discussions/18692",
          "updatedAt": "2022-06-21T20:31:25Z",
          "publishedAt": "2021-08-24T14:23:43Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "Can you explain a little more what you are trying to do here? Is your AuxKernel operating on a nodal or an elemental variable?",
                  "url": "https://github.com/idaholab/moose/discussions/18692#discussioncomment-1228250",
                  "updatedAt": "2022-06-21T20:31:25Z",
                  "publishedAt": "2021-08-24T15:54:58Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "KhaledNabilSharafeldin"
                          },
                          "bodyText": "I have nodal AuxVariable that I am operating on, in computeValue() i'm returning _u[qp] + (*_rho[_component])[_qp] the way I am initializing that variable _rho[_component] = &coupledValue(\"rho\", _component)\nboth the variable i am operating on and rho are nodal variables, rho is a nonlinearvariable solved using a kernel.\nI am making sure that this kind of updating is done ON the nodes correctly, that both _u and rho refer to the same node.\nHope that this clears it up",
                          "url": "https://github.com/idaholab/moose/discussions/18692#discussioncomment-1228359",
                          "updatedAt": "2022-06-21T20:31:36Z",
                          "publishedAt": "2021-08-24T16:15:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "loganharbour"
                          },
                          "bodyText": "If the AuxKernel is nodal (meaning its variable = is nodal), all of the structures given with coupledValue()[0] and _u[_qp == 0] will be the nodal value. That is, the system is smart enough to know that if you've got a nodal variable, all of the coupling will also be performed in a nodal sense. The physical location of _u[0] and coupledValue('some_var')[0] should be _q_point[0] == <the node>\nWith this, I don't believe you need coupleDofValues() at all. coupledValue() will give you the result that you want.",
                          "url": "https://github.com/idaholab/moose/discussions/18692#discussioncomment-1228515",
                          "updatedAt": "2022-06-21T20:31:38Z",
                          "publishedAt": "2021-08-24T16:47:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "rwcarlsen"
                          },
                          "bodyText": "@loganharbour is correct.  Nodal aux kernels set _qp to zero and the coupledValue interface automatically handles the nodal vs elemental aspect of objects.  If you just use coupledValue moose should just do the right thing.",
                          "url": "https://github.com/idaholab/moose/discussions/18692#discussioncomment-1228600",
                          "updatedAt": "2022-06-21T20:31:37Z",
                          "publishedAt": "2021-08-24T17:03:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "KhaledNabilSharafeldin"
                          },
                          "bodyText": "Thanks @loganharbour  and @rwcarlsen  for clarification, but generally speaking, if I wanted to use coupleDofValues() is there a simple way to index it to the current node? just curious.\nI marked it as solved since the responses solved my problem.",
                          "url": "https://github.com/idaholab/moose/discussions/18692#discussioncomment-1228680",
                          "updatedAt": "2022-08-29T20:05:52Z",
                          "publishedAt": "2021-08-24T17:17:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "loganharbour"
                          },
                          "bodyText": "All kernels (whether it be a Kernel or an AuxKernel) have their evaluation loop defined based on the variable type. That is:\n\nIf nodal, execute computeValue() once for each node, set _qp = 0 and fill all data structures (variable values, points, gradients, etc) for just _qp = 0\nIf elemental, fill data structures with data for each quadrature point, and evaluate computeValue() for each quadrature point, setting _qp appropriately.\n\nBecause of how we set things up, the \"current node\" in the case of a nodal kernel will always have a local index of 0. That is, you should see coupledValue('var')[0] == coupledDofValues('var')[0] and coupledDofValues('var').size() == 1. Therefore, there shouldn't really be a reason to use the dof values in this context.",
                          "url": "https://github.com/idaholab/moose/discussions/18692#discussioncomment-1228800",
                          "updatedAt": "2022-08-29T20:05:54Z",
                          "publishedAt": "2021-08-24T17:33:30Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "EXODIFF errors in MOOSE on an HPC cluster",
          "author": {
            "login": "adfboyd"
          },
          "bodyText": "Hello,\nI was advised to start a new discussion on the remaining errors I am experiencing in testing MOOSE on a cluster. The final test results from ./run_tests have been attached. Any input on how to fix these would be much appreciated. (the previous discussion on installation and testing can be found here: #17946)\nfinaltestresults.txt\nThank you!",
          "url": "https://github.com/idaholab/moose/discussions/18094",
          "updatedAt": "2022-08-23T15:11:31Z",
          "publishedAt": "2021-06-16T11:09:10Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "milljm"
                  },
                  "bodyText": "Sorry, I don't think I was clear; The developers will want the error before the 'Final Test Results'. The full error listed before the final results contains the reason why exodiff failed.",
                  "url": "https://github.com/idaholab/moose/discussions/18094#discussioncomment-878169",
                  "updatedAt": "2022-08-23T15:11:39Z",
                  "publishedAt": "2021-06-16T13:59:09Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "adfboyd"
                          },
                          "bodyText": "Apologies - the whole thing is here:\nslurm-331094.txt",
                          "url": "https://github.com/idaholab/moose/discussions/18094#discussioncomment-878401",
                          "updatedAt": "2022-08-23T15:11:40Z",
                          "publishedAt": "2021-06-16T14:25:41Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "lindsayad"
                  },
                  "bodyText": "Most of the errors are due to missing python packages. Definitely matplotlib is missing. Is numpy available?\nOne exodiff is trivial (0 vs. 1e-17 ... I don't even know why that would have registered because we have a default floor of 1e-10) so I wouldn't worry about that. The contact-related exodiffs are interesting...basically the same result but different sign. I'm going to have to look into that a bit more to understand how that could be.\nThen there are a couple of out-of-memory errors due to extensive mesh refinement I think.\nOverall I wouldn't be too worried about these errors.",
                  "url": "https://github.com/idaholab/moose/discussions/18094#discussioncomment-878807",
                  "updatedAt": "2022-08-23T15:11:46Z",
                  "publishedAt": "2021-06-16T15:04:33Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "Actually this warning looks a little scarier:\nWarning: unable to write derivative cache file.",
                          "url": "https://github.com/idaholab/moose/discussions/18094#discussioncomment-878814",
                          "updatedAt": "2022-08-23T15:11:43Z",
                          "publishedAt": "2021-06-16T15:06:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "@dschwen any thoughts on the above warning? Looks to be out of fparser_ad.cc",
                          "url": "https://github.com/idaholab/moose/discussions/18094#discussioncomment-878828",
                          "updatedAt": "2022-08-23T15:11:44Z",
                          "publishedAt": "2021-06-16T15:07:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "@adfboyd I think it would actually be helpful for me if you could send the file moose/test/tests/geomsearch/3d_moving_penetration/pl_test4q_out.e",
                          "url": "https://github.com/idaholab/moose/discussions/18094#discussioncomment-878953",
                          "updatedAt": "2022-08-23T15:11:44Z",
                          "publishedAt": "2021-06-16T15:29:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@lindsayad I ll add the python package dependencies to the tests that are not checking\nAlso there s an error showing cli_args mispelled. I ll look at that",
                          "url": "https://github.com/idaholab/moose/discussions/18094#discussioncomment-879428",
                          "updatedAt": "2022-09-15T20:46:43Z",
                          "publishedAt": "2021-06-16T17:09:54Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "A few of the test failures are due to a typo in the diffs to analyze_jacobian.py.\nIt should be cli_args instead of cli_ags.\nThere s other failures with that test scripts, not exodiffs though",
                          "url": "https://github.com/idaholab/moose/discussions/18094#discussioncomment-880602",
                          "updatedAt": "2022-09-15T20:46:43Z",
                          "publishedAt": "2021-06-16T22:28:08Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "LabrosV"
                          },
                          "bodyText": "Actually this warning looks a little scarier:\nWarning: unable to write derivative cache file.\n\n\nI'm getting the same warning when running my app on an HPC cluster.\nConvergence and results look fine. Any way to fix it?",
                          "url": "https://github.com/idaholab/moose/discussions/18094#discussioncomment-1228119",
                          "updatedAt": "2022-09-15T20:46:46Z",
                          "publishedAt": "2021-08-24T15:31:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "This should not influence the results according to @dschwen in #16464",
                          "url": "https://github.com/idaholab/moose/discussions/18094#discussioncomment-1228182",
                          "updatedAt": "2022-09-15T20:46:47Z",
                          "publishedAt": "2021-08-24T15:46:11Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "adfboyd"
                  },
                  "bodyText": "You can find the file here: https://drive.google.com/file/d/1GI8hdW2Zt3ZtjFnSjkk6UOecYkDMIY65/view?usp=sharing .\nPlease let me know if there's a better way of sharing this type of thing, it's not supported on Github.",
                  "url": "https://github.com/idaholab/moose/discussions/18094#discussioncomment-879286",
                  "updatedAt": "2022-08-23T15:11:47Z",
                  "publishedAt": "2021-06-16T16:35:31Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "lindsayad"
                          },
                          "bodyText": "I don't know what to think. The normal should always point out of the primary domain but it's pointing into the domain on your system for that single time step. Moreover, distance should be positive if the faces are penetrated (which they are here) and it's negative. That's a direct result of the wrong normal computation though. @friedmud is the original architect of the FindContactPoint code. Maybe he has some thoughts but I don't see anything in the code that would make me think the result should ever be wrong just because you're on a different architecture. For @friedmud or others, the TLDR is we have this:\n\nand we are getting the exodiff\ngeomsearch/3d_moving_penetration.pl_test4q: Nodal variables:\ngeomsearch/3d_moving_penetration.pl_test4q:    distance             rel diff:  4.2426407e-01 ~ -4.2426407e-01 = 2.00000e+00 (node 37)\ngeomsearch/3d_moving_penetration.pl_test4q:    normal_x             rel diff: -7.0710678e-01 ~  7.0710678e-01 = 2.00000e+00 (node 37)\ngeomsearch/3d_moving_penetration.pl_test4q:    normal_y             rel diff: -7.0710678e-01 ~  7.0710678e-01 = 2.00000e+00 (node 37)\n\nHere's with the primary block volume so that you know which way the normal should point (it's correct in the gold file, file 1)",
                          "url": "https://github.com/idaholab/moose/discussions/18094#discussioncomment-879780",
                          "updatedAt": "2022-08-23T15:11:46Z",
                          "publishedAt": "2021-06-16T18:08:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@friedmud any thoughts on this?",
                          "url": "https://github.com/idaholab/moose/discussions/18094#discussioncomment-977131",
                          "updatedAt": "2022-08-23T15:12:55Z",
                          "publishedAt": "2021-07-08T00:46:39Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}