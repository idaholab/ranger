{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyNC0wNC0xOVQxMTowMzoxMC0wNjowMM4AY5Se"
    },
    "edges": [
      {
        "node": {
          "title": "Issue with petsc finding MPI",
          "author": {
            "login": "TheGreatCid"
          },
          "bodyText": "Hi all,\nI am installing MOOSE on an HPC.\nIn my environment I have\n\nOpenMPI 4.1\nGCC 11.3.0\npython 3.11.5\ncmake 3.25.2\n\nI am receiving the following error when running ./update_and_rebuild_petch.sh\nUnable to find mpi in default locations!\n\nThis is odd since, which mpicc returns a valid directory.\nI try to manually specify the directory with --with-mpi-dir but that returns the same error. The configure.log file for petsc does not shed anymore light than the output I posted.\nI have a valid ./moose_profile with all of the options specified in the HPC tutorial. Using mpich instead of openMPI is not an option for me. At least the cluster I am using does not have any mpich modules.\nAny insight would be much appreciated!",
          "url": "https://github.com/idaholab/moose/discussions/26801",
          "updatedAt": "2024-04-22T12:55:10Z",
          "publishedAt": "2024-02-13T15:12:32Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "milljm"
                  },
                  "bodyText": "I am not sure where we mention a specific MPI wrapper on our HPC instructions: https://mooseframework.inl.gov/getting_started/installation/hpc_install_moose.html\nGiven that you go to mention it, can you post the link to the instructions you followed (I am asking because those instructions may be wrong, and I'd like to fix them!)\nLooks like you meet all the version requirements. Can you post the results of running the diagnostic script?\ncd moose/scripts\n./diagnostics.sh\nAlso, the above diagnostic script does not print out each compiler variable (I really need to add that). So can you also provide what the following returns as well?\necho $CC $CXX $FC $F90 $F77\nwhich $CC $CXX $FC $F90 $F77\nwhich $($CC -show | cut -d\\  -f1)\nwhich $($CXX -show | cut -d\\  -f1)\nwhich $($FC -show | cut -d\\  -f1)\nwhich $($F90 -show | cut -d\\  -f1)\nwhich $($F77 -show | cut -d\\  -f1)\nThanks!",
                  "url": "https://github.com/idaholab/moose/discussions/26801#discussioncomment-8455477",
                  "updatedAt": "2024-02-13T15:38:54Z",
                  "publishedAt": "2024-02-13T15:38:53Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "TheGreatCid"
                          },
                          "bodyText": "Thanks for your reply!\nI've heard that mpich is more stable than openMPI (however I could be wrong there). I did follow the instructions you posted\nHere is the output from the diagnostics.sh\nTue Feb 13 08:47:24 MST 2024\n\nSystem Arch: LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch Distributor ID: RedHatEnterprise Description: Red Hat Enterprise Linux release 8.8 (Ootpa) Release: 8.8 Codename: Ootpa\n\nMOOSE Package Version: Custom Build\n\nCPU Count: 112\n\nMemory Free: 159265.184 MB\nmpicc: error while loading shared libraries: libimf.so: cannot open shared object file: No such file or directory\n\nVariable `which $CC` check:\n/opt/toss/openmpi/4.1/intel/bin/mpicc\n\n$CC --version:\n\nmpicc: error while loading shared libraries: libimf.so: cannot open shared object file: No such file or directory\nmpicc: error while loading shared libraries: libimf.so: cannot open shared object file: No such file or directory\nmpicc: error while loading shared libraries: libimf.so: cannot open shared object file: No such file or directory\n./diagnostics.sh: line 40: --version: command not found\n\nMPICC:\nwhich mpicc:\n        /opt/toss/openmpi/4.1/intel/bin/mpicc\nmpicc -show:\n\n\nCOMPILER :\n\n\nPython:\n        /projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/bin/python\n        Python 3.11.5\n\nMODULES:\n\nCurrently Loaded Modules:\n  1) aue/anaconda3/2023.09   2) cmake/3.25.2   3) openmpi-intel/4.1\n\n \n\nPETSC_DIR not set\n\nENVIRONMENT:\n ;;\n ;;\n ;;\n}\n}\n}\n}\nACLOCAL_PATH=/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/share/aclocal\nBASH_ENV=/usr/share/lmod/lmod/init/bash\nBASH_FUNC_ml%%=() {  eval \"$($LMOD_DIR/ml_cmd \"$@\")\"\nBASH_FUNC_module%%=() {  if [ -z \"${LMOD_SH_DBG_ON+x}\" ]; then\nBASH_FUNC_scl%%=() {  if [ \"$1\" = \"load\" -o \"$1\" = \"unload\" ]; then\nBASH_FUNC_which%%=() {  ( alias;\n case \"$-\" in \nCC=mpicc\n_CE_CONDA=\n_CE_M=\nCMAKE_PREFIX_PATH=/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2\nCONDA_EXE=/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/bin/conda\nCONDA_PYTHON_EXE=/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/bin/python\nCONDA_SHLVL=0\nCPATH=/opt/toss/openmpi/4.1/intel/include\nCVS_RSH=ssh\nCXX=mpicxx\nDBUS_SESSION_BUS_ADDRESS=unix:abstract=/tmp/dbus-AM4aCaf358,guid=c87fa47ce5d057361519431065c6699c\nDEFAULT_MODULES_LOADED=1\nDISPLAY=localhost:19.0\n echo \"Shell debugging restarted\" 1>&2;\n echo \"Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for Lmod's output\" 1>&2;\n else\n esac;\n eval \"$($LMOD_CMD shell \"$@\")\" && eval \"$(${LMOD_SETTARG_CMD:-:} -s sh)\";\n eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@\n eval \"module $@\";\nF77=mpif77\nF90=mpif90\nFC=mpif90\n fi\n fi;\n fi;\n fi;\nGDK_BACKEND=x11\nGUESTFISH_INIT=\nGUESTFISH_OUTPUT=\nGUESTFISH_PS1=\\[\\]><fs>\\[\\] \nGUESTFISH_RESTORE=                                                                                                                                                                                                                            \nHISTCONTROL=ignoredups\nHISTSIZE=1000\nHOME=/ascldap/users/detorre\nHOSTNAME=manzano-login2\nhttp_proxy=http://user:pass@proxy.sandia.gov:80\nhttps_proxy=http://user:pass@proxy.sandia.gov:80\n if [ -n \"${__lmod_sh_dbg:-}\" ]; then\n if [ -n \"${__lmod_sh_dbg:-}\" ]; then\nI_MPI_FABRICS=shm:tmi\nI_MPI_FAIR_READ_SPIN_COUNT=10000\nI_MPI_FALLBACK_DEVICE=disable\nI_MPI_PERHOST=allcores\nI_MPI_TMI_PROVIDER=psm2\nIPATH_NO_BACKTRACE=1\nIPATH_NO_CPUAFFINITY=1\nKDEDIRS=/usr\nKRB5CCNAME=FILE:/tmp/krb5cc_135142_nzm6xEjkvN\nLANG=en_US.UTF-8\nLANGUAGE=\nLD_LIBRARY_PATH=/opt/toss/openmpi/4.1/intel/lib:/usr/lib64/nvidia:/projects/netpub/gnu/11.3.0-toss3/lib64:/projects/netpub/gnu/11.3.0-toss3/lib::\nLESSOPEN=||/usr/bin/lesspipe.sh %s\n_LMFILES_=/apps/modules/modulefiles-apps/aue/anaconda3/2023.09:/apps/modules/modulefiles/cmake/3.25.2:/opt/toss/modules/modulefiles/openmpi-intel/4.1\nLMOD_CHECK_FOR_VALID_MODULE_FILES=yes\nLMOD_CMD=/usr/share/lmod/lmod/libexec/lmod\nLMOD_DIR=/usr/share/lmod/lmod/libexec\nLMOD_FAMILY_MPI=openmpi-intel\nLMOD_FAMILY_MPI_VERSION=4.1\n __lmod_my_status=$?;\nLMOD_PKG=/usr/share/lmod/lmod\n__LMOD_REF_COUNT_ACLOCAL_PATH=/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/share/aclocal:1\n__LMOD_REF_COUNT_CMAKE_PREFIX_PATH=/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2:1\n__LMOD_REF_COUNT_CPATH=/opt/toss/openmpi/4.1/intel/include:1\n__LMOD_REF_COUNT_LD_LIBRARY_PATH=/opt/toss/openmpi/4.1/intel/lib:1;/usr/lib64/nvidia:1;/projects/netpub/gnu/11.3.0-toss3/lib64:1;/projects/netpub/gnu/11.3.0-toss3/lib:1;:1\n__LMOD_REF_COUNT_MANPATH=/opt/toss/openmpi/4.1/intel/man:1;/opt/toss/openmpi/4.1/intel/share/man:1;/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/share/man:1;/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/man:1;/projects/netpub/gnu/11.3.0-toss3/man:1;/usr/share/lmod/lmod/share/man:1;:1\n__LMOD_REF_COUNT_MODULEPATH=/etc/scl/modulefiles:2;/projects/sierra/modules/generated/modulefiles:1;/etc/modulefiles:1;/usr/share/modulefiles:1;/usr/share/Modules/modulefiles:1;/apps/modules/modulefiles-apps:1;/apps/modules/modulefiles:1;/opt/toss/modules/modulefiles:1;/usr/share/modulefiles/Linux:1;/usr/share/modulefiles/Core:1;/usr/share/lmod/lmod/modulefiles/Core:1\n__LMOD_REF_COUNT_PATH=/opt/toss/openmpi/4.1/intel/bin:1;/projects/cmake/toss4/3.25.2/bin:1;/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/condabin:1;/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/bin:1;/projects/netpub/gnu/11.3.0-toss3/bin:1;/ascldap/users/detorre/moose-compilers/mambaforge3/bin:4;/usr/lpp/mmfs/bin:1;/opt/ibm/MCStore/bin:1;/opt/ibm/MCStore/scripts:1;/usr/lib64/ccache:1;/usr/local/bin:1;/usr/bin:1;/usr/local/sbin:1;/usr/sbin:1\n__LMOD_REF_COUNT_PKG_CONFIG_PATH=/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/share/pkgconfig:1;/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/lib/pkgconfig:1\nLMOD_ROOT=/usr/share/lmod\nLMOD_SETTARG_FULL_SUPPORT=no\n __lmod_sh_dbg='v'\n __lmod_sh_dbg='vx'\n __lmod_sh_dbg='x'\nLMOD_sys=Linux\nLMOD_SYSTEM_NAME=manzano\nLMOD_VERSION=8.7.30\nLOADEDMODULES=aue/anaconda3/2023.09:cmake/3.25.2:openmpi-intel/4.1\nLOGNAME=detorre\nLS_COLORS=rs=0:di=38;5;33:ln=38;5;51:mh=00:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=01;05;37;41:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;40:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo=38;5;9:*.t7z=38;5;9:*.zip=38;5;9:*.z=38;5;9:*.dz=38;5;9:*.gz=38;5;9:*.lrz=38;5;9:*.lz=38;5;9:*.lzo=38;5;9:*.xz=38;5;9:*.zst=38;5;9:*.tzst=38;5;9:*.bz2=38;5;9:*.bz=38;5;9:*.tbz=38;5;9:*.tbz2=38;5;9:*.tz=38;5;9:*.deb=38;5;9:*.rpm=38;5;9:*.jar=38;5;9:*.war=38;5;9:*.ear=38;5;9:*.sar=38;5;9:*.rar=38;5;9:*.alz=38;5;9:*.ace=38;5;9:*.zoo=38;5;9:*.cpio=38;5;9:*.7z=38;5;9:*.rz=38;5;9:*.cab=38;5;9:*.wim=38;5;9:*.swm=38;5;9:*.dwm=38;5;9:*.esd=38;5;9:*.jpg=38;5;13:*.jpeg=38;5;13:*.mjpg=38;5;13:*.mjpeg=38;5;13:*.gif=38;5;13:*.bmp=38;5;13:*.pbm=38;5;13:*.pgm=38;5;13:*.ppm=38;5;13:*.tga=38;5;13:*.xbm=38;5;13:*.xpm=38;5;13:*.tif=38;5;13:*.tiff=38;5;13:*.png=38;5;13:*.svg=38;5;13:*.svgz=38;5;13:*.mng=38;5;13:*.pcx=38;5;13:*.mov=38;5;13:*.mpg=38;5;13:*.mpeg=38;5;13:*.m2v=38;5;13:*.mkv=38;5;13:*.webm=38;5;13:*.ogm=38;5;13:*.mp4=38;5;13:*.m4v=38;5;13:*.mp4v=38;5;13:*.vob=38;5;13:*.qt=38;5;13:*.nuv=38;5;13:*.wmv=38;5;13:*.asf=38;5;13:*.rm=38;5;13:*.rmvb=38;5;13:*.flc=38;5;13:*.avi=38;5;13:*.fli=38;5;13:*.flv=38;5;13:*.gl=38;5;13:*.dl=38;5;13:*.xcf=38;5;13:*.xwd=38;5;13:*.yuv=38;5;13:*.cgm=38;5;13:*.emf=38;5;13:*.ogv=38;5;13:*.ogx=38;5;13:*.aac=38;5;45:*.au=38;5;45:*.flac=38;5;45:*.m4a=38;5;45:*.mid=38;5;45:*.midi=38;5;45:*.mka=38;5;45:*.mp3=38;5;45:*.mpc=38;5;45:*.ogg=38;5;45:*.ra=38;5;45:*.wav=38;5;45:*.oga=38;5;45:*.opus=38;5;45:*.spx=38;5;45:*.xspf=38;5;45:\nMAIL=/var/spool/mail/detorre\nMANPATH=/opt/toss/openmpi/4.1/intel/man:/opt/toss/openmpi/4.1/intel/share/man:/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/share/man:/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/man:/projects/netpub/gnu/11.3.0-toss3/man:/usr/share/lmod/lmod/share/man::\nMODULEPATH=/etc/scl/modulefiles:/projects/sierra/modules/generated/modulefiles:/etc/modulefiles:/usr/share/modulefiles:/usr/share/Modules/modulefiles:/apps/modules/modulefiles-apps:/apps/modules/modulefiles:/opt/toss/modules/modulefiles:/usr/share/modulefiles/Linux:/usr/share/modulefiles/Core:/usr/share/lmod/lmod/modulefiles/Core\nMODULEPATH_ROOT=/usr/share/modulefiles\nMODULESHOME=/usr/share/lmod/lmod\n_ModuleTable001_=X01vZHVsZVRhYmxlXyA9IHsKTVR2ZXJzaW9uID0gMywKY19yZWJ1aWxkVGltZSA9IDcyMDAuMCwKY19zaG9ydFRpbWUgPSAwLjIwMzYyNjE1NTg1MzI3LApkZXB0aFQgPSB7fSwKZmFtaWx5ID0gewptcGkgPSAib3Blbm1waS1pbnRlbCIsCn0sCm1UID0gewpbImF1ZS9hbmFjb25kYTMiXSA9IHsKZm4gPSAiL2FwcHMvbW9kdWxlcy9tb2R1bGVmaWxlcy1hcHBzL2F1ZS9hbmFjb25kYTMvMjAyMy4wOSIsCmZ1bGxOYW1lID0gImF1ZS9hbmFjb25kYTMvMjAyMy4wOSIsCmxvYWRPcmRlciA9IDEsCnByb3BUID0ge30sCnN0YWNrRGVwdGggPSAwLApzdGF0dXMgPSAiYWN0aXZlIiwKdXNlck5hbWUgPSAiYXVlL2FuYWNvbmRhMy8yMDIzLjA5IiwKd1YgPSAiMDAwMDAyMDIzLjAwMDAw\n_ModuleTable002_=MDAwOS4qemZpbmFsIiwKfSwKY21ha2UgPSB7CmZuID0gIi9hcHBzL21vZHVsZXMvbW9kdWxlZmlsZXMvY21ha2UvMy4yNS4yIiwKZnVsbE5hbWUgPSAiY21ha2UvMy4yNS4yIiwKbG9hZE9yZGVyID0gMiwKcHJvcFQgPSB7fSwKc3RhY2tEZXB0aCA9IDAsCnN0YXR1cyA9ICJhY3RpdmUiLAp1c2VyTmFtZSA9ICJjbWFrZS8zLjI1LjIiLAp3ViA9ICIwMDAwMDAwMDMuMDAwMDAwMDI1LjAwMDAwMDAwMi4qemZpbmFsIiwKfSwKWyJvcGVubXBpLWludGVsIl0gPSB7CmZuID0gIi9vcHQvdG9zcy9tb2R1bGVzL21vZHVsZWZpbGVzL29wZW5tcGktaW50ZWwvNC4xIiwKZnVsbE5hbWUgPSAib3Blbm1waS1pbnRlbC80LjEiLApsb2FkT3JkZXIgPSAzLApwcm9wVCA9IHt9LApzdGFja0Rl\n_ModuleTable003_=cHRoID0gMCwKc3RhdHVzID0gImFjdGl2ZSIsCnVzZXJOYW1lID0gIm9wZW5tcGktaW50ZWwvNC4xIiwKd1YgPSAiMDAwMDAwMDA0LjAwMDAwMDAwMS4qemZpbmFsIiwKfSwKfSwKbXBhdGhBID0gewoiL2V0Yy9zY2wvbW9kdWxlZmlsZXMiLCAiL3Byb2plY3RzL3NpZXJyYS9tb2R1bGVzL2dlbmVyYXRlZC9tb2R1bGVmaWxlcyIsICIvZXRjL21vZHVsZWZpbGVzIiwgIi91c3Ivc2hhcmUvbW9kdWxlZmlsZXMiLCAiL3Vzci9zaGFyZS9Nb2R1bGVzL21vZHVsZWZpbGVzIiwgIi9hcHBzL21vZHVsZXMvbW9kdWxlZmlsZXMtYXBwcyIsICIvYXBwcy9tb2R1bGVzL21vZHVsZWZpbGVzIgosICIvb3B0L3Rvc3MvbW9kdWxlcy9tb2R1bGVmaWxlcyIsICIvdXNyL3NoYXJlL21vZHVsZWZp\n_ModuleTable004_=bGVzL0xpbnV4IiwgIi91c3Ivc2hhcmUvbW9kdWxlZmlsZXMvQ29yZSIsICIvdXNyL3NoYXJlL2xtb2QvbG1vZC9tb2R1bGVmaWxlcy9Db3JlIiwKfSwKc3lzdGVtQmFzZU1QQVRIID0gIi9ldGMvc2NsL21vZHVsZWZpbGVzOi9wcm9qZWN0cy9zaWVycmEvbW9kdWxlcy9nZW5lcmF0ZWQvbW9kdWxlZmlsZXM6L2V0Yy9zY2wvbW9kdWxlZmlsZXM6L2V0Yy9tb2R1bGVmaWxlczovdXNyL3NoYXJlL21vZHVsZWZpbGVzOi91c3Ivc2hhcmUvTW9kdWxlcy9tb2R1bGVmaWxlczovYXBwcy9tb2R1bGVzL21vZHVsZWZpbGVzLWFwcHM6L2FwcHMvbW9kdWxlcy9tb2R1bGVmaWxlczovb3B0L3Rvc3MvbW9kdWxlcy9tb2R1bGVmaWxlczovdXNyL3NoYXJlL21vZHVsZWZpbGVzL0xpbnV4Oi91\n_ModuleTable005_=c3Ivc2hhcmUvbW9kdWxlZmlsZXMvQ29yZTovdXNyL3NoYXJlL2xtb2QvbG1vZC9tb2R1bGVmaWxlcy9Db3JlIiwKfQo=\n_ModuleTable_Sz_=5\nMPICH_MPI=openmpi-4.1-intel-4.1.4\nMPI_HOME=/opt/toss/openmpi/4.1/intel\nMPIHOME=/opt/toss/openmpi/4.1/intel\nMPI_NAME=openmpi-4.1-intel\nMPI_ROOT=/opt/toss/openmpi/4.1/intel\nMPIROOT=/opt/toss/openmpi/4.1/intel\nMPI_VERSION=4.1.4\nOLDPWD=/pscratch/detorre/raccoon/moose/petsc\nOMPI_MCA_btl=^openib\nOMPI_MCA_btl_openib_allow_ib=true\nOMPI_MCA_btl_openib_ib_retry_count=7\nOMPI_MCA_btl_openib_ib_timeout=21\nOMPI_MCA_mtl=psm2\nOMPI_MCA_pml=cm\nPATH=/ascldap/users/detorre/moose-compilers/mambaforge3/bin:/opt/toss/openmpi/4.1/intel/bin:/projects/cmake/toss4/3.25.2/bin:/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/condabin:/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/bin:/projects/netpub/gnu/11.3.0-toss3/bin:/ascldap/users/detorre/moose-compilers/mambaforge3/bin:/usr/lpp/mmfs/bin:/opt/ibm/MCStore/bin:/opt/ibm/MCStore/scripts:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin\nPKG_CONFIG_PATH=/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/share/pkgconfig:/projects/aue/hpc/builds/x86_64/rhel8/ba17d7f2/anaconda3/install/linux-rhel8-x86_64/gcc-10.3.0/anaconda3-2023.09-0-zmej2r2/lib/pkgconfig\nPSM2_DEVICES=self,shm,hfi\nPWD=/pscratch/detorre/raccoon/moose/scripts\n return $__lmod_my_status\nSBATCH_NO_REQUEUE=1\nS_COLORS=auto\n set +$__lmod_sh_dbg;\n set -$__lmod_sh_dbg;\nSHELL=/bin/bash\nSHLVL=2\nSNLCLUSTER=manzano\nSNLNETWORK=srn\nSNLSITE=nm\nSNLSYSTEM=cts1\nSQUEUE_FORMAT=%.10i %.9P %.8j %.8u %.2t %.10M %.6D %R\nSRUN_EXPORT_ENV=ALL\nSRUN_NO_REQUEUE=1\nSSH_ASKPASS=/usr/libexec/openssh/gnome-ssh-askpass\nSSH_CLIENT=10.203.21.56 52556 22\nSSH_CONNECTION=10.203.21.56 52556 205.137.89.235 22\nSSH_TTY=/dev/pts/11\nTERM=xterm-256color\n unset __lmod_sh_dbg;\nUSER=detorre\n_=/usr/bin/env\n /usr/bin/scl \"$@\";\n *v*)\n *v*x*)\nwhich_declare=declare -f\n *x*)\nXDG_DATA_DIRS=/ascldap/users/detorre/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share:/usr/share\nXDG_SESSION_ID=141\nXMODIFIERS=@im=ibus\n\n\nAnd here is the output from the echo command\nmpicc mpicxx mpif90 mpif90 mpif77",
                          "url": "https://github.com/idaholab/moose/discussions/26801#discussioncomment-8455714",
                          "updatedAt": "2024-02-13T15:54:33Z",
                          "publishedAt": "2024-02-13T15:54:32Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Sorry for the delayed response...\nThere are a number of issues I see:\n\nYou have a path set to what may be our Conda packaged compilers, and your systems OpenMPI\nPATH=/ascldap/users/detorre/moose-compilers/mambaforge3/bin:/opt/toss/openmpi/4.1/intel/bin: <trimmed the rest>\n\n\nThe Intel compiler has given us a lot of issues in the past, and that is why you see it listed as \"Not supported\". But you are of course free to make the attempt! That said, are there any GNU compilers available on your cluster?\nYikes!\n$CC --version:\n\nmpicc: error while loading shared libraries: libimf.so: cannot open shared object file: No such file or directory\n\nWe need to figure out why this is the case. I am hopeful it is due to having both the Conda compiler stack in your PATH as well as the Intel OpenMPI compiler (fixing bullet point 1 may solve this issue).\n\nI think that is about it... The bottom line:\nDo not conda activate moose-compilers (if that is what you are doing). We need to get rid of this value in your PATH:\n/ascldap/users/detorre/moose-compilers/mambaforge3/bin\n\nAnd, are there any GNU compilers available? OpenMPI is fine, but we would suggest trying to find one that wraps to GCC instead of Intel.",
                          "url": "https://github.com/idaholab/moose/discussions/26801#discussioncomment-8456527",
                          "updatedAt": "2024-02-13T17:05:09Z",
                          "publishedAt": "2024-02-13T17:05:08Z",
                          "isAnswer": true
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "TheGreatCid"
                          },
                          "bodyText": "I implemented your suggestions, and it seems to be working now. Thank you!",
                          "url": "https://github.com/idaholab/moose/discussions/26801#discussioncomment-8457608",
                          "updatedAt": "2024-02-13T18:53:46Z",
                          "publishedAt": "2024-02-13T18:53:46Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "lvkas521424"
                  },
                  "bodyText": "",
                  "url": "https://github.com/idaholab/moose/discussions/26801#discussioncomment-9153622",
                  "updatedAt": "2024-04-23T02:59:57Z",
                  "publishedAt": "2024-04-18T10:53:41Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nTo find those when compiling you set the CC FC and CXX environment variables to the respective mpicc/f90/cxx\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/26801#discussioncomment-9153688",
                          "updatedAt": "2024-04-18T10:59:22Z",
                          "publishedAt": "2024-04-18T10:59:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Depending on how new your copy of MOOSE is, we have updated our diagnostic script to print out some useful information.\ncd moose/scripts\n./diagnostics.sh\nShould return a bunch OKs if it sees things are good to go. Address any failures, or warnings.\nAn example run on my machine without 'activating' my environment (aka this would be a failure):\n##################################################################################################\nInfluential Environment Variables\n\nCONDA_CHANNEL=https://conda.software.inl.gov/public\nCONDA_DEFAULT_ENV=base\nCONDA_EXE=/Users/milljm/miniforge3/bin/conda\nCONDA_PREFIX=/Users/milljm/miniforge3\nCONDA_PROMPT_MODIFIER=(base) \nCONDA_PYTHON_EXE=/Users/milljm/miniforge3/bin/python\nCONDA_SHLVL=1\nCURL_CA_BUNDLE=/opt/certs/CAINLROOT_B64.crt\nMOOSE_JOBS=6\nMOOSE_TERM_FORMAT=tpnsc\nPATH=/Users/milljm/miniforge3/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/opt/X11/bin:/usr/local/MacGPG2/bin:/Users/milljm/Applications/Visual Studio Code.app/Contents/Resources/app/bin\nREQUESTS_CA_BUNDLE=/opt/certs/CAINLROOT_B64.crt\nSSL_CERT_FILE=/opt/certs/CAINLROOT_B64.crt\n\n##################################################################################################\nCompiler(s) (CC CXX FC F77 F90):\n\nCC              not set\nCXX             not set\nFC              not set\nF77             not set\nF90             not set\n\nFAIL: One or more compiler environment variables not set\n\n##################################################################################################\nPython Sanity Checks\n\nVerify `/usr/bin/env python3 --version` (reporting as: Python 3.10.13),\nmatches versions for: `which python3 && which python`\n\nOK\n\n##################################################################################################\nPython Modules (TestHarness, run-ability)\n\nOK\n\n##################################################################################################\nCONDA MOOSE Packages\n\nWARNING: Not using Conda MOOSE packages, or `conda activate moose` not\nperformed",
                          "url": "https://github.com/idaholab/moose/discussions/26801#discussioncomment-9154200",
                          "updatedAt": "2024-04-18T11:58:41Z",
                          "publishedAt": "2024-04-18T11:56:39Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lvkas521424"
                          },
                          "bodyText": "Hi, Guillaume and milljm, sorry, due to our closed computer network environment, I am unable to provide you with incorrect output information and can only use photography.After using ./diagnostics.sh , some errors have occurred\n\nlvkas",
                          "url": "https://github.com/idaholab/moose/discussions/26801#discussioncomment-9160701",
                          "updatedAt": "2024-04-19T00:48:03Z",
                          "publishedAt": "2024-04-19T00:48:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "looks like the environment variables are properly set for the compiler.\nYou are missing a python package we need though. Can you install packaging? We usually get it from conda but you might have to do this differently since you are offline",
                          "url": "https://github.com/idaholab/moose/discussions/26801#discussioncomment-9160806",
                          "updatedAt": "2024-04-19T01:11:28Z",
                          "publishedAt": "2024-04-19T01:11:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "lvkas521424"
                          },
                          "bodyText": "Sure, I can package packaging from the external network and deliver them to the internal network via email.Hope it\u2018s useful.",
                          "url": "https://github.com/idaholab/moose/discussions/26801#discussioncomment-9160823",
                          "updatedAt": "2024-04-19T01:15:24Z",
                          "publishedAt": "2024-04-19T01:15:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "uh oh. Python 3.12\nI dont think we support that one. @milljm can confirm/\nDo you have another one installed by any chance?",
                          "url": "https://github.com/idaholab/moose/discussions/26801#discussioncomment-9169215",
                          "updatedAt": "2024-04-19T17:19:01Z",
                          "publishedAt": "2024-04-19T17:19:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "nevermind. seems we checked last week and python 3.12 is supported. Or at least 3.12.3 is",
                          "url": "https://github.com/idaholab/moose/discussions/26801#discussioncomment-9169235",
                          "updatedAt": "2024-04-19T17:20:38Z",
                          "publishedAt": "2024-04-19T17:20:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Can you get a picture of the top portion of the output? Seeing influential environment paths would be helpful in this instance.\nI am leaning toward this being an issue with you supplying a custom compiler, while the build scripts might be picking up another one elsewhere (PATH, LD_LIBRARY_PATH, related issues)",
                          "url": "https://github.com/idaholab/moose/discussions/26801#discussioncomment-9188708",
                          "updatedAt": "2024-04-22T12:55:10Z",
                          "publishedAt": "2024-04-22T12:55:10Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "C++ help",
          "author": {
            "login": "ashishdhole"
          },
          "bodyText": "Hello,\nI want to add a condition in my kernel where residual of ACGrGrPoly > residual of ACInterface (i.e. driving pressure > gradient energy density). I am very new to C++ and not able to get how to do it.\nI have tried\nReal interRes = (_grad_u[_qp] * _kappa[_qp] * _L[_qp] * _grad_test[_i][_qp]);\nif (std::abs((_L[_qp] * _mu[_qp] * (op * op * op - op + 3.0 * op * SumOPj)) > std::abs(interRes))\n\nbut I am not sure if this is how it is done. Can you please help me. The residual of ACInterface has _L[_qp] term which is not in the ACGrGrPoly. can you please help. Thank you",
          "url": "https://github.com/idaholab/moose/discussions/27431",
          "updatedAt": "2024-04-21T21:41:03Z",
          "publishedAt": "2024-04-21T16:44:06Z",
          "category": {
            "name": "Q&A Modules: Phase field"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "hello\ncomparing two numbers like this in C++ is fine.\nHowever,\n(_grad_u[_qp] * _kappa[_qp] * _L[_qp] * _grad_test[_i][_qp]);\n\nlooks like the residual of a diffusion-like operator so it seems correct.\nhowever\nstd::abs((_L[_qp] * _mu[_qp] * (op * op * op - op + 3.0 * op * SumOPj)\n\ndoes not look like a weak form residual? Unless the 'test' function is implicitly in one of the terms?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/27431#discussioncomment-9181252",
                  "updatedAt": "2024-04-21T20:01:46Z",
                  "publishedAt": "2024-04-21T20:01:46Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "ashishdhole"
                          },
                          "bodyText": "the second part is the residual of ACGrGrPoly, which has test function in offdiagonaljacobian and not in residual part. do I have to add the test function in my relation?",
                          "url": "https://github.com/idaholab/moose/discussions/27431#discussioncomment-9181667",
                          "updatedAt": "2024-04-21T21:12:17Z",
                          "publishedAt": "2024-04-21T21:12:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "ok I am not familiar with this kernel but I dot think the residual does not have the test function\nI looked up the base classes and I found\ntemplate <typename T>\nReal\nACBulk<T>::precomputeQpResidual()\n{\n  // Get free energy derivative from function\n  Real dFdop = computeDFDOP(Residual);\n\n  // Set residual\n  return _L[_qp] * dFdop;\n}\n\nthe output of precomputeQpResidual() from a KernelValue class is multiplied by the test function.\nso you should instead multiply by the test function",
                          "url": "https://github.com/idaholab/moose/discussions/27431#discussioncomment-9181687",
                          "updatedAt": "2024-04-21T21:19:12Z",
                          "publishedAt": "2024-04-21T21:19:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ashishdhole"
                          },
                          "bodyText": "If I put test function in the relation, the make happens successfully, but when running input file, it terminates the calculations at time=0. but if I am not putting this condition, my input file runs perfectly fine. Not sure what exactly is happening.",
                          "url": "https://github.com/idaholab/moose/discussions/27431#discussioncomment-9181730",
                          "updatedAt": "2024-04-21T21:31:08Z",
                          "publishedAt": "2024-04-21T21:31:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "how does it terminate? through non-convergence or through a crash / seg fault?",
                          "url": "https://github.com/idaholab/moose/discussions/27431#discussioncomment-9181738",
                          "updatedAt": "2024-04-21T21:34:14Z",
                          "publishedAt": "2024-04-21T21:34:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ashishdhole"
                          },
                          "bodyText": "segmentation fault",
                          "url": "https://github.com/idaholab/moose/discussions/27431#discussioncomment-9181744",
                          "updatedAt": "2024-04-21T21:35:16Z",
                          "publishedAt": "2024-04-21T21:35:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "you ll want to recompile in debug mode and re-run. This may give you a backtrace that tells you why it seg faults\nif not, follow these instructions to obtain that information\nhttps://mooseframework.inl.gov/application_development/debugging.html",
                          "url": "https://github.com/idaholab/moose/discussions/27431#discussioncomment-9181767",
                          "updatedAt": "2024-04-21T21:41:03Z",
                          "publishedAt": "2024-04-21T21:41:03Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Setup issue: Test timeout, LibMesh version blank.",
          "author": {
            "login": "calebfowler-tamu"
          },
          "bodyText": "I am trying to setup MOOSE on a laptop for personal research and study. I have been able to follow the instructions here: https://mooseframework.inl.gov/getting_started/installation/conda.html successfully through cloning MOOSE.\nIn making the test script by make -j 6 I was running into issues that weren't giving much info, so I switched to make -j 1 as I saw recommended on a number of other posts to avoid running out of memory.\nThis was the printed error when make failed:\nCompiling C++ (in opt mode) /home/calebfowler/mooseWorkspace/moose/framework/build/unity_src/variables_Unity.C... x86_64-conda-linux-gnu-c++: fatal error: Killed signal terminated program cc1plus compilation terminated.\nBut proceeding from there, I eventually hadthe test 'fvkernels/mms/harmonic_interpolation.average/tris' timeout, giving the full output which I'll comment below. I noticed that no version was listed for LibMesh.\nfvkernels/mms/harmonic_interpolation.average/tris: Working Directory: /home/calebfowler/mooseWorkspace/moose/test/tests/fvkernels/mms/harmonic_interpolation fvkernels/mms/harmonic_interpolation.average/tris: Running command: python3 -m unittest -v test.TestAverageTriangles fvkernels/mms/harmonic_interpolation.average/tris: Framework Information: fvkernels/mms/harmonic_interpolation.average/tris: MOOSE Version:           git commit bde1548eec on 2023-12-14 fvkernels/mms/harmonic_interpolation.average/tris: LibMesh Version: fvkernels/mms/harmonic_interpolation.average/tris: PETSc Version:           3.20.1 fvkernels/mms/harmonic_interpolation.average/tris: SLEPc Version:           3.20.0 fvkernels/mms/harmonic_interpolation.average/tris: Current Time:            Thu Dec 14 15:06:04 2023 fvkernels/mms/harmonic_interpolation.average/tris: Executable Timestamp:    Thu Dec 14 14:59:51 2023\nAs such, I have tried rebuilding LibMes, following the instructions here, to no avail. Some of the critical output is listed below https://mooseframework.inl.gov/getting_started/installation/build_libmesh.html.\nNo LSB modules are available. Submodule 'libmesh' (https://github.com/libMesh/libmesh) registered for path 'libmesh' Cloning into '/home/calebfowler/mooseWorkspace/moose/libmesh'...\nchecking for built-in XDR support... yes checking for boostlib >= 1.57.0... configure: We could not detect the boost libraries (version 1.57 or higher). If you have a staged boost library (still not installed) please specify $BOOST_ROOT in your environment and do not give a PATH to --with-boost option.  If you are sure you have boost installed, then check your version number looking in <boost/version.hpp>. See http://randspringer.de/boost for more documentation. <<< External boost installation *not* found... will try to configure for libmesh's internal boost >>> checking for boostlib >= 1.57.0... yes <<< Using libmesh-provided boost in ./contrib >>> checking for boost::movelib::unique_ptr support... yes note: Checking /lib and /include for MPI note: Could not find /lib/libmpi(.a/.so/.dylib) mpicxx Compiler Supports MPI configure: error: *** PETSc was not found, but --enable-petsc-required was specified. Running make -j 6... make: *** No targets specified and no makefile found.  Stop.",
          "url": "https://github.com/idaholab/moose/discussions/26345",
          "updatedAt": "2024-04-21T12:14:21Z",
          "publishedAt": "2023-12-14T21:35:57Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "calebfowler-tamu"
                  },
                  "bodyText": "fvkernels/mms/harmonic_interpolation.average/tris: Working Directory: /home/calebfowler/mooseWorkspace/moose/test/tests/fvkernels/mms/harmonic_interpolation fvkernels/mms/harmonic_interpolation.average/tris: Running command: python3 -m unittest -v test.TestAverageTriangles fvkernels/mms/harmonic_interpolation.average/tris: Framework Information: fvkernels/mms/harmonic_interpolation.average/tris: MOOSE Version:           git commit bde1548eec on 2023-12-14 fvkernels/mms/harmonic_interpolation.average/tris: LibMesh Version: fvkernels/mms/harmonic_interpolation.average/tris: PETSc Version:           3.20.1 fvkernels/mms/harmonic_interpolation.average/tris: SLEPc Version:           3.20.0 fvkernels/mms/harmonic_interpolation.average/tris: Current Time:            Thu Dec 14 15:06:04 2023 fvkernels/mms/harmonic_interpolation.average/tris: Executable Timestamp:    Thu Dec 14 14:59:51 2023 fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Parallelism: fvkernels/mms/harmonic_interpolation.average/tris:   Num Processors:          1 fvkernels/mms/harmonic_interpolation.average/tris:   Num Threads:             1 fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Mesh: fvkernels/mms/harmonic_interpolation.average/tris:   Parallel Type:           replicated fvkernels/mms/harmonic_interpolation.average/tris:   Mesh Dimension:          2 fvkernels/mms/harmonic_interpolation.average/tris:   Spatial Dimension:       2 fvkernels/mms/harmonic_interpolation.average/tris:   Nodes:                   9 fvkernels/mms/harmonic_interpolation.average/tris:   Elems:                   8 fvkernels/mms/harmonic_interpolation.average/tris:   Num Subdomains:          2 fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Nonlinear System: fvkernels/mms/harmonic_interpolation.average/tris:   Num DOFs:                8 fvkernels/mms/harmonic_interpolation.average/tris:   Num Local DOFs:          8 fvkernels/mms/harmonic_interpolation.average/tris:   Variables:               \"v\" fvkernels/mms/harmonic_interpolation.average/tris:   Finite Element Types:    \"MONOMIAL\" fvkernels/mms/harmonic_interpolation.average/tris:   Approximation Orders:    \"CONSTANT\" fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Execution Information: fvkernels/mms/harmonic_interpolation.average/tris:   Executioner:             Steady fvkernels/mms/harmonic_interpolation.average/tris:   Solver Mode:             NEWTON fvkernels/mms/harmonic_interpolation.average/tris:   MOOSE Preconditioner:    SMP (auto) fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Postprocessor Values: fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: | time           | error          | h              | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris:  0 Nonlinear |R| = 1.594123e+02 fvkernels/mms/harmonic_interpolation.average/tris:       0 Linear |R| = 1.594123e+02 fvkernels/mms/harmonic_interpolation.average/tris:       1 Linear |R| = 2.738872e-02 fvkernels/mms/harmonic_interpolation.average/tris:       2 Linear |R| = 1.178621e-06 fvkernels/mms/harmonic_interpolation.average/tris:  1 Nonlinear |R| = 1.178621e-06 fvkernels/mms/harmonic_interpolation.average/tris:  Solve Converged! fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Postprocessor Values: fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: | time           | error          | h              | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 | fvkernels/mms/harmonic_interpolation.average/tris: |   1.000000e+00 |   5.811146e-02 |   7.071068e-01 | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Framework Information: fvkernels/mms/harmonic_interpolation.average/tris: MOOSE Version:           git commit bde1548eec on 2023-12-14 fvkernels/mms/harmonic_interpolation.average/tris: LibMesh Version: fvkernels/mms/harmonic_interpolation.average/tris: PETSc Version:           3.20.1 fvkernels/mms/harmonic_interpolation.average/tris: SLEPc Version:           3.20.0 fvkernels/mms/harmonic_interpolation.average/tris: Current Time:            Thu Dec 14 15:06:04 2023 fvkernels/mms/harmonic_interpolation.average/tris: Executable Timestamp:    Thu Dec 14 14:59:51 2023 fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Parallelism: fvkernels/mms/harmonic_interpolation.average/tris:   Num Processors:          1 fvkernels/mms/harmonic_interpolation.average/tris:   Num Threads:             1 fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Mesh: fvkernels/mms/harmonic_interpolation.average/tris:   Parallel Type:           replicated fvkernels/mms/harmonic_interpolation.average/tris:   Mesh Dimension:          2 fvkernels/mms/harmonic_interpolation.average/tris:   Spatial Dimension:       2 fvkernels/mms/harmonic_interpolation.average/tris:   Nodes:                   25 fvkernels/mms/harmonic_interpolation.average/tris:   Elems:                   32 fvkernels/mms/harmonic_interpolation.average/tris:   Num Subdomains:          2 fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Nonlinear System: fvkernels/mms/harmonic_interpolation.average/tris:   Num DOFs:                32 fvkernels/mms/harmonic_interpolation.average/tris:   Num Local DOFs:          32 fvkernels/mms/harmonic_interpolation.average/tris:   Variables:               \"v\" fvkernels/mms/harmonic_interpolation.average/tris:   Finite Element Types:    \"MONOMIAL\" fvkernels/mms/harmonic_interpolation.average/tris:   Approximation Orders:    \"CONSTANT\" fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Execution Information: fvkernels/mms/harmonic_interpolation.average/tris:   Executioner:             Steady fvkernels/mms/harmonic_interpolation.average/tris:   Solver Mode:             NEWTON fvkernels/mms/harmonic_interpolation.average/tris:   MOOSE Preconditioner:    SMP (auto) fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Postprocessor Values: fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: | time           | error          | h              | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris:  0 Nonlinear |R| = 2.104827e+02 fvkernels/mms/harmonic_interpolation.average/tris:       0 Linear |R| = 2.104827e+02 fvkernels/mms/harmonic_interpolation.average/tris:       1 Linear |R| = 1.188994e-01 fvkernels/mms/harmonic_interpolation.average/tris:       2 Linear |R| = 7.903575e-04 fvkernels/mms/harmonic_interpolation.average/tris:  1 Nonlinear |R| = 7.903575e-04 fvkernels/mms/harmonic_interpolation.average/tris:       0 Linear |R| = 7.903575e-04 fvkernels/mms/harmonic_interpolation.average/tris:       1 Linear |R| = 2.142164e-06 fvkernels/mms/harmonic_interpolation.average/tris:       2 Linear |R| = 2.994724e-09 fvkernels/mms/harmonic_interpolation.average/tris:  2 Nonlinear |R| = 2.994729e-09 fvkernels/mms/harmonic_interpolation.average/tris:  Solve Converged! fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Postprocessor Values: fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: | time           | error          | h              | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 | fvkernels/mms/harmonic_interpolation.average/tris: |   1.000000e+00 |   4.064836e-02 |   3.535534e-01 | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Framework Information: fvkernels/mms/harmonic_interpolation.average/tris: MOOSE Version:           git commit bde1548eec on 2023-12-14 fvkernels/mms/harmonic_interpolation.average/tris: LibMesh Version: fvkernels/mms/harmonic_interpolation.average/tris: PETSc Version:           3.20.1 fvkernels/mms/harmonic_interpolation.average/tris: SLEPc Version:           3.20.0 fvkernels/mms/harmonic_interpolation.average/tris: Current Time:            Thu Dec 14 15:06:05 2023 fvkernels/mms/harmonic_interpolation.average/tris: Executable Timestamp:    Thu Dec 14 14:59:51 2023 fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Parallelism: fvkernels/mms/harmonic_interpolation.average/tris:   Num Processors:          1 fvkernels/mms/harmonic_interpolation.average/tris:   Num Threads:             1 fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Mesh: fvkernels/mms/harmonic_interpolation.average/tris:   Parallel Type:           replicated fvkernels/mms/harmonic_interpolation.average/tris:   Mesh Dimension:          2 fvkernels/mms/harmonic_interpolation.average/tris:   Spatial Dimension:       2 fvkernels/mms/harmonic_interpolation.average/tris:   Nodes:                   81 fvkernels/mms/harmonic_interpolation.average/tris:   Elems:                   128 fvkernels/mms/harmonic_interpolation.average/tris:   Num Subdomains:          2 fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Nonlinear System: fvkernels/mms/harmonic_interpolation.average/tris:   Num DOFs:                128 fvkernels/mms/harmonic_interpolation.average/tris:   Num Local DOFs:          128 fvkernels/mms/harmonic_interpolation.average/tris:   Variables:               \"v\" fvkernels/mms/harmonic_interpolation.average/tris:   Finite Element Types:    \"MONOMIAL\" fvkernels/mms/harmonic_interpolation.average/tris:   Approximation Orders:    \"CONSTANT\" fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Execution Information: fvkernels/mms/harmonic_interpolation.average/tris:   Executioner:             Steady fvkernels/mms/harmonic_interpolation.average/tris:   Solver Mode:             NEWTON fvkernels/mms/harmonic_interpolation.average/tris:   MOOSE Preconditioner:    SMP (auto) fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Postprocessor Values: fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: | time           | error          | h              | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris:  0 Nonlinear |R| = 2.853799e+02 fvkernels/mms/harmonic_interpolation.average/tris:       0 Linear |R| = 2.853799e+02 fvkernels/mms/harmonic_interpolation.average/tris:       1 Linear |R| = 6.025500e-01 fvkernels/mms/harmonic_interpolation.average/tris:       2 Linear |R| = 7.192565e-03 fvkernels/mms/harmonic_interpolation.average/tris:       3 Linear |R| = 5.581953e-05 fvkernels/mms/harmonic_interpolation.average/tris:  1 Nonlinear |R| = 5.581953e-05 fvkernels/mms/harmonic_interpolation.average/tris:       0 Linear |R| = 5.581953e-05 fvkernels/mms/harmonic_interpolation.average/tris:       1 Linear |R| = 6.546158e-07 fvkernels/mms/harmonic_interpolation.average/tris:       2 Linear |R| = 5.134356e-09 fvkernels/mms/harmonic_interpolation.average/tris:       3 Linear |R| = 4.753799e-11 fvkernels/mms/harmonic_interpolation.average/tris:  2 Nonlinear |R| = 4.753939e-11 fvkernels/mms/harmonic_interpolation.average/tris:  Solve Converged! fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Postprocessor Values: fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: | time           | error          | h              | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 | fvkernels/mms/harmonic_interpolation.average/tris: |   1.000000e+00 |   2.295142e-02 |   1.767767e-01 | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Framework Information: fvkernels/mms/harmonic_interpolation.average/tris: MOOSE Version:           git commit bde1548eec on 2023-12-14 fvkernels/mms/harmonic_interpolation.average/tris: LibMesh Version: fvkernels/mms/harmonic_interpolation.average/tris: PETSc Version:           3.20.1 fvkernels/mms/harmonic_interpolation.average/tris: SLEPc Version:           3.20.0 fvkernels/mms/harmonic_interpolation.average/tris: Current Time:            Thu Dec 14 15:06:05 2023 fvkernels/mms/harmonic_interpolation.average/tris: Executable Timestamp:    Thu Dec 14 14:59:51 2023 fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Parallelism: fvkernels/mms/harmonic_interpolation.average/tris:   Num Processors:          1 fvkernels/mms/harmonic_interpolation.average/tris:   Num Threads:             1 fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Mesh: fvkernels/mms/harmonic_interpolation.average/tris:   Parallel Type:           replicated fvkernels/mms/harmonic_interpolation.average/tris:   Mesh Dimension:          2 fvkernels/mms/harmonic_interpolation.average/tris:   Spatial Dimension:       2 fvkernels/mms/harmonic_interpolation.average/tris:   Nodes:                   289 fvkernels/mms/harmonic_interpolation.average/tris:   Elems:                   512 fvkernels/mms/harmonic_interpolation.average/tris:   Num Subdomains:          2 fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Nonlinear System: fvkernels/mms/harmonic_interpolation.average/tris:   Num DOFs:                512 fvkernels/mms/harmonic_interpolation.average/tris:   Num Local DOFs:          512 fvkernels/mms/harmonic_interpolation.average/tris:   Variables:               \"v\" fvkernels/mms/harmonic_interpolation.average/tris:   Finite Element Types:    \"MONOMIAL\" fvkernels/mms/harmonic_interpolation.average/tris:   Approximation Orders:    \"CONSTANT\" fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Execution Information: fvkernels/mms/harmonic_interpolation.average/tris:   Executioner:             Steady fvkernels/mms/harmonic_interpolation.average/tris:   Solver Mode:             NEWTON fvkernels/mms/harmonic_interpolation.average/tris:   MOOSE Preconditioner:    SMP (auto) fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Postprocessor Values: fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: | time           | error          | h              | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris:  0 Nonlinear |R| = 3.941863e+02 fvkernels/mms/harmonic_interpolation.average/tris:       0 Linear |R| = 3.941863e+02 fvkernels/mms/harmonic_interpolation.average/tris:       1 Linear |R| = 1.085823e+00 fvkernels/mms/harmonic_interpolation.average/tris:       2 Linear |R| = 2.897496e-02 fvkernels/mms/harmonic_interpolation.average/tris:       3 Linear |R| = 4.281595e-04 fvkernels/mms/harmonic_interpolation.average/tris:  1 Nonlinear |R| = 4.281595e-04 fvkernels/mms/harmonic_interpolation.average/tris:       0 Linear |R| = 4.281595e-04 fvkernels/mms/harmonic_interpolation.average/tris:       1 Linear |R| = 8.778613e-06 fvkernels/mms/harmonic_interpolation.average/tris:       2 Linear |R| = 2.135415e-07 fvkernels/mms/harmonic_interpolation.average/tris:       3 Linear |R| = 3.446505e-09 fvkernels/mms/harmonic_interpolation.average/tris:  2 Nonlinear |R| = 3.446498e-09 fvkernels/mms/harmonic_interpolation.average/tris:  Solve Converged! fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Postprocessor Values: fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: | time           | error          | h              | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 | fvkernels/mms/harmonic_interpolation.average/tris: |   1.000000e+00 |   1.192730e-02 |   8.838835e-02 | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Framework Information: fvkernels/mms/harmonic_interpolation.average/tris: MOOSE Version:           git commit bde1548eec on 2023-12-14 fvkernels/mms/harmonic_interpolation.average/tris: LibMesh Version: fvkernels/mms/harmonic_interpolation.average/tris: PETSc Version:           3.20.1 fvkernels/mms/harmonic_interpolation.average/tris: SLEPc Version:           3.20.0 fvkernels/mms/harmonic_interpolation.average/tris: Current Time:            Thu Dec 14 15:06:05 2023 fvkernels/mms/harmonic_interpolation.average/tris: Executable Timestamp:    Thu Dec 14 14:59:51 2023 fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Parallelism: fvkernels/mms/harmonic_interpolation.average/tris:   Num Processors:          1 fvkernels/mms/harmonic_interpolation.average/tris:   Num Threads:             1 fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Mesh: fvkernels/mms/harmonic_interpolation.average/tris:   Parallel Type:           replicated fvkernels/mms/harmonic_interpolation.average/tris:   Mesh Dimension:          2 fvkernels/mms/harmonic_interpolation.average/tris:   Spatial Dimension:       2 fvkernels/mms/harmonic_interpolation.average/tris:   Nodes:                   1089 fvkernels/mms/harmonic_interpolation.average/tris:   Elems:                   2048 fvkernels/mms/harmonic_interpolation.average/tris:   Num Subdomains:          2 fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Nonlinear System: fvkernels/mms/harmonic_interpolation.average/tris:   Num DOFs:                2048 fvkernels/mms/harmonic_interpolation.average/tris:   Num Local DOFs:          2048 fvkernels/mms/harmonic_interpolation.average/tris:   Variables:               \"v\" fvkernels/mms/harmonic_interpolation.average/tris:   Finite Element Types:    \"MONOMIAL\" fvkernels/mms/harmonic_interpolation.average/tris:   Approximation Orders:    \"CONSTANT\" fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Execution Information: fvkernels/mms/harmonic_interpolation.average/tris:   Executioner:             Steady fvkernels/mms/harmonic_interpolation.average/tris:   Solver Mode:             NEWTON fvkernels/mms/harmonic_interpolation.average/tris:   MOOSE Preconditioner:    SMP (auto) fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Postprocessor Values: fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: | time           | error          | h              | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris:  0 Nonlinear |R| = 5.505286e+02 fvkernels/mms/harmonic_interpolation.average/tris:       0 Linear |R| = 5.505286e+02 fvkernels/mms/harmonic_interpolation.average/tris:       1 Linear |R| = 1.178934e+00 fvkernels/mms/harmonic_interpolation.average/tris:       2 Linear |R| = 3.070637e-02 fvkernels/mms/harmonic_interpolation.average/tris:       3 Linear |R| = 1.068884e-03 fvkernels/mms/harmonic_interpolation.average/tris:  1 Nonlinear |R| = 1.068884e-03 fvkernels/mms/harmonic_interpolation.average/tris:       0 Linear |R| = 1.068884e-03 fvkernels/mms/harmonic_interpolation.average/tris:       1 Linear |R| = 3.484288e-05 fvkernels/mms/harmonic_interpolation.average/tris:       2 Linear |R| = 7.812917e-07 fvkernels/mms/harmonic_interpolation.average/tris:       3 Linear |R| = 1.678239e-08 fvkernels/mms/harmonic_interpolation.average/tris:       4 Linear |R| = 4.068220e-10 fvkernels/mms/harmonic_interpolation.average/tris:  2 Nonlinear |R| = 4.068274e-10 fvkernels/mms/harmonic_interpolation.average/tris:  Solve Converged! fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: Postprocessor Values: fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: | time           | error          | h              | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: |   0.000000e+00 |   0.000000e+00 |   0.000000e+00 | fvkernels/mms/harmonic_interpolation.average/tris: |   1.000000e+00 |   6.037172e-03 |   4.419417e-02 | fvkernels/mms/harmonic_interpolation.average/tris: +----------------+----------------+----------------+ fvkernels/mms/harmonic_interpolation.average/tris: fvkernels/mms/harmonic_interpolation.average/tris: test (test.TestAverageTriangles) ... fvkernels/mms/harmonic_interpolation.average/tris ................................ [FINISHED] FAILED (TIMEOUT)",
                  "url": "https://github.com/idaholab/moose/discussions/26345#discussioncomment-7858076",
                  "updatedAt": "2023-12-14T21:38:36Z",
                  "publishedAt": "2023-12-14T21:38:35Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nSwitching to make -j 1 was the right call. Now you can see if make -j 2 ... works, just to save a little time in the future.\nYou are building on your laptop so probably using mamba to get petsc and libmesh (as a moose-petsc and moose-petsc package).\nSo you should not run the script to install libmesh from moose/scripts.\nA timeout can just mean your laptop is slower than any machine we test with.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/26345#discussioncomment-7858089",
                  "updatedAt": "2023-12-14T21:39:55Z",
                  "publishedAt": "2023-12-14T21:39:55Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "nhnkkhang"
                  },
                  "bodyText": "I have the same issue. I used INL HPC, follow the instruction here: https://mooseframework.inl.gov/moose/getting_started/installation/inl_hpc_install_moose.html. I can run the ./update_and_rebuild_petsc.sh, but when I run: ./update_and_rebuild_libmesh.sh, it return like this\nconfigure: error: cannot find flags to link with the Boost system library (libboost-system)\nconfigure: error: ../../../contrib/metaphysicl/configure failed for contrib/metaphysicl\nRunning make -j 6...\nmake: *** No targets specified and no makefile found.  Stop.\n` . Then I still tried to build moose and run to the same error: \n`In file included from /home/nguykhan/Work_Moose/moose/framework/build/unity_src/userobjects_Unity.C:55:\n/home/nguykhan/Work_Moose/moose/framework/src/userobjects/NodalPatchRecoveryBase.C:18:10: fatal error: libmesh/parallel_eigen.h: No such file or directory\n   18 | #include \"libmesh/parallel_eigen.h\"\n      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nmake: *** [/home/nguykhan/Work_Moose/moose/framework/build.mk:150: /home/nguykhan/Work_Moose/moose/framework/build/unity_src/userobjects_Unity.x86_64-pc-linux-gnu.opt.lo] Error 1\nmake: *** Waiting for unfinished jobs....\nIn file included from /home/nguykhan/Work_Moose/moose/framework/build/unity_src/utils_Unity.C:2:\n/home/nguykhan/Work_Moose/moose/framework/src/utils/ADFParser.C: In member function 'bool ADFParser::JITCompile()':\n/home/nguykhan/Work_Moose/moose/framework/src/utils/ADFParser.C:36:30: error: no matching function for call to 'ADFParser::JITCompileHelper(const char [7], const char [1], std::__cxx11::basic_string<char>, const long unsigned int&)'\n   36 |     result = JITCompileHelper(\n      |              ~~~~~~~~~~~~~~~~^\n   37 |         \"ADReal\", \"\", \"#include \\\"\" + std::string(include_path_env) + \"\\\"\\n\", type_hash);\n      |         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nIn file included from /home/nguykhan/Work_Moose/moose/framework/build/header_symlinks/ADFParser.h:15,\n                 from /home/nguykhan/Work_Moose/moose/framework/src/utils/ADFParser.C:10:\n/apps/local/libmesh/1.8.0/include/libmesh/fparser_ad.hh:147:8: note: candidate: 'bool FunctionParserADBase<Value_t>::JITCompileHelper(const std::string&, const std::string&, const std::string&) [with Value_t = double; std::string = std::__cxx11::basic_string<char>]'\n  147 |   bool JITCompileHelper(const std::string & Value_t_name,\n      |        ^~~~~~~~~~~~~~~~\n/apps/local/libmesh/1.8.0/include/libmesh/fparser_ad.hh:147:8: note:   candidate expects 3 arguments, 4 provided\n/home/nguykhan/Work_Moose/moose/framework/src/utils/ADFParser.C:44:32: error: no matching function for call to 'ADFParser::JITCompileHelper(const char [7], const char [1], std::__cxx11::basic_string<char>, const long unsigned int&)'\n   44 |       result = JITCompileHelper(\"ADReal\", \"\", \"#include \\\"\" + include_path + \"\\\"\\n\", type_hash);\n      |                ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n/apps/local/libmesh/1.8.0/include/libmesh/fparser_ad.hh:147:8: note: candidate: 'bool FunctionParserADBase<Value_t>::JITCompileHelper(const std::string&, const std::string&, const std::string&) [with Value_t = double; std::string = std::__cxx11::basic_string<char>]'\n  147 |   bool JITCompileHelper(const std::string & Value_t_name,\n      |        ^~~~~~~~~~~~~~~~\n/apps/local/libmesh/1.8.0/include/libmesh/fparser_ad.hh:147:8: note:   candidate expects 3 arguments, 4 provided\n/home/nguykhan/Work_Moose/moose/framework/src/utils/ADFParser.C:47:32: error: no matching function for call to 'ADFParser::JITCompileHelper(const char [7], const char [307], const char [21], const long unsigned int&)'\n   47 |       result = JITCompileHelper(\"ADReal\", ADFPARSER_INCLUDES, \"#include \\\"ADReal.h\\\"\\n\", type_hash);\n      |                ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n/apps/local/libmesh/1.8.0/include/libmesh/fparser_ad.hh:147:8: note: candidate: 'bool FunctionParserADBase<Value_t>::JITCompileHelper(const std::string&, const std::string&, const std::string&) [with Value_t = double; std::string = std::__cxx11::basic_string<char>]'\n  147 |   bool JITCompileHelper(const std::string & Value_t_name,\n      |        ^~~~~~~~~~~~~~~~\n/apps/local/libmesh/1.8.0/include/libmesh/fparser_ad.hh:147:8: note:   candidate expects 3 arguments, 4 provided\nIn file included from /home/nguykhan/Work_Moose/moose/framework/build/unity_src/variables_Unity.C:7:\n/home/nguykhan/Work_Moose/moose/framework/src/variables/MooseVariableData.C: In constructor 'MooseVariableData<OutputType>::MooseVariableData(const MooseVariableField<OutputType>&, SystemBase&, THREAD_ID, Moose::ElementType, const libMesh::QBase* const&, const libMesh::QBase* const&, const libMesh::Node* const&, const libMesh::Elem* const&)':\n/home/nguykhan/Work_Moose/moose/framework/src/variables/MooseVariableData.C:79:61: error: 'RAVIART_THOMAS' was not declared in this scope\n   79 |       _fe_type.family == MONOMIAL_VEC || _fe_type.family == RAVIART_THOMAS)\n      |                                                             ^~~~~~~~~~~~~~\nIn file included from /home/nguykhan/Work_Moose/moose/framework/build/unity_src/systems_Unity.C:2:\n/home/nguykhan/Work_Moose/moose/framework/src/systems/AuxiliarySystem.C: In member function 'virtual void AuxiliarySystem::addVariable(const std::string&, const std::string&, InputParameters&)':\n/home/nguykhan/Work_Moose/moose/framework/src/systems/AuxiliarySystem.C:234:61: error: 'RAVIART_THOMAS' was not declared in this scope\n  234 |         fe_type.family == MONOMIAL_VEC || fe_type.family == RAVIART_THOMAS)\n      |                                                             ^~~~~~~~~~~~~~\nIn file included from /home/nguykhan/Work_Moose/moose/framework/build/unity_src/base_Unity.C:4:\n/home/nguykhan/Work_Moose/moose/framework/src/base/Assembly.C: In member function 'void Assembly::buildVectorFE(libMesh::FEType) const':\n/home/nguykhan/Work_Moose/moose/framework/src/base/Assembly.C:474:24: error: 'RAVIART_THOMAS' was not declared in this scope\n  474 |     if (type.family == RAVIART_THOMAS)\n      |                        ^~~~~~~~~~~~~~\n/home/nguykhan/Work_Moose/moose/framework/src/base/Assembly.C: In member function 'void Assembly::buildVectorFaceFE(libMesh::FEType) const':\n/home/nguykhan/Work_Moose/moose/framework/src/base/Assembly.C:507:24: error: 'RAVIART_THOMAS' was not declared in this scope\n  507 |     if (type.family == RAVIART_THOMAS)\n      |                        ^~~~~~~~~~~~~~\n/home/nguykhan/Work_Moose/moose/framework/src/base/Assembly.C: In member function 'void Assembly::buildVectorNeighborFE(libMesh::FEType) const':\n/home/nguykhan/Work_Moose/moose/framework/src/base/Assembly.C:536:24: error: 'RAVIART_THOMAS' was not declared in this scope\n  536 |     if (type.family == RAVIART_THOMAS)\n      |                        ^~~~~~~~~~~~~~\n/home/nguykhan/Work_Moose/moose/framework/src/base/Assembly.C: In member function 'void Assembly::buildVectorFaceNeighborFE(libMesh::FEType) const':\n/home/nguykhan/Work_Moose/moose/framework/src/base/Assembly.C:566:24: error: 'RAVIART_THOMAS' was not declared in this scope\n  566 |     if (type.family == RAVIART_THOMAS)\n      |                        ^~~~~~~~~~~~~~\nIn file included from /home/nguykhan/Work_Moose/moose/framework/build/unity_src/systems_Unity.C:12:\n/home/nguykhan/Work_Moose/moose/framework/src/systems/SystemBase.C: In member function 'virtual void SystemBase::addVariable(const std::string&, const std::string&, InputParameters&)':\n/home/nguykhan/Work_Moose/moose/framework/src/systems/SystemBase.C:734:61: error: 'RAVIART_THOMAS' was not declared in this scope\n  734 |         fe_type.family == MONOMIAL_VEC || fe_type.family == RAVIART_THOMAS)\n      |                                                             ^~~~~~~~~~~~~~\nmake: *** [/home/nguykhan/Work_Moose/moose/framework/build.mk:150: /home/nguykhan/Work_Moose/moose/framework/build/unity_src/variables_Unity.x86_64-pc-linux-gnu.opt.lo] Error 1\nmake: *** [/home/nguykhan/Work_Moose/moose/framework/build.mk:150: /home/nguykhan/Work_Moose/moose/framework/build/unity_src/base_Unity.x86_64-pc-linux-gnu.opt.lo] Error 1\nmake: *** [/home/nguykhan/Work_Moose/moose/framework/build.mk:150: /home/nguykhan/Work_Moose/moose/framework/build/unity_src/systems_Unity.x86_64-pc-linux-gnu.opt.lo] Error 1\nmake: *** [/home/nguykhan/Work_Moose/moose/framework/build.mk:150: /home/nguykhan/Work_Moose/moose/framework/build/unity_src/utils_Unity.x86_64-pc-linux-gnu.opt.lo] Error 1\n\nWhat should I do in this case?",
                  "url": "https://github.com/idaholab/moose/discussions/26345#discussioncomment-7871877",
                  "updatedAt": "2023-12-16T22:39:34Z",
                  "publishedAt": "2023-12-16T16:55:05Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nWhich MPI modules did you load?\nYou can type 'module list' or run the diagnostics script in moose/scripts to get that\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/26345#discussioncomment-7871945",
                          "updatedAt": "2023-12-16T17:04:57Z",
                          "publishedAt": "2023-12-16T17:04:56Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "nhnkkhang"
                          },
                          "bodyText": "Thank you for your support!\nI resolved the issue. Apparently, I have a local libmesh (different version with MOOSE), that caused the issue.",
                          "url": "https://github.com/idaholab/moose/discussions/26345#discussioncomment-7873541",
                          "updatedAt": "2023-12-16T21:38:41Z",
                          "publishedAt": "2023-12-16T21:38:40Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "lvkas521424"
                  },
                  "bodyText": "Hello, @GiudGiud  also encountered the issue of \"XDR not found\" while installing on HPC,\nRunning make - j 6\nMake: * * * * No targets specified and no makefiles found stop\nlvkas",
                  "url": "https://github.com/idaholab/moose/discussions/26345#discussioncomment-9178492",
                  "updatedAt": "2024-04-21T10:38:41Z",
                  "publishedAt": "2024-04-21T10:38:41Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nYou can pass a flag to install without it. It's tied to checkpointing, XDR is the binary format\nwe are actually supporting a non-binary (ascii) format that  had  the same performance pretty much\nOr you need to install libtirpc-devel on your cluster.",
                          "url": "https://github.com/idaholab/moose/discussions/26345#discussioncomment-9178920",
                          "updatedAt": "2024-04-21T15:24:37Z",
                          "publishedAt": "2024-04-21T12:14:21Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Unit conversion of mesh between the Reactor module and the THM module",
          "author": {
            "login": "NorthMagic"
          },
          "bodyText": "Hello, everyone! I'm using THM's HeatStructureFromFile3D, in which the mesh file is generated by Reactor module. Unfortunately, the unit of mesh from Reactor module is cm but that of THM module is m. I don't know how to do the unit conversion between HeatStructureFromFile3D (mesh file from Reactor module: cm) and FlowChannel1Phase (mesh from THM module: m). Could you please give me some suggestions?",
          "url": "https://github.com/idaholab/moose/discussions/27430",
          "updatedAt": "2024-04-21T05:36:13Z",
          "publishedAt": "2024-04-21T01:38:52Z",
          "category": {
            "name": "Q&A Modules: Thermal Hydraulics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou can use the coordinate transformation system to perform the conversion between units\nThe parameters to set are in the mesh block\nMore details can be found here on the Mesh syntax page\nCoordinate Systems\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/27430#discussioncomment-9177090",
                  "updatedAt": "2024-04-21T15:26:00Z",
                  "publishedAt": "2024-04-21T02:19:33Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "NorthMagic"
                          },
                          "bodyText": "Thanks for your help, it was fixed.",
                          "url": "https://github.com/idaholab/moose/discussions/27430#discussioncomment-9177490",
                          "updatedAt": "2024-04-21T05:36:11Z",
                          "publishedAt": "2024-04-21T05:36:11Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Restart from Last step",
          "author": {
            "login": "wangzhaohao"
          },
          "bodyText": "Hi, I am using restart in MOOSE\nI want to use the previous solid mechanics output, which is .e file.\nI have set the disp_x disp_y disp_z, there seems no problem.\nBut I meet a problem is the stress is zero, what should i do to transfer it?\nThanks for you help!",
          "url": "https://github.com/idaholab/moose/discussions/27405",
          "updatedAt": "2024-04-21T02:53:00Z",
          "publishedAt": "2024-04-18T09:23:37Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou cannot use restart from exodus for solid mechanics. You have to use the checkpoint system for restarting\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/27405#discussioncomment-9153695",
                  "updatedAt": "2024-04-18T11:00:19Z",
                  "publishedAt": "2024-04-18T11:00:17Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "wangzhaohao"
                          },
                          "bodyText": "thanks, i find checkpoint in this link\nand a question is there no mention of how to use. Can you give me some examples or other links?",
                          "url": "https://github.com/idaholab/moose/discussions/27405#discussioncomment-9154180",
                          "updatedAt": "2024-04-18T11:53:52Z",
                          "publishedAt": "2024-04-18T11:53:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You add a Checkpoint object to your Outputs block to create the checkpoint\nand use the restart_file_base parameter in the Problem to use it.\nthere will be examples in the repository. grep -r restart_file_base will find them",
                          "url": "https://github.com/idaholab/moose/discussions/27405#discussioncomment-9154270",
                          "updatedAt": "2024-04-18T12:06:09Z",
                          "publishedAt": "2024-04-18T12:06:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "wangzhaohao"
                          },
                          "bodyText": "Thanks, I find some examples, and I do as follow:\n[Problem]\n   restart_file_base = XAPR_heatsource_out_fuelRod0_cp/LATEST\n   force_restart = true\n[]\nthe XAPR_heatsource_out_fuleRod0_cp is a subApp, So I use force_restart\nBut seems doesn't work\nWARNING! There are options you set that were not used!\nWARNING! could be spelling mistake, etc!\nThere is one unused database option. It is:\nOption left: name:-i value: pulse_project/pulse_heat.i source: command line\nI use the command in zsh is  mpiexec -n 40 ./pulse-opt -i pulse_project/pulse_heat.i",
                          "url": "https://github.com/idaholab/moose/discussions/27405#discussioncomment-9177068",
                          "updatedAt": "2024-04-21T02:13:41Z",
                          "publishedAt": "2024-04-21T02:13:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "wangzhaohao"
                          },
                          "bodyText": "Oh, I find the reason\nthe end time should follow the LATEST",
                          "url": "https://github.com/idaholab/moose/discussions/27405#discussioncomment-9177147",
                          "updatedAt": "2024-04-21T02:52:57Z",
                          "publishedAt": "2024-04-21T02:52:56Z",
                          "isAnswer": true
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "TensorMechanics error regarding transient vs. Steady solve",
          "author": {
            "login": "cmcheron"
          },
          "bodyText": "I'm trying to obtain a steady state solution for a nuclear fuel rod thermal mechanical problem and keep running into the following error:\n*** ERROR ***\nall_strain: Calling \"coupled[Vector][Gradient/Dot]Old[er]\" on variable \"displacements\" when using a \"Steady\" executioner is not allowed. This value is available only in transient simulations.\nI know there's likely a couple errors in my input file since I'm pretty new to MOOSE, but I really can't figure out what is causing this specific error.  I expect there to be some type of mechanical contact between the cladding and fuel surfaces, but haven't been able to figure out how to set that up yet.\nFor context I' include my set up for the contact and TensorMechanics modules:\n[ThermalContact]\n[fuel_gap] # Assume gap is filled with helium\ntype = GapHeatTransfer\nvariable = temperature\ngap_geometry_type = 'CYLINDER'\ngap_conductivity = 0.002556 #  W/cm^2-K\nprimary = fuel_surface\nsecondary = clad_surface\nquadrature = true\nemissivity_primary = 0.4\nemissivity_secondary = 0.4\n[]\n[]\n[Contact]\n[gap]\nprimary = fuel_surface\nsecondary = clad_surface\nmodel = frictionless\nformulation = mortar\n[]\n[]\nThe below is causing problems.\n[Modules/TensorMechanics/Master]\n[all] # all stresses developed\nadd_variables = true\nstrain = FINITE\neigenstrain_names = thermal\ngenerate_output = 'vonmises_stress'\nvolumetric_locking_connection = true\ntemperature = temperature\n[]\n[]",
          "url": "https://github.com/idaholab/moose/discussions/27427",
          "updatedAt": "2024-04-20T19:50:31Z",
          "publishedAt": "2024-04-20T19:43:38Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "cmcheron"
                  },
                  "bodyText": "nevermind I found my problem. Needed to use Transient Solve and not Steady Solve since Contact tracks the change in mesh over time",
                  "url": "https://github.com/idaholab/moose/discussions/27427#discussioncomment-9176035",
                  "updatedAt": "2024-04-20T19:50:32Z",
                  "publishedAt": "2024-04-20T19:50:31Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Function for Initial_condition",
          "author": {
            "login": "ananthanarasimhanj"
          },
          "bodyText": "I have a variable \"e\" as shown in the below code.  I want to expression (or function) in the place of constant value given to \"initial_condition\". what should I change?\n[Variables]\n  [e]\n    family = SCALAR\n    order = FIRST\n    initial_condition = 13.81551056 \n    #scaling = 1e-3\n  []\n[]\n\n[ScalarKernels]\n\n  [de_dt]\n    type = ODETimeDerivativeLog\n    variable = e\n  []\n\n[]",
          "url": "https://github.com/idaholab/moose/discussions/27417",
          "updatedAt": "2024-04-20T16:38:17Z",
          "publishedAt": "2024-04-19T12:53:43Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou use the Scalar----ICs, in the [ICs] block to set an initial condition to ScalarVariables.\nHere's a link to the one that can use a function\nhttps://mooseframework.inl.gov/source/ics/FunctionScalarIC.html\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/27417#discussioncomment-9169166",
                  "updatedAt": "2024-04-19T17:14:32Z",
                  "publishedAt": "2024-04-19T17:14:32Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "ananthanarasimhanj"
                          },
                          "bodyText": "Sure. Is it also allowed to do this through AuxVariables/AuxScalarKernels?",
                          "url": "https://github.com/idaholab/moose/discussions/27417#discussioncomment-9171634",
                          "updatedAt": "2024-04-19T23:52:08Z",
                          "publishedAt": "2024-04-19T23:52:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "That s ok too\nYou can execute them on InITIAL it is also the same as an initial condition",
                          "url": "https://github.com/idaholab/moose/discussions/27417#discussioncomment-9172288",
                          "updatedAt": "2024-04-20T03:59:35Z",
                          "publishedAt": "2024-04-20T03:59:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ananthanarasimhanj"
                          },
                          "bodyText": "Thanks... below one looks OK?\n[AuxScalarKernels]\n  [H2]\n    type = ParsedAuxScalar\n    variable = H2\n    args = 'Nd'\n    function = 'log(Nd)'\n    execute_on = 'INITIAL'\n  []",
                          "url": "https://github.com/idaholab/moose/discussions/27417#discussioncomment-9172294",
                          "updatedAt": "2024-04-20T04:02:40Z",
                          "publishedAt": "2024-04-20T04:02:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ananthanarasimhanj"
                          },
                          "bodyText": "There is also another question for me... this variable I want to initialize using an expression is a chemical species, and it has to have time derivative kernel. Is it possible to use an expression for the nonlinear variable instead of using as aux? for example below code, where initial condition is a constant has to be replaced with an expression. How should we do that?\n[Variables]\n  [H2]\n    family = SCALAR\n    order = FIRST\n    initial_condition = 58\n  []\n []\n\n[ScalarKernels]\n\n  [H2]\n    type = ODETimeDerivativeLog\n    variable = H2\n  []\n []",
                          "url": "https://github.com/idaholab/moose/discussions/27417#discussioncomment-9172319",
                          "updatedAt": "2024-04-20T04:16:12Z",
                          "publishedAt": "2024-04-20T04:14:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nFor more complex intiializations you use the ICs block instead of specify it directly in the Variable\nThere are many ICs objects you can use.\nAuxkernel on initial are also still an option imo\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/27417#discussioncomment-9172928",
                          "updatedAt": "2024-04-20T16:37:46Z",
                          "publishedAt": "2024-04-20T07:32:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ananthanarasimhanj"
                          },
                          "bodyText": "But, variable in AuxKernel, its value will be held constant throughout the simulation and cannot be subjected to a time derivative as this is considered to be a chemical species in this problem statement. Isnt it?",
                          "url": "https://github.com/idaholab/moose/discussions/27417#discussioncomment-9174712",
                          "updatedAt": "2024-04-20T14:46:14Z",
                          "publishedAt": "2024-04-20T14:46:13Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "They can be modified by auxkernels on each time step\nI dont know actually if the time derivative is not computed. It would be worth checking",
                          "url": "https://github.com/idaholab/moose/discussions/27417#discussioncomment-9175254",
                          "updatedAt": "2024-04-20T16:38:18Z",
                          "publishedAt": "2024-04-20T16:38:17Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "One test failed",
          "author": {
            "login": "Always-kimi"
          },
          "bodyText": "Hi all\uff0c\nThere is one tests failed when I executed ./run_tests -j 8. Does anyone know how to fix it\uff1f\nmake_install.test_copy_install/copy_tests ................................... FAILED (EXPECTED OUTPUT MISSING)\n--------------------------------------------------------------------------------------------------------------\nRan 4299 tests in 631.2 seconds. Average test time 0.9 seconds, maximum test time 34.6 seconds.\n4298 passed, 99 skipped, 0 pending, 1 FAILED\n\nthe detailed information are as follows:\nmake_install.test_copy_install/setup_fake_test_structure ..................................... [re-running] OK\nmake_install.test_copy_install/copy_tests: Working Directory: /home/kimi/projects/moose/test/tests/make_install/../../../make_install_test\nmake_install.test_copy_install/copy_tests: Running command: /home/kimi/projects/moose/test/moose_test-opt --copy-inputs tests\nmake_install.test_copy_install/copy_tests: sh: 1: rsync: not found\nmake_install.test_copy_install/copy_tests: \nmake_install.test_copy_install/copy_tests: \nmake_install.test_copy_install/copy_tests: *** ERROR ***\nmake_install.test_copy_install/copy_tests: Failed to copy the requested directory.\nmake_install.test_copy_install/copy_tests: \nmake_install.test_copy_install/copy_tests: Stack frames: 10\nmake_install.test_copy_install/copy_tests: 0: libMesh::print_trace(std::ostream&)\nmake_install.test_copy_install/copy_tests: 1: moose::internal::mooseErrorRaw(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)\nmake_install.test_copy_install/copy_tests: 2: void mooseError<char const (&) [40]>(char const (&) [40])\nmake_install.test_copy_install/copy_tests: 3: MooseApp::copyInputs() const\nmake_install.test_copy_install/copy_tests: 4: MooseApp::run()\nmake_install.test_copy_install/copy_tests: 5: /home/kimi/projects/moose/test/moose_test-opt(+0x20eb) [0x55f602f850eb]\nmake_install.test_copy_install/copy_tests: 6: main\nmake_install.test_copy_install/copy_tests: 7: /lib/x86_64-linux-gnu/libc.so.6(+0x2724a) [0x7f921344624a]\nmake_install.test_copy_install/copy_tests: 8: __libc_start_main\nmake_install.test_copy_install/copy_tests: 9: /home/kimi/projects/moose/test/moose_test-opt(+0x2396) [0x55f602f85396]\nmake_install.test_copy_install/copy_tests: application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\nmake_install.test_copy_install/copy_tests: [unset]: write_line error; fd=-1 buf=:cmd=abort exitcode=1\nmake_install.test_copy_install/copy_tests: :\nmake_install.test_copy_install/copy_tests: system msg for write_line failure : Bad file descriptor\nmake_install.test_copy_install/copy_tests: ################################################################################\nmake_install.test_copy_install/copy_tests: \nmake_install.test_copy_install/copy_tests: Unable to match the following pattern against the program's output:\nmake_install.test_copy_install/copy_tests: \nmake_install.test_copy_install/copy_tests: Directory successfully copied\nmake_install.test_copy_install/copy_tests: \nmake_install.test_copy_install/copy_tests: ################################################################################\nmake_install.test_copy_install/copy_tests: Tester failed, reason: EXPECTED OUTPUT MISSING\nmake_install.test_copy_install/copy_tests: \nmake_install.test_copy_install/copy_tests [previous results: EXPECTED OUT...] FAILED (EXPECTED OUTPUT MISSING)\n\n\nFinal Test Results:\n--------------------------------------------------------------------------------------------------------------\nmake_install.test_copy_install/setup_fake_test_structure ..................................... [re-running] OK\nmake_install.test_copy_install/copy_tests [previous results: EXPECTED OUT...] FAILED (EXPECTED OUTPUT MISSING)\n--------------------------------------------------------------------------------------------------------------\nRan 2 tests in 5.5 seconds. Average test time 0.1 seconds, maximum test time 0.1 seconds.\n1 passed, 0 skipped, 0 pending, 1 FAILED",
          "url": "https://github.com/idaholab/moose/discussions/27423",
          "updatedAt": "2024-04-22T13:13:03Z",
          "publishedAt": "2024-04-20T06:46:07Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou don't need to have this test passing to use moose\nIf you really want to, then install rsync because the test requires it to copy a directory to install something\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/27423#discussioncomment-9172770",
                  "updatedAt": "2024-04-20T06:56:28Z",
                  "publishedAt": "2024-04-20T06:56:27Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Always-kimi"
                          },
                          "bodyText": "Got it, thanks\uff01",
                          "url": "https://github.com/idaholab/moose/discussions/27423#discussioncomment-9172951",
                          "updatedAt": "2024-04-20T07:39:53Z",
                          "publishedAt": "2024-04-20T07:39:52Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Help with installing MOOSE on HPC",
          "author": {
            "login": "smpark7"
          },
          "bodyText": "I'm trying to install MOOSE on the ALCF Polaris HPC. I have the following environment:\nmodule swap PrgEnv-nvhpc PrgEnv-gnu/8.3.3\nmodule load cray-hdf5-parallel/1.12.2.3\nmodule load cray-fftw/3.3.10.3\nmodule load cray-python/3.9.13.1\nmodule load cmake/3.23.2\nexport CC=mpicc CXX=mpicxx FC=mpif90 F90=mpif90 F77=mpif77\n\nWhen running the update_and_rebuild_petsc.sh script, the installation appears to stall indefinitely at the step Configuring SUPERLU_DIST with CMake; this may take several minutes. Here's the config log after terminating the PETSc installation.\nconfigure.log\nThe installation appears to be stuck while running the following command:\nExecuting: /soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin/cmake .. -DCMAKE_INSTALL_PREFIX=/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose -DCMAKE_INSTALL_NAME_DIR:STRING=\"/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib\" -DCMAKE_INSTALL_LIBDIR:STRING=\"lib\" -DCMAKE_VERBOSE_MAKEFILE=1 -DCMAKE_BUILD_TYPE=Release -DCMAKE_AR=\"/usr/bin/ar\" -DCMAKE_C_COMPILER=\"mpicc\" -DMPI_C_COMPILER=\"/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/bin/mpicc\" -DCMAKE_RANLIB=/usr/bin/ranlib -DCMAKE_C_FLAGS:STRING=\"-fPIC -Wno-lto-type-mismatch -Wno-stringop-overflow -g -O -fopenmp\" -DCMAKE_C_FLAGS_DEBUG:STRING=\"-fPIC -Wno-lto-type-mismatch -Wno-stringop-overflow -g -O -fopenmp\" -DCMAKE_C_FLAGS_RELEASE:STRING=\"-fPIC -Wno-lto-type-mismatch -Wno-stringop-overflow -g -O -fopenmp\" -DCMAKE_CXX_COMPILER=\"mpicxx\" -DMPI_CXX_COMPILER=\"/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/bin/mpicxx\" -DCMAKE_CXX_FLAGS:STRING=\"-Wno-lto-type-mismatch -Wno-psabi -g -O -std=c++17 -fPIC -fopenmp\" -DCMAKE_CXX_FLAGS_DEBUG:STRING=\"-Wno-lto-type-mismatch -Wno-psabi -g -O -std=c++17 -fPIC -fopenmp\" -DCMAKE_CXX_FLAGS_RELEASE:STRING=\"-Wno-lto-type-mismatch -Wno-psabi -g -O -std=c++17 -fPIC -fopenmp\" -DCMAKE_Fortran_COMPILER=\"mpif90\" -DMPI_Fortran_COMPILER=\"/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/bin/mpif90\" -DCMAKE_Fortran_FLAGS:STRING=\"-fPIC -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -g -O -fopenmp -fallow-argument-mismatch\" -DCMAKE_Fortran_FLAGS_DEBUG:STRING=\"-fPIC -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -g -O -fopenmp -fallow-argument-mismatch\" -DCMAKE_Fortran_FLAGS_RELEASE:STRING=\"-fPIC -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -g -O -fopenmp -fallow-argument-mismatch\" -DCMAKE_EXE_LINKER_FLAGS:STRING=\" -fopenmp\" -DBUILD_SHARED_LIBS:BOOL=ON -DUSE_XSDK_DEFAULTS=YES -DTPL_BLAS_LIBRARIES=\"-Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lflapack -Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lfblas -lm -ldl -Wl,-rpath,/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -L/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -lmpifort_gnu_91 -lmpi_gnu_91 -lgfortran -lm -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0 -L/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0 -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib64 -L/opt/cray/pe/gcc/12.2.0/snos/lib64 -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib -L/opt/cray/pe/gcc/12.2.0/snos/lib -lgfortran -lm -lgcc_s -lquadmath\" -DTPL_LAPACK_LIBRARIES=\"-Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lflapack -Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lfblas -lm -ldl -Wl,-rpath,/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -L/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -lmpifort_gnu_91 -lmpi_gnu_91 -lgfortran -lm -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0 -L/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0 -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib64 -L/opt/cray/pe/gcc/12.2.0/snos/lib64 -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib -L/opt/cray/pe/gcc/12.2.0/snos/lib -lgfortran -lm -lgcc_s -lquadmath\" -DTPL_PARMETIS_INCLUDE_DIRS=\"/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/include;/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/include\" -DTPL_PARMETIS_LIBRARIES=\"-Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lparmetis -Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lmetis -lm\" -DXSDK_INDEX_SIZE=64 -DXSDK_ENABLE_Fortran=ON -Denable_tests=0 -Denable_examples=0 -DMPI_C_COMPILE_FLAGS:STRING=\"\" -DMPI_C_INCLUDE_PATH:STRING=\"\" -DMPI_C_HEADER_DIR:STRING=\"\" -DMPI_C_LIBRARIES:STRING=\"\"",
          "url": "https://github.com/idaholab/moose/discussions/27379",
          "updatedAt": "2024-04-20T01:52:39Z",
          "publishedAt": "2024-04-16T01:23:37Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nDoes it consistently get stuck there?\n@milljm\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/27379#discussioncomment-9129811",
                  "updatedAt": "2024-04-16T12:44:42Z",
                  "publishedAt": "2024-04-16T12:44:41Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "The log looks fine up to that point to me. What happens when we execute that cmake command manually?\ncd /lus/eagle/projects/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/externalpackages/git.superlu_dist/petsc-build\n\n\n/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin/cmake .. -DCMAKE_INSTALL_PREFIX=/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose -DCMAKE_INSTALL_NAME_DIR:STRING=\"/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib\" -DCMAKE_INSTALL_LIBDIR:STRING=\"lib\" -DCMAKE_VERBOSE_MAKEFILE=1 -DCMAKE_BUILD_TYPE=Release -DCMAKE_AR=\"/usr/bin/ar\" -DCMAKE_C_COMPILER=\"mpicc\" -DMPI_C_COMPILER=\"/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/bin/mpicc\" -DCMAKE_RANLIB=/usr/bin/ranlib -DCMAKE_C_FLAGS:STRING=\"-fPIC -Wno-lto-type-mismatch -Wno-stringop-overflow -g -O -fopenmp\" -DCMAKE_C_FLAGS_DEBUG:STRING=\"-fPIC -Wno-lto-type-mismatch -Wno-stringop-overflow -g -O -fopenmp\" -DCMAKE_C_FLAGS_RELEASE:STRING=\"-fPIC -Wno-lto-type-mismatch -Wno-stringop-overflow -g -O -fopenmp\" -DCMAKE_CXX_COMPILER=\"mpicxx\" -DMPI_CXX_COMPILER=\"/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/bin/mpicxx\" -DCMAKE_CXX_FLAGS:STRING=\"-Wno-lto-type-mismatch -Wno-psabi -g -O -std=c++17 -fPIC -fopenmp\" -DCMAKE_CXX_FLAGS_DEBUG:STRING=\"-Wno-lto-type-mismatch -Wno-psabi -g -O -std=c++17 -fPIC -fopenmp\" -DCMAKE_CXX_FLAGS_RELEASE:STRING=\"-Wno-lto-type-mismatch -Wno-psabi -g -O -std=c++17 -fPIC -fopenmp\" -DCMAKE_Fortran_COMPILER=\"mpif90\" -DMPI_Fortran_COMPILER=\"/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/bin/mpif90\" -DCMAKE_Fortran_FLAGS:STRING=\"-fPIC -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -g -O -fopenmp -fallow-argument-mismatch\" -DCMAKE_Fortran_FLAGS_DEBUG:STRING=\"-fPIC -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -g -O -fopenmp -fallow-argument-mismatch\" -DCMAKE_Fortran_FLAGS_RELEASE:STRING=\"-fPIC -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -g -O -fopenmp -fallow-argument-mismatch\" -DCMAKE_EXE_LINKER_FLAGS:STRING=\" -fopenmp\" -DBUILD_SHARED_LIBS:BOOL=ON -DUSE_XSDK_DEFAULTS=YES -DTPL_BLAS_LIBRARIES=\"-Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lflapack -Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lfblas -lm -ldl -Wl,-rpath,/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -L/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -lmpifort_gnu_91 -lmpi_gnu_91 -lgfortran -lm -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0 -L/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0 -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib64 -L/opt/cray/pe/gcc/12.2.0/snos/lib64 -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib -L/opt/cray/pe/gcc/12.2.0/snos/lib -lgfortran -lm -lgcc_s -lquadmath\" -DTPL_LAPACK_LIBRARIES=\"-Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lflapack -Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lfblas -lm -ldl -Wl,-rpath,/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -L/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -lmpifort_gnu_91 -lmpi_gnu_91 -lgfortran -lm -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0 -L/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0 -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib64 -L/opt/cray/pe/gcc/12.2.0/snos/lib64 -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib -L/opt/cray/pe/gcc/12.2.0/snos/lib -lgfortran -lm -lgcc_s -lquadmath\" -DTPL_PARMETIS_INCLUDE_DIRS=\"/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/include;/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/include\" -DTPL_PARMETIS_LIBRARIES=\"-Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lparmetis -Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lmetis -lm\" -DXSDK_INDEX_SIZE=64 -DXSDK_ENABLE_Fortran=ON -Denable_tests=0 -Denable_examples=0 -DMPI_C_COMPILE_FLAGS:STRING=\"\" -DMPI_C_INCLUDE_PATH:STRING=\"\" -DMPI_C_HEADER_DIR:STRING=\"\" -DMPI_C_LIBRARIES:STRING=\"\"\nI have noticed the path changes slightly. But I suspect this is a symbolic link. And, previous contribs in the log built just fine so I don't suspect this to be an issue. But can you explain the following:\n/eagle\n# verses\n/lus/eagle",
                          "url": "https://github.com/idaholab/moose/discussions/27379#discussioncomment-9130784",
                          "updatedAt": "2024-04-16T14:13:05Z",
                          "publishedAt": "2024-04-16T14:12:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "smpark7"
                          },
                          "bodyText": "Thank you for looking through the log.\n\nDoes it consistently get stuck there?\n\nYes it always stalls there. If I disable superlu_dist, it stalls while configuring scalapack.\n\nThe log looks fine up to that point to me. What happens when we execute that cmake command manually?\n\nIt stalls after outputting the following:\n-- The C compiler identification is GNU 12.2.0\n-- The CXX compiler identification is GNU 12.2.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/bin/mpicc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/bin/mpicxx - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- The Fortran compiler identification is GNU 12.2.0\n-- Detecting Fortran compiler ABI info\n-- Detecting Fortran compiler ABI info - done\n-- Check for working Fortran compiler: /opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/bin/mpif90 - skipped\n-- SuperLU_DIST will be built as a shared library.\n-- SuperLU_DIST will also be built as a static library.\n-- Using 64 bit integer for index size.\n\n\nI have noticed the path changes slightly. But I suspect this is a symbolic link. And, previous contribs in the log built just fine so I don't suspect this to be an issue. But can you explain the following:\n\nI think /lus/eagle/projects is the path to the actual directory, and it is mounted on /eagle.",
                          "url": "https://github.com/idaholab/moose/discussions/27379#discussioncomment-9133460",
                          "updatedAt": "2024-04-16T17:56:04Z",
                          "publishedAt": "2024-04-16T17:56:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Sorry for the delayed response, doctors appointment...\nI have never seen Cmake stall in such a benign looking way. Lets try to interact with Cmake with its ncruses interface. Might produce an error rather than halt. Run the same command as before, except instead of cmake it will be ccmake:\ncd /lus/eagle/projects/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/externalpackages/git.superlu_dist/petsc-build\n\n\n/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin/ccmake .. -DCMAKE_INSTALL_PREFIX=/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose -DCMAKE_INSTALL_NAME_DIR:STRING=\"/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib\" -DCMAKE_INSTALL_LIBDIR:STRING=\"lib\" -DCMAKE_VERBOSE_MAKEFILE=1 -DCMAKE_BUILD_TYPE=Release -DCMAKE_AR=\"/usr/bin/ar\" -DCMAKE_C_COMPILER=\"mpicc\" -DMPI_C_COMPILER=\"/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/bin/mpicc\" -DCMAKE_RANLIB=/usr/bin/ranlib -DCMAKE_C_FLAGS:STRING=\"-fPIC -Wno-lto-type-mismatch -Wno-stringop-overflow -g -O -fopenmp\" -DCMAKE_C_FLAGS_DEBUG:STRING=\"-fPIC -Wno-lto-type-mismatch -Wno-stringop-overflow -g -O -fopenmp\" -DCMAKE_C_FLAGS_RELEASE:STRING=\"-fPIC -Wno-lto-type-mismatch -Wno-stringop-overflow -g -O -fopenmp\" -DCMAKE_CXX_COMPILER=\"mpicxx\" -DMPI_CXX_COMPILER=\"/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/bin/mpicxx\" -DCMAKE_CXX_FLAGS:STRING=\"-Wno-lto-type-mismatch -Wno-psabi -g -O -std=c++17 -fPIC -fopenmp\" -DCMAKE_CXX_FLAGS_DEBUG:STRING=\"-Wno-lto-type-mismatch -Wno-psabi -g -O -std=c++17 -fPIC -fopenmp\" -DCMAKE_CXX_FLAGS_RELEASE:STRING=\"-Wno-lto-type-mismatch -Wno-psabi -g -O -std=c++17 -fPIC -fopenmp\" -DCMAKE_Fortran_COMPILER=\"mpif90\" -DMPI_Fortran_COMPILER=\"/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/bin/mpif90\" -DCMAKE_Fortran_FLAGS:STRING=\"-fPIC -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -g -O -fopenmp -fallow-argument-mismatch\" -DCMAKE_Fortran_FLAGS_DEBUG:STRING=\"-fPIC -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -g -O -fopenmp -fallow-argument-mismatch\" -DCMAKE_Fortran_FLAGS_RELEASE:STRING=\"-fPIC -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -g -O -fopenmp -fallow-argument-mismatch\" -DCMAKE_EXE_LINKER_FLAGS:STRING=\" -fopenmp\" -DBUILD_SHARED_LIBS:BOOL=ON -DUSE_XSDK_DEFAULTS=YES -DTPL_BLAS_LIBRARIES=\"-Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lflapack -Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lfblas -lm -ldl -Wl,-rpath,/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -L/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -lmpifort_gnu_91 -lmpi_gnu_91 -lgfortran -lm -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0 -L/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0 -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib64 -L/opt/cray/pe/gcc/12.2.0/snos/lib64 -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib -L/opt/cray/pe/gcc/12.2.0/snos/lib -lgfortran -lm -lgcc_s -lquadmath\" -DTPL_LAPACK_LIBRARIES=\"-Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lflapack -Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lfblas -lm -ldl -Wl,-rpath,/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -L/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -lmpifort_gnu_91 -lmpi_gnu_91 -lgfortran -lm -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0 -L/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0 -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib64 -L/opt/cray/pe/gcc/12.2.0/snos/lib64 -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib -L/opt/cray/pe/gcc/12.2.0/snos/lib -lgfortran -lm -lgcc_s -lquadmath\" -DTPL_PARMETIS_INCLUDE_DIRS=\"/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/include;/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/include\" -DTPL_PARMETIS_LIBRARIES=\"-Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lparmetis -Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lmetis -lm\" -DXSDK_INDEX_SIZE=64 -DXSDK_ENABLE_Fortran=ON -Denable_tests=0 -Denable_examples=0 -DMPI_C_COMPILE_FLAGS:STRING=\"\" -DMPI_C_INCLUDE_PATH:STRING=\"\" -DMPI_C_HEADER_DIR:STRING=\"\" -DMPI_C_LIBRARIES:STRING=\"\"\n#                                                                 ^ instead of cmake\nOnce you enter interactive mode, press c to start configuring. See how far we get! Thanks for being patient with me!",
                          "url": "https://github.com/idaholab/moose/discussions/27379#discussioncomment-9142018",
                          "updatedAt": "2024-04-17T12:07:25Z",
                          "publishedAt": "2024-04-17T12:05:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "smpark7"
                          },
                          "bodyText": "I appreciate your help with this issue! With ccmake, it also stalls after the line -- Using 64 bit integer for index size. The progress bar on the bottom is at 12%.",
                          "url": "https://github.com/idaholab/moose/discussions/27379#discussioncomment-9147530",
                          "updatedAt": "2024-04-17T20:32:44Z",
                          "publishedAt": "2024-04-17T20:32:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Well, this next trick is going to require a little fineness;\nWe need two terminals, one to run the cmake command (or ccmake && press c). And another to see all running processes in your name, to try and see just what Cmake is further executing that we might try and troubleshoot. Can you setup the procedure as follows:\ncd /lus/eagle/projects/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/externalpackages/git.superlu_dist/petsc-build\n\n\n/soft/buildtools/cmake/cmake-3.23.2/cmake-3.23.2-linux-x86_64/bin/cmake .. -DCMAKE_INSTALL_PREFIX=/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose -DCMAKE_INSTALL_NAME_DIR:STRING=\"/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib\" -DCMAKE_INSTALL_LIBDIR:STRING=\"lib\" -DCMAKE_VERBOSE_MAKEFILE=1 -DCMAKE_BUILD_TYPE=Release -DCMAKE_AR=\"/usr/bin/ar\" -DCMAKE_C_COMPILER=\"mpicc\" -DMPI_C_COMPILER=\"/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/bin/mpicc\" -DCMAKE_RANLIB=/usr/bin/ranlib -DCMAKE_C_FLAGS:STRING=\"-fPIC -Wno-lto-type-mismatch -Wno-stringop-overflow -g -O -fopenmp\" -DCMAKE_C_FLAGS_DEBUG:STRING=\"-fPIC -Wno-lto-type-mismatch -Wno-stringop-overflow -g -O -fopenmp\" -DCMAKE_C_FLAGS_RELEASE:STRING=\"-fPIC -Wno-lto-type-mismatch -Wno-stringop-overflow -g -O -fopenmp\" -DCMAKE_CXX_COMPILER=\"mpicxx\" -DMPI_CXX_COMPILER=\"/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/bin/mpicxx\" -DCMAKE_CXX_FLAGS:STRING=\"-Wno-lto-type-mismatch -Wno-psabi -g -O -std=c++17 -fPIC -fopenmp\" -DCMAKE_CXX_FLAGS_DEBUG:STRING=\"-Wno-lto-type-mismatch -Wno-psabi -g -O -std=c++17 -fPIC -fopenmp\" -DCMAKE_CXX_FLAGS_RELEASE:STRING=\"-Wno-lto-type-mismatch -Wno-psabi -g -O -std=c++17 -fPIC -fopenmp\" -DCMAKE_Fortran_COMPILER=\"mpif90\" -DMPI_Fortran_COMPILER=\"/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/bin/mpif90\" -DCMAKE_Fortran_FLAGS:STRING=\"-fPIC -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -g -O -fopenmp -fallow-argument-mismatch\" -DCMAKE_Fortran_FLAGS_DEBUG:STRING=\"-fPIC -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -g -O -fopenmp -fallow-argument-mismatch\" -DCMAKE_Fortran_FLAGS_RELEASE:STRING=\"-fPIC -ffree-line-length-none -ffree-line-length-0 -Wno-lto-type-mismatch -g -O -fopenmp -fallow-argument-mismatch\" -DCMAKE_EXE_LINKER_FLAGS:STRING=\" -fopenmp\" -DBUILD_SHARED_LIBS:BOOL=ON -DUSE_XSDK_DEFAULTS=YES -DTPL_BLAS_LIBRARIES=\"-Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lflapack -Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lfblas -lm -ldl -Wl,-rpath,/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -L/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -lmpifort_gnu_91 -lmpi_gnu_91 -lgfortran -lm -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0 -L/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0 -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib64 -L/opt/cray/pe/gcc/12.2.0/snos/lib64 -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib -L/opt/cray/pe/gcc/12.2.0/snos/lib -lgfortran -lm -lgcc_s -lquadmath\" -DTPL_LAPACK_LIBRARIES=\"-Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lflapack -Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lfblas -lm -ldl -Wl,-rpath,/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -L/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -lmpifort_gnu_91 -lmpi_gnu_91 -lgfortran -lm -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0 -L/opt/cray/pe/gcc/12.2.0/snos/lib/gcc/x86_64-suse-linux/12.2.0 -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib64 -L/opt/cray/pe/gcc/12.2.0/snos/lib64 -Wl,-rpath,/opt/cray/pe/gcc/12.2.0/snos/lib -L/opt/cray/pe/gcc/12.2.0/snos/lib -lgfortran -lm -lgcc_s -lquadmath\" -DTPL_PARMETIS_INCLUDE_DIRS=\"/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/include;/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/include\" -DTPL_PARMETIS_LIBRARIES=\"-Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lparmetis -Wl,-rpath,/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -L/eagle/Hybrid-Sn-Diffusion/projects/moltres/moose/petsc/arch-moose/lib -lmetis -lm\" -DXSDK_INDEX_SIZE=64 -DXSDK_ENABLE_Fortran=ON -Denable_tests=0 -Denable_examples=0 -DMPI_C_COMPILE_FLAGS:STRING=\"\" -DMPI_C_INCLUDE_PATH:STRING=\"\" -DMPI_C_HEADER_DIR:STRING=\"\" -DMPI_C_LIBRARIES:STRING=\"\"\nAnd right after the hang begins, in the other terminal execute this command:\nps -ef | grep youruserid >~/ps_outout.log\nPlease scrub this log for any personal/identifiable/sensitive information before posting/attaching. Your free to trim everything you think is not related. But the fact will be Cmake will be executing other commands, and we should include those in the log you post.\nEdit: Oh, and definitely replace youruserid with your actual User ID on the system :)",
                          "url": "https://github.com/idaholab/moose/discussions/27379#discussioncomment-9147612",
                          "updatedAt": "2024-04-17T20:46:48Z",
                          "publishedAt": "2024-04-17T20:46:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "smpark7"
                          },
                          "bodyText": "Here's the log.\nps_output.log",
                          "url": "https://github.com/idaholab/moose/discussions/27379#discussioncomment-9147739",
                          "updatedAt": "2024-04-17T21:04:12Z",
                          "publishedAt": "2024-04-17T21:04:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "oh wow, those are very long lines. I might need more of an expert here. If I recall correctly, there is a limit as to the length of the command... Perhaps this is getting in our way. I even see something a bit odd with one of the last bits of these lines:\n<trimmed> -L/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/lib -lmpi_gnu_91 -I/opt/cray/p\n\n/opt/cray/p doesn't \"feel\" right. Unless that directory exists? Can you check?\n@permcody or @roystgnr I consider very low level Linux dudes. Care to comment on my command line length statement?",
                          "url": "https://github.com/idaholab/moose/discussions/27379#discussioncomment-9147816",
                          "updatedAt": "2024-04-17T21:14:18Z",
                          "publishedAt": "2024-04-17T21:14:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Closer inspection it almost looks like the line is repeating somehow (I don't have an idea for a fix yet)",
                          "url": "https://github.com/idaholab/moose/discussions/27379#discussioncomment-9147828",
                          "updatedAt": "2024-04-17T21:16:47Z",
                          "publishedAt": "2024-04-17T21:16:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "smpark7"
                          },
                          "bodyText": "Yea I think the lines are cut off either by a command length or output length limit. Each line length is 131072, which equals 2^17. /opt/cray/p doesn't exist",
                          "url": "https://github.com/idaholab/moose/discussions/27379#discussioncomment-9147886",
                          "updatedAt": "2024-04-17T21:30:11Z",
                          "publishedAt": "2024-04-17T21:25:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "I don't suppose there is another version of Cmake we can try? Our Conda packages are using 3.28.3 (for reference).\nAlthough, I am doubting its a cmake bug.",
                          "url": "https://github.com/idaholab/moose/discussions/27379#discussioncomment-9147923",
                          "updatedAt": "2024-04-17T21:31:32Z",
                          "publishedAt": "2024-04-17T21:31:32Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Superconductivity in MOOSE",
          "author": {
            "login": "gj19866"
          },
          "bodyText": "Hi there,\nI just wanted to say thank you for all the assistance from the MOOSE team, especially @GiudGiud, in the fruition of my current research.\nIt has been great to see MOOSE applied to the field of superconductivity.\nAll the best,\nGillian\nFinal_report_Final_checks (2).pdf",
          "url": "https://github.com/idaholab/moose/discussions/27413",
          "updatedAt": "2024-04-19T17:03:10Z",
          "publishedAt": "2024-04-18T23:24:53Z",
          "category": {
            "name": "Show and tell"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Thanks for sharing! We are glad to see people pushing MOOSE into new fields!\nIf you made an app or a repo feel free to share a link for posterity. I may have missed it but I didn't see it in your report.",
                  "url": "https://github.com/idaholab/moose/discussions/27413#discussioncomment-9169047",
                  "updatedAt": "2024-04-19T17:03:11Z",
                  "publishedAt": "2024-04-19T17:03:10Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}