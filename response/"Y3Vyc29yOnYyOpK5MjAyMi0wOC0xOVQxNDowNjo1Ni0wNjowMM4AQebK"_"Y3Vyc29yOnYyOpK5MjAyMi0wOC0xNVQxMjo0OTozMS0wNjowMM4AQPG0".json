{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMi0wOC0xNVQxMjo0OTozMS0wNjowMM4AQPG0"
    },
    "edges": [
      {
        "node": {
          "title": "Minimizer in MOOSE?",
          "author": {
            "login": "heinono1"
          },
          "bodyText": "Does MOOSE have a tool or utility for direct minimization? In many problems, the static solution (or asymptotic solution) of the governing equation is an energy minimum, in particular for a large class of problems in which the energy of the system is strictly monotonically non-increasing. If one is interested in the asymptotic stationary solution, one can either time-integrate until the energy does not decrease any more, or do a direct energy minimization. Is there such a direct energy minimization path implemented in MOOSE?",
          "url": "https://github.com/idaholab/moose/discussions/21880",
          "updatedAt": "2022-09-02T20:02:02Z",
          "publishedAt": "2022-08-18T15:10:31Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@lynnmunday might have something for you in isopod",
                  "url": "https://github.com/idaholab/moose/discussions/21880#discussioncomment-3424017",
                  "updatedAt": "2022-08-18T15:23:15Z",
                  "publishedAt": "2022-08-18T15:23:13Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "heinono1"
                  },
                  "bodyText": "Cool - thanks! I'll look into Isopod.\n\nCheers,\nOlle\n\u2026\nOn Thu, Aug 18, 2022 at 10:23 AM Guillaume Giudicelli < ***@***.***> wrote:\n @lynnmunday <https://github.com/lynnmunday> might have something for you\n in isopod\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#21880 (comment)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/AEKZEF4DSIULJW6K7DKFUMLVZZIO3ANCNFSM565RXSYA>\n .\n You are receiving this because you authored the thread.Message ID:\n ***@***.***>\n\n\n-- \nOlle Heinonen\n***@***.***",
                  "url": "https://github.com/idaholab/moose/discussions/21880#discussioncomment-3424201",
                  "updatedAt": "2022-08-18T15:46:05Z",
                  "publishedAt": "2022-08-18T15:46:04Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "Isopod essentially interfaces with PETSc/TAO. You could also use trilinos's ROL.",
                          "url": "https://github.com/idaholab/moose/discussions/21880#discussioncomment-3426036",
                          "updatedAt": "2022-08-18T18:50:44Z",
                          "publishedAt": "2022-08-18T18:50:43Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "It's coming to MOOSE\n#21887",
                  "url": "https://github.com/idaholab/moose/discussions/21880#discussioncomment-3426767",
                  "updatedAt": "2022-08-18T21:10:47Z",
                  "publishedAt": "2022-08-18T21:10:46Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "heinono1"
                  },
                  "bodyText": "Super!\n\u2026\nOn Thu, Aug 18, 2022 at 4:10 PM Guillaume Giudicelli < ***@***.***> wrote:\n It's coming to MOOSE\n #21887 <#21887>\n\n \u2014\n Reply to this email directly, view it on GitHub\n <#21880 (comment)>,\n or unsubscribe\n <https://github.com/notifications/unsubscribe-auth/AEKZEF7R3FGN2TJJ3NOGGT3VZ2RGBANCNFSM565RXSYA>\n .\n You are receiving this because you authored the thread.Message ID:\n ***@***.***>\n\n\n-- \nOlle Heinonen\n***@***.***",
                  "url": "https://github.com/idaholab/moose/discussions/21880#discussioncomment-3426785",
                  "updatedAt": "2022-08-18T21:14:18Z",
                  "publishedAt": "2022-08-18T21:14:17Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How the Jacobian is defined in Moose",
          "author": {
            "login": "Oops-Qiao"
          },
          "bodyText": "Hello,\nToday I just read something in the following link\nhttps://mooseframework.inl.gov/syntax/Kernels/index.html#824e4ebf-e857-4e31-9fbf-290950ee3314\nwhere it mentioned:  The Jacobian, which is the derivative of [Eq. (2)] ...\nI am curious if this is the same way as we studied finite element method course. Or this is just a special way for Jacobian in Moose.\nThanks for your answers.\nBest regards,\nQiao",
          "url": "https://github.com/idaholab/moose/discussions/21876",
          "updatedAt": "2022-09-02T20:02:14Z",
          "publishedAt": "2022-08-18T08:50:39Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "Oops-Qiao"
                  },
                  "bodyText": "If I remember correctly, Jacobian is defined as the derivative of physical domain coordinates with respect to the reference domain.\nWhy the derivative of Eq 2 is equivalent to this Jacobian mapping?",
                  "url": "https://github.com/idaholab/moose/discussions/21876#discussioncomment-3422083",
                  "updatedAt": "2022-08-18T11:24:51Z",
                  "publishedAt": "2022-08-18T11:24:51Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "Yeah, I think this confuses many people.\nWikipedia says this: https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant\nSo Jacobian is a general definition. The three most common \"specializations\" you may find in MOOSE or any FEM packages are\n\nThe derivative of the residual vector w.r.t. all dofs\nThe derivative of the element coordinates w.r.t. the reference element, in the context of parametric mapping.\nThe derivative of the coordinates in the current/deformed configuration (after applying displacements) w.r.t. the coordinates in the reference configuration, in the context of mechanics.",
                  "url": "https://github.com/idaholab/moose/discussions/21876#discussioncomment-3422894",
                  "updatedAt": "2022-08-18T13:20:42Z",
                  "publishedAt": "2022-08-18T13:20:41Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "Yeah, sorry I don't check my school emails very often. I just updated my github profile.",
                          "url": "https://github.com/idaholab/moose/discussions/21876#discussioncomment-3423044",
                          "updatedAt": "2022-08-18T13:37:45Z",
                          "publishedAt": "2022-08-18T13:37:45Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Error message on HPC but no error on local machine",
          "author": {
            "login": "xueyang94"
          },
          "bodyText": "I have a remote branch that I have checked out on both local machine and on HPC (of my University). On the local machine, I can compile and run simulations without issue. On HPC, compiling phase_field-opt receives a warning:\n\n/usr/bin/ld: warning: libpmi.so.0, needed by /apps/mpi/gcc/9.3.0/openmpi/4.1.1/lib64/libmpi_usempif08.so, may conflict with libpmi.so.1\n\nWhen running simulations on HPC, I receive an error below. No error on local machine. Any suggestions on how to remove this error? Thanks.\n\n[c0701a-s3:6130 :0:6130] Caught signal 11 (Segmentation fault: address not mapped to object at address (nil))\n==== backtrace (tid:   6130) ====\n0 0x000000000004ee05 ucs_debug_print_backtrace()  ???:0\n1 0x000000000016ab91 __memcmp_sse4_1()  :0\n2 0x0000000000325208 std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits, std::allocator >, std::pair<std::__cxx11::basic_string<char, std::char_traits, std::allocator > const, std::unique_ptr<libMesh::Parameters::Value, std::default_deletelibMesh::Parameters::Value > >, std::_Select1st<std::pair<std::__cxx11::basic_string<char, std::char_traits, std::allocator > const, std::unique_ptr<libMesh::Parameters::Value, std::default_deletelibMesh::Parameters::Value > > >, std::less, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits, std::allocator > const, std::unique_ptr<libMesh::Parameters::Value, std::default_deletelibMesh::Parameters::Value > > > >::_M_find_tr<std::basic_string_view<char, std::char_traits >, void>()  ???:0\n3 0x00000000005ac051 libMesh::Parameters::have_parameter()  ???:0\n4 0x0000000000a38077 MaterialPropertyInterface::deducePropertyName()  ???:0\n5 0x00000000005f11f4 DerivativeMaterialInterface<JvarMapKernelInterface >::getMaterialPropertyDerivative<double, false>()  ???:0\n6 0x00000000005d5a2f NestKKSSplitCHCRes::NestKKSSplitCHCRes()  ???:0\n7 0x00000000006b2349 Registry::build<NestKKSSplitCHCRes, MooseObject>()  ???:0\n8 0x0000000001b0e4ea Factory::create()  ???:0\n9 0x00000000011c3c5e NonlinearSystemBase::addKernel()  ???:0\n10 0x0000000000de1c8e FEProblemBase::addKernel()  ???:0\n11 0x000000000140056a AddKernelAction::act()  ???:0\n12 0x00000000013f899d Action::timedAct()  ???:0\n13 0x0000000001404ed1 ActionWarehouse::executeActionsWithAction()  ???:0\n14 0x000000000140534f ActionWarehouse::executeAllActions()  ???:0\n15 0x0000000001b1c812 MooseApp::runInputFile()  ???:0\n16 0x0000000001b1feef MooseApp::run()  ???:0\n17 0x0000000000402a0a main()  ???:0\n18 0x0000000000022545 __libc_start_main()  ???:0\n19 0x0000000000402c80 _start()  ???:0\n=================================\nsalloc: Relinquishing job allocation 45222753\nsalloc: Job allocation 45222753 has been revoked.",
          "url": "https://github.com/idaholab/moose/discussions/21866",
          "updatedAt": "2022-08-18T12:50:16Z",
          "publishedAt": "2022-08-17T13:08:08Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "The warning seems unrelated to the backtrace.",
                  "url": "https://github.com/idaholab/moose/discussions/21866#discussioncomment-3415052",
                  "updatedAt": "2022-08-17T13:43:20Z",
                  "publishedAt": "2022-08-17T13:43:19Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "I agree.\nCan you try the following:\n\nrun in debug mode see if you hit an assert\nrun in debug mode with a debugger active and get a full backtrace with line numbers\nreplace this ParsedMaterial with a dummy material and see if it runs ok. If so, we should try to modify the JIT parameters of this material\n\nare you running this in parallel?",
                  "url": "https://github.com/idaholab/moose/discussions/21866#discussioncomment-3417648",
                  "updatedAt": "2022-08-17T20:13:09Z",
                  "publishedAt": "2022-08-17T20:13:08Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "xueyang94"
                          },
                          "bodyText": "I found out that I had a syntax error in a kernel I wrote (NestKKSSplitCHCRes). The problem is solved. Thanks for the debugging info!",
                          "url": "https://github.com/idaholab/moose/discussions/21866#discussioncomment-3422660",
                          "updatedAt": "2022-08-18T12:50:15Z",
                          "publishedAt": "2022-08-18T12:50:14Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "ODE stepper",
          "author": {
            "login": "heinono1"
          },
          "bodyText": "I am using the BDF2 and AB2PredictorCorrector set for time-stepping. I also use postprocessors to monitor values of variables and energies etc. I noted something weird in the Postprocessors. Here is what I have in relevnt blocks:\n[Executioner]\n  type = Transient\n  solve_type = ''PJFNK'\n  scheme = 'BDF2'\n [./TimeStepper]\n   type = AB2PredictorCorrector\n  e_max = 5.e-3\n  e_tol = 5.e-3\n ....\n[../]\n[]\n\nBut the postprocessors return 0 for any execute_on other than 'nonlinear', eg\n[Postprocessors]\n  [./<mx>]\n  type = ElementAverageValue\n  variable = mag_y\n  evaluate_on = 'nonlinear'\n  [../]\n[]\n\nAny other value for evaluate_on gives me zero. This seems to be related to the BDF2/AB2PredictorCorrector. Am I doing something wrong here or missing something?\nThanks,\nOlle",
          "url": "https://github.com/idaholab/moose/discussions/21870",
          "updatedAt": "2022-09-02T20:04:43Z",
          "publishedAt": "2022-08-17T15:35:48Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nShould be execute_on not evaluate_on.\nIs mag_y a nonlinear variable or an auxvariable?\nTime stepping and time integration schemes are independent, you should be able to swap them out almost freely. Could you please see if BDF2 or AB2PC is the problem?\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21870#discussioncomment-3417293",
                  "updatedAt": "2022-08-17T19:10:48Z",
                  "publishedAt": "2022-08-17T19:10:47Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "heinono1"
                          },
                          "bodyText": "Thanks, Guillaume. I did have the correct 'execute_on' in the input file - I incorrectly typed 'evaluate_on' in the posting.\nSwapping crank-nicolson or nothing (default implicit euler) for BDF2 gives errors (unknown time integrator) even though I follow the syntax in https://mooseframework.inl.gov/source/timesteppers/AB2PredictorCorrector.html. So not sure what is going on.\nCheers,\nOlle",
                          "url": "https://github.com/idaholab/moose/discussions/21870#discussioncomment-3418175",
                          "updatedAt": "2022-08-17T22:02:35Z",
                          "publishedAt": "2022-08-17T22:02:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "heinono1"
                          },
                          "bodyText": "Oh, mag_y is a nonlinear variable.",
                          "url": "https://github.com/idaholab/moose/discussions/21870#discussioncomment-3418179",
                          "updatedAt": "2022-08-17T22:03:36Z",
                          "publishedAt": "2022-08-17T22:03:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "You should be able to remove the AB2 and use BDF2 on its own.\nTbh I think AB2 is going to be the problem here",
                          "url": "https://github.com/idaholab/moose/discussions/21870#discussioncomment-3419695",
                          "updatedAt": "2022-08-18T04:44:19Z",
                          "publishedAt": "2022-08-18T04:44:19Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Correctness of ComputeLagrangianWrappedStress",
          "author": {
            "login": "Flolaffel"
          },
          "bodyText": "Hello,\nI recently experimented with ComputeLagrangianWrappedStress and expected that I'd be able to replicate the results I get when using the StressDivergenceTensors-System exactly. But that wasn't the case. I used the standard material model of ComputeFiniteStrain and ComputeFiniteStrainElasticStress. The input is as follows\n[Mesh]\n  type = GeneratedMesh\n  dim = 3\n  nx = 3\n  ny = 3\n  nz = 3\n  elem_type = HEX8\n[]\n\n[GlobalParams]\n  displacements = 'disp_x disp_y disp_z'\n[]\n\n[Modules/TensorMechanics/Master]\n  [all]\n    strain = FINITE\n    add_variables = true\n    generate_output = 'stress_xx stress_yy stress_zz stress_xy'\n  []\n[]\n\n[Functions]\n  [tdisp]\n    type = ParsedFunction\n    value = '0.5 * t' \n  []\n  [tdisp_quer]\n    type = ParsedFunction\n    value = '0.5 * y * t' \n  []\n[]\n\n[BCs]\n  [bottom_y]\n    type = DirichletBC\n    variable = disp_y\n    boundary = bottom\n    value = 0\n  []\n  [bottom_x]\n    type = DirichletBC\n    variable = disp_x\n    boundary = bottom\n    value = 0\n  []\n  [bottom_z]\n    type = DirichletBC\n    variable = disp_z\n    boundary = bottom\n    value = 0\n  []\n  [left_x]\n    type = DirichletBC\n    variable = disp_x\n    boundary = left\n    value = 0\n  []\n  [right_x]\n    type = DirichletBC\n    variable = disp_x\n    boundary = right\n    value = 0\n  []\n  [front_z]\n    type = DirichletBC\n    variable = disp_z\n    boundary = front\n    value = 0\n  []\n  [back_z]\n    type = DirichletBC\n    variable = disp_z\n    boundary = back\n    value = 0\n  []\n  [tdisp]\n    type = FunctionDirichletBC\n    variable = disp_y\n    boundary = top\n    function = tdisp\n  []\n[]\n\n[Materials]\n  [elasticity]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 30\n    poissons_ratio = 0.4\n  []\n  [stress]\n    type = ComputeFiniteStrainElasticStress\n  []\n[]\n\n[Preconditioning]\n  [smp]\n    type = SMP\n    full = true\n    petsc_options_iname = '-pc_type'\n    petsc_options_value = 'lu'\n  []\n[]\n\n[Executioner]\n  type = Transient\n  solve_type = Newton\n  end_time = 1\n  dt = 0.25\n[]\n\n[Outputs]\n  exodus = true\n  csv = true\n[]\n\n[Postprocessors]\n  [cauchy_xx]\n    type = ElementAverageValue\n    variable = stress_xx\n    execute_on = 'initial timestep_end'\n  []\n  [cauchy_yy]\n    type = ElementAverageValue\n    variable = stress_yy\n    execute_on = 'initial timestep_end'\n  []\n  [cauchy_xy]\n    type = ElementAverageValue\n    variable = stress_xy\n    execute_on = 'initial timestep_end'\n  []\n  [cauchy_zz]\n    type = ElementAverageValue\n    variable = stress_zz\n    execute_on = 'initial timestep_end'\n  []\n[]\n\nThe corresponding input with lagrangian kernel system is\n[Mesh]\n  type = GeneratedMesh\n  dim = 3\n  nx = 3\n  ny = 3\n  nz = 3\n  elem_type = HEX8\n[]\n\n[GlobalParams]\n  displacements = 'disp_x disp_y disp_z'\n  #large_kinematics = true\n[]\n\n[Modules/TensorMechanics/Master]\n  [all]\n    new_system = true\n    formulation = TOTAL\n    strain = FINITE\n    add_variables = true\n    generate_output = 'cauchy_stress_xx cauchy_stress_yy cauchy_stress_zz cauchy_stress_xy'\n  []\n[]\n\n[Functions]\n  [tdisp]\n    type = ParsedFunction\n    value = '0.5 * t' \n  []\n  [tdisp_quer]\n    type = ParsedFunction\n    value = '0.5 * y * t' \n  []\n[]\n\n[BCs]\n  [bottom_y]\n    type = DirichletBC\n    variable = disp_y\n    boundary = bottom\n    value = 0\n  []\n  [bottom_x]\n    type = DirichletBC\n    variable = disp_x\n    boundary = bottom\n    value = 0\n  []\n  [bottom_z]\n    type = DirichletBC\n    variable = disp_z\n    boundary = bottom\n    value = 0\n  []\n  [left_x]\n    type = DirichletBC\n    variable = disp_x\n    boundary = left\n    value = 0\n  []\n  [right_x]\n    type = DirichletBC\n    variable = disp_x\n    boundary = right\n    value = 0\n  []\n  [front_z]\n    type = DirichletBC\n    variable = disp_z\n    boundary = front\n    value = 0\n  []\n  [back_z]\n    type = DirichletBC\n    variable = disp_z\n    boundary = back\n    value = 0\n  []\n  [tdisp]\n    type = FunctionDirichletBC\n    variable = disp_y\n    boundary = top\n    function = tdisp\n  []\n[]\n\n[Materials]\n  [elasticity]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 30\n    poissons_ratio = 0.4\n  []\n  [stress]\n    type = ComputeLagrangianWrappedStress\n  []\n  [stress_base]\n    type = ComputeFiniteStrainElasticStress\n  []\n[]\n\n[Preconditioning]\n  [smp]\n    type = SMP\n    full = true\n    petsc_options_iname = '-pc_type'\n    petsc_options_value = 'lu'\n  []\n[]\n\n[Executioner]\n  type = Transient\n  solve_type = Newton\n  end_time = 1\n  dt = 0.25\n[]\n\n[Outputs]\n  exodus = true\n  csv = true\n[]\n\n[Postprocessors]\n  [cauchy_xx]\n    type = ElementAverageValue\n    variable = cauchy_stress_xx\n    execute_on = 'initial timestep_end'\n  []\n  [cauchy_yy]\n    type = ElementAverageValue\n    variable = cauchy_stress_yy\n    execute_on = 'initial timestep_end'\n  []\n  [cauchy_xy]\n    type = ElementAverageValue\n    variable = cauchy_stress_xy\n    execute_on = 'initial timestep_end'\n  []\n  [cauchy_zz]\n    type = ElementAverageValue\n    variable = cauchy_stress_zz\n    execute_on = 'initial timestep_end'\n  []\n[]\n\nNot using large_kinematics in the GlobalParams yields results that are closer to the previous input but still not quite equal. Is the behaviour of not being able to replicate results exactly expected?\nFurthermore I noticed that both inputs seem to react to the timestep size. Changing the timestep size changes the stresses in the output. I feel like that shouldn't be the case. What am I missing?",
          "url": "https://github.com/idaholab/moose/discussions/21684",
          "updatedAt": "2022-09-02T20:04:57Z",
          "publishedAt": "2022-07-26T04:50:27Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "reverendbedford"
                  },
                  "bodyText": "When using the ComputeLagrangianWrappedStress class you should always use a small deformation MOOSE material model.  So in your case you should use ComputeLinearElasticStress.  The ComputeLagrangianWrappedStress \"takes care\" of making the small deformation model respond in a large deformations context by advecting the response with an objective stress rate.\nYou should always have the stress calculator use large_kinematics when you're using finite strain, as the Jacobian won't be correct otherwise.\nEven with these changes you won't get an exact match between the old and new models, as we are using a different objective rate.\nThere is a PR to add the Green-Naghdi objective rate.  If you use that rate (instead of the default Truesdell rate) and take small time steps you should closely match the old formulation.",
                  "url": "https://github.com/idaholab/moose/discussions/21684#discussioncomment-3231120",
                  "updatedAt": "2022-07-26T13:18:05Z",
                  "publishedAt": "2022-07-26T13:18:04Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Flolaffel"
                          },
                          "bodyText": "Thanks for your reply. Right now I can say:\n\nWhen using ComputeLinearElasticStress instead of ComputeFiniteStrainElasticStress in my second input I just get an error that tells me to use  ComputeFiniteStrainElasticStress instead. What else do I have to do?\nOk, that's what I initially thought too.\nand 4. I will read into that, thank you.",
                          "url": "https://github.com/idaholab/moose/discussions/21684#discussioncomment-3250980",
                          "updatedAt": "2022-07-26T19:33:50Z",
                          "publishedAt": "2022-07-26T19:33:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "reverendbedford"
                          },
                          "bodyText": "For the second input file?\nTo be clear: \"Lagrangian\" kernels need small deformation (ComputeLinearElasticStress) + ComputeLagrangianWrappedStress.  Old kernels need  ComputeFiniteStrainElasticStress.",
                          "url": "https://github.com/idaholab/moose/discussions/21684#discussioncomment-3251526",
                          "updatedAt": "2022-07-26T19:37:04Z",
                          "publishedAt": "2022-07-26T19:37:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "reverendbedford"
                          },
                          "bodyText": "Nevermind this seems to be a bug.  I'll fix it.",
                          "url": "https://github.com/idaholab/moose/discussions/21684#discussioncomment-3251713",
                          "updatedAt": "2022-07-26T19:38:44Z",
                          "publishedAt": "2022-07-26T19:38:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "reverendbedford"
                          },
                          "bodyText": "I partly lied: you can use ComputeFiniteStrainElasticStress because we zero out the rotations being fed into the original moose material update.  However you should be able to use ComputeLinearElasticStress both practically and logically.",
                          "url": "https://github.com/idaholab/moose/discussions/21684#discussioncomment-3273470",
                          "updatedAt": "2022-07-28T20:36:36Z",
                          "publishedAt": "2022-07-28T20:36:35Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "reverendbedford"
                          },
                          "bodyText": "Okay, fix is here, #21721",
                          "url": "https://github.com/idaholab/moose/discussions/21684#discussioncomment-3274867",
                          "updatedAt": "2022-07-28T21:05:26Z",
                          "publishedAt": "2022-07-28T21:05:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Flolaffel"
                          },
                          "bodyText": "Thanks, now ComputeLinearElasticStress works and delivers the same results as ComputeFiniteStrainElasticStress.\nI have a follow-up question though: In you earlier comment you said to use the Green-Naghdi rate to match the results from the StressDivergenceTensors-System closely. But upon reading Rashid (1993) that is referenced in your ComputeFiniteStrain documentation it seems to me that the StressDivergenceTensors-System should use the Jaumann rate. Am I wrong?",
                          "url": "https://github.com/idaholab/moose/discussions/21684#discussioncomment-3378104",
                          "updatedAt": "2022-08-11T18:23:04Z",
                          "publishedAt": "2022-08-11T18:23:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "reverendbedford"
                          },
                          "bodyText": "My understanding is they advect with the rotation (R).  But I see the paper claims it's an implementation of the Jaumann rate.\nI guess one solution is to run the same model with the \"old\" and \"new\" systems, using both rates for the \"new\" system and see which one is closest.",
                          "url": "https://github.com/idaholab/moose/discussions/21684#discussioncomment-3415249",
                          "updatedAt": "2022-08-17T14:11:45Z",
                          "publishedAt": "2022-08-17T14:11:45Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "A question about Heat conduction solve using displaced mesh",
          "author": {
            "login": "xuxiaobei1995"
          },
          "bodyText": "Hi all,\nI'm confused about an issue on the heat conduction solve using displaced mesh. Take a simple thermo-mechanical coupled case as an example:\n\nIt is an elongated rectangle with a uniform heat source; the geometry and condition is shown in the figure above. For this case, after the elongation, the analytical solution of the left side temperature is 2 (Tleft - Tright  = 0.5 * qv * conduction_length / k = 1). However, the result from MOOSE is 2.346. If I set use_displaced_mesh = true in the heat source kernel, the result is 3, which is still different from the analytical solution. Does any one know how these results are computed? The input file of this case is shown below, which can be run in any application including the heat conduction & tensor mechanics modules.\n\n[GlobalParams]\n  displacements = 'disp_x disp_y'\n[]\n\n[Mesh]\n  type = GeneratedMesh\n  dim = 2\n  nx = 10\n  ny = 10\n  xmin = 0\n  xmax = 1\n  ymin = 0\n  ymax = 1\n[]\n\n[Variables]\n  [temp]\n  []\n[]\n\n[Kernels]\n  [heatconduction]\n    type = HeatConduction\n    variable = temp\n  []\n  [heatsource]\n    type = HeatSource\n    variable = temp\n    value = 1\n    #use_displaced_mesh = true\n  []\n[]\n\n[Modules/TensorMechanics/Master]\n  [./tm]\n    strain = FINITE\n    add_variables = true\n    decomposition_method = EigenSolution\n    use_displaced_mesh = true\n  [../]\n[]\n\n[Materials]\n  [./elasticity_tensor]\n    type = ComputeIsotropicElasticityTensor\n    youngs_modulus = 2.1e5\n    poissons_ratio = 0.3\n  [../]\n  [./stress]\n    type = ComputeFiniteStrainElasticStress\n  [../]\n  [./hc_mat]\n    type = HeatConductionMaterial\n    thermal_conductivity = 1.0\n  []\n[]\n\n[BCs]\n  [./left]\n    type = PresetBC\n    variable = disp_x \n    boundary = left\n    value = 0.0\n  [../]\n  [./bottom]\n    type = PresetBC\n    variable = disp_y \n    boundary = bottom\n    value = 0.0\n  [../]\n  [./right]\n    type = FunctionPresetBC\n    variable = disp_x \n    boundary = right\n    function = '0.02*t'\n  []\n  [./right_temp]\n    type = PresetBC\n    variable = temp\n    boundary = right\n    value = 1.0\n  []\n[]\n\n[Preconditioning]\n  [./SMP]\n    type = SMP\n    full = true\n  [../]\n[]\n\n[Executioner]\n  type = Transient\n  dt = 1\n  end_time = 50\n\n  solve_type = 'PJFNK'\n\n  petsc_options_iname = '-pc_type -sub_pc_type -pc_asm_overlap -ksp_gmres_restart'\n  line_search = none\n  petsc_options_value = 'asm lu 20 101'\n[]\n\n[Outputs]\n  exodus = true\n  csv = true\n[]",
          "url": "https://github.com/idaholab/moose/discussions/21451",
          "updatedAt": "2022-08-17T08:59:41Z",
          "publishedAt": "2022-06-29T08:34:56Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "T,xx + 1 = 0\nT = -0.5x^2 + ax + b\nT(2) = 1, so 2a + b = 3\nT,x(0) = 0, so a = 0, b = 3\nT = -0.5x^2 + 3\nT(0) = 3\n\nCheers",
                  "url": "https://github.com/idaholab/moose/discussions/21451#discussioncomment-3047437",
                  "updatedAt": "2022-06-29T10:41:52Z",
                  "publishedAt": "2022-06-29T10:41:51Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "xuxiaobei1995"
                          },
                          "bodyText": "Sorry, I just found I made a mistake when solving the analytical solution... T(0) = 3 is the right result. Big thanks to you!",
                          "url": "https://github.com/idaholab/moose/discussions/21451#discussioncomment-3048120",
                          "updatedAt": "2022-06-29T12:32:55Z",
                          "publishedAt": "2022-06-29T12:32:54Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "js-jixu"
                          },
                          "bodyText": "I'm also studying thermomechanical coupling now, and I have some questions. Can I connect with you? My email address is on my homepage.",
                          "url": "https://github.com/idaholab/moose/discussions/21451#discussioncomment-3412965",
                          "updatedAt": "2022-08-17T08:59:42Z",
                          "publishedAt": "2022-08-17T08:59:41Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Basic question related to coupling in kernels",
          "author": {
            "login": "KhaledNabilSharafeldin"
          },
          "bodyText": "Hello,\nI have a very basic question related to  params.addRequiredCoupledVar in kernels.\nI am currently passing more variables than what actually being used by the kernel using this param. I briefly looked into the code for it and it seems like it's accumulating dependency list, so I am imagining that the code would assume that all the variables are dependent on one another,\nSo my question is would this lead to a slow down or higher memory usage for my app? and if yes, is there a way to only read the variable value without coupling it? like passing it as a vector of variable names and use &coupledValue() funciton?\nThanks in advance,\nKhaled",
          "url": "https://github.com/idaholab/moose/discussions/21796",
          "updatedAt": "2022-09-02T20:11:17Z",
          "publishedAt": "2022-08-08T13:55:02Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nIf you dont couple it, then you risk the system not updating the coupled value (re-initializing it on every new quadrature point) when you try to use it. In general, if you need it you should couple it. And if you dont, you dont need to pass it in the arguments in the input file. You can always pass a constant in place of a variable if you dont plan to use that variable.\nThere wont be an increased memory usage.\nThere could be a slow down but it's very unlikely to be significant. As to all matters of performance, first make it work, then profile it if you need more performance. So I would not worry about this at this point.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21796#discussioncomment-3351702",
                  "updatedAt": "2022-08-08T18:03:07Z",
                  "publishedAt": "2022-08-08T18:03:06Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "KhaledNabilSharafeldin"
                          },
                          "bodyText": "I have done some profiling, but nothing sticks out to me as out of place. Any recommendations would be really valuable. The code is both slow and consuming a lot of memory.\n\nMemory\n\n\n\nCPU",
                          "url": "https://github.com/idaholab/moose/discussions/21796#discussioncomment-3351820",
                          "updatedAt": "2022-08-08T18:23:13Z",
                          "publishedAt": "2022-08-08T18:23:12Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Seems the numerical solve is costing most of the time and the memory\nCould you please paste your executioner block here?",
                          "url": "https://github.com/idaholab/moose/discussions/21796#discussioncomment-3351931",
                          "updatedAt": "2022-08-08T18:44:02Z",
                          "publishedAt": "2022-08-08T18:44:01Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "KhaledNabilSharafeldin"
                          },
                          "bodyText": "Might also want to add that during jacobian calculation in the subapp1, memory usage doubles in size\n\n Mainapp \n[Executioner]\n  type = Transient\n\n  solve_type = 'newton'\n  line_search = none\n     petsc_options = '-ksp_monitor'\n\n\n  petsc_options_iname = '-pc_type'\n  petsc_options_value = 'lu'\n  # petsc_options_iname = '-pc_type -pc_hypre_type  -pc_hypre_boomeramg_strong_threshold  -pc_hypre_boomeramg_agg_nl  -pc_hypre_boomeramg_agg_num_paths -pc_hypre_boomeramg_max_levels  -pc_hypre_boomeramg_coarsen_type  -pc_hypre_boomeramg_interp_type -pc_hypre_boomeramg_P_max -pc_hypre_boomeramg_truncfactor '\n  # petsc_options_value = 'hypre    boomeramg       0.7                                   4                           5                                 25                              HIMS                              ext+i                           2                         0.3                             '\n  # -pc_type hypre -pc_hypre_type boomeramg -pc_hypre_boomeramg_strong_threshold 0.7  -pc_hypre_boomeramg_agg_nl 4 -pc_hypre_boomeramg_agg_num_paths 5 -pc_hypre_boomeramg_max_levels 25 -pc_hypre_boomeramg_coarsen_type HMIS -pc_hypre_boomeramg_interp_type ext+i -pc_hypre_boomeramg_P_max 2 -pc_hypre_boomeramg_truncfactor 0.3\n  l_max_its = 2\n  l_tol = 1e-14\n  nl_max_its = 20\n  nl_rel_tol = 1e-8\n  nl_abs_tol = 1e-5\n\n  start_time = 6500.0\n  # end_time = 1000000.0\n  num_steps = 10\n  [./TimeStepper]\n    type = TransientTransport\n    postprocessor = rss_max\n    cfl = 0.5\n    dt = 1e-6\n    dt_max1 = 100\n    dt_max2 = 100\n    n = 12\n    n_slip = 12\n  [../]  \n[]\n\n\nSubapp1\n[Executioner]\n  type = Transient\n  solve_type = 'linear'                  # PJFNK JFNK NEWTON FD LINEAR\n  l_tol = 1.0e-5                         # Linear Tolerance         : 1.0e-5\n  l_abs_tol = 1.0e-8\n  l_max_its = 500                       # Max Linear Iterations    : 10000\n  # petsc_options_iname = '-pc_type -pc_hypre_type -ksp_gmres_restart -pc_hypre_boomeramg_strong_threshold'\n  # petsc_options_value = 'hypre    boomeramg      31                 0.7'\n   petsc_options_iname = '-pc_type -sub_pc_type -sub_pc_factor_shift_type '\n   petsc_options_value = 'asm      ilu           nonzero                  '\n   \n  # petsc_options_iname = '-pc_type -ksp_gmres_restart -sub_pc_type -sub_pc_factor_shift_type'\n  # petsc_options_value = 'asm      200                ilu           NONZERO' \n  # scheme = explicit-euler\n  # start_time = 55000\n  # dt = 20\n  [./TimeStepper]\n    type = TransientTransport\n    postprocessor = rss_max\n    cfl = 0.45\n    dt = 20\n    dt_max1 = 20\n    dt_max2 = 10\n    n_slip = 12\n    dt_time_change = 360\n  [../] \n  # reset_dt = true\n\n  # num_steps = 1000\n\n  automatic_scaling = true\n  compute_scaling_once = true \n  # end_time = 30\n  # num_steps = 20\n[]\n\n\nSubapp2\n[Executioner]\n  type = Transient\n  #solve_type = 'LINEAR'                  # PJFNK JFNK NEWTON FD LINEAR\n  #l_tol = 1.0e-11                         # Linear Tolerance         : 1.0e-5\n  #l_max_its = 10000                       # Max Linear Iterations    : 10000\n  # petsc_options_iname = '-pc_type   -pc_hypre_type  -pc_hypre_boomeramg_strong_threshold'\n  # petsc_options_value = 'hypre      boomeramg       0.7'\n  # start_time = 55000\n\n  # start_time = 75\n\n  # petsc_options_iname = '-pc_type -sub_pc_type -sub_pc_factor_shift_type '\n  # petsc_options_value = 'asm      ilu           nonzero                  '\n  # automatic_scaling = true\n  # compute_scaling_once = false \n  # num_steps = 1\n  # dt = 0.5\n  # dt_max = 0.1\n  [./TimeIntegrator]\n    # type = ImplicitEuler\n    # type = BDF2\n    #type = CrankNicolson\n    # type = ImplicitMidpoint\n    # type = LStableDirk2\n    # type = LStableDirk3\n    # type = LStableDirk4\n    # type = AStableDirk4\n    #\n    # Explicit methods\n    type = ActuallyExplicitEuler\n    # type = ExplicitEuler\n    #type = ExplicitMidpoint\n    # type = Heun\n    # type = Ralston\n  [../]\n  # [./TimeStepper]\n  #   type = TransientTransport\n  #   postprocessor = rss_maxs\n  #   cfl = 0.45\n  #   dt = 1e-6\n  #   dt_max = 50\n  #   n_slip = 12\n  # [../] \n  # scheme = implicit-euler\n[]",
                          "url": "https://github.com/idaholab/moose/discussions/21796#discussioncomment-3351987",
                          "updatedAt": "2022-08-08T18:59:26Z",
                          "publishedAt": "2022-08-08T18:54:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Do you get decent performance out of the subapps?\nFor the main app,\n  petsc_options_iname = '-pc_type'\n  petsc_options_value = 'lu'\n\nthis is a direct solve and it's expensive for large systems. This is the Mat_Aij operations in the CPU profiling iirc.\nYou should try a more scalable preconditioner if you want to see large gains in memory and CPU time\nFor a small improvement,\nYou may also try using a different solver package for LU, see if you get better performance. We have found strumpack to outperform the others quite often. Use the -pc_factor_mat_solver_package petsc option (in iname) and strumpack for the value.",
                          "url": "https://github.com/idaholab/moose/discussions/21796#discussioncomment-3354062",
                          "updatedAt": "2022-08-09T01:47:53Z",
                          "publishedAt": "2022-08-09T01:47:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "KhaledNabilSharafeldin"
                          },
                          "bodyText": "Do you get decent performance out of the subapps?\n\nYes! any suggestions for the more scalable preconditioners?\nand I'll give strumpack a shot and see how that goes.\nThanks again!\nBest,\nKhaled",
                          "url": "https://github.com/idaholab/moose/discussions/21796#discussioncomment-3354488",
                          "updatedAt": "2022-08-09T03:33:51Z",
                          "publishedAt": "2022-08-09T03:33:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "have you tried asm with an ilu sub pc?\nwhat about field splits? (there's a bug on schur right now, I m working on fixing it)",
                          "url": "https://github.com/idaholab/moose/discussions/21796#discussioncomment-3354547",
                          "updatedAt": "2022-08-09T03:58:45Z",
                          "publishedAt": "2022-08-09T03:58:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "KhaledNabilSharafeldin"
                          },
                          "bodyText": "I think i've tried both asm and ilu, I am currently testing more with them.\nHowever, currently I'm using a custom mesh by generating it in a main file and reading the .e file output. The mesh uses 2 different mesh elements PYR5 and TET. I switched this up to only PYR5 generated by moose and right away I'm getting substantially lower memory usage. I am still testing and finding the PC that works best but the mesh change made it solvable at the moment.\nDo you have any reasoning why would the different meshes would make such a substantial change? mind you, they are approximately the same number of elements.\nThanks again @GiudGiud for recommendations and solutions!\nBest,\nKhaled",
                          "url": "https://github.com/idaholab/moose/discussions/21796#discussioncomment-3376329",
                          "updatedAt": "2022-08-11T14:44:50Z",
                          "publishedAt": "2022-08-11T14:43:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hard to know. Sometimes meshes are more true to the physics (think a hex mesh aligned with a fluid flow for example) and you get better solutions. I dont know about the speed of the solve though!\n@roystgnr might have some insights on this",
                          "url": "https://github.com/idaholab/moose/discussions/21796#discussioncomment-3376490",
                          "updatedAt": "2022-08-11T14:54:24Z",
                          "publishedAt": "2022-08-11T14:54:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "roystgnr"
                          },
                          "bodyText": "In case others come across this later, let me talk about the more common phenomena first:\nMesh changes to be more true to the physics can greatly speed up a solve on a difficult nonlinear problem by getting you to a solution in fewer quasiNewton steps - e.g. in a compressible flow, a hex mesh that's not aligned well enough with a shock can be slower to solve on or can be impossible to solve on depending on your formulation.\nIf you're not using either an overkill preconditioner like LU or a horribly weak preconditioner like Jacobi, I'm afraid that even seemingly trivial changes to a mesh can lead to unexpected changes in iteration counts and solve speeds.  The most runtime-efficient preconditioners take your mesh partitioning into account, with something like a Schwarz method or Block Jacobi method (for communication between processors) wrapped around a stronger preconditioner within each subpartition's diagonal block of the matrix ... and that means the strength of your preconditioner as a whole strongly depends on your partitioning.  That's a reasonable practical concern to worry about, but unfortunately it combines with a less reasonable practical concern: mesh partitioning is kind of a black art.  ParMETIS (our default for DistributedMesh) can sometimes give weird results, METIS (our default for ReplicatedMesh) can sometimes give even weirder results (despite having easier constraints on the algorithm; I don't understand why myself), and even our simple alternatives like space-filling-curves can give results very dependent on mesh geometry and processor count.  With a worse partitioning (less-\"compact\" partitions with more \"surface\" at the interfaces between them) you get a double-whammy - the reduced compactness makes your sub-preconditioners less effective so you need more iterations, and the increased surface means more communication between blocks so each iteration takes longer to boot.\nBut that all said ...\nyour problem isn't solve speed, it's solve memory, isn't it?  That's much more surprising to me.  There's a minor effect where going from tets to pyramids (on the same number of nodes) increases your sparsity pattern and so increases your memory requirements (both for the matrix itself and for ILU sub-preconditioners), and of course in the other direction a pyramid Mesh itself takes less memory since it's got fewer elements for the same number of nodes.  I'm not sure what else could be important.  I guess if a mesh change takes your preconditioner from \"really good\" to \"mediocre\", that can lead to an increased number of Krylov vectors being saved (by methods like GMRES, the PETSc default, which save them) and that would take more memory, but unless you're manually changing ksp_gmres_restart it'll throw out and reconstruct those vectors after every 30 iterations, so the memory requirements can't get out of control.\nI'd love to see a before-and-after of memory usage graphs; there's got to be some factor I'm missing.",
                          "url": "https://github.com/idaholab/moose/discussions/21796#discussioncomment-3377789",
                          "updatedAt": "2022-08-11T17:47:28Z",
                          "publishedAt": "2022-08-11T17:47:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "KhaledNabilSharafeldin"
                          },
                          "bodyText": "@roystgnr thank you very much for the elaborate explanation! this kind of extended replies help a lot understanding the different systems better.\nThe issue with my application first and foremost is memory usage, then speed. I am mostly drawing comparisons in memory usage and performance based on a similar code/model that we have written using PETSc in fortran. The ultimate goal is to get the MOOSE application up to speed and functionality of the previous one.\nThe mesh size I am trying to run is about 3.8-4.2 million elements, do you have a rough estimate how much memory would be needed to run a similar mesh? We have 3 multiapps that share the same mesh and about a 100 of Moose's \"nonlinear variables\", which are not necessarily nonlinear or directly coupled. Currently I've tested on a single highmemory cluster node that has 128 cores and 1TB of RAM and it still exceeded that limit.\n\nI think i've tried both asm and ilu, I am currently testing more with them. However, currently I'm using a custom mesh by generating it in a main file and reading the .e file output. The mesh uses 2 different mesh elements PYR5 and TET. I switched this up to only PYR5 generated by moose and right away I'm getting substantially lower memory usage. I am still testing and finding the PC that works best but the mesh change made it solvable at the moment.\nDo you have any reasoning why would the different meshes would make such a substantial change? mind you, they are approximately the same number of elements.\nThanks again @GiudGiud for recommendations and solutions!\nBest, Khaled\n\nalso regarding this, it did indeed reduce memory usage, however the increase was not as drastic as I initially seen since I got confused by DistributedRectilinearMeshGenerator which allows for PYRAMID type elements but instead defaults for HEX elements without any warnings or errors, so I used instead GeneratedMeshGenerator in distributed.\nThanks again @GiudGiud and @roystgnr!\nBest,\nKhaled",
                          "url": "https://github.com/idaholab/moose/discussions/21796#discussioncomment-3407059",
                          "updatedAt": "2022-08-16T14:20:45Z",
                          "publishedAt": "2022-08-16T14:20:45Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "ArrayDiffusion and ArrayDGDiffusion",
          "author": {
            "login": "dingqiushi"
          },
          "bodyText": "Why is it necessary to distinguish between diagonal and non-diagonal elements when calculating Jacobi matrices in ArrayDiffusion.C, and why is it necessary to use two functions, computeQpJacobian() and computeQpOffDiagJacobian()? Why is there no computeQpOffDiagJacobian()  in ArrayDGDiffusion.C, only computeQpJacobian() ?",
          "url": "https://github.com/idaholab/moose/discussions/21857",
          "updatedAt": "2022-08-20T07:34:30Z",
          "publishedAt": "2022-08-16T13:38:04Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nwhen there is no off-diagonal coefficients, we dont implement this routine. Are we missing a coefficient for that kernel?\nIt s necessary to have the contributions split between diagonal and off-diagonal because for preconditioning we sometimes only use the diagonal of the Jacobian instead of the full Jacobian. Having this separate routine allows us to form this preconditioning.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21857#discussioncomment-3406799",
                  "updatedAt": "2022-08-16T13:50:59Z",
                  "publishedAt": "2022-08-16T13:50:59Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Meaning of TaggingInterface",
          "author": {
            "login": "matthiasneuner"
          },
          "bodyText": "Hi, I would like to understand the purpose and the function of the TaggingInterface.\nWhat is the purpose and the meaning? What is a tag? Why is there a need for a tag? Unfortunately, the doxygen documentation is not very helpful in this regard ..\nAlso, I cannot really understand how it works.\nTo me it seems that _re_blocks and _ke_blocks are populated by kernels, but I cannot see at which stage and by which module those attributes are accessed. That means, I cannot see when and where does attributes are accessed during the final assembly of the equation system.\nWhen should the TaggingInterface be used, and why are some modules (e.g., NodalConstraint) not using it?\nThank you in advance!",
          "url": "https://github.com/idaholab/moose/discussions/21855",
          "updatedAt": "2022-09-02T20:11:22Z",
          "publishedAt": "2022-08-16T03:49:43Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nMy understanding is that it's meant for two things:\n\neasy storage and retrieval of vectors (fields) in the system class. I used vector tags for saving the iterates in PicardSolve and SecantSolve\nrelatedly, a way to store the contribution of a single kernel to a residual to examine it for debugging\n\nSome modules are not using it because they predate the interface most likely, and there was not necessarily funding to take on a refactor.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21855#discussioncomment-3403148",
                  "updatedAt": "2022-08-22T04:17:59Z",
                  "publishedAt": "2022-08-16T04:03:23Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Error after updating env: \"missing symbol called\"",
          "author": {
            "login": "marinasessim"
          },
          "bodyText": "Hello,\nI'm getting an error on an older app after updating the moose environment. I get this error when I run any input file or the tests.\n\"dyld[xxxxx]: missing symbol called\"\nI cleaned up before recompiling by running \"make clobberall\", which was marked as the solution for this error on a similar thread.\nI'm attaching the outputs of what I think is the equivalent of \"ldd executable\" on a mac (otool -L).\nPlease let me know if there is any other information that can help identify the source. The application is a few months old, please let me know if there are deprecated functions/syntax/anything that could be causing this error.\nThanks for your attention!\nmacaw-opt_output.txt",
          "url": "https://github.com/idaholab/moose/discussions/21703",
          "updatedAt": "2022-09-02T20:11:33Z",
          "publishedAt": "2022-07-27T16:09:20Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nmake clobberall is often insufficient\nwe tend to use git clean -xfd these days\nplease make sure to commit ALL your work before running this command.\nWhat is macaw? What does it do?\nWe have a controlled app with the same name, so I'd just like to check\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21703#discussioncomment-3263469",
                  "updatedAt": "2022-07-27T18:14:59Z",
                  "publishedAt": "2022-07-27T18:14:58Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "marinasessim"
                          },
                          "bodyText": "Hi Guillaume,\nUnfortunately \"git clean -xfd\" did not do the trick either.\nI updated the environment \"conda update --all\" and rebased moose again. It compiles and no test fails. I did notice a warning this time, it shows up at compilation time (same for Macaw):\n(moose) [msessim][~/projects/macaw]> make -j 4\nChecking if conda packages exist...\nChecking if libmesh version is up to date...\n/Users/msessim/projects/moose/framework/moose.mk:34: The moose-libmesh conda package build number does match the current version of MOOSE. Please run \"conda update --all\" in your MOOSE environment.\nChecking if petsc version is up to date...\n/Users/msessim/projects/moose/framework/moose.mk:50: The moose-petsc conda package build number does match the current version of MOOSE. Please run \"conda update --all\" in your MOOSE environment.\n(...)\nI had just performed \"conda update --all\". Is there any other package update that I am missing?\nI still get the \"missing symbol called\" when I run the tests in Macaw, or any input file. Let me know if there is any info that will help here.\nThanks for your interest in Macaw. I created Macaw (https://github.com/marinasessim/macaw) to share some of the work I did in grad school with Dr.Tonks (https://doi.org/10.1016/j.commatsci.2021.111156). We used phase-field to simulate the oxidation of carbon fibers in a material that is used in heat shields of spacecraft. I did not know we had another Macaw application, bummer!\nMarina",
                          "url": "https://github.com/idaholab/moose/discussions/21703#discussioncomment-3275151",
                          "updatedAt": "2022-07-28T21:51:30Z",
                          "publishedAt": "2022-07-28T21:51:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nSo conda update --all can fail to update sometimes. What versions do you have of the packages?\nmoose-libmesh             2022.06.06              build_3    https://conda.software.inl.gov/public\nmoose-mpich               4.0.2                   build_0    https://conda.software.inl.gov/public\nmoose-petsc               3.16.5                  build_6    https://conda.software.inl.gov/public\nmoose-tools               2022.06.16       py39h5a31bc0_0    https://conda.software.inl.gov/public\n\nif you have something older than those, you may be hitting this.\nIn this case, it's easiest to first try mamba update moose-libmesh moose-petsc then if this fails, wipe the conda environment and reinstall them\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/21703#discussioncomment-3281036",
                          "updatedAt": "2022-07-29T14:13:41Z",
                          "publishedAt": "2022-07-29T14:13:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "marinasessim"
                          },
                          "bodyText": "Hi Guillaume, sorry for the delay. This was the version of the aforementioned packages:\nmoose-libmesh             2022.06.06              build_0    https://conda.software.inl.gov/public\nmoose-libmesh-vtk         9.1.0            py39hc1bf809_7    https://conda.software.inl.gov/public\nmoose-mpich               4.0.1                   build_2    https://conda.software.inl.gov/public\nmoose-petsc               3.16.5                  build_3    https://conda.software.inl.gov/public\nmoose-tools               2022.04.18       py39hf94b855_0    https://conda.software.inl.gov/public\nI'm in the process of updating the moose env now (starting over since neither \"conda update --all\" or the \"mamba update moose-libmesh moose-petsc\" worked). I asked a colleague (@timcalvert) to try it out, he has a fresh environment with the updated packages and the \"missing symbol called\" problem persists. We also tried in our linux cluster in UF and same is true there. Do you have any other suggestions about what could be happening? Let me know if there is anything I can share to help shed some light.\nThanks for your help!\nMarina",
                          "url": "https://github.com/idaholab/moose/discussions/21703#discussioncomment-3352332",
                          "updatedAt": "2022-08-08T19:42:03Z",
                          "publishedAt": "2022-08-08T19:41:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "the \"missing symbol called\" error suggests there are old compiled objects in the repository. I would use the git clean -xfd routine in the moose/ folder and try again. Note that you need to have saved / committed all your work before using that.\nThe package versions are slightly outdated. You probably got newer versions when re-creating the environment from scratch?\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/21703#discussioncomment-3354039",
                          "updatedAt": "2022-08-09T01:41:56Z",
                          "publishedAt": "2022-08-09T01:41:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "marinasessim"
                          },
                          "bodyText": "Hello,\nI removed the old environment and created a new one. MOOSE tests were compiled and ran with no problems. The packages are now updated:\nmoose-libmesh             2022.08.05              build_0    https://conda.software.inl.gov/public\nmoose-libmesh-vtk         9.1.0               h30e85a9_11    https://conda.software.inl.gov/public\nmoose-mpich               4.0.2                   build_1    https://conda.software.inl.gov/public\nmoose-petsc               3.16.6                  build_0    https://conda.software.inl.gov/public\nmoose-tools               2022.07.18      py310hcffbc79_0    https://conda.software.inl.gov/public\n\nI performed both git clean -xfd and make clobberall in the macaw dir. Macaw was recompiled, but the tests still hit the \"missing symbol called\" error. I'm attaching the outputs of ./run_tests. I will try to run it in debugging mode next.\ntests_missing_symbol_out.txt",
                          "url": "https://github.com/idaholab/moose/discussions/21703#discussioncomment-3400366",
                          "updatedAt": "2022-08-15T17:43:01Z",
                          "publishedAt": "2022-08-15T17:43:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Is MOOSE a submodule of macaw in your build?\nif so you may want to use\ngit submodule for each git clean -xfd in the macaw dir as well",
                          "url": "https://github.com/idaholab/moose/discussions/21703#discussioncomment-3400387",
                          "updatedAt": "2022-08-15T17:46:59Z",
                          "publishedAt": "2022-08-15T17:46:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "marinasessim"
                          },
                          "bodyText": "Hi Guillaume,\nI don't think I have moose as a submodule, there is no /moose in the macaw dir and I set export MOOSE_DIR=\"$HOME/projects/moose\" in the bash_profile.\nI just tried  git submodule foreach git clean -xfd in addition to git clean -xfd and make clobberall in the macaw dir.  Same error while running tests (\"missing symbol called\").",
                          "url": "https://github.com/idaholab/moose/discussions/21703#discussioncomment-3400640",
                          "updatedAt": "2022-08-15T18:23:41Z",
                          "publishedAt": "2022-08-15T18:23:41Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "if it s not a submodule, can you clean moose with the git clean -xfd command in projects/moose?\nmissing symbol called could also be a missing function definition I think",
                          "url": "https://github.com/idaholab/moose/discussions/21703#discussioncomment-3400794",
                          "updatedAt": "2022-08-15T18:49:32Z",
                          "publishedAt": "2022-08-15T18:49:31Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}