{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMS0wNy0yNVQxNDowNTowNS0wNjowMM4AMXQ1"
    },
    "edges": [
      {
        "node": {
          "title": "Generating subdomains from the element list?",
          "author": {
            "login": "Traiwit"
          },
          "bodyText": "Hi guys,\nis it possible to generate subdomains by element list?\nfrom the list under meshgenerators, it doesn't seem like there is one that can do what I want\nI could do that by using FileMeshGenerator to import pre-defined subdomains from .inp file (Abaqus).\n\nmy ultimate goal is to first, create cohesive surfaces from the subdomains (red/large blocks) using BreakMeshByBlockGenerator then impose the mining-step subdomains into the main mesh (black/small blocks)\nso generating subdomains from the element list will help in the later part.\n\n@jiangwen84 @arovinelli, gents, I think this approach can help with my CZM problem, what do you think?\nKind regards,\nTraiwit",
          "url": "https://github.com/idaholab/moose/discussions/18444",
          "updatedAt": "2022-06-22T15:58:21Z",
          "publishedAt": "2021-07-26T01:25:52Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThere is not. You can use https://mooseframework.inl.gov/source/meshgenerators/ParsedSubdomainMeshGenerator.html to sort of define the block based on the location of the centroid of the element, which could work here if you convert each block shape into a combinatorial geometry expression.\nIf you want to work with a list, I think modifying the .inp file, or specifying subblocks in abacus is the way to go here.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/18444#discussioncomment-1059627",
                  "updatedAt": "2022-06-22T15:58:35Z",
                  "publishedAt": "2021-07-27T01:24:09Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "arovinelli"
                  },
                  "bodyText": "ElementSubdomainIDGenerator\nis similar to what you need.\nThe bad part is you have to give all elements a new block id.\nThe element list is implied to be 0,1,2,...,N\nThe example below will give to elements  0,1,2,3,4 block id 0 2 3 1 2\n  [./subdomains]\n    type = ElementSubdomainIDGenerator\n    input = gmg\n    subdomain_ids = '0 2 3 1 2'\n  []",
                  "url": "https://github.com/idaholab/moose/discussions/18444#discussioncomment-1061865",
                  "updatedAt": "2022-06-22T15:58:43Z",
                  "publishedAt": "2021-07-27T12:11:43Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "use of a current kernel variable in another kernel as a coefficient",
          "author": {
            "login": "TLWise"
          },
          "bodyText": "I would like to use the resulting value for u1 as a coefficient in the solution of value u2 where u1 is defined in the formula d/dt(u1) = C1u1 and u2 is defined in the formula d/dt(u2)=C2u2+C3*u1. Is there a kernel or a function that I may apply to provide me with the desired coupling?",
          "url": "https://github.com/idaholab/moose/discussions/18450",
          "updatedAt": "2024-01-11T17:17:05Z",
          "publishedAt": "2021-07-26T23:44:21Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nYou can couple the two equations. I believe the example for coupled force does something very similar to what you want to do.\nIf you want to use the \"resulting\" value, so solve one equation, then solve the next one, you will need to use a MultiApp.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/18450#discussioncomment-1058832",
                  "updatedAt": "2024-01-11T17:17:12Z",
                  "publishedAt": "2021-07-27T00:00:56Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "TLWise"
                          },
                          "bodyText": "Thank you. I was able to do what I wanted by using combinations of CoefReaction and CoupledForce.",
                          "url": "https://github.com/idaholab/moose/discussions/18450#discussioncomment-1059703",
                          "updatedAt": "2024-01-11T17:17:12Z",
                          "publishedAt": "2021-07-27T01:56:28Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to output the concentration gradient energy contribution from \"CHInterface\"",
          "author": {
            "login": "wenpeng231024"
          },
          "bodyText": "Hi everyone,\nI use \"CHInterface\" to calculate the concentration gradient energy. Does anyone know how to output this part gradient energy?\n\nhttps://mooseframework.inl.gov/source/kernels/CHInterface.html\nThanks,\nPeng",
          "url": "https://github.com/idaholab/moose/discussions/18447",
          "updatedAt": "2024-05-29T23:39:41Z",
          "publishedAt": "2021-07-26T15:05:23Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "dschwen"
                  },
                  "bodyText": "You can use the TotalFreeEnergy AuxKernel. Just set F_name = 0 to ignore the bulk contribution.",
                  "url": "https://github.com/idaholab/moose/discussions/18447#discussioncomment-1058553",
                  "updatedAt": "2024-05-29T23:40:25Z",
                  "publishedAt": "2021-07-26T21:53:38Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "wenpeng231024"
                          },
                          "bodyText": "Thanks for your reply. I will try this.",
                          "url": "https://github.com/idaholab/moose/discussions/18447#discussioncomment-1059595",
                          "updatedAt": "2024-05-29T23:40:25Z",
                          "publishedAt": "2021-07-27T01:07:17Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "MOOSE setting all my material properties to zero?",
          "author": {
            "login": "richmondodufisan"
          },
          "bodyText": "In my material file, I declare the property using:\n_len_scale(declareADProperty(\"length_scale\")),\nand after computing it in computeQpProperties, I added a print statement there- its outputting exactly what it's supposed to. Then in the kernel, I \"get\" this value with:\n_len_scale(getADMaterialPropertyByName(\"length_scale\")),\nprinting this outputs 0.000000. The same for all other material properties. I find this strange because its worked in the past. If I understand correctly, the string in quotes declared in the material file is what is being retrieved when I use \"getADMaterialPropertyByName\", right? The only different thing I could think of is that this is the first time I'm running a transient solve.\nThe material property is a constant, btw. That is, what I computed in computeQpProperties depends only on values given from the user in the input file, so it's not supposed to change or be affected by anything else.\nI checked other materials, and they are all 0.000000 as well. I only caught this after I got an arithmetic exception somewhere else because of a division operation.",
          "url": "https://github.com/idaholab/moose/discussions/18409",
          "updatedAt": "2022-06-03T08:23:57Z",
          "publishedAt": "2021-07-22T00:30:27Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nIs _len_scale declared like this in the header?\nconst ADMaterialProperty & _len_scale ;\nIf len_scale is not declared as a reference, that would be the problem. But there's a guard (non-reference routine is private in MaterialProperty.h) against assigning it to a value so I doubt it's that.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/18409#discussioncomment-1035716",
                  "updatedAt": "2022-06-03T08:24:42Z",
                  "publishedAt": "2021-07-22T03:36:38Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "Hi,\nYes it's declared like this:\nconst ADMaterialProperty & _len_scale;\nwith a (Real) after ADMaterialProperty in angled brackets",
                          "url": "https://github.com/idaholab/moose/discussions/18409#discussioncomment-1035790",
                          "updatedAt": "2022-06-03T08:24:45Z",
                          "publishedAt": "2021-07-22T04:29:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "for both the material and the kernel right",
                          "url": "https://github.com/idaholab/moose/discussions/18409#discussioncomment-1035803",
                          "updatedAt": "2022-06-03T08:24:45Z",
                          "publishedAt": "2021-07-22T04:37:56Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "Yes for both",
                          "url": "https://github.com/idaholab/moose/discussions/18409#discussioncomment-1035805",
                          "updatedAt": "2022-06-03T08:24:45Z",
                          "publishedAt": "2021-07-22T04:39:14Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "But in the material, it's not const though, since I have to assign _len_scale[_qp] in computeQpProperties",
                          "url": "https://github.com/idaholab/moose/discussions/18409#discussioncomment-1035807",
                          "updatedAt": "2022-06-03T08:24:57Z",
                          "publishedAt": "2021-07-22T04:41:17Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@rwcarlsen on mat props.\nnot sure what is going on here",
                          "url": "https://github.com/idaholab/moose/discussions/18409#discussioncomment-1038005",
                          "updatedAt": "2022-06-03T08:25:00Z",
                          "publishedAt": "2021-07-22T14:26:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "rwcarlsen"
                          },
                          "bodyText": "Could you show more snippets of the code where you declare the property in the material, compute/assign the property,  retrieve the property in another object, and use the property in another object?  Are you using any non-default execute_on parameters for any objects, etc.?  Just need more context to help you diagnose.",
                          "url": "https://github.com/idaholab/moose/discussions/18409#discussioncomment-1038851",
                          "updatedAt": "2022-06-03T08:24:57Z",
                          "publishedAt": "2021-07-22T17:40:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "_len_scale[_qp] = some calc involving _user_len_scale and some other params gotten with \"getParam\" from the input file\nprintf(\"from user %f \\n \",_user_len_scale);\nprintf(\"from material %f  \\n \",_len_scale[_qp]);\n/// These print statements print exactly what I expect\n///Kernel\n/// header file declaration\nconst ADMaterialProperty & _len_scale;\n/// source file constructor\n_len_scale(getADMaterialPropertyByName(\"length_scale\")),\nprintf(\"from kernel lenscale %f, init_porosity %f \\n\",_len_scale[_qp], _porosity_0[_qp]);\n/// prints 0.00000 for both values\nI also tried changing the \"length_scale\" in the getADMaterialPropertyByName to just a random string and I got an error, so it's apparently picking it from my previous declaration, I don't know why it's setting it to zero.\nI also noticed something else this morning- a variable in my kernel, \"porosity\" is set to a constantIC, but when I printed it at the start of the computeQpResidual, it is also 0.00000 as well. But, in the .e file that's created, the initial value I set in the input file is shown correctly on the mesh.\nI tried running it on a remote machine as well with far more computing power, but I got the same error.",
                          "url": "https://github.com/idaholab/moose/discussions/18409#discussioncomment-1039135",
                          "updatedAt": "2022-06-03T08:24:57Z",
                          "publishedAt": "2021-07-22T19:05:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "rwcarlsen"
                          },
                          "bodyText": "Is the property declared as an AD property to begin with?  AD and non-AD properties are tracked separately.  So you need to declare+retrieve them consistently as AD and non-AD.  Also - just to confirm - the declaration for the member var is const ADMaterialProperty<Real> & _len_scale; - right?  Also just want to confirm - are your printfs in your kernel for the property values inside the computeQpResidual function?  Because inside the constructor or other functions, the properties will not be computed/ready.",
                          "url": "https://github.com/idaholab/moose/discussions/18409#discussioncomment-1039716",
                          "updatedAt": "2022-06-03T08:25:46Z",
                          "publishedAt": "2021-07-22T22:00:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "And _len_scale[_qp] = some calc is being done in computeValue of the material right?",
                          "url": "https://github.com/idaholab/moose/discussions/18409#discussioncomment-1039941",
                          "updatedAt": "2022-06-03T08:25:48Z",
                          "publishedAt": "2021-07-22T23:45:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "Yes, the values from the user are initialized with ADReal, and then getParam<Real> to retrieve them. They're then calculated in computeQpProperties() (@GiudGiud, not computeValue(), is that the function I should be overriding?), and assigned to variables declared like const ADMaterialProperty<Real> & _len_scale; . Then, in each of the kernels, they are retrieved with _len_scale(getADMaterialPropertyByName(\"length_scale\")),\nAnd yes I added the print statement inside computeQpResidual(). I also added print statements to the other kernels, but the solver isn't getting to them yet.",
                          "url": "https://github.com/idaholab/moose/discussions/18409#discussioncomment-1040186",
                          "updatedAt": "2022-06-03T08:25:48Z",
                          "publishedAt": "2021-07-23T01:52:38Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "rwcarlsen"
                  },
                  "bodyText": "Could you show the code in your header and .C files where you define your material property member variable and retrieve it?  Sometimes people have silly mistakes like retrieving the material property and storing it into a material property value instead of a reference member variable.\n[edit] - never mind - looks like you declared the member var right. Hmmm...",
                  "url": "https://github.com/idaholab/moose/discussions/18409#discussioncomment-1038839",
                  "updatedAt": "2022-06-03T08:24:55Z",
                  "publishedAt": "2021-07-22T17:36:17Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Peacock issues with Bigsur",
          "author": {
            "login": "malanzey"
          },
          "bodyText": "Hello\nI am new to moose and I am running it on Mac OS Bigsur 11.5. I have read that there were some issues including failing moose tests and segmentation fault while running peacock. I have been trying to find a solution yet I am not sure if all these issues were resolved or still under development.\nthank you",
          "url": "https://github.com/idaholab/moose/discussions/18440",
          "updatedAt": "2022-06-20T15:28:56Z",
          "publishedAt": "2021-07-25T19:58:20Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "milljm"
                  },
                  "bodyText": "BigSur will by default install a version of XCode that does not work (see discussion #17722 for details).\nIf the above is not applicative to you, can you copy/paste the error(s) you are receiving along with the command that invoked them? Please refrain from pasting screen shots where possible.",
                  "url": "https://github.com/idaholab/moose/discussions/18440#discussioncomment-1051133",
                  "updatedAt": "2022-06-20T15:28:56Z",
                  "publishedAt": "2021-07-26T12:51:02Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "malanzey"
                          },
                          "bodyText": "Hi\nThank you for your response. I have resolved the peacock issue and it is working fine where I applied the following fix after reinstalling moose, instead of installing moose-libmesh and moos-tools through\nconda install moose-tools\nconda install moose-libmesh\nI installed it by typing the following in terminal\nconda deactivate\nconda install mamba\nmamba create -n moose moose-libmesh moose-tools\nconda activate moose\nThis have solved my issue which I believe was python version related. However as for the failing moose tests I am still having a large number of tests failing. The test I ran was\ncd ~/projects/moose/test\nmake -j 4\n./run_tests -j 4\nand that resulted to the following:\ntime_integrators/convergence.implicit_midpoint/level0 ..................................................... OK\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject ........ [min_cpus=2] FAILED (CRASH)\nsamplers/distribute.scale/execute ............................................................. FAILED (CRASH)\nperformance.multiprocess/mpi ................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nreporters/base.base .............................................................. [min_cpus=2] FAILED (CRASH)\nmesh/custom_partitioner.group/custom_linear_partitioner .......................... [min_cpus=2] FAILED (CRASH)\nreporters/mesh_info.info/default ................................................. [min_cpus=2] FAILED (CRASH)\nsystem_interfaces.mpi .......................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\noutputs/vtk.files/parallel ....................................................... [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/parallel_consistency.test ................................... [min_cpus=2] FAILED (CRASH)\nmesh/custom_partitioner.group/custom_linear_partitioner_displacement ............. [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/csv_reader.parallel ......................................... [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/parallel_consistency.broadcast .............................. [min_cpus=2] FAILED (CRASH)\nmesh/custom_partitioner.group/custom_linear_partitioner_restart .................. [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/csv_reader.tester_fail ..................... [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\ndgkernels/2d_diffusion_dg.proper_ghosting_with_action_parallel ................... [min_cpus=2] FAILED (CRASH)\noutputs/json/distributed.info/default ............................................ [min_cpus=2] FAILED (CRASH)\noutputs/vtk.solution/diff_serial_mesh_parallel ................................... [min_cpus=2] FAILED (CRASH)\nmisc/exception.parallel_exception_residual_transient_non_zero_rank ............... [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/work_balance.work_balance/replicated ........................ [min_cpus=2] FAILED (CRASH)\nmeshgenerators/meta_data_store.test_meta_data_with_use_split ..................... [min_cpus=2] FAILED (CRASH)\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer ......................... [min_cpus=2] FAILED (CRASH)\nrestart/kernel_restartable.parallel_error/error1 ................................. [min_cpus=2] FAILED (CRASH)\nmesh/checkpoint.test_2 ........................................................... [min_cpus=2] FAILED (CRASH)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor ................ [min_cpus=3] FAILED (CRASH)\noutputs/xml.parallel/replicated .................................................. [min_cpus=3] FAILED (CRASH)\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank ............... [min_cpus=2] FAILED (CRASH)\npostprocessors/num_residual_eval.test ............................................ [min_cpus=2] FAILED (CRASH)\nmultiapps/max_procs_per_app.test ................................................. [min_cpus=3] FAILED (CRASH)\ninterfacekernels/2d_interface.parallel_fdp_test .................................. [min_cpus=2] FAILED (CRASH)\nmesh/splitting.use_split ......................................................... [min_cpus=3] FAILED (CRASH)\ntransfers/multiapp_nearest_node_transfer.parallel ................................ [min_cpus=2] FAILED (CRASH)\npreconditioners/hmg.hmg ........................................ [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\noutputs/variables.nemesis_hide ................................................... [min_cpus=2] FAILED (CRASH)\nics/depend_on_uo.ic_depend_on_uo ................................................. [min_cpus=2] FAILED (CRASH)\ninterfaces/random.parallel_verification .......................................... [min_cpus=2] FAILED (CRASH)\npreconditioners/hmg.hmg_3D ..................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nbcs/periodic.testperiodic_dp ..................................................... [min_cpus=2] FAILED (CRASH)\nsystem_interfaces.partitioner/parmetis ........................................... [min_cpus=2] FAILED (CRASH)\noutputs/xml.parallel/distributed ................................................. [min_cpus=3] FAILED (CRASH)\nmisc/exception.parallel_error_residual_transient_non_zero_rank .. [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\nfvkernels/mms/non-orthogonal.compact ............................................. [min_cpus=2] FAILED (CRASH)\nmeshgenerators/distributed_rectilinear/partition.2D_3 ............................ [min_cpus=3] FAILED (CRASH)\nics/depend_on_uo.scalar_ic_from_uo ............................................... [min_cpus=2] FAILED (CRASH)\ninterfaces/random.parallel_verification_uo ....................................... [min_cpus=2] FAILED (CRASH)\nmisc/exception.parallel_error_jacobian_transient_non_zero_rank .. [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\nmesh/unique_ids.replicated_mesh .................................................. [min_cpus=2] FAILED (CRASH)\nmesh/splitting.check/pre_split ................................. [min_cpus=3] FAILED (EXPECTED OUTPUT MISSING)\nrestart/restartable_types.parallel/first ......................................... [min_cpus=2] FAILED (CRASH)\npreconditioners/hmg.hmg_strumpack .............................. [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nrelationship_managers/geometric_neighbors.geometric_edge_neighbor_3D ............. [min_cpus=3] FAILED (CRASH)\nRan 2583 tests in 263.7 seconds. Average test time 0.3 seconds, maximum test time 24.5 seconds.\n2533 passed, 73 skipped, 0 pending, 50 FAILED\nMAX FAILURES REACHED\nAny advice on what is causing all these tests to fail?\nThank you for your time.",
                          "url": "https://github.com/idaholab/moose/discussions/18440#discussioncomment-1051963",
                          "updatedAt": "2022-06-20T15:28:59Z",
                          "publishedAt": "2021-07-26T15:48:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Looks like parallel tests are failing... can you post an actual failure (the results you posted are confusingly part of a \"Final abridged list of results\"). The actual more detailed failure can be found if you scroll up a ways.",
                          "url": "https://github.com/idaholab/moose/discussions/18440#discussioncomment-1052161",
                          "updatedAt": "2022-06-20T15:29:16Z",
                          "publishedAt": "2021-07-26T16:36:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "For example:\n\u276f ./run_tests -i duplicate_outputs\ntests/test_harness.b ............................................................... [skipped dependency] SKIP\ntests/test_harness.d: Working Directory: /Users/milljm/projects/moose/test/tests/test_harness\ntests/test_harness.d: Running command: /Users/milljm/projects/moose/test/moose_test-opt -i good.i Outputs/exodus=true --error --error-unused --error-override --no-gdb-backtrace\ntests/test_harness.d: Output file will over write pre-existing output file:\ntests/test_harness.d: \tgood_out.e\ntests/test_harness.d: \ntests/test_harness.d ......................................................... FAILED (OUTFILE RACE CONDITION)\ntests/test_harness.a: Working Directory: /Users/milljm/projects/moose/test/tests/test_harness\ntests/test_harness.a: Running command: /Users/milljm/projects/moose/test/moose_test-opt -i good.i Outputs/exodus=true --error --error-unused --error-override --no-gdb-backtrace\ntests/test_harness.a: Output file will over write pre-existing output file:\ntests/test_harness.a: \tgood_out.e\ntests/test_harness.a: \ntests/test_harness.a ......................................................... FAILED (OUTFILE RACE CONDITION)\ntests/test_harness.c ............................................................... [skipped dependency] SKIP\n\n\nFinal Test Results:\n--------------------------------------------------------------------------------------------------------------\ntests/test_harness.b ............................................................... [skipped dependency] SKIP\ntests/test_harness.c ............................................................... [skipped dependency] SKIP\ntests/test_harness.d ......................................................... FAILED (OUTFILE RACE CONDITION)\ntests/test_harness.a ......................................................... FAILED (OUTFILE RACE CONDITION)\n--------------------------------------------------------------------------------------------------------------\nRan 2 tests in 0.1 seconds. Average test time 0.0 seconds, maximum test time 0.0 seconds.\n0 passed, 2 skipped, 0 pending, 2 FAILED\nWhat you pasted was \"Final Test Results\". But the actual error as to why, is further up the log.",
                          "url": "https://github.com/idaholab/moose/discussions/18440#discussioncomment-1052190",
                          "updatedAt": "2022-06-20T15:29:20Z",
                          "publishedAt": "2021-07-26T16:42:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "malanzey"
                          },
                          "bodyText": "Hi\nI apologize for the confusion here are the detailed script for the failed tests\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/transfers/multiapp_vector_pp_transfer\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i master.i --error --error-unused --error-override --no-gdb-backtrace\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIR_Init_thread(586)..............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_init(324).................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: (unknown)(): Invalid group\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIR_Init_thread(586)..............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_init(324).................:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: (unknown)(): Invalid group\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Exit Code: 8\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: ################################################################################\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer: Tester failed, reason: CRASH\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer:\ntransfers/multiapp_vector_pp_transfer.vector_pp_transfer ......................... [min_cpus=2] FAILED (CRASH)\nparser/vector_range_checking.vector_elem_checks/elementcompare_real ....................................... OK\ntransfers/reporter_transfer.errors/from_multi_app ......................................................... OK\nrestart/kernel_restartable.thread_error/threads_error .............................. [skipped dependency] SKIP\nrestart/kernel_restartable.parallel_error/error1: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/restart/kernel_restartable\nrestart/kernel_restartable.parallel_error/error1: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i kernel_restartable.i --error --error-unused --error-override --no-gdb-backtrace\nrestart/kernel_restartable.parallel_error/error1: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrestart/kernel_restartable.parallel_error/error1: MPIR_Init_thread(586)..............:\nrestart/kernel_restartable.parallel_error/error1: MPID_Init(224).....................: channel initialization failed\nrestart/kernel_restartable.parallel_error/error1: MPIDI_CH3_Init(105)................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_init(324).................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(175).............:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_get_business_card(401):\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nrestart/kernel_restartable.parallel_error/error1: (unknown)(): Invalid group\nrestart/kernel_restartable.parallel_error/error1: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrestart/kernel_restartable.parallel_error/error1: MPIR_Init_thread(586)..............:\nrestart/kernel_restartable.parallel_error/error1: MPID_Init(224).....................: channel initialization failed\nrestart/kernel_restartable.parallel_error/error1: MPIDI_CH3_Init(105)................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_init(324).................:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(175).............:\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_get_business_card(401):\nrestart/kernel_restartable.parallel_error/error1: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nrestart/kernel_restartable.parallel_error/error1: (unknown)(): Invalid group\nrestart/kernel_restartable.parallel_error/error1:\nrestart/kernel_restartable.parallel_error/error1:\nrestart/kernel_restartable.parallel_error/error1: Exit Code: 8\nrestart/kernel_restartable.parallel_error/error1: ################################################################################\nrestart/kernel_restartable.parallel_error/error1: Tester failed, reason: CRASH\nrestart/kernel_restartable.parallel_error/error1:\nrestart/kernel_restartable.parallel_error/error1 ................................. [min_cpus=2] FAILED (CRASH)\nrestart/kernel_restartable.thread_error/with_threads ............................... [skipped dependency] SKIP\nrestart/kernel_restartable.parallel_error/error2 ................................... [skipped dependency] SKIP\nrestart/restart_diffusion.restart_with_variable_rename .................................................... OK\ntransfers/multiapp_conservative_transfer.nearest_point_vector_pps ......................................... OK\ntransfers/multiapp_mesh_function_transfer.errors/missed_point ............................................. OK\nmultiapps/check_error.bad_positions ....................................................................... OK\ntransfers/multiapp_userobject_transfer.3d_1d_err .......................................................... OK\nmultiapps/full_solve_multiapp.solve_not_converge/continue ................................................. OK\ninterfacekernels/2d_interface.no_mallocs_during_scaling ................................................... OK\ntransfers/multiapp_postprocessor_interpolation_transfer.test_error ........................................ OK\nvariables/fe_hier.group/test_hier_1_3d .................................................................... OK\nvariables/optionally_coupled.catch_out_of_bound_default_access/coupledValue ............................... OK\ninterfacekernels/1d_interface.single_variable_jacobian_test ............................................... OK\nmultiapps/picard.steady ................................................................................... OK\nrestrictable/block_api_test.has/hasBlocks_ANY_BLOCK_ID .................................................... OK\nfunctions/solution_function.rot4 .......................................................................... OK\nfvkernels/mms/advective-outflow.VanLeerLimiter ............................................................ OK\nfunctions/piecewise_multilinear.oneDb ..................................................................... OK\nrestrictable/boundary_api_test.mat/hasBoundaryMaterialProperty_false ...................................... OK\nreporters/base.errors/requested_different_type ............................................................ OK\noutputs/json/distributed.info/default: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/outputs/json/distributed\noutputs/json/distributed.info/default: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i distributed.i --error --error-unused --error-override --no-gdb-backtrace\noutputs/json/distributed.info/default: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/json/distributed.info/default: MPIR_Init_thread(586)..............:\noutputs/json/distributed.info/default: MPID_Init(224).....................: channel initialization failed\noutputs/json/distributed.info/default: MPIDI_CH3_Init(105)................:\noutputs/json/distributed.info/default: MPID_nem_init(324).................:\noutputs/json/distributed.info/default: MPID_nem_tcp_init(175).............:\noutputs/json/distributed.info/default: MPID_nem_tcp_get_business_card(401):\noutputs/json/distributed.info/default: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\noutputs/json/distributed.info/default: (unknown)(): Invalid group\noutputs/json/distributed.info/default: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/json/distributed.info/default: MPIR_Init_thread(586)..............:\noutputs/json/distributed.info/default: MPID_Init(224).....................: channel initialization failed\noutputs/json/distributed.info/default: MPIDI_CH3_Init(105)................:\noutputs/json/distributed.info/default: MPID_nem_init(324).................:\noutputs/json/distributed.info/default: MPID_nem_tcp_init(175).............:\noutputs/json/distributed.info/default: MPID_nem_tcp_get_business_card(401):\noutputs/json/distributed.info/default: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\noutputs/json/distributed.info/default: (unknown)(): Invalid group\noutputs/json/distributed.info/default:\noutputs/json/distributed.info/default:\noutputs/json/distributed.info/default: Exit Code: 8\noutputs/json/distributed.info/default: ################################################################################\noutputs/json/distributed.info/default: Tester failed, reason: CRASH\noutputs/json/distributed.info/default:\noutputs/json/distributed.info/default ............................................ [min_cpus=2] FAILED (CRASH)\nfunctions/parsed.function_curl ............................................................................ OK\nmultiapps/steffensen_postprocessor.pp_transient/app_begin_transfers_end ................................... OK\noutputs/variables.show_hide ............................................................................... OK\noutputs/csv.restart/restart_part2_append .................................................................. OK\noutputs/intervals.no_intermediate ......................................................................... OK\noutputs/console.file_scalar_aux ........................................................................... OK\noutputs/iterative.start_stop/exodus_inline ................................................................ OK\nmultiapps/secant_postprocessor.pp_transient/app_begin_transfers_end ....................................... OK\nmultiapps/picard_postprocessor.pp_transient/app_begin_transfers_end ....................................... OK\nmultiapps/secant.variables_transient/app_begin_transfers_end .............................................. OK\noutputs/debug.show_top_residuals_debug .................................................................... OK\ntag.systems/test_tag_DG_kernels ........................................................................... OK\ninterfaces/reporterinterface.has_errors/name .............................................................. OK\ninterfaces/vectorpostprocessorinterface.missing_errors/by_param ........................................... OK\ninterfaces/postprocessorinterface.has_errors/name ......................................................... OK\nmultiapps/steffensen.variables_transient/app_begin_transfers_end .......................................... OK\nnodalkernels/constraint_enforcement.upper_and_lower_bound/non_singular .................................... OK\nics/from_exodus_solution.array_nodal_var_1 ................................................................ OK\nusability.normal .......................................................................................... OK\ninterfaces/userobjectinterface.has_uo/param ............................................................... OK\nmesh/mesh_generation.annular_errors/annular_except6 ....................................................... OK\nmisc/jacobian.zero_jacobian_2D_ok ......................................................................... OK\ngeomsearch/2d_moving_penetration.pl_test2tt ............................................................... OK\ngeomsearch/3d_moving_penetration.pl_test2tt ............................................................... OK\nauxkernels/solution_aux.exodus_elem_map ................................................................... OK\nkernels/conservative_advection.upwinding/none_in_none_out ................................................. OK\nkernels/array_kernels.test_diffusion_reaction_transient ................................................... OK\nkernels/hfem.variable_robin ............................................................................... OK\nkernels/simple_transient_diffusion.automatic-scaling-done-per-time-step-parallel-preconditioner ........... OK\nmesh_modifiers/rename_block.error/except6 ................................................................. OK\nmesh/high_order_elems.test_tet4_refine .................................................................... OK\nkernels/vector_fe.lagrange_vec_1d_jac ..................................................................... OK\nexecutioners/eigen_executioners.test_deficient_B_eigen .................................................... OK\nmesh_modifiers/add_side_sets_from_bounding_box.overlap/test_overlapping ................................... OK\nmaterials/ad_piecewise_linear_interpolation_material.test-jac ............................................. OK\noutputs/oversample.example/ex02 ........................................................................... OK\nmaterials/derivative_material_interface.mutliblock ........................................................ OK\nmaterials/material.mat_block_boundary_check ............................................................... OK\nfvkernels/fv_simple_diffusion.3d_dirichlet ................................................................ OK\nmaterials/stateful_prop.storage/spatial_bnd_only .......................................................... OK\nfvkernels/mms/non-orthogonal.compact: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/fvkernels/mms/non-orthogonal\nfvkernels/mms/non-orthogonal.compact: Running command: python -m unittest -v test.TestCompactADR\nfvkernels/mms/non-orthogonal.compact: The 'mms' package requires sympy for symbolic function evaluation, it can be installed by running pip install sympy --user.\nfvkernels/mms/non-orthogonal.compact: Running: /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i advection-diffusion-reaction.i Mesh/uniform_refine=0\nfvkernels/mms/non-orthogonal.compact: test (test.TestCompactADR) ... Fatal error in MPI_Init_thread: Invalid group, error stack:\nfvkernels/mms/non-orthogonal.compact: MPIR_Init_thread(586)..............:\nfvkernels/mms/non-orthogonal.compact: MPID_Init(224).....................: channel initialization failed\nfvkernels/mms/non-orthogonal.compact: MPIDI_CH3_Init(105)................:\nfvkernels/mms/non-orthogonal.compact: MPID_nem_init(324).................:\nfvkernels/mms/non-orthogonal.compact: MPID_nem_tcp_init(175).............:\nfvkernels/mms/non-orthogonal.compact: MPID_nem_tcp_get_business_card(401):\nfvkernels/mms/non-orthogonal.compact: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nfvkernels/mms/non-orthogonal.compact: (unknown)(): Invalid group\nfvkernels/mms/non-orthogonal.compact: ERROR\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact: ======================================================================\nfvkernels/mms/non-orthogonal.compact: ERROR: test (test.TestCompactADR)\nfvkernels/mms/non-orthogonal.compact: ----------------------------------------------------------------------\nfvkernels/mms/non-orthogonal.compact: Traceback (most recent call last):\nfvkernels/mms/non-orthogonal.compact:   File \"/Users/mohammadalabdullah/projects/moose/test/tests/fvkernels/mms/non-orthogonal/test.py\", line 7, in test\nfvkernels/mms/non-orthogonal.compact:     df1 = mms.run_spatial('advection-diffusion-reaction.i', 7, mpi=2)\nfvkernels/mms/non-orthogonal.compact:   File \"/Users/mohammadalabdullah/projects/moose/python/mms/runner.py\", line 129, in run_spatial\nfvkernels/mms/non-orthogonal.compact:     return _runner(*args, rtype=SPATIAL, **kwargs)\nfvkernels/mms/non-orthogonal.compact:   File \"/Users/mohammadalabdullah/projects/moose/python/mms/runner.py\", line 102, in _runner\nfvkernels/mms/non-orthogonal.compact:     raise IOError(\"The CSV output does not exist: {}\".format(csv))\nfvkernels/mms/non-orthogonal.compact: OSError: The CSV output does not exist: None\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact: ----------------------------------------------------------------------\nfvkernels/mms/non-orthogonal.compact: Ran 1 test in 0.116s\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact: FAILED (errors=1)\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact: Exit Code: 1\nfvkernels/mms/non-orthogonal.compact: ################################################################################\nfvkernels/mms/non-orthogonal.compact: Tester failed, reason: CRASH\nfvkernels/mms/non-orthogonal.compact:\nfvkernels/mms/non-orthogonal.compact ............................................. [min_cpus=2] FAILED (CRASH)\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/userobjects/setup_interface_count\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i general.i --error --error-unused --error-override --no-gdb-backtrace\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Fatal error in MPI_Init_thread: Invalid group, error stack:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPIR_Init_thread(586)..............:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_Init(224).....................: channel initialization failed\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPIDI_CH3_Init(105)................:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_init(324).................:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_init(175).............:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_get_business_card(401):\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: (unknown)(): Invalid group\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Fatal error in MPI_Init_thread: Invalid group, error stack:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPIR_Init_thread(586)..............:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_Init(224).....................: channel initialization failed\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPIDI_CH3_Init(105)................:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_init(324).................:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_init(175).............:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_get_business_card(401):\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: (unknown)(): Invalid group\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Exit Code: 8\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: ################################################################################\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject: Tester failed, reason: CRASH\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject:\nuserobjects/setup_interface_count.setup_interface_count/GeneralUserObject ........ [min_cpus=2] FAILED (CRASH)\nmeshgenerators/rename_boundary_generator.errors/inconsistent_size ......................................... OK\ntime_steppers/iteration_adaptive.shrink_init_dt ........................................................... OK\nmeshgenerators/break_mesh_by_block_generator.surrounding_block_restricted/split_interface_only ............ OK\nuserobjects/layered_average.1d_displaced .................................................................. OK\nconstraints/equal_value_embedded_constraint.penalty/2D_2D ................................................. OK\npostprocessors/num_nodes.test_split: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/postprocessors/num_nodes\npostprocessors/num_nodes.test_split: Running command: mpiexec -n 4 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i num_nodes.i --use-split --split-file generated.cpr Outputs/file_base=num_nodes_split_out --error --error-unused --error-override --no-gdb-backtrace\npostprocessors/num_nodes.test_split: Fatal error in MPI_Init_thread: Invalid group, error stack:\npostprocessors/num_nodes.test_split: MPIR_Init_thread(586)..............:\npostprocessors/num_nodes.test_split: MPID_Init(224).....................: channel initialization failed\npostprocessors/num_nodes.test_split: MPIDI_CH3_Init(105)................:\npostprocessors/num_nodes.test_split: MPID_nem_init(324).................:\npostprocessors/num_nodes.test_split: MPID_nem_tcp_init(175).............:\npostprocessors/num_nodes.test_split: MPID_nem_tcp_get_business_card(401):\npostprocessors/num_nodes.test_split: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\npostprocessors/num_nodes.test_split: (unknown)(): Invalid group\npostprocessors/num_nodes.test_split: Fatal error in MPI_Init_thread: Invalid group, error stack:\npostprocessors/num_nodes.test_split: MPIR_Init_thread(586)..............:\npostprocessors/num_nodes.test_split: MPID_Init(224).....................: channel initialization failed\npostprocessors/num_nodes.test_split: MPIDI_CH3_Init(105)................:\npostprocessors/num_nodes.test_split: MPID_nem_init(324).................:\npostprocessors/num_nodes.test_split: MPID_nem_tcp_init(175).............:\npostprocessors/num_nodes.test_split: MPID_nem_tcp_get_business_card(401):\npostprocessors/num_nodes.test_split: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\npostprocessors/num_nodes.test_split: (unknown)(): Invalid group\npostprocessors/num_nodes.test_split: Fatal error in MPI_Init_thread: Invalid group, error stack:\npostprocessors/num_nodes.test_split: MPIR_Init_thread(586)..............:\npostprocessors/num_nodes.test_split: MPID_Init(224).....................: channel initialization failed\npostprocessors/num_nodes.test_split: MPIDI_CH3_Init(105)................:\npostprocessors/num_nodes.test_split: MPID_nem_init(324).................:\npostprocessors/num_nodes.test_split: MPID_nem_tcp_init(175).............:\npostprocessors/num_nodes.test_split: MPID_nem_tcp_get_business_card(401):\npostprocessors/num_nodes.test_split: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\npostprocessors/num_nodes.test_split: (unknown)(): Invalid group\npostprocessors/num_nodes.test_split: Fatal error in MPI_Init_thread: Invalid group, error stack:\npostprocessors/num_nodes.test_split: MPIR_Init_thread(586)..............:\npostprocessors/num_nodes.test_split: MPID_Init(224).....................: channel initialization failed\npostprocessors/num_nodes.test_split: MPIDI_CH3_Init(105)................:\npostprocessors/num_nodes.test_split: MPID_nem_init(324).................:\npostprocessors/num_nodes.test_split: MPID_nem_tcp_init(175).............:\npostprocessors/num_nodes.test_split: MPID_nem_tcp_get_business_card(401):\npostprocessors/num_nodes.test_split: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\npostprocessors/num_nodes.test_split: (unknown)(): Invalid group\npostprocessors/num_nodes.test_split: Fatal error in MPI_Init_thread: Invalid group, error stack:\npostprocessors/num_nodes.test_split: MPIR_Init_thread(586)..............:\npostprocessors/num_nodes.test_split: MPID_Init(224).....................: channel initialization failed\npostprocessors/num_nodes.test_split: MPIDI_CH3_Init(105)................:\npostprocessors/num_nodes.test_split: MPID_nem_init(324).................:\npostprocessors/num_nodes.test_split: MPID_nem_tcp_init(175).............:\npostprocessors/num_nodes.test_split: MPID_nem_tcp_get_business_card(401):\npostprocessors/num_nodes.test_split: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\npostprocessors/num_nodes.test_split: (unknown)(): Invalid group\npostprocessors/num_nodes.test_split: Fatal error in MPI_Init_thread: Invalid group, error stack:\npostprocessors/num_nodes.test_split: MPIR_Init_thread(586)..............:\npostprocessors/num_nodes.test_split: MPID_Init(224).....................: channel initialization failed\npostprocessors/num_nodes.test_split: MPIDI_CH3_Init(105)................:\npostprocessors/num_nodes.test_split: MPID_nem_init(324).................:\npostprocessors/num_nodes.test_split: MPID_nem_tcp_init(175).............:\npostprocessors/num_nodes.test_split: MPID_nem_tcp_get_business_card(401):\npostprocessors/num_nodes.test_split: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\npostprocessors/num_nodes.test_split: (unknown)(): Invalid group\npostprocessors/num_nodes.test_split: Fatal error in MPI_Init_thread: Invalid group, error stack:\npostprocessors/num_nodes.test_split: MPIR_Init_thread(586)..............:\npostprocessors/num_nodes.test_split: MPID_Init(224).....................: channel initialization failed\npostprocessors/num_nodes.test_split: MPIDI_CH3_Init(105)................:\npostprocessors/num_nodes.test_split: MPID_nem_init(324).................:\npostprocessors/num_nodes.test_split: MPID_nem_tcp_init(175).............:\npostprocessors/num_nodes.test_split: MPID_nem_tcp_get_business_card(401):\npostprocessors/num_nodes.test_split: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\npostprocessors/num_nodes.test_split: (unknown)(): Invalid group\npostprocessors/num_nodes.test_split: Fatal error in MPI_Init_thread: Invalid group, error stack:\npostprocessors/num_nodes.test_split: MPIR_Init_thread(586)..............:\npostprocessors/num_nodes.test_split: MPID_Init(224).....................: channel initialization failed\npostprocessors/num_nodes.test_split: MPIDI_CH3_Init(105)................:\npostprocessors/num_nodes.test_split: MPID_nem_init(324).................:\npostprocessors/num_nodes.test_split: MPID_nem_tcp_init(175).............:\npostprocessors/num_nodes.test_split: MPID_nem_tcp_get_business_card(401):\npostprocessors/num_nodes.test_split: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\npostprocessors/num_nodes.test_split: (unknown)(): Invalid group\npostprocessors/num_nodes.test_split:\npostprocessors/num_nodes.test_split:\npostprocessors/num_nodes.test_split: Exit Code: 8\npostprocessors/num_nodes.test_split: ################################################################################\npostprocessors/num_nodes.test_split: Tester failed, reason: CRASH\npostprocessors/num_nodes.test_split:\npostprocessors/num_nodes.test_split .............................................. [min_cpus=4] FAILED (CRASH)\nmesh/checkpoint.test_4: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/mesh/checkpoint\nmesh/checkpoint.test_4: Running command: mpiexec -n 4 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i checkpoint_split.i Outputs/file_base=test_4 --use-split --split-file checkpoint_split_in --error --error-unused --error-override --no-gdb-backtrace\nmesh/checkpoint.test_4: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_4: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_4: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_4: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_4: MPID_nem_init(324).................:\nmesh/checkpoint.test_4: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_4: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_4: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmesh/checkpoint.test_4: (unknown)(): Invalid group\nmesh/checkpoint.test_4: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_4: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_4: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_4: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_4: MPID_nem_init(324).................:\nmesh/checkpoint.test_4: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_4: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_4: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmesh/checkpoint.test_4: (unknown)(): Invalid group\nmesh/checkpoint.test_4: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_4: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_4: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_4: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_4: MPID_nem_init(324).................:\nmesh/checkpoint.test_4: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_4: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_4: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmesh/checkpoint.test_4: (unknown)(): Invalid group\nmesh/checkpoint.test_4: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_4: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_4: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_4: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_4: MPID_nem_init(324).................:\nmesh/checkpoint.test_4: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_4: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_4: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmesh/checkpoint.test_4: (unknown)(): Invalid group\nmesh/checkpoint.test_4: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_4: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_4: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_4: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_4: MPID_nem_init(324).................:\nmesh/checkpoint.test_4: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_4: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_4: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmesh/checkpoint.test_4: (unknown)(): Invalid group\nmesh/checkpoint.test_4: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_4: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_4: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_4: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_4: MPID_nem_init(324).................:\nmesh/checkpoint.test_4: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_4: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_4: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmesh/checkpoint.test_4: (unknown)(): Invalid group\nmesh/checkpoint.test_4: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_4: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_4: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_4: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_4: MPID_nem_init(324).................:\nmesh/checkpoint.test_4: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_4: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_4: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmesh/checkpoint.test_4: (unknown)(): Invalid group\nmesh/checkpoint.test_4: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_4: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_4: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_4: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_4: MPID_nem_init(324).................:\nmesh/checkpoint.test_4: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_4: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_4: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmesh/checkpoint.test_4: (unknown)(): Invalid group\nmesh/checkpoint.test_4:\nmesh/checkpoint.test_4:\nmesh/checkpoint.test_4: Exit Code: 8\nmesh/checkpoint.test_4: ################################################################################\nmesh/checkpoint.test_4: Tester failed, reason: CRASH\nmesh/checkpoint.test_4:\nmesh/checkpoint.test_4 ........................................................... [min_cpus=4] FAILED (CRASH)\ndampers/min_damping.min_nodal_damping: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/dampers/min_damping\ndampers/min_damping.min_nodal_damping: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i min_nodal_damping.i --error --error-unused --error-override --no-gdb-backtrace --n-threads=2\ndampers/min_damping.min_nodal_damping: Fatal error in MPI_Init_thread: Invalid group, error stack:\ndampers/min_damping.min_nodal_damping: MPIR_Init_thread(586)..............:\ndampers/min_damping.min_nodal_damping: MPID_Init(224).....................: channel initialization failed\ndampers/min_damping.min_nodal_damping: MPIDI_CH3_Init(105)................:\ndampers/min_damping.min_nodal_damping: MPID_nem_init(324).................:\ndampers/min_damping.min_nodal_damping: MPID_nem_tcp_init(175).............:\ndampers/min_damping.min_nodal_damping: MPID_nem_tcp_get_business_card(401):\ndampers/min_damping.min_nodal_damping: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\ndampers/min_damping.min_nodal_damping: (unknown)(): Invalid group\ndampers/min_damping.min_nodal_damping: ################################################################################\ndampers/min_damping.min_nodal_damping:\ndampers/min_damping.min_nodal_damping: Unable to match the following pattern against the program's output:\ndampers/min_damping.min_nodal_damping:\ndampers/min_damping.min_nodal_damping: From damper: 'limit' damping below min_damping\ndampers/min_damping.min_nodal_damping:\ndampers/min_damping.min_nodal_damping: ################################################################################\ndampers/min_damping.min_nodal_damping: Tester failed, reason: EXPECTED OUTPUT MISSING\ndampers/min_damping.min_nodal_damping:\ndampers/min_damping.min_nodal_damping ............ [min_cpus=2,min_threads=2] FAILED (EXPECTED OUTPUT MISSING)\nmultiapps/max_procs_per_app.test: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/multiapps/max_procs_per_app\nmultiapps/max_procs_per_app.test: Running command: mpiexec -n 3 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i master.i --error --error-unused --error-override --no-gdb-backtrace\nmultiapps/max_procs_per_app.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmultiapps/max_procs_per_app.test: MPIR_Init_thread(586)..............:\nmultiapps/max_procs_per_app.test: MPID_Init(224).....................: channel initialization failed\nmultiapps/max_procs_per_app.test: MPIDI_CH3_Init(105)................:\nmultiapps/max_procs_per_app.test: MPID_nem_init(324).................:\nmultiapps/max_procs_per_app.test: MPID_nem_tcp_init(175).............:\nmultiapps/max_procs_per_app.test: MPID_nem_tcp_get_business_card(401):\nmultiapps/max_procs_per_app.test: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmultiapps/max_procs_per_app.test: (unknown)(): Invalid group\nmultiapps/max_procs_per_app.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmultiapps/max_procs_per_app.test: MPIR_Init_thread(586)..............:\nmultiapps/max_procs_per_app.test: MPID_Init(224).....................: channel initialization failed\nmultiapps/max_procs_per_app.test: MPIDI_CH3_Init(105)................:\nmultiapps/max_procs_per_app.test: MPID_nem_init(324).................:\nmultiapps/max_procs_per_app.test: MPID_nem_tcp_init(175).............:\nmultiapps/max_procs_per_app.test: MPID_nem_tcp_get_business_card(401):\nmultiapps/max_procs_per_app.test: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmultiapps/max_procs_per_app.test: (unknown)(): Invalid group\nmultiapps/max_procs_per_app.test:\nmultiapps/max_procs_per_app.test:\nmultiapps/max_procs_per_app.test: Exit Code: 8\nmultiapps/max_procs_per_app.test: ################################################################################\nmultiapps/max_procs_per_app.test: Tester failed, reason: CRASH\nmultiapps/max_procs_per_app.test:\nmultiapps/max_procs_per_app.test ................................................. [min_cpus=3] FAILED (CRASH)\nsamplers/base.parallel/threads ............................................................ [min_threads=2] OK\ntransfers/multiapp_mesh_function_transfer.errors/mismatch_exec_on ......................................... OK\ntransfers/multiapp_postprocessor_interpolation_transfer.fetype_error ...................................... OK\nvariables/optionally_coupled.catch_out_of_bound_default_access/coupled .................................... OK\ninterfacekernels/1d_interface.ik_save_in .................................................................. OK\nvariables/fe_hier.group/test_hier_3_3d .................................................................... OK\nrestrictable/block_api_test.has/blockIDs .................................................................. OK\nfunctions/solution_function.scale_transl .................................................................. OK\nfunctions/piecewise_multilinear.time ...................................................................... OK\nmultiapps/picard.steady_fixed_picard_its .................................................................. OK\nreporters/base.special_types .............................................................................. OK\noutputs/json/distributed.info/limit ....................................................................... OK\noutputs/exodus.nodal_output ............................................................................... OK\nfunctions/parsed.vals_error ............................................................................... OK\noutputs/variables.nemesis_hide: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/outputs/variables\noutputs/variables.nemesis_hide: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i nemesis_hide.i --error --error-unused --error-override --no-gdb-backtrace\noutputs/variables.nemesis_hide: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/variables.nemesis_hide: MPIR_Init_thread(586)..............:\noutputs/variables.nemesis_hide: MPID_Init(224).....................: channel initialization failed\noutputs/variables.nemesis_hide: MPIDI_CH3_Init(105)................:\noutputs/variables.nemesis_hide: MPID_nem_init(324).................:\noutputs/variables.nemesis_hide: MPID_nem_tcp_init(175).............:\noutputs/variables.nemesis_hide: MPID_nem_tcp_get_business_card(401):\noutputs/variables.nemesis_hide: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\noutputs/variables.nemesis_hide: (unknown)(): Invalid group\noutputs/variables.nemesis_hide: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/variables.nemesis_hide: MPIR_Init_thread(586)..............:\noutputs/variables.nemesis_hide: MPID_Init(224).....................: channel initialization failed\noutputs/variables.nemesis_hide: MPIDI_CH3_Init(105)................:\noutputs/variables.nemesis_hide: MPID_nem_init(324).................:\noutputs/variables.nemesis_hide: MPID_nem_tcp_init(175).............:\noutputs/variables.nemesis_hide: MPID_nem_tcp_get_business_card(401):\noutputs/variables.nemesis_hide: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\noutputs/variables.nemesis_hide: (unknown)(): Invalid group\noutputs/variables.nemesis_hide:\noutputs/variables.nemesis_hide:\noutputs/variables.nemesis_hide: Exit Code: 8\noutputs/variables.nemesis_hide: ################################################################################\noutputs/variables.nemesis_hide: Tester failed, reason: CRASH\noutputs/variables.nemesis_hide:\noutputs/variables.nemesis_hide ................................................... [min_cpus=2] FAILED (CRASH)\nmultiapps/steffensen_postprocessor.pp_transient/app_end_transfers_begin ................................... OK\noutputs/console.file_solve_log ............................................................................ OK\noutputs/intervals.no_final_repeat ......................................................................... OK\noutputs/iterative.start_stop/exodus_start_time ............................................................ OK\nmultiapps/secant_postprocessor.pp_transient/app_end_transfers_begin ....................................... OK\nmultiapps/picard_postprocessor.pp_transient/app_end_transfers_begin ....................................... OK\ninterfaces/random.parallel_verification: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/interfaces/random\ninterfaces/random.parallel_verification: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i random.i --error --error-unused --error-override --no-gdb-backtrace\ninterfaces/random.parallel_verification: Fatal error in MPI_Init_thread: Invalid group, error stack:\ninterfaces/random.parallel_verification: MPIR_Init_thread(586)..............:\ninterfaces/random.parallel_verification: MPID_Init(224).....................: channel initialization failed\ninterfaces/random.parallel_verification: MPIDI_CH3_Init(105)................:\ninterfaces/random.parallel_verification: MPID_nem_init(324).................:\ninterfaces/random.parallel_verification: MPID_nem_tcp_init(175).............:\ninterfaces/random.parallel_verification: MPID_nem_tcp_get_business_card(401):\ninterfaces/random.parallel_verification: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\ninterfaces/random.parallel_verification: (unknown)(): Invalid group\ninterfaces/random.parallel_verification: Fatal error in MPI_Init_thread: Invalid group, error stack:\ninterfaces/random.parallel_verification: MPIR_Init_thread(586)..............:\ninterfaces/random.parallel_verification: MPID_Init(224).....................: channel initialization failed\ninterfaces/random.parallel_verification: MPIDI_CH3_Init(105)................:\ninterfaces/random.parallel_verification: MPID_nem_init(324).................:\ninterfaces/random.parallel_verification: MPID_nem_tcp_init(175).............:\ninterfaces/random.parallel_verification: MPID_nem_tcp_get_business_card(401):\ninterfaces/random.parallel_verification: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\ninterfaces/random.parallel_verification: (unknown)(): Invalid group\ninterfaces/random.parallel_verification:\ninterfaces/random.parallel_verification:\ninterfaces/random.parallel_verification: Exit Code: 8\ninterfaces/random.parallel_verification: ################################################################################\ninterfaces/random.parallel_verification: Tester failed, reason: CRASH\ninterfaces/random.parallel_verification:\ninterfaces/random.parallel_verification .......................................... [min_cpus=2] FAILED (CRASH)\ninterfaces/random.test_par_mesh .................................................... [skipped dependency] SKIP\ninterfaces/random.threads_verification ............................................. [skipped dependency] SKIP\nmultiapps/secant.variables_transient/app_end_transfers_begin .............................................. OK\ninterfaces/vectorpostprocessorinterface.missing_errors/by_param_vector .................................... OK\ninterfaces/postprocessorinterface.missing_errors/by_param ................................................. OK\nnodalkernels/constraint_enforcement.upper_and_lower_bound/amg_fail ........................................ OK\nics/from_exodus_solution.array_nodal_var_2 ................................................................ OK\ninterfaces/userobjectinterface.has_uo/name ................................................................ OK\nfvkernels/mms/advective-outflow.MinModLimiter ............................................................. OK\nmultiapps/steffensen.variables_transient/app_end_transfers_begin .......................................... OK\nmesh/mesh_generation.annular/annulus ...................................................................... OK\nmisc/jacobian.jacobian_zero_2D ............................................................................ OK\nmesh/unique_ids.replicated_mesh: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/mesh/unique_ids\nmesh/unique_ids.replicated_mesh: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i unique_ids.i --error --error-unused --error-override --no-gdb-backtrace\nmesh/unique_ids.replicated_mesh: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/unique_ids.replicated_mesh: MPIR_Init_thread(586)..............:\nmesh/unique_ids.replicated_mesh: MPID_Init(224).....................: channel initialization failed\nmesh/unique_ids.replicated_mesh: MPIDI_CH3_Init(105)................:\nmesh/unique_ids.replicated_mesh: MPID_nem_init(324).................:\nmesh/unique_ids.replicated_mesh: MPID_nem_tcp_init(175).............:\nmesh/unique_ids.replicated_mesh: MPID_nem_tcp_get_business_card(401):\nmesh/unique_ids.replicated_mesh: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmesh/unique_ids.replicated_mesh: (unknown)(): Invalid group\nmesh/unique_ids.replicated_mesh:\nmesh/unique_ids.replicated_mesh:\nmesh/unique_ids.replicated_mesh: Exit Code: 8\nmesh/unique_ids.replicated_mesh: ################################################################################\nmesh/unique_ids.replicated_mesh: Tester failed, reason: CRASH\nmesh/unique_ids.replicated_mesh:\nmesh/unique_ids.replicated_mesh .................................................. [min_cpus=2] FAILED (CRASH)\nmesh/unique_ids.distributed_mesh ................................................... [skipped dependency] SKIP\nkernels/array_kernels.test_body_force ..................................................................... OK\ngeomsearch/3d_moving_penetration.pl_test2qtt .............................................................. OK\nkernels/hfem.robin_displaced .............................................................................. OK\nmesh/high_order_elems.test_tet10_refine ................................................................... OK\nmesh_modifiers/add_side_sets_from_bounding_box.overlap/test_overlapping_sidesets_error .................... OK\nmesh_modifiers/block_deleter.delete/BlockDeleterTest8 ..................................................... OK\nkernels/vector_fe.ad_lagrange_vec ......................................................................... OK\nmaterials/derivative_material_interface.warn .............................................................. OK\nmaterials/material.test ................................................................................... OK\noutputs/oversample.example/ex02_adapt ..................................................................... OK\ngeomsearch/3d_moving_penetration_smoothing.overlapping/pl_test4nns ........................................ OK\nmeshgenerators/combiner_generator.errors/mismatch_pos ..................................................... OK\nmaterials/output.group/multiple_files ..................................................................... OK\nfvkernels/fv_simple_diffusion.1d_dirichlet ................................................................ OK\ntime_steppers/timesequence_stepper.restart/timesequence_restart2 .......................................... OK\nmaterials/stateful_prop.stateful_copy ..................................................................... OK\nmeshgenerators/distributed_rectilinear/partition.2D_3: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/meshgenerators/distributed_rectilinear/partition\nmeshgenerators/distributed_rectilinear/partition.2D_3: Running command: mpiexec -n 3 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i squarish_partition.i --error --error-unused --error-override --no-gdb-backtrace\nmeshgenerators/distributed_rectilinear/partition.2D_3: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIR_Init_thread(586)..............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIDI_CH3_Init(105)................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_init(324).................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(175).............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmeshgenerators/distributed_rectilinear/partition.2D_3: (unknown)(): Invalid group\nmeshgenerators/distributed_rectilinear/partition.2D_3: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIR_Init_thread(586)..............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_Init(224).....................: channel initialization failed\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPIDI_CH3_Init(105)................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_init(324).................:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(175).............:\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_get_business_card(401):\nmeshgenerators/distributed_rectilinear/partition.2D_3: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmeshgenerators/distributed_rectilinear/partition.2D_3: (unknown)(): Invalid group\nmeshgenerators/distributed_rectilinear/partition.2D_3:\nmeshgenerators/distributed_rectilinear/partition.2D_3:\nmeshgenerators/distributed_rectilinear/partition.2D_3: Exit Code: 8\nmeshgenerators/distributed_rectilinear/partition.2D_3: ################################################################################\nmeshgenerators/distributed_rectilinear/partition.2D_3: Tester failed, reason: CRASH\nmeshgenerators/distributed_rectilinear/partition.2D_3:\nmeshgenerators/distributed_rectilinear/partition.2D_3 ............................ [min_cpus=3] FAILED (CRASH)\ntime_steppers/iteration_adaptive.shrink_init_dt_restart ................................................... OK\nuserobjects/layered_average.layered_average/layered_average_block ......................................... OK\nconstraints/equal_value_embedded_constraint.penalty/2D_3D ................................................. OK\npostprocessors/find_value_on_line.line_out_of_bounds ...................................................... OK\ntime_integrators/convergence.explicit_midpoint/level0 ..................................................... OK\npostprocessors/num_iterations.methods/explicit_euler ...................................................... OK\nproblems/eigen_problem/eigensolvers.monolith_newton ....................................................... OK\nparser/vector_range_checking.outofbounds .................................................................. OK\nbcs/periodic.testlevel1 ................................................................................... OK\ntransfers/multiapp_nearest_node_transfer.two_way_many_apps ................................................ OK\ndgkernels/2d_diffusion_dg.no_additional_rms ............................................................... OK\ntransfers/reporter_transfer.clone/serial .................................................................. OK\nrestart/restart_diffusion.restart_use_end_part1 ........................................................... OK\nmultiapps/check_error.sub_cycling_and_catch_up ............................................................ OK\ntransfers/multiapp_userobject_transfer.two_pipes .......................................................... OK\ninterfacekernels/1d_interface.ik_save_in_other_side ....................................................... OK\nrestrictable/block_api_test.has/isBlockSubset ............................................................. OK\nvariables/fe_hier.group/test_hier_2_3d .................................................................... OK\nfunctions/solution_function.scale_mult .................................................................... OK\nfunctions/piecewise_multilinear.twoDa ..................................................................... OK\nmultiapps/picard.steady_with_custom_convergence_check ..................................................... OK\noutputs/exodus.discontinuous .............................................................................. OK\noutputs/csv.align ......................................................................................... OK\noutputs/console.norms ..................................................................................... OK\ntransfers/multiapp_conservative_transfer.userobject_transfer .............................................. OK\noutputs/iterative.start_stop/output_step_window ........................................................... OK\nmultiapps/steffensen_postprocessor.pp_transient/app_end_transfers_end ..................................... OK\noutputs/debug.show_top_residuals_nonlinear_only ........................................................... OK\ntag.systems/test_tag_itegratedBCs ......................................................................... OK\ninterfaces/random.test_uo ................................................................................. OK\nmultiapps/secant_postprocessor.pp_transient/app_end_transfers_end ......................................... OK\ninterfaces/vectorpostprocessorinterface.missing_errors/by_name ............................................ OK\ninterfaces/postprocessorinterface.missing_errors/by_name .................................................. OK\nnodalkernels/constraint_enforcement.vi/rsls ............................................................... OK\ninterfaces/userobjectinterface.has_uo/param_T ............................................................. OK\nmultiapps/secant.variables_transient/app_end_transfers_end ................................................ OK\nmultiapps/picard_postprocessor.pp_transient/app_end_transfers_end ......................................... OK\nmesh/mesh_generation.annular/annulus_sector ............................................................... OK\ngeomsearch/2d_moving_penetration.pl_test2qtt .............................................................. OK\nmultiapps/steffensen.variables_transient/app_end_transfers_end ............................................ OK\nmisc/jacobian.inf_nan ..................................................................................... OK\nauxkernels/solution_aux.exodus_elemental .................................................................. OK\nkernels/hfem.robin_adpatation ............................................................................. OK\nmesh/high_order_elems.test_prism6_refine .................................................................. OK\nmesh_modifiers/block_deleter.delete/BlockDeleterTest9 ..................................................... OK\ngeomsearch/3d_moving_penetration.pl_test3 ................................................................. OK\nkernels/vector_fe.ad_lagrange_vec_jacobian ................................................................ OK\nmaterials/derivative_material_interface.bad_evaluation/nan ................................................ OK\nmaterials/material.check_test ............................................................................. OK\nmeshgenerators/combiner_generator.errors/mismatch_pos_from_file ........................................... OK\nmaterials/output.group/steady ............................................................................. OK\ngeomsearch/3d_moving_penetration_smoothing.overlapping/pl_test4nnstt ...................................... OK\ntime_steppers/timesequence_stepper.restart/timesequence_restart3 .......................................... OK\nfvkernels/mms/advective-outflow.SOULimiter ................................................................ OK\nfvkernels/fv_simple_diffusion.unstructured-rz ............................................................. OK\nuserobjects/setup_interface_count.setup_interface_count/ElementUserObject ................................. OK\nmeshgenerators/break_mesh_by_block_generator.surrounding_block_restricted/split_all ....................... OK\ntime_steppers/iteration_adaptive.pps_lim .................................................................. OK\nuserobjects/layered_average.layered_average/block_restricted .............................................. OK\nconstraints/equal_value_embedded_constraint.penalty/3D_3D ................................................. OK\npostprocessors/find_value_on_line.depth_exceeded .......................................................... OK\nmaterials/stateful_prop.adaptivity/spatially_const ........................................................ OK\npostprocessors/num_iterations.methods/explicit_midpoint ................................................... OK\ntime_integrators/convergence.explicit_midpoint/level1 ..................................................... OK\nparser/vector_range_checking.checkempty ................................................................... OK\nproblems/eigen_problem/eigensolvers.ne_deficient .......................................................... OK\ntransfers/reporter_transfer.clone/parallel .............................. [min_cpus=6,insufficient slots] SKIP\ntransfers/multiapp_nearest_node_transfer.parallel: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/transfers/multiapp_nearest_node_transfer\ntransfers/multiapp_nearest_node_transfer.parallel: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i parallel_master.i --error --error-unused --error-override --no-gdb-backtrace\ntransfers/multiapp_nearest_node_transfer.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_nearest_node_transfer.parallel: MPIR_Init_thread(586)..............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_nearest_node_transfer.parallel: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_init(324).................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\ntransfers/multiapp_nearest_node_transfer.parallel: (unknown)(): Invalid group\ntransfers/multiapp_nearest_node_transfer.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\ntransfers/multiapp_nearest_node_transfer.parallel: MPIR_Init_thread(586)..............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_Init(224).....................: channel initialization failed\ntransfers/multiapp_nearest_node_transfer.parallel: MPIDI_CH3_Init(105)................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_init(324).................:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(175).............:\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_get_business_card(401):\ntransfers/multiapp_nearest_node_transfer.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\ntransfers/multiapp_nearest_node_transfer.parallel: (unknown)(): Invalid group\ntransfers/multiapp_nearest_node_transfer.parallel:\ntransfers/multiapp_nearest_node_transfer.parallel:\ntransfers/multiapp_nearest_node_transfer.parallel: Exit Code: 8\ntransfers/multiapp_nearest_node_transfer.parallel: ################################################################################\ntransfers/multiapp_nearest_node_transfer.parallel: Tester failed, reason: CRASH\ntransfers/multiapp_nearest_node_transfer.parallel:\ntransfers/multiapp_nearest_node_transfer.parallel ................................ [min_cpus=2] FAILED (CRASH)\nrestart/restart_diffusion.restart_use_end_part2 ........................................................... OK\nsamplers/base.parallel/mpi: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/samplers/base\nsamplers/base.parallel/mpi: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i mpi.i --allow-test-objects --error --error-unused --error-override --no-gdb-backtrace\nsamplers/base.parallel/mpi: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsamplers/base.parallel/mpi: MPIR_Init_thread(586)..............:\nsamplers/base.parallel/mpi: MPID_Init(224).....................: channel initialization failed\nsamplers/base.parallel/mpi: MPIDI_CH3_Init(105)................:\nsamplers/base.parallel/mpi: MPID_nem_init(324).................:\nsamplers/base.parallel/mpi: MPID_nem_tcp_init(175).............:\nsamplers/base.parallel/mpi: MPID_nem_tcp_get_business_card(401):\nsamplers/base.parallel/mpi: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nsamplers/base.parallel/mpi: (unknown)(): Invalid group\nsamplers/base.parallel/mpi:\nsamplers/base.parallel/mpi:\nsamplers/base.parallel/mpi: Exit Code: 8\nsamplers/base.parallel/mpi: ################################################################################\nsamplers/base.parallel/mpi: Tester failed, reason: CRASH\nsamplers/base.parallel/mpi:\nsamplers/base.parallel/mpi ....................................................... [min_cpus=2] FAILED (CRASH)\nrestrictable/block_api_test.mat/hasBlockMaterialProperty_true ............................................. OK\ninterfacekernels/1d_interface.reaction_1D_steady_CSVDiff .................................................. OK\nfunctions/solution_function.nonexistent_var_err ........................................................... OK\noutputs/csv.sort .......................................................................................... OK\nbcs/periodic.testperiodic ................................................................................. OK\noutputs/console.timing .................................................................................... OK\nmultiapps/picard.steady_with_pseudo_transient_sub ......................................................... OK\nfunctions/piecewise_multilinear.twoDb ..................................................................... OK\noutputs/iterative.start_stop/output_start_step ............................................................ OK\noutputs/debug.show_top_residuals_scalar ................................................................... OK\ntag.systems/test_tag_interface_kernels .................................................................... OK\ninterfaces/vectorpostprocessorinterface.missing_errors/by_name_vector ..................................... OK\nnodalkernels/constraint_enforcement.vi/rsls_amg ........................................................... OK\nmultiapps/secant_postprocessor.pp_transient/app_begin_transfers_begin_secant_sub .......................... OK\nmultiapps/steffensen_postprocessor.pp_transient/app_begin_transfers_begin_steffensen_sub .................. OK\ntransfers/multiapp_conservative_transfer.userobject_transfer_csv .......................................... OK\ninterfaces/userobjectinterface.has_uo/name_T .............................................................. OK\nmesh/mesh_generation.annular/disc ......................................................................... OK\ngeomsearch/2d_moving_penetration.pl_test3 ................................................................. OK\nmultiapps/picard_postprocessor.pp_transient/app_begin_transfers_begin_picard_sub .......................... OK\nmultiapps/secant.variables_transient/app_begin_transfers_begin_secant_sub ................................. OK\nauxkernels/solution_aux.exodus_elemental_only ............................................................. OK\nmultiapps/steffensen.variables_transient/app_begin_transfers_begin_steffensen_sub ......................... OK\nkernels/hfem.variable_dirichlet ........................................................................... OK\nmesh/high_order_elems.test_prism15_refine ................................................................. OK\nmesh_modifiers/block_deleter.delete/BlockDeleterTest10 .................................................... OK\nbcs/dmg_periodic.check_one_step: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/bcs/dmg_periodic\nbcs/dmg_periodic.check_one_step: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i dmg_periodic_bc.i /UserObjects/uo/type=CheckGhostedBoundaries /UserObjects/uo/total_num_bdry_sides=160 Outputs/hide=\"pid\" Outputs/exodus=false Executioner/num_steps=1 --error --error-unused --error-override --no-gdb-backtrace\nbcs/dmg_periodic.check_one_step: Fatal error in MPI_Init_thread: Invalid group, error stack:\nbcs/dmg_periodic.check_one_step: MPIR_Init_thread(586)..............:\nbcs/dmg_periodic.check_one_step: MPID_Init(224).....................: channel initialization failed\nbcs/dmg_periodic.check_one_step: MPIDI_CH3_Init(105)................:\nbcs/dmg_periodic.check_one_step: MPID_nem_init(324).................:\nbcs/dmg_periodic.check_one_step: MPID_nem_tcp_init(175).............:\nbcs/dmg_periodic.check_one_step: MPID_nem_tcp_get_business_card(401):\nbcs/dmg_periodic.check_one_step: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nbcs/dmg_periodic.check_one_step: (unknown)(): Invalid group\nbcs/dmg_periodic.check_one_step:\nbcs/dmg_periodic.check_one_step:\nbcs/dmg_periodic.check_one_step: Exit Code: 8\nbcs/dmg_periodic.check_one_step: ################################################################################\nbcs/dmg_periodic.check_one_step: Tester failed, reason: CRASH\nbcs/dmg_periodic.check_one_step:\nbcs/dmg_periodic.check_one_step .................................................. [min_cpus=2] FAILED (CRASH)\nmaterials/derivative_material_interface.bad_evaluation/warning ............................................ OK\nmaterials/output.invalid_outputs .......................................................................... OK\ngeomsearch/3d_moving_penetration.pl_test3q ................................................................ OK\nfvkernels/mms/non-orthogonal.extended: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/fvkernels/mms/non-orthogonal\nfvkernels/mms/non-orthogonal.extended: Running command: python -m unittest -v test.TestExtendedADR\nfvkernels/mms/non-orthogonal.extended: The 'mms' package requires sympy for symbolic function evaluation, it can be installed by running pip install sympy --user.\nfvkernels/mms/non-orthogonal.extended: Running: /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i extended-adr.i Mesh/uniform_refine=0\nfvkernels/mms/non-orthogonal.extended: test (test.TestExtendedADR) ... Fatal error in MPI_Init_thread: Invalid group, error stack:\nfvkernels/mms/non-orthogonal.extended: MPIR_Init_thread(586)..............:\nfvkernels/mms/non-orthogonal.extended: MPID_Init(224).....................: channel initialization failed\nfvkernels/mms/non-orthogonal.extended: MPIDI_CH3_Init(105)................:\nfvkernels/mms/non-orthogonal.extended: MPID_nem_init(324).................:\nfvkernels/mms/non-orthogonal.extended: MPID_nem_tcp_init(175).............:\nfvkernels/mms/non-orthogonal.extended: MPID_nem_tcp_get_business_card(401):\nfvkernels/mms/non-orthogonal.extended: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nfvkernels/mms/non-orthogonal.extended: (unknown)(): Invalid group\nfvkernels/mms/non-orthogonal.extended: ERROR\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended: ======================================================================\nfvkernels/mms/non-orthogonal.extended: ERROR: test (test.TestExtendedADR)\nfvkernels/mms/non-orthogonal.extended: ----------------------------------------------------------------------\nfvkernels/mms/non-orthogonal.extended: Traceback (most recent call last):\nfvkernels/mms/non-orthogonal.extended:   File \"/Users/mohammadalabdullah/projects/moose/test/tests/fvkernels/mms/non-orthogonal/test.py\", line 23, in test\nfvkernels/mms/non-orthogonal.extended:     df1 = mms.run_spatial('extended-adr.i', 7, mpi=2)\nfvkernels/mms/non-orthogonal.extended:   File \"/Users/mohammadalabdullah/projects/moose/python/mms/runner.py\", line 129, in run_spatial\nfvkernels/mms/non-orthogonal.extended:     return _runner(*args, rtype=SPATIAL, **kwargs)\nfvkernels/mms/non-orthogonal.extended:   File \"/Users/mohammadalabdullah/projects/moose/python/mms/runner.py\", line 102, in _runner\nfvkernels/mms/non-orthogonal.extended:     raise IOError(\"The CSV output does not exist: {}\".format(csv))\nfvkernels/mms/non-orthogonal.extended: OSError: The CSV output does not exist: None\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended: ----------------------------------------------------------------------\nfvkernels/mms/non-orthogonal.extended: Ran 1 test in 0.123s\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended: FAILED (errors=1)\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended: Exit Code: 1\nfvkernels/mms/non-orthogonal.extended: ################################################################################\nfvkernels/mms/non-orthogonal.extended: Tester failed, reason: CRASH\nfvkernels/mms/non-orthogonal.extended:\nfvkernels/mms/non-orthogonal.extended ............................................ [min_cpus=2] FAILED (CRASH)\nmeshgenerators/break_mesh_by_block_generator.surrounding_block_restricted/no_transition ................... OK\ntime_steppers/iteration_adaptive.reject_large_dt .......................................................... OK\nuserobjects/layered_average.block_restricted_num_layers ................................................... OK\npostprocessors/num_iterations.methods/explicit_tvd_rk2 .................................................... OK\ntime_integrators/convergence.explicit_midpoint/level2 ..................................................... OK\ntransfers/reporter_transfer.clone_type/type_specified ................... [min_cpus=6,insufficient slots] SKIP\nmaterials/stateful_prop.adaptivity/spatially_varying ...................................................... OK\npreconditioners/hmg.hmg: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/preconditioners/hmg\npreconditioners/hmg.hmg: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i diffusion_hmg.i --error --error-unused --error-override --no-gdb-backtrace\npreconditioners/hmg.hmg: Fatal error in MPI_Init_thread: Invalid group, error stack:\npreconditioners/hmg.hmg: MPIR_Init_thread(586)..............:\npreconditioners/hmg.hmg: MPID_Init(224).....................: channel initialization failed\npreconditioners/hmg.hmg: MPIDI_CH3_Init(105)................:\npreconditioners/hmg.hmg: MPID_nem_init(324).................:\npreconditioners/hmg.hmg: MPID_nem_tcp_init(175).............:\npreconditioners/hmg.hmg: MPID_nem_tcp_get_business_card(401):\npreconditioners/hmg.hmg: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\npreconditioners/hmg.hmg: (unknown)(): Invalid group\npreconditioners/hmg.hmg: Fatal error in MPI_Init_thread: Invalid group, error stack:\npreconditioners/hmg.hmg: MPIR_Init_thread(586)..............:\npreconditioners/hmg.hmg: MPID_Init(224).....................: channel initialization failed\npreconditioners/hmg.hmg: MPIDI_CH3_Init(105)................:\npreconditioners/hmg.hmg: MPID_nem_init(324).................:\npreconditioners/hmg.hmg: MPID_nem_tcp_init(175).............:\npreconditioners/hmg.hmg: MPID_nem_tcp_get_business_card(401):\npreconditioners/hmg.hmg: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\npreconditioners/hmg.hmg: (unknown)(): Invalid group\npreconditioners/hmg.hmg: ################################################################################\npreconditioners/hmg.hmg:\npreconditioners/hmg.hmg: Unable to match the following pattern against the program's output:\npreconditioners/hmg.hmg:\npreconditioners/hmg.hmg: using\\s+allatonce\\s+MatPtAP()\\s+implementation\npreconditioners/hmg.hmg:\npreconditioners/hmg.hmg: ################################################################################\npreconditioners/hmg.hmg: Tester failed, reason: EXPECTED OUTPUT MISSING\npreconditioners/hmg.hmg:\npreconditioners/hmg.hmg ........................................ [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nproblems/eigen_problem/eigensolvers.nonlinear_laplace ..................................................... OK\nsamplers/base.parallel/iter ............................................................................... OK\nsystem_interfaces.mpi: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/system_interfaces\nsystem_interfaces.mpi: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i input.i --error --error-unused --error-override --no-gdb-backtrace\nsystem_interfaces.mpi: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.mpi: MPIR_Init_thread(586)..............:\nsystem_interfaces.mpi: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.mpi: MPIDI_CH3_Init(105)................:\nsystem_interfaces.mpi: MPID_nem_init(324).................:\nsystem_interfaces.mpi: MPID_nem_tcp_init(175).............:\nsystem_interfaces.mpi: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.mpi: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nsystem_interfaces.mpi: (unknown)(): Invalid group\nsystem_interfaces.mpi: ################################################################################\nsystem_interfaces.mpi:\nsystem_interfaces.mpi: Unable to match the following pattern against the program's output:\nsystem_interfaces.mpi:\nsystem_interfaces.mpi: Num Processors:\\s+2\\s+Num Threads:\\s+1\nsystem_interfaces.mpi:\nsystem_interfaces.mpi: ################################################################################\nsystem_interfaces.mpi: Tester failed, reason: EXPECTED OUTPUT MISSING\nsystem_interfaces.mpi:\nsystem_interfaces.mpi .......................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nrestrictable/block_api_test.mat/hasBlockMaterialProperty_false ............................................ OK\nfunctions/solution_function.solution_function/grad_p1 ..................................................... OK\noutputs/console.transient ................................................................................. OK\nbcs/periodic.testperiodic_vector .......................................................................... OK\nfunctions/piecewise_multilinear.twoD_const ................................................................ OK\nmultiapps/picard.steady_with_postprocessor_convergence .................................................... OK\noutputs/debug.show_material_properties_consumed ........................................................... OK\noutputs/iterative.start_stop/output_end_step .............................................................. OK\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/misc/exception\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i parallel_exception_residual_transient.i Kernels/exception/rank=1 --error --error-unused --error-override --no-gdb-backtrace\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPIR_Init_thread(586)..............:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_Init(224).....................: channel initialization failed\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPIDI_CH3_Init(105)................:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_init(324).................:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_init(175).............:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_get_business_card(401):\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: (unknown)(): Invalid group\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPIR_Init_thread(586)..............:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_Init(224).....................: channel initialization failed\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPIDI_CH3_Init(105)................:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_init(324).................:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_init(175).............:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_get_business_card(401):\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: (unknown)(): Invalid group\nmisc/exception.parallel_exception_residual_transient_non_zero_rank:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Exit Code: 8\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: ################################################################################\nmisc/exception.parallel_exception_residual_transient_non_zero_rank: Tester failed, reason: CRASH\nmisc/exception.parallel_exception_residual_transient_non_zero_rank:\nmisc/exception.parallel_exception_residual_transient_non_zero_rank ............... [min_cpus=2] FAILED (CRASH)\ntag.systems/test_tag_dirac_kernels ........................................................................ OK\nnodalkernels/constraint_enforcement.vi/ssls ............................................................... OK\nfvkernels/mms/advective-outflow.kt/KTLimitedCD ............................................................ OK\ntransfers/multiapp_conservative_transfer.negative_adjuster ................................................ OK\nmesh/mesh_generation.annular/disc_sector .................................................................. OK\ngeomsearch/2d_moving_penetration.pl_test3q ................................................................ OK\nmultiapps/steffensen_postprocessor.pp_transient/app_begin_transfers_end_steffensen_sub .................... OK\nmultiapps/secant_postprocessor.pp_transient/app_begin_transfers_end_secant_sub ............................ OK\nauxkernels/solution_aux.exodus_direct ..................................................................... OK\nkernels/hfem.robin_distributed_mesh ....................................................................... OK\nmultiapps/picard_postprocessor.pp_transient/app_begin_transfers_end_picard_sub ............................ OK\nmesh/high_order_elems.test_prism18_refine ................................................................. OK\nmesh_modifiers/block_deleter.delete/BlockDeleterTest12 .................................................... OK\nmultiapps/secant.variables_transient/app_begin_transfers_end_secant_sub ................................... OK\nkernels/vector_fe.jacobian ................................................................................ OK\nmaterials/derivative_material_interface.bad_evaluation/error .............................................. OK\nmaterials/material.exception/serial ....................................................................... OK\nmaterials/output.warn_unsupported_types ................................................................... OK\ntime_steppers/timesequence_stepper.restart_failure/timesequence_restart_failure_1 ......................... OK\ngeomsearch/3d_moving_penetration_smoothing.overlapping/pl_test4ns ......................................... OK\nmultiapps/steffensen.variables_transient/app_begin_transfers_end_steffensen_sub ........................... OK\nuserobjects/setup_interface_count.setup_interface_count/SideUserObject .................................... OK\nvectorpostprocessors/line_value_sampler.parallel: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/vectorpostprocessors/line_value_sampler\nvectorpostprocessors/line_value_sampler.parallel: Running command: mpiexec -n 3 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i line_value_sampler.i --error --error-unused --error-override --no-gdb-backtrace\nvectorpostprocessors/line_value_sampler.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/line_value_sampler.parallel: MPIR_Init_thread(586)..............:\nvectorpostprocessors/line_value_sampler.parallel: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/line_value_sampler.parallel: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/line_value_sampler.parallel: MPID_nem_init(324).................:\nvectorpostprocessors/line_value_sampler.parallel: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/line_value_sampler.parallel: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/line_value_sampler.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nvectorpostprocessors/line_value_sampler.parallel: (unknown)(): Invalid group\nvectorpostprocessors/line_value_sampler.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/line_value_sampler.parallel: MPIR_Init_thread(586)..............:\nvectorpostprocessors/line_value_sampler.parallel: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/line_value_sampler.parallel: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/line_value_sampler.parallel: MPID_nem_init(324).................:\nvectorpostprocessors/line_value_sampler.parallel: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/line_value_sampler.parallel: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/line_value_sampler.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nvectorpostprocessors/line_value_sampler.parallel: (unknown)(): Invalid group\nvectorpostprocessors/line_value_sampler.parallel:\nvectorpostprocessors/line_value_sampler.parallel:\nvectorpostprocessors/line_value_sampler.parallel: Exit Code: 8\nvectorpostprocessors/line_value_sampler.parallel: ################################################################################\nvectorpostprocessors/line_value_sampler.parallel: Tester failed, reason: CRASH\nvectorpostprocessors/line_value_sampler.parallel:\nvectorpostprocessors/line_value_sampler.parallel ................................. [min_cpus=3] FAILED (CRASH)\ngeomsearch/3d_moving_penetration.pl_test3tt ............................................................... OK\ntime_steppers/iteration_adaptive.piecewise_linear ......................................................... OK\nuserobjects/layered_average.block_restricted_bounding_block ............................................... OK\ntransfers/reporter_transfer.clone_type/error ............................ [min_cpus=5,insufficient slots] SKIP\ntime_integrators/convergence.explicit_euler/level0 ........................................................ OK\npostprocessors/num_iterations.methods/heun ................................................................ OK\nmaterials/stateful_prop.many_stateful_props ............................................................... OK\npreconditioners/hmg.hmg_3D: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/preconditioners/hmg\npreconditioners/hmg.hmg_3D: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i diffusion_hmg.i Mesh/dmg/dim=3 Mesh/dmg/nz=10 Outputs/file_base=diffusion_hmg_3d_out -log_view --error --error-unused --error-override --no-gdb-backtrace\npreconditioners/hmg.hmg_3D: Fatal error in MPI_Init_thread: Invalid group, error stack:\npreconditioners/hmg.hmg_3D: MPIR_Init_thread(586)..............:\npreconditioners/hmg.hmg_3D: MPID_Init(224).....................: channel initialization failed\npreconditioners/hmg.hmg_3D: MPIDI_CH3_Init(105)................:\npreconditioners/hmg.hmg_3D: MPID_nem_init(324).................:\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_init(175).............:\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_get_business_card(401):\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\npreconditioners/hmg.hmg_3D: (unknown)(): Invalid group\npreconditioners/hmg.hmg_3D: Fatal error in MPI_Init_thread: Invalid group, error stack:\npreconditioners/hmg.hmg_3D: MPIR_Init_thread(586)..............:\npreconditioners/hmg.hmg_3D: MPID_Init(224).....................: channel initialization failed\npreconditioners/hmg.hmg_3D: MPIDI_CH3_Init(105)................:\npreconditioners/hmg.hmg_3D: MPID_nem_init(324).................:\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_init(175).............:\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_get_business_card(401):\npreconditioners/hmg.hmg_3D: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\npreconditioners/hmg.hmg_3D: (unknown)(): Invalid group\npreconditioners/hmg.hmg_3D: ################################################################################\npreconditioners/hmg.hmg_3D:\npreconditioners/hmg.hmg_3D: Unable to match the following pattern against the program's output:\npreconditioners/hmg.hmg_3D:\npreconditioners/hmg.hmg_3D: PETSc\\s+Preconditioner:\\s+hmg\\s+strong_threshold:\\s+0.7\npreconditioners/hmg.hmg_3D:\npreconditioners/hmg.hmg_3D: ################################################################################\npreconditioners/hmg.hmg_3D: Tester failed, reason: EXPECTED OUTPUT MISSING\npreconditioners/hmg.hmg_3D:\npreconditioners/hmg.hmg_3D ..................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nrestart/restart_diffusion.restart_use_end_error_check ..................................................... OK\nsamplers/base.global_vs_local/base_1rank .................................................................. OK\nproblems/eigen_problem/eigensolvers.coupled_system ........................................................ OK\ninterfacekernels/1d_interface.reaction_1D_steady_ExoDiff .................................................. OK\nsystem_interfaces.thread/openmp ........................................................... [min_threads=2] OK\nbcs/periodic.testperiodic_dp: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/bcs/periodic\nbcs/periodic.testperiodic_dp: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i periodic_bc_displaced_problem.i --error --error-unused --error-override --no-gdb-backtrace\nbcs/periodic.testperiodic_dp: Fatal error in MPI_Init_thread: Invalid group, error stack:\nbcs/periodic.testperiodic_dp: MPIR_Init_thread(586)..............:\nbcs/periodic.testperiodic_dp: MPID_Init(224).....................: channel initialization failed\nbcs/periodic.testperiodic_dp: MPIDI_CH3_Init(105)................:\nbcs/periodic.testperiodic_dp: MPID_nem_init(324).................:\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_init(175).............:\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_get_business_card(401):\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nbcs/periodic.testperiodic_dp: (unknown)(): Invalid group\nbcs/periodic.testperiodic_dp: Fatal error in MPI_Init_thread: Invalid group, error stack:\nbcs/periodic.testperiodic_dp: MPIR_Init_thread(586)..............:\nbcs/periodic.testperiodic_dp: MPID_Init(224).....................: channel initialization failed\nbcs/periodic.testperiodic_dp: MPIDI_CH3_Init(105)................:\nbcs/periodic.testperiodic_dp: MPID_nem_init(324).................:\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_init(175).............:\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_get_business_card(401):\nbcs/periodic.testperiodic_dp: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nbcs/periodic.testperiodic_dp: (unknown)(): Invalid group\nbcs/periodic.testperiodic_dp:\nbcs/periodic.testperiodic_dp:\nbcs/periodic.testperiodic_dp: Exit Code: 8\nbcs/periodic.testperiodic_dp: ################################################################################\nbcs/periodic.testperiodic_dp: Tester failed, reason: CRASH\nbcs/periodic.testperiodic_dp:\nbcs/periodic.testperiodic_dp ..................................................... [min_cpus=2] FAILED (CRASH)\noutputs/console.transient_perf_int ........................................................................ OK\nfunctions/solution_function.solution_function/grad_p2 ..................................................... OK\ninterfaces/random.parallel_verification_uo: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/interfaces/random\ninterfaces/random.parallel_verification_uo: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i random_uo.i --error --error-unused --error-override --no-gdb-backtrace\ninterfaces/random.parallel_verification_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\ninterfaces/random.parallel_verification_uo: MPIR_Init_thread(586)..............:\ninterfaces/random.parallel_verification_uo: MPID_Init(224).....................: channel initialization failed\ninterfaces/random.parallel_verification_uo: MPIDI_CH3_Init(105)................:\ninterfaces/random.parallel_verification_uo: MPID_nem_init(324).................:\ninterfaces/random.parallel_verification_uo: MPID_nem_tcp_init(175).............:\ninterfaces/random.parallel_verification_uo: MPID_nem_tcp_get_business_card(401):\ninterfaces/random.parallel_verification_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\ninterfaces/random.parallel_verification_uo: (unknown)(): Invalid group\ninterfaces/random.parallel_verification_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\ninterfaces/random.parallel_verification_uo: MPIR_Init_thread(586)..............:\ninterfaces/random.parallel_verification_uo: MPID_Init(224).....................: channel initialization failed\ninterfaces/random.parallel_verification_uo: MPIDI_CH3_Init(105)................:\ninterfaces/random.parallel_verification_uo: MPID_nem_init(324).................:\ninterfaces/random.parallel_verification_uo: MPID_nem_tcp_init(175).............:\ninterfaces/random.parallel_verification_uo: MPID_nem_tcp_get_business_card(401):\ninterfaces/random.parallel_verification_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\ninterfaces/random.parallel_verification_uo: (unknown)(): Invalid group\ninterfaces/random.parallel_verification_uo:\ninterfaces/random.parallel_verification_uo:\ninterfaces/random.parallel_verification_uo: Exit Code: 8\ninterfaces/random.parallel_verification_uo: ################################################################################\ninterfaces/random.parallel_verification_uo: Tester failed, reason: CRASH\ninterfaces/random.parallel_verification_uo:\ninterfaces/random.parallel_verification_uo ....................................... [min_cpus=2] FAILED (CRASH)\ninterfaces/random.test_uo_par_mesh ................................................. [skipped dependency] SKIP\ninterfaces/random.threads_verification_uo .......................................... [skipped dependency] SKIP\nmultiapps/picard.steady_with_postprocessor_diff_convergence ............................................... OK\nmisc/exception.parallel_exception_jacobian_transient ...................................................... OK\ntag.systems/test_tag_nodal_kernels ........................................................................ OK\nnodalkernels/constraint_enforcement.vi/ssls_amg ........................................................... OK\ntransfers/multiapp_conservative_transfer.skip_adjustment .................................................. OK\nmesh/mesh_generation.spiral_annular_mesh .................................................................. OK\ngeomsearch/2d_moving_penetration.pl_test3tt ............................................................... OK\nmultiapps/secant_postprocessor.pp_transient/app_end_transfers_begin_secant_sub ............................ OK\nmultiapps/steffensen_postprocessor.pp_transient/app_end_transfers_begin_steffensen_sub .................... OK\nauxkernels/solution_aux.exodus_interp ..................................................................... OK\nkernels/hfem.accurate_lower_d_volumes/2d .................................................................. OK\nmultiapps/picard_postprocessor.pp_transient/app_end_transfers_begin_picard_sub ............................ OK\nmesh/high_order_elems.test_pyramid5 ....................................................................... OK\nmaterials/derivative_material_interface.bad_evaluation/exception .......................................... OK\nmultiapps/secant.variables_transient/app_end_transfers_begin_secant_sub ................................... OK\ntime_steppers/timesequence_stepper.restart_failure/timesequence_restart_failure2 .......................... OK\nkernels/vector_fe.coupled_gradient_dot_em_gauge ........................................................... OK\nmaterials/output.show_added_aux_vars ...................................................................... OK\nuserobjects/setup_interface_count.setup_interface_count/InternalSideUserObject ............................ OK\ngeomsearch/3d_moving_penetration_smoothing.overlapping/pl_test4nstt ....................................... OK\nmultiapps/steffensen.variables_transient/app_end_transfers_begin_steffensen_sub ........................... OK\nmeshgenerators/break_mesh_by_block_generator.surrounding_block_restricted/transition_split ................ OK\nvectorpostprocessors/line_value_sampler.scaling ........................................................... OK\ngeomsearch/3d_moving_penetration.pl_test4 ................................................................. OK\ntime_steppers/iteration_adaptive.multi_piecewise_linear_function_point .................................... OK\ntime_integrators/convergence.explicit_euler/level1 ........................................................ OK\npostprocessors/num_iterations.methods/implicit_euler ...................................................... OK\nmaterials/stateful_prop.ad/reg ............................................................................ OK\npreconditioners/hmg.hmg_strumpack: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/preconditioners/hmg\npreconditioners/hmg.hmg_strumpack: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i diffusion_hmg.i Mesh/dmg/dim=3 Mesh/dmg/nz=10 Outputs/file_base=diffusion_strumpack_3d_out -pc_type lu -pc_factor_mat_solver_type strumpack -snes_view --error --error-unused --error-override --no-gdb-backtrace\npreconditioners/hmg.hmg_strumpack: Fatal error in MPI_Init_thread: Invalid group, error stack:\npreconditioners/hmg.hmg_strumpack: MPIR_Init_thread(586)..............:\npreconditioners/hmg.hmg_strumpack: MPID_Init(224).....................: channel initialization failed\npreconditioners/hmg.hmg_strumpack: MPIDI_CH3_Init(105)................:\npreconditioners/hmg.hmg_strumpack: MPID_nem_init(324).................:\npreconditioners/hmg.hmg_strumpack: MPID_nem_tcp_init(175).............:\npreconditioners/hmg.hmg_strumpack: MPID_nem_tcp_get_business_card(401):\npreconditioners/hmg.hmg_strumpack: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\npreconditioners/hmg.hmg_strumpack: (unknown)(): Invalid group\npreconditioners/hmg.hmg_strumpack: Fatal error in MPI_Init_thread: Invalid group, error stack:\npreconditioners/hmg.hmg_strumpack: MPIR_Init_thread(586)..............:\npreconditioners/hmg.hmg_strumpack: MPID_Init(224).....................: channel initialization failed\npreconditioners/hmg.hmg_strumpack: MPIDI_CH3_Init(105)................:\npreconditioners/hmg.hmg_strumpack: MPID_nem_init(324).................:\npreconditioners/hmg.hmg_strumpack: MPID_nem_tcp_init(175).............:\npreconditioners/hmg.hmg_strumpack: MPID_nem_tcp_get_business_card(401):\npreconditioners/hmg.hmg_strumpack: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\npreconditioners/hmg.hmg_strumpack: (unknown)(): Invalid group\npreconditioners/hmg.hmg_strumpack: ################################################################################\npreconditioners/hmg.hmg_strumpack:\npreconditioners/hmg.hmg_strumpack: Unable to match the following pattern against the program's output:\npreconditioners/hmg.hmg_strumpack:\npreconditioners/hmg.hmg_strumpack: package\\s+used\\s+to\\s+perform\\s+factorization:\\s+strumpack\npreconditioners/hmg.hmg_strumpack:\npreconditioners/hmg.hmg_strumpack: ################################################################################\npreconditioners/hmg.hmg_strumpack: Tester failed, reason: EXPECTED OUTPUT MISSING\npreconditioners/hmg.hmg_strumpack:\npreconditioners/hmg.hmg_strumpack .............................. [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nsamplers/base.global_vs_local/base_2rank: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/samplers/base\nsamplers/base.global_vs_local/base_2rank: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i global_vs_local.i --allow-test-objects --error --error-unused --error-override --no-gdb-backtrace\nsamplers/base.global_vs_local/base_2rank: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsamplers/base.global_vs_local/base_2rank: MPIR_Init_thread(586)..............:\nsamplers/base.global_vs_local/base_2rank: MPID_Init(224).....................: channel initialization failed\nsamplers/base.global_vs_local/base_2rank: MPIDI_CH3_Init(105)................:\nsamplers/base.global_vs_local/base_2rank: MPID_nem_init(324).................:\nsamplers/base.global_vs_local/base_2rank: MPID_nem_tcp_init(175).............:\nsamplers/base.global_vs_local/base_2rank: MPID_nem_tcp_get_business_card(401):\nsamplers/base.global_vs_local/base_2rank: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nsamplers/base.global_vs_local/base_2rank: (unknown)(): Invalid group\nsamplers/base.global_vs_local/base_2rank:\nsamplers/base.global_vs_local/base_2rank:\nsamplers/base.global_vs_local/base_2rank: Exit Code: 8\nsamplers/base.global_vs_local/base_2rank: ################################################################################\nsamplers/base.global_vs_local/base_2rank: Tester failed, reason: CRASH\nsamplers/base.global_vs_local/base_2rank:\nsamplers/base.global_vs_local/base_2rank ......................................... [min_cpus=2] FAILED (CRASH)\nproblems/eigen_problem/eigensolvers.eigen_scalar_kernel ................................................... OK\nsystem_interfaces.partitioner/parmetis: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/system_interfaces\nsystem_interfaces.partitioner/parmetis: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i input.i --error --error-unused --error-override --no-gdb-backtrace\nsystem_interfaces.partitioner/parmetis: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.partitioner/parmetis: MPIR_Init_thread(586)..............:\nsystem_interfaces.partitioner/parmetis: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.partitioner/parmetis: MPIDI_CH3_Init(105)................:\nsystem_interfaces.partitioner/parmetis: MPID_nem_init(324).................:\nsystem_interfaces.partitioner/parmetis: MPID_nem_tcp_init(175).............:\nsystem_interfaces.partitioner/parmetis: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.partitioner/parmetis: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nsystem_interfaces.partitioner/parmetis: (unknown)(): Invalid group\nsystem_interfaces.partitioner/parmetis:\nsystem_interfaces.partitioner/parmetis:\nsystem_interfaces.partitioner/parmetis: Exit Code: 8\nsystem_interfaces.partitioner/parmetis: ################################################################################\nsystem_interfaces.partitioner/parmetis: Tester failed, reason: CRASH\nsystem_interfaces.partitioner/parmetis:\nsystem_interfaces.partitioner/parmetis ........................................... [min_cpus=2] FAILED (CRASH)\nnodalkernels/constant_rate.group/threaded ................................................. [min_threads=3] OK\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/misc/exception\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i parallel_exception_jacobian_transient.i Kernels/exception/rank=1 --error --error-unused --error-override --no-gdb-backtrace\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: MPIR_Init_thread(586)..............:\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: MPID_Init(224).....................: channel initialization failed\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: MPIDI_CH3_Init(105)................:\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: MPID_nem_init(324).................:\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: MPID_nem_tcp_init(175).............:\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: MPID_nem_tcp_get_business_card(401):\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: (unknown)(): Invalid group\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: MPIR_Init_thread(586)..............:\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: MPID_Init(224).....................: channel initialization failed\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: MPIDI_CH3_Init(105)................:\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: MPID_nem_init(324).................:\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: MPID_nem_tcp_init(175).............:\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: MPID_nem_tcp_get_business_card(401):\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: (unknown)(): Invalid group\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: Exit Code: 8\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: ################################################################################\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank: Tester failed, reason: CRASH\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank:\nmisc/exception.parallel_exception_jacobian_transient_non_zero_rank ............... [min_cpus=2] FAILED (CRASH)\nmultiapps/picard.postprocessor_convergence_history ........................................................ OK\nnodalkernels/constraint_enforcement.vi/rsls_pjfnk ......................................................... OK\nmesh/mesh_generation.ringleb_mesh ......................................................................... OK\ngeomsearch/2d_moving_penetration.pl_test3qtt .............................................................. OK\nauxkernels/solution_aux.exodus_interp_restart/part1 ....................................................... OK\nmultiapps/secant_postprocessor.pp_transient/app_end_transfers_end_secant_sub .............................. OK\nkernels/hfem.accurate_lower_d_volumes/3d .................................................................. OK\nmultiapps/steffensen_postprocessor.pp_transient/app_end_transfers_end_steffensen_sub ...................... OK\nmesh/high_order_elems.test_pyramid13 ...................................................................... OK\nfvkernels/mms/advective-outflow.kt/KTLimitedUpwind ........................................................ OK\nkernels/vector_fe.coupled_gradient_dot_jacobian ........................................................... OK\nmultiapps/picard_postprocessor.pp_transient/app_end_transfers_end_picard_sub .............................. OK\nmultiapps/secant.variables_transient/app_end_transfers_end_secant_sub ..................................... OK\nmaterials/derivative_material_interface.postprocessor_coupling/parsed_material ............................ OK\nuserobjects/setup_interface_count.setup_interface_count/NodalSideUserObject ............................... OK\nmeshgenerators/break_mesh_by_block_generator.block_pairs_restricted ....................................... OK\ngeomsearch/3d_moving_penetration_smoothing.overlapping/pl_test4qns ........................................ OK\nvectorpostprocessors/line_value_sampler.delimiter ......................................................... OK\nrestart/restartable_types.parallel/second .......................................... [skipped dependency] SKIP\nrestart/restartable_types.parallel/first: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/restart/restartable_types\nrestart/restartable_types.parallel/first: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i restartable_types.i --error --error-unused --error-override --no-gdb-backtrace\nrestart/restartable_types.parallel/first: Fatal error in MPI_Init_thread: Invalid group, error stack:\nrestart/restartable_types.parallel/first: MPIR_Init_thread(586)..............:\nrestart/restartable_types.parallel/first: MPID_Init(224).....................: channel initialization failed\nrestart/restartable_types.parallel/first: MPIDI_CH3_Init(105)................:\nrestart/restartable_types.parallel/first: MPID_nem_init(324).................:\nrestart/restartable_types.parallel/first: MPID_nem_tcp_init(175).............:\nrestart/restartable_types.parallel/first: MPID_nem_tcp_get_business_card(401):\nrestart/restartable_types.parallel/first: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nrestart/restartable_types.parallel/first: (unknown)(): Invalid group\nrestart/restartable_types.parallel/first:\nrestart/restartable_types.parallel/first:\nrestart/restartable_types.parallel/first: Exit Code: 8\nrestart/restartable_types.parallel/first: ################################################################################\nrestart/restartable_types.parallel/first: Tester failed, reason: CRASH\nrestart/restartable_types.parallel/first:\nrestart/restartable_types.parallel/first ......................................... [min_cpus=2] FAILED (CRASH)\ntime_steppers/iteration_adaptive.multi_piecewise_linear_function_change ................................... OK\nmultiapps/steffensen.variables_transient/app_end_transfers_end_steffensen_sub ............................. OK\npostprocessors/num_iterations.methods/implicit_midpoint ................................................... OK\nmaterials/stateful_prop.ad/ad ............................................................................. OK\nfunctions/image_function.threshold_adapt_parallel: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/functions/image_function\nfunctions/image_function.threshold_adapt_parallel: Running command: mpiexec -n 3 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i threshold_adapt_parallel.i Outputs/exodus=true --error --error-unused --error-override --no-gdb-backtrace\nfunctions/image_function.threshold_adapt_parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\nfunctions/image_function.threshold_adapt_parallel: MPIR_Init_thread(586)..............:\nfunctions/image_function.threshold_adapt_parallel: MPID_Init(224).....................: channel initialization failed\nfunctions/image_function.threshold_adapt_parallel: MPIDI_CH3_Init(105)................:\nfunctions/image_function.threshold_adapt_parallel: MPID_nem_init(324).................:\nfunctions/image_function.threshold_adapt_parallel: MPID_nem_tcp_init(175).............:\nfunctions/image_function.threshold_adapt_parallel: MPID_nem_tcp_get_business_card(401):\nfunctions/image_function.threshold_adapt_parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nfunctions/image_function.threshold_adapt_parallel: (unknown)(): Invalid group\nfunctions/image_function.threshold_adapt_parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\nfunctions/image_function.threshold_adapt_parallel: MPIR_Init_thread(586)..............:\nfunctions/image_function.threshold_adapt_parallel: MPID_Init(224).....................: channel initialization failed\nfunctions/image_function.threshold_adapt_parallel: MPIDI_CH3_Init(105)................:\nfunctions/image_function.threshold_adapt_parallel: MPID_nem_init(324).................:\nfunctions/image_function.threshold_adapt_parallel: MPID_nem_tcp_init(175).............:\nfunctions/image_function.threshold_adapt_parallel: MPID_nem_tcp_get_business_card(401):\nfunctions/image_function.threshold_adapt_parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nfunctions/image_function.threshold_adapt_parallel: (unknown)(): Invalid group\nfunctions/image_function.threshold_adapt_parallel:\nfunctions/image_function.threshold_adapt_parallel:\nfunctions/image_function.threshold_adapt_parallel: Exit Code: 8\nfunctions/image_function.threshold_adapt_parallel: ################################################################################\nfunctions/image_function.threshold_adapt_parallel: Tester failed, reason: CRASH\nfunctions/image_function.threshold_adapt_parallel:\nfunctions/image_function.threshold_adapt_parallel ................................ [min_cpus=3] FAILED (CRASH)\ninterfacekernels/1d_interface.reaction_1D_transient ....................................................... OK\nsystem_interfaces.partitioner/ptscotch: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/system_interfaces\nsystem_interfaces.partitioner/ptscotch: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i input.i --error --error-unused --error-override --no-gdb-backtrace\nsystem_interfaces.partitioner/ptscotch: Fatal error in MPI_Init_thread: Invalid group, error stack:\nsystem_interfaces.partitioner/ptscotch: MPIR_Init_thread(586)..............:\nsystem_interfaces.partitioner/ptscotch: MPID_Init(224).....................: channel initialization failed\nsystem_interfaces.partitioner/ptscotch: MPIDI_CH3_Init(105)................:\nsystem_interfaces.partitioner/ptscotch: MPID_nem_init(324).................:\nsystem_interfaces.partitioner/ptscotch: MPID_nem_tcp_init(175).............:\nsystem_interfaces.partitioner/ptscotch: MPID_nem_tcp_get_business_card(401):\nsystem_interfaces.partitioner/ptscotch: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nsystem_interfaces.partitioner/ptscotch: (unknown)(): Invalid group\nsystem_interfaces.partitioner/ptscotch:\nsystem_interfaces.partitioner/ptscotch:\nsystem_interfaces.partitioner/ptscotch: Exit Code: 8\nsystem_interfaces.partitioner/ptscotch: ################################################################################\nsystem_interfaces.partitioner/ptscotch: Tester failed, reason: CRASH\nsystem_interfaces.partitioner/ptscotch:\nsystem_interfaces.partitioner/ptscotch ........................................... [min_cpus=2] FAILED (CRASH)\nfunctions/piecewise_multilinear.fourDa .................................................................... OK\ninterfaces/random.material_serial ......................................................................... OK\nics/random_ic_test.test_threaded .......................................................... [min_threads=2] OK\nmisc/exception.parallel_exception_initial_condition ....................................................... OK\ntag.systems/test_eigen .................................................................................... OK\nmesh/mesh_generation.annular_except1_deprecated ........................................................... OK\nnodalkernels/constraint_enforcement.unbounded ............................................................. OK\nmesh/custom_partitioner.group/custom_linear_partitioner: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/mesh/custom_partitioner\nmesh/custom_partitioner.group/custom_linear_partitioner: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i custom_linear_partitioner_test.i --error --error-unused --error-override --no-gdb-backtrace\nmesh/custom_partitioner.group/custom_linear_partitioner: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIR_Init_thread(586)..............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_Init(224).....................: channel initialization failed\nmesh/custom_partitioner.group/custom_linear_partitioner: MPIDI_CH3_Init(105)................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_init(324).................:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(175).............:\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_get_business_card(401):\nmesh/custom_partitioner.group/custom_linear_partitioner: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmesh/custom_partitioner.group/custom_linear_partitioner: (unknown)(): Invalid group\nmesh/custom_partitioner.group/custom_linear_partitioner:\nmesh/custom_partitioner.group/custom_linear_partitioner:\nmesh/custom_partitioner.group/custom_linear_partitioner: Exit Code: 8\nmesh/custom_partitioner.group/custom_linear_partitioner: ################################################################################\nmesh/custom_partitioner.group/custom_linear_partitioner: Tester failed, reason: CRASH\nmesh/custom_partitioner.group/custom_linear_partitioner:\nmesh/custom_partitioner.group/custom_linear_partitioner .......................... [min_cpus=2] FAILED (CRASH)\nrunWorker Exception: Traceback (most recent call last):\nFile \"/Users/mohammadalabdullah/projects/moose/python/TestHarness/schedulers/Scheduler.py\", line 445, in runJob\nself.queueJobs(Jobs, j_lock)\nFile \"/Users/mohammadalabdullah/projects/moose/python/TestHarness/schedulers/Scheduler.py\", line 266, in queueJobs\nself.__runner_pool_jobs.add(self.run_pool.apply_async(self.runJob, (job, Jobs, j_lock)))\nFile \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/multiprocessing/pool.py\", line 455, in apply_async\nself._check_running()\nFile \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/multiprocessing/pool.py\", line 350, in _check_running\nraise ValueError(\"Pool not running\")\nValueError: Pool not running\nrunWorker Exception: Traceback (most recent call last):\nFile \"/Users/mohammadalabdullah/projects/moose/python/TestHarness/schedulers/Scheduler.py\", line 445, in runJob\nself.queueJobs(Jobs, j_lock)\nFile \"/Users/mohammadalabdullah/projects/moose/python/TestHarness/schedulers/Scheduler.py\", line 266, in queueJobs\nself.__runner_pool_jobs.add(self.run_pool.apply_async(self.runJob, (job, Jobs, j_lock)))\nFile \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/multiprocessing/pool.py\", line 455, in apply_async\nself._check_running()\nFile \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/multiprocessing/pool.py\", line 350, in _check_running\nraise ValueError(\"Pool not running\")\nValueError: Pool not running",
                          "url": "https://github.com/idaholab/moose/discussions/18440#discussioncomment-1052231",
                          "updatedAt": "2021-07-26T16:53:21Z",
                          "publishedAt": "2021-07-26T16:53:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "malanzey"
                          },
                          "bodyText": "There is also more from the top\nsamplers/distribute.scale/execute: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/samplers/distribute\nsamplers/distribute.scale/execute: Running command: /Users/mohammadalabdullah/projects/moose/test/tests/samplers/distribute/execute.py\nsamplers/distribute.scale/execute: mpiexec -n 1 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i distribute.i Outputs/file_base=distribute_1 Postprocessors/test/test_type=getGlobalSamples Samplers/sampler/num_rows=1\nsamplers/distribute.scale/execute: Traceback (most recent call last):\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/projects/moose/test/tests/samplers/distribute/execute.py\", line 47, in \nsamplers/distribute.scale/execute:     execute('distribute.i', 'distribute_none', 1, args.processors, 'getGlobalSamples')\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/projects/moose/test/tests/samplers/distribute/execute.py\", line 29, in execute\nsamplers/distribute.scale/execute:     local = pandas.read_csv('{}.csv'.format(file_base))\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/site-packages/pandas/io/parsers.py\", line 605, in read_csv\nsamplers/distribute.scale/execute:     return _read(filepath_or_buffer, kwds)\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/site-packages/pandas/io/parsers.py\", line 457, in _read\nsamplers/distribute.scale/execute:     parser = TextFileReader(filepath_or_buffer, **kwds)\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/site-packages/pandas/io/parsers.py\", line 814, in init\nsamplers/distribute.scale/execute:     self._engine = self._make_engine(self.engine)\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1045, in _make_engine\nsamplers/distribute.scale/execute:     return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1862, in init\nsamplers/distribute.scale/execute:     self._open_handles(src, kwds)\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1357, in _open_handles\nsamplers/distribute.scale/execute:     self.handles = get_handle(\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/site-packages/pandas/io/common.py\", line 642, in get_handle\nsamplers/distribute.scale/execute:     handle = open(\nsamplers/distribute.scale/execute: FileNotFoundError: [Errno 2] No such file or directory: 'distribute_1.csv'\nsamplers/distribute.scale/execute: mpiexec -n 1 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i distribute.i Outputs/file_base=distribute_1 Postprocessors/test/test_type=getGlobalSamples Samplers/sampler/num_rows=1\nsamplers/distribute.scale/execute: Traceback (most recent call last):\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/projects/moose/test/tests/samplers/distribute/execute.py\", line 47, in \nsamplers/distribute.scale/execute:     execute('distribute.i', 'distribute_none', 1, args.processors, 'getGlobalSamples')\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/projects/moose/test/tests/samplers/distribute/execute.py\", line 29, in execute\nsamplers/distribute.scale/execute:     local = pandas.read_csv('{}.csv'.format(file_base))\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/site-packages/pandas/io/parsers.py\", line 605, in read_csv\nsamplers/distribute.scale/execute:     return _read(filepath_or_buffer, kwds)\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/site-packages/pandas/io/parsers.py\", line 457, in _read\nsamplers/distribute.scale/execute:     parser = TextFileReader(filepath_or_buffer, **kwds)\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/site-packages/pandas/io/parsers.py\", line 814, in init\nsamplers/distribute.scale/execute:     self._engine = self._make_engine(self.engine)\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1045, in _make_engine\nsamplers/distribute.scale/execute:     return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1862, in init\nsamplers/distribute.scale/execute:     self._open_handles(src, kwds)\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1357, in _open_handles\nsamplers/distribute.scale/execute:     self.handles = get_handle(\nsamplers/distribute.scale/execute:   File \"/Users/mohammadalabdullah/miniconda3/envs/moose/lib/python3.9/site-packages/pandas/io/common.py\", line 642, in get_handle\nsamplers/distribute.scale/execute:     handle = open(\nsamplers/distribute.scale/execute: FileNotFoundError: [Errno 2] No such file or directory: 'distribute_1.csv'\nsamplers/distribute.scale/execute:\nsamplers/distribute.scale/execute:\nsamplers/distribute.scale/execute: Exit Code: 1\nsamplers/distribute.scale/execute: ################################################################################\nsamplers/distribute.scale/execute: Tester failed, reason: CRASH\nsamplers/distribute.scale/execute:\nsamplers/distribute.scale/execute ............................................................. FAILED (CRASH)\nsamplers/distribute.scale/plot ..................................................... [skipped dependency] SKIP\nactions/meta_action.test_meta_action ...................................................................... OK\nactions/meta_action_multiple_tasks.circle_quads ......................................................\nreporters/iteration_info.info/default ..................................................................... OK\nreporters/base.base: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/reporters/base\nreporters/base.base: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i base.i --allow-test-objects --error --error-unused --error-override --no-gdb-backtrace\nreporters/base.base: Fatal error in MPI_Init_thread: Invalid group, error stack:\nreporters/base.base: MPIR_Init_thread(586)..............:\nreporters/base.base: MPID_Init(224).....................: channel initialization failed\nreporters/base.base: MPIDI_CH3_Init(105)................:\nreporters/base.base: MPID_nem_init(324).................:\nreporters/base.base: MPID_nem_tcp_init(175).............:\nreporters/base.base: MPID_nem_tcp_get_business_card(401):\nreporters/base.base: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nreporters/base.base: (unknown)(): Invalid group\nreporters/base.base:\nreporters/base.base:\nreporters/base.base: Exit Code: 8\nreporters/base.base: ################################################################################\nreporters/base.base: Tester failed, reason: CRASH\nreporters/base.base:\nreporters/base.base .............................................................. [min_cpus=2] FAILED (CRASH)\nvbcs/fv_neumannbc.fvbcs_internal ......................................................................... OK\nperformance.multiprocess/mpi: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/performance\nperformance.multiprocess/mpi: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i input.i --error --error-unused --error-override --no-gdb-backtrace\nperformance.multiprocess/mpi: Fatal error in MPI_Init_thread: Invalid group, error stack:\nperformance.multiprocess/mpi: MPIR_Init_thread(586)..............:\nperformance.multiprocess/mpi: MPID_Init(224).....................: channel initialization failed\nperformance.multiprocess/mpi: MPIDI_CH3_Init(105)................:\nperformance.multiprocess/mpi: MPID_nem_init(324).................:\nperformance.multiprocess/mpi: MPID_nem_tcp_init(175).............:\nperformance.multiprocess/mpi: MPID_nem_tcp_get_business_card(401):\nperformance.multiprocess/mpi: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nperformance.multiprocess/mpi: (unknown)(): Invalid group\nperformance.multiprocess/mpi: ################################################################################\nperformance.multiprocess/mpi:\nperformance.multiprocess/mpi: Unable to match the following pattern against the program's output:\nperformance.multiprocess/mpi:\nperformance.multiprocess/mpi: Num Processors:\\s+2\\s+Num Threads:\\s+1\nperformance.multiprocess/mpi:\nperformance.multiprocess/mpi: ################################################################################\nperformance.multiprocess/mpi: Tester failed, reason: EXPECTED OUTPUT MISSING\nperformance.multiprocess/mpi:\nperformance.multiprocess/mpi ................................... [min_cpus=2] FAILED (EXPECTED OUTPUT MISSING)\nrestrictable/check_error.errors/mesh_null ................................................................. OK\nrestrictable/boundary_api_test.ids/boundaryIDs ............................................................ OK\nreporters/constant_reporter.errors/no_values .............................................................. OK\nreporters/base.error ...................................................................................... OK\nreporters/declare_initial_setup.declareInitialSetup/initialSetup_with_info ................................ OK\nreporters/iteration_info.info/limit ....................................................................... OK\noutputs/misc.getOutputs ................................................................................... OK\noutputs/displaced.non_displaced_fallback .................................................................. OK\noutputs/checkpoint.interval/test_recover .................................................................. OK\noutputs/transferred_scalar_variable.group/csv ............................................................. OK\noutputs/postprocessor.invalid_outputs ..................................................................... OK\nfunctions/parsed.transient ................................................................................ OK\noutputs/dofmap.group/simple_screen ........................................................................ OK\noutputs/tecplot.test_append ............................................................................... OK\noutputs/displacement.displaced_eq_test .................................................................... OK\noutputs/exodus.input ...................................................................................... OK\noutputs/output_on.show_outputs ............................................................................ OK\noutputs/vtk.files/parallel: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/outputs/vtk\noutputs/vtk.files/parallel: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i vtk_parallel.i --error --error-unused --error-override --no-gdb-backtrace\noutputs/vtk.files/parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/vtk.files/parallel: MPIR_Init_thread(586)..............:\noutputs/vtk.files/parallel: MPID_Init(224).....................: channel initialization failed\noutputs/vtk.files/parallel: MPIDI_CH3_Init(105)................:\noutputs/vtk.files/parallel: MPID_nem_init(324).................:\noutputs/vtk.files/parallel: MPID_nem_tcp_init(175).............:\noutputs/vtk.files/parallel: MPID_nem_tcp_get_business_card(401):\noutputs/vtk.files/parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\noutputs/vtk.files/parallel: (unknown)(): Invalid group\noutputs/vtk.files/parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/vtk.files/parallel: MPIR_Init_thread(586)..............:\noutputs/vtk.files/parallel: MPID_Init(224).....................: channel initialization failed\noutputs/vtk.files/parallel: MPIDI_CH3_Init(105)................:\noutputs/vtk.files/parallel: MPID_nem_init(324).................:\noutputs/vtk.files/parallel: MPID_nem_tcp_init(175).............:\noutputs/vtk.files/parallel: MPID_nem_tcp_get_business_card(401):\noutputs/vtk.files/parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\noutputs/vtk.files/parallel: (unknown)(): Invalid group\noutputs/vtk.files/parallel:\noutputs/vtk.files/parallel:\noutputs/vtk.files/parallel: Exit Code: 8\noutputs/vtk.files/parallel: ################################################################################\noutputs/vtk.files/parallel: Tester failed, reason: CRASH\noutputs/vtk.files/parallel:\noutputs/vtk.files/parallel ....................................................... [min_cpus=2] FAILED (CRASH)\nmarkers/two_circle_marker.group/two_circle_marker_gaussian_ic ............................................. OK\nvectorpostprocessors/work_balance.work_balance/replicated: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/vectorpostprocessors/work_balance\nvectorpostprocessors/work_balance.work_balance/replicated: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i work_balance.i --error --error-unused --error-override --no-gdb-backtrace\nvectorpostprocessors/work_balance.work_balance/replicated: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/work_balance.work_balance/replicated: MPIR_Init_thread(586)..............:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/work_balance.work_balance/replicated: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_init(324).................:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nvectorpostprocessors/work_balance.work_balance/replicated: (unknown)(): Invalid group\nvectorpostprocessors/work_balance.work_balance/replicated: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/work_balance.work_balance/replicated: MPIR_Init_thread(586)..............:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/work_balance.work_balance/replicated: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_init(324).................:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/work_balance.work_balance/replicated: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nvectorpostprocessors/work_balance.work_balance/replicated: (unknown)(): Invalid group\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated: Exit Code: 8\nvectorpostprocessors/work_balance.work_balance/replicated: ################################################################################\nvectorpostprocessors/work_balance.work_balance/replicated: Tester failed, reason: CRASH\nvectorpostprocessors/work_balance.work_balance/replicated:\nvectorpostprocessors/work_balance.work_balance/replicated ........................ [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/elements_along_plane.3d .............................................................. OK\nvectorpostprocessors/parallel_consistency.test: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/vectorpostprocessors/parallel_consistency\nvectorpostprocessors/parallel_consistency.test: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i parallel_consistency.i --error --error-unused --error-override --no-gdb-backtrace\nvectorpostprocessors/parallel_consistency.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.test: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.test: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.test: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nvectorpostprocessors/parallel_consistency.test: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.test: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/parallel_consistency.test: MPIR_Init_thread(586)..............:\nvectorpostprocessors/parallel_consistency.test: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/parallel_consistency.test: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_init(324).................:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/parallel_consistency.test: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nvectorpostprocessors/parallel_consistency.test: (unknown)(): Invalid group\nvectorpostprocessors/parallel_consistency.test:\nvectorpostprocessors/parallel_consistency.test:\nvectorpostprocessors/parallel_consistency.test: Exit Code: 8\nvectorpostprocessors/parallel_consistency.test: ################################################################################\nvectorpostprocessors/parallel_consistency.test: Tester failed, reason: CRASH\nvectorpostprocessors/parallel_consistency.test:\nvectorpostprocessors/parallel_consistency.test ................................... [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/csv_reader.parallel: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/vectorpostprocessors/csv_reader\nvectorpostprocessors/csv_reader.parallel: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i read.i UserObjects/tester/rank=1 Outputs/csv=false --error --error-unused --error-override --no-gdb-backtrace\nvectorpostprocessors/csv_reader.parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/csv_reader.parallel: MPIR_Init_thread(586)..............:\nvectorpostprocessors/csv_reader.parallel: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/csv_reader.parallel: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/csv_reader.parallel: MPID_nem_init(324).................:\nvectorpostprocessors/csv_reader.parallel: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/csv_reader.parallel: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/csv_reader.parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nvectorpostprocessors/csv_reader.parallel: (unknown)(): Invalid group\nvectorpostprocessors/csv_reader.parallel:\nvectorpostprocessors/csv_reader.parallel:\nvectorpostprocessors/csv_reader.parallel: Exit Code: 8\nvectorpostprocessors/csv_reader.parallel: ################################################################################\nvectorpostprocessors/csv_reader.parallel: Tester failed, reason: CRASH\nvectorpostprocessors/csv_reader.parallel:\nvectorpostprocessors/csv_reader.parallel ......................................... [min_cpus=2] FAILED (CRASH)\nvectorpostprocessors/intersection_points_along_line.intersecting_elems/3d ................................. OK\nvectorpostprocessors/element_id_counters.element_counter_with_ids_on_side ...\nfunctions/image_function.3d ............................................................................... OK\nreporters/mesh_info.info/default: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/reporters/mesh_info\nreporters/mesh_info.info/default: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i mesh_info.i --error --error-unused --error-override --no-gdb-backtrace\nreporters/mesh_info.info/default: Fatal error in MPI_Init_thread: Invalid group, error stack:\nreporters/mesh_info.info/default: MPIR_Init_thread(586)..............:\nreporters/mesh_info.info/default: MPID_Init(224).....................: channel initialization failed\nreporters/mesh_info.info/default: MPIDI_CH3_Init(105)................:\nreporters/mesh_info.info/default: MPID_nem_init(324).................:\nreporters/mesh_info.info/default: MPID_nem_tcp_init(175).............:\nreporters/mesh_info.info/default: MPID_nem_tcp_get_business_card(401):\nreporters/mesh_info.info/default: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nreporters/mesh_info.info/default: (unknown)(): Invalid group\nreporters/mesh_info.info/default:\nreporters/mesh_info.info/default: ################################################################################\nreporters/mesh_info.info/default: Tester failed, reason: CRASH\nreporters/mesh_info.info/default:\nreporters/mesh_info.info/default ................................................. [min_cpus=2] FAILED (CRASH)\nreporters/constant_reporter.errors/no_names ............................................................... OK\npreconditioners/fdp.jacobian_fdp_coloring_diagonal_test_fail .............................................. OK\nreporters/declare_initial_setup.decalareInitialSetup_with_get ............................................. OK\nreporters/iteration_info.info/steady ...................................................................... OK\noutputs/misc.getOutputs_with_names ........................................................................ OK\noutputs/displaced.mesh_use_displaced_mesh_false ........................................................... OK\noutputs/checkpoint.default/recover_half_transient ......................................................... OK\noutputs/postprocessor.console ............................................................................. OK\nfunctions/parsed.vector ................................................................................... OK\noutputs/dofmap.group/uniform_refine ....................................................................... OK\noutputs/exodus.enable_initial ............................................................................. OK\noutputs/displacement.displacement_transient_test .......................................................... OK\noutputs/vtk.solution/diff_serial_mesh ..................................................................... OK\noutputs/common.console/scalar_variables ................................................................... OK\noutputs/system_info.mesh .................................................................................. OK\noutputs/iterative.iterative/exodus_steady ................................................................. OK\noutputs/variables.test_nonexistent ........................................................................ OK\noutputs/intervals.output_final ............................................................................ OK\noutputs/error.reserved/none_reserved ...................................................................... OK\noutputs/csv.no_time ....................................................................................... OK\noutputs/csv_final_and_latest.no_link ...................................................................... OK\noutputs/oversample.group/adapt ............................................................................ OK\noutputs/recover.part1_latest .............................................................................. OK\noutputs/console.scalar_variables .......................................................................... OK\nfviks/one-var-diffusion.run_except3 ....................................................................... OK\noutputs/perf_graph/multi_app.perf/with_full ............................................................... OK\nics/vector_function_ic.comp_x_error ....................................................................... OK\nics/depend_on_uo.ic_depend_on_uo: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/ics/depend_on_uo\nics/depend_on_uo.ic_depend_on_uo: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i geometric_neighbors_ic.i --error --error-unused --error-override --no-gdb-backtrace\nics/depend_on_uo.ic_depend_on_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\nics/depend_on_uo.ic_depend_on_uo: MPIR_Init_thread(586)..............:\nics/depend_on_uo.ic_depend_on_uo: MPID_Init(224).....................: channel initialization failed\nics/depend_on_uo.ic_depend_on_uo: MPIDI_CH3_Init(105)................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_init(324).................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(175).............:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_get_business_card(401):\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nics/depend_on_uo.ic_depend_on_uo: (unknown)(): Invalid group\nics/depend_on_uo.ic_depend_on_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\nics/depend_on_uo.ic_depend_on_uo: MPIR_Init_thread(586)..............:\nics/depend_on_uo.ic_depend_on_uo: MPID_Init(224).....................: channel initialization failed\nics/depend_on_uo.ic_depend_on_uo: MPIDI_CH3_Init(105)................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_init(324).................:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(175).............:\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_get_business_card(401):\nics/depend_on_uo.ic_depend_on_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nics/depend_on_uo.ic_depend_on_uo: (unknown)(): Invalid group\nics/depend_on_uo.ic_depend_on_uo:\nics/depend_on_uo.ic_depend_on_uo:\nics/depend_on_uo.ic_depend_on_uo: Exit Code: 8\nics/depend_on_uo.ic_depend_on_uo: ################################################################################\nics/depend_on_uo.ic_depend_on_uo: Tester failed, reason: CRASH\nics/depend_on_uo.ic_depend_on_uo:\nics/depend_on_uo.ic_depend_on_uo ................................................. [min_cpus=2] FAILED (CRASH)\nics/check_error.ics_on_same_block_both_global ..............................\nortar/displaced-gap-conductance-2d-non-conforming.linear_exact_verification .............................. OK\nvectorpostprocessors/csv_reader.tester_fail: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/vectorpostprocessors/csv_reader\nvectorpostprocessors/csv_reader.tester_fail: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i read.i UserObjects/tester/rank=1 UserObjects/tester/gold='1 2 3' Outputs/csv=false --error --error-unused --error-override --no-gdb-backtrace --keep-cout --redirect-output tester_fail\nvectorpostprocessors/csv_reader.tester_fail: Fatal error in MPI_Init_thread: Invalid group, error stack:\nvectorpostprocessors/csv_reader.tester_fail: MPIR_Init_thread(586)..............:\nvectorpostprocessors/csv_reader.tester_fail: MPID_Init(224).....................: channel initialization failed\nvectorpostprocessors/csv_reader.tester_fail: MPIDI_CH3_Init(105)................:\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_init(324).................:\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_tcp_init(175).............:\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_tcp_get_business_card(401):\nvectorpostprocessors/csv_reader.tester_fail: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nvectorpostprocessors/csv_reader.tester_fail: (unknown)(): Invalid group\nvectorpostprocessors/csv_reader.tester_fail: ################################################################################\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: Unable to match the following pattern against the program's output:\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: The supplied gold data does not match the VPP data on the given rank.\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail: ################################################################################\nvectorpostprocessors/csv_reader.tester_fail: Tester failed, reason: EXPECTED ERROR MISSING\nvectorpostprocessors/csv_reader.tester_fail:\nvectorpostprocessors/csv_reader.tester_fail ..................... [min_cpus=2] FAILED (EXPECTED ERROR MISSING)\noutputs/dofmap.group/transient ............................................................................ OK\noutputs/vtk.solution/diff_serial_mesh_parallel: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/outputs/vtk\noutputs/vtk.solution/diff_serial_mesh_parallel: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i vtk_diff_serial_mesh_parallel.i --error --error-unused --error-override --no-gdb-backtrace\noutputs/vtk.solution/diff_serial_mesh_parallel: Fatal error in MPI_Init_thread: Invalid group, error stack:\noutputs/vtk.solution/diff_serial_mesh_parallel: MPIR_Init_thread(586)..............:\noutputs/vtk.solution/diff_serial_mesh_parallel: MPID_Init(224).....................: channel initialization failed\noutputs/vtk.solution/diff_serial_mesh_parallel: MPIDI_CH3_Init(105)................:\noutputs/vtk.solution/diff_serial_mesh_parallel: MPID_nem_init(324).................:\noutputs/vtk.solution/diff_serial_mesh_parallel: MPID_nem_tcp_init(175).............:\noutputs/vtk.solution/diff_serial_mesh_parallel: MPID_nem_tcp_get_business_card(401):\noutputs/vtk.solution/diff_serial_mesh_parallel: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\noutputs/vtk.solution/diff_serial_mesh_parallel: (unknown)(): Invalid group\noutputs/vtk.solution/diff_serial_mesh_parallel:\noutputs/vtk.solution/diff_serial_mesh_parallel: ################################################################################\noutputs/vtk.solution/diff_serial_mesh_parallel: Tester failed, reason: CRASH\noutputs/vtk.solution/diff_serial_mesh_parallel:\noutputs/vtk.solution/diff_serial_mesh_parallel ................................... [min_cpus=2] FAILED (CRASH)\noutputs/exodus.output_all ................................................................................. OK\nostprocessors/find_value_on_line.above_max_default_continue .............................................. OK\nmesh/checkpoint.test_2: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/mesh/checkpoint\nmesh/checkpoint.test_2: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i checkpoint_split.i Outputs/file_base=test_2 --use-split --split-file checkpoint_split_in --error --error-unused --error-override --no-gdb-backtrace\nmesh/checkpoint.test_2: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_2: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_2: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_2: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_2: MPID_nem_init(324).................:\nmesh/checkpoint.test_2: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_2: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_2: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmesh/checkpoint.test_2: (unknown)(): Invalid group\nmesh/checkpoint.test_2: Fatal error in MPI_Init_thread: Invalid group, error stack:\nmesh/checkpoint.test_2: MPIR_Init_thread(586)..............:\nmesh/checkpoint.test_2: MPID_Init(224).....................: channel initialization failed\nmesh/checkpoint.test_2: MPIDI_CH3_Init(105)................:\nmesh/checkpoint.test_2: MPID_nem_init(324).................:\nmesh/checkpoint.test_2: MPID_nem_tcp_init(175).............:\nmesh/checkpoint.test_2: MPID_nem_tcp_get_business_card(401):\nmesh/checkpoint.test_2: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nmesh/checkpoint.test_2: (unknown)(): Invalid group\nmesh/checkpoint.test_2:\nmesh/checkpoint.test_2:\nmesh/checkpoint.test_2: Exit Code: 8\nmesh/checkpoint.test_2: ################################################################################\nmesh/checkpoint.test_2: Tester failed, reason: CRASH\nmesh/checkpoint.test_2:\nmesh/checkpoint.test_2 ........................................................... [min_cpus=2] FAILED (CRASH)\nutils/2d_linear_interpolation.xyz_error ................................................................... OK\nmisc/max_var_n_dofs_per_elem.3d .......................................................\ninterfacekernels/1d_interface.jacobian_test ............................................................... OK\ninterfacekernels/2d_interface.parallel_fdp_test: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/interfacekernels/2d_interface\ninterfacekernels/2d_interface.parallel_fdp_test: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i coupled_value_coupled_flux.i Preconditioning/smp/type=FDP --error-unused --error-override --no-gdb-backtrace\ninterfacekernels/2d_interface.parallel_fdp_test: Fatal error in MPI_Init_thread: Invalid group, error stack:\ninterfacekernels/2d_interface.parallel_fdp_test: MPIR_Init_thread(586)..............:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_Init(224).....................: channel initialization failed\ninterfacekernels/2d_interface.parallel_fdp_test: MPIDI_CH3_Init(105)................:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_init(324).................:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_init(175).............:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_get_business_card(401):\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\ninterfacekernels/2d_interface.parallel_fdp_test: (unknown)(): Invalid group\ninterfacekernels/2d_interface.parallel_fdp_test: Fatal error in MPI_Init_thread: Invalid group, error stack:\ninterfacekernels/2d_interface.parallel_fdp_test: MPIR_Init_thread(586)..............:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_Init(224).....................: channel initialization failed\ninterfacekernels/2d_interface.parallel_fdp_test: MPIDI_CH3_Init(105)................:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_init(324).................:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_init(175).............:\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_get_business_card(401):\ninterfacekernels/2d_interface.parallel_fdp_test: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\ninterfacekernels/2d_interface.parallel_fdp_test: (unknown)(): Invalid group\ninterfacekernels/2d_interface.parallel_fdp_test:\ninterfacekernels/2d_interface.parallel_fdp_test:\ninterfacekernels/2d_interface.parallel_fdp_test: Exit Code: 8\ninterfacekernels/2d_interface.parallel_fdp_test: ################################################################################\ninterfacekernels/2d_interface.parallel_fdp_test: Tester failed, reason: CRASH\ninterfacekernels/2d_interface.parallel_fdp_test:\ninterfacekernels/2d_interface.parallel_fdp_test .................................. [min_cpus=2] FAILED (CRASH)\nmultiapps/picard.abs_tol .................................................................................. OK\noutputs/iterative.vtk ..................................................................................... OK\nics/depend_on_uo.scalar_ic_from_uo: Working Directory: /Users/mohammadalabdullah/projects/moose/test/tests/ics/depend_on_uo\nics/depend_on_uo.scalar_ic_from_uo: Running command: mpiexec -n 2 /Users/mohammadalabdullah/projects/moose/test/moose_test-opt -i scalar_ic_from_uo.i --error --error-unused --error-override --no-gdb-backtrace\nics/depend_on_uo.scalar_ic_from_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\nics/depend_on_uo.scalar_ic_from_uo: MPIR_Init_thread(586)..............:\nics/depend_on_uo.scalar_ic_from_uo: MPID_Init(224).....................: channel initialization failed\nics/depend_on_uo.scalar_ic_from_uo: MPIDI_CH3_Init(105)................:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_init(324).................:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_init(175).............:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_get_business_card(401):\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nics/depend_on_uo.scalar_ic_from_uo: (unknown)(): Invalid group\nics/depend_on_uo.scalar_ic_from_uo: Fatal error in MPI_Init_thread: Invalid group, error stack:\nics/depend_on_uo.scalar_ic_from_uo: MPIR_Init_thread(586)..............:\nics/depend_on_uo.scalar_ic_from_uo: MPID_Init(224).....................: channel initialization failed\nics/depend_on_uo.scalar_ic_from_uo: MPIDI_CH3_Init(105)................:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_init(324).................:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_init(175).............:\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_get_business_card(401):\nics/depend_on_uo.scalar_ic_from_uo: MPID_nem_tcp_init(373).............: gethostbyname failed, D0-81-7A-D7-B7-96 (errno 0)\nics/depend_on_uo.scalar_ic_from_uo: (unknown)(): Invalid group\nics/depend_on_uo.scalar_ic_from_uo:\nics/depend_on_uo.scalar_ic_from_uo:\nics/depend_on_uo.scalar_ic_from_uo: Exit Code: 8\nics/depend_on_uo.scalar_ic_from_uo: ################################################################################\nics/depend_on_uo.scalar_ic_from_uo: Tester failed, reason: CRASH\nics/depend_on_uo.scalar_ic_from_uo:\nics/depend_on_uo.scalar_ic_from_uo ............................................... [min_cpus=2] FAILED (CRASH)",
                          "url": "https://github.com/idaholab/moose/discussions/18440#discussioncomment-1052254",
                          "updatedAt": "2023-01-23T20:24:37Z",
                          "publishedAt": "2021-07-26T16:56:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Thank you! Looks like you are running into the common mpich gethostbyname failure.\nWe have a section dedicated to this error on our FAQ. The exact link is: https://mooseframework.inl.gov/help/troubleshooting.html#failingtests\nHere is an except of those FAQs that pertains to this issue (I modified it just a touch to be easier to read):\n\n\ngethostbyname failed, localhost (errno 3)\nThis is a fairly common occurrence which happens when your internal network stack / route, is not correctly configured for the local loopback device. Thankfully, there is an easy fix:\n\n\nObtain your hostname:\n$> hostname\nmycoolname\n\n\nLinux & Macintosh : Add the results of hostname to your /etc/hosts file. Like so:\n$> sudo vi /etc/hosts\n\n127.0.0.1  localhost\n\n# The following lines are desirable for IPv6 capable hosts\n::1        localhost ip6-localhost ip6-loopback\nff02::1    ip6-allnodes\nff02::2    ip6-allrouters\n\n127.0.0.1  mycoolname  # <--- add this line to the end of your hosts file\nEveryone's host file is different. But the results of adding the necessary line described above will be the same.\n\n\nMacintosh only, 2nd method:\n$> sudo scutil --set HostName mycoolname\nWe have received reports where the second method sometimes does not work.",
                          "url": "https://github.com/idaholab/moose/discussions/18440#discussioncomment-1052315",
                          "updatedAt": "2023-04-20T16:02:35Z",
                          "publishedAt": "2021-07-26T17:06:28Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "malanzey"
                          },
                          "bodyText": "Thank you very much the first method worked !",
                          "url": "https://github.com/idaholab/moose/discussions/18440#discussioncomment-1052640",
                          "updatedAt": "2023-04-20T16:02:35Z",
                          "publishedAt": "2021-07-26T18:23:26Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Updating Libmesh issue",
          "author": {
            "login": "ahmad681"
          },
          "bodyText": "Hi,\nI am having a problem with compiling my code. Last night it was working great and I was getting great results. i havent changed anything since last night. Earlier this morning, I \"make clean\" and tried to compile and it keeps telling me\nThere appears to be a libMesh update in progress for\nthe branch of MOOSE you are operating on (only the\nmaster branch contains publicly available moose-libmesh\npackages).\nYou must provide your own libMesh either by:\n1. Uninstall moose-libmesh and build it manually via\n   moose/scripts/update_and_rebuild_libmesh.sh\n\nor\n2. Use conda build to build your own moose-libmesh\npackage.\n/home/ahmad68/projects/moose/framework/moose.mk:15: *** Build failed.  Stop.\nI updated the libmesh by building my own libmesh, and still same error. Could you please help me with that?\nThanks",
          "url": "https://github.com/idaholab/moose/discussions/18245",
          "updatedAt": "2022-08-23T14:33:29Z",
          "publishedAt": "2021-07-02T18:26:42Z",
          "category": {
            "name": "Q&A Meshing"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "ahmad681"
                  },
                  "bodyText": "Note: I asked my groupmates, they have never faced this issue before. It doesn't make sense to me yet why I would have to update my library.\nI updated it using this website https://mooseframework.inl.gov/moose/help/faq/faq_build_libmesh.html\nbut it still not working. I also updated Conda, still not working too and it gives me same messege.",
                  "url": "https://github.com/idaholab/moose/discussions/18245#discussioncomment-958089",
                  "updatedAt": "2022-08-23T14:33:31Z",
                  "publishedAt": "2021-07-02T18:35:49Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hello\nCan you please first try to clean the build from the repository?\nFirst, just try make clobberall. If that is not enough, commit ALL your changes, then run git clean -xfd (will remove everything that is not committed).\nThen we have just updated libmesh and petsc a week ago, so it s worth updating your moose and libmesh.\nSo\ngit fetch origin; git pull origin to get the latest moose\nconda update --all to update libmesh.\nBest,\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/18245#discussioncomment-958105",
                          "updatedAt": "2022-08-23T14:33:39Z",
                          "publishedAt": "2021-07-02T18:41:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ahmad681"
                          },
                          "bodyText": "Thanks for the reply Guillaume. I reached to conda update --all\nbut I am getting Segmentation fault messege.",
                          "url": "https://github.com/idaholab/moose/discussions/18245#discussioncomment-958130",
                          "updatedAt": "2022-08-23T14:33:42Z",
                          "publishedAt": "2021-07-02T18:49:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "What platform? What operating system?\nTry conda clean -a see if it removes the segfault. Some python packages you could have pip-installed can cause seg faults for conda, but it s rare",
                          "url": "https://github.com/idaholab/moose/discussions/18245#discussioncomment-958145",
                          "updatedAt": "2022-08-23T14:33:47Z",
                          "publishedAt": "2021-07-02T18:54:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ahmad681"
                          },
                          "bodyText": "Okay, the libmesh has now been updated successfully and when I did conda update --all, no problem popped up. But Now a problem is popping up\nIt is not being able to recognize -fno-plt ! which I am not sure what this is. Thanks a lot for the help\nCompiling C++ (in opt mode) /home/ahmad68/projects/moose/framework/build/unity_src/src_Unity.C...\nCompiling C++ (in opt mode) /home/ahmad68/projects/moose/framework/build/unity_src/outputs_Unity.C...\nCompiling C++ (in opt mode) /home/ahmad68/projects/moose/framework/build/unity_src/outputs_png_Unity.C...\nCompiling C++ (in opt mode) /home/ahmad68/projects/moose/framework/build/unity_src/outputs_formatters_Unity.C...\nCompiling C++ (in opt mode) /home/ahmad68/projects/moose/framework/build/unity_src/controls_Unity.C...\nCompiling C++ (in opt mode) /home/ahmad68/projects/moose/framework/build/unity_src/reporters_Unity.C...\nCompiling C++ (in opt mode) /home/ahmad68/projects/moose/framework/build/unity_src/timeintegrators_Unity.C...\nCompiling C++ (in opt mode) /home/ahmad68/projects/moose/framework/build/unity_src/fvbcs_Unity.C...\nCompiling C++ (in opt mode) /home/ahmad68/projects/moose/framework/build/unity_src/meshgenerators_Unity.C...\nCompiling C++ (in opt mode) /home/ahmad68/projects/moose/framework/build/unity_src/relationshipmanagers_Unity.C...\nCompiling C++ (in opt mode) /home/ahmad68/projects/moose/framework/build/unity_src/vectorpostprocessors_Unity.C...\nCompiling C++ (in opt mode) /home/ahmad68/projects/moose/framework/build/unity_src/problems_Unity.C...\nCompiling C++ (in opt mode) /home/ahmad68/projects/moose/framework/build/unity_src/partitioner_Unity.C...\nCompiling C++ (in opt mode) /home/ahmad68/projects/moose/framework/build/unity_src/timesteppers_Unity.C...\ng++: error: unrecognized command line option '-fno-plt'\ng++: error: unrecognized command line option '-fno-plt'\ng++: error: unrecognized command line option '-fno-plt'\nmake: *** [/home/ahmad68/projects/moose/framework/build/unity_src/src_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1\nmake: *** Waiting for unfinished jobs....\nmake: *** [/home/ahmad68/projects/moose/framework/build/unity_src/outputs_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1\nmake: *** [/home/ahmad68/projects/moose/framework/build/unity_src/outputs_png_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1\ng++: error: unrecognized command line option '-fno-plt'\nmake: *** [/home/ahmad68/projects/moose/framework/build/unity_src/outputs_formatters_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1\ng++: error: unrecognized command line option '-fno-plt'\ng++: error: unrecognized command line option '-fno-plt'\nmake: *** [/home/ahmad68/projects/moose/framework/build/unity_src/controls_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1\ng++: error: unrecognized command line option '-fno-plt'\nmake: *** [/home/ahmad68/projects/moose/framework/build/unity_src/timeintegrators_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1\nmake: *** [/home/ahmad68/projects/moose/framework/build/unity_src/reporters_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1\ng++: error: unrecognized command line option '-fno-plt'\nmake: *** [/home/ahmad68/projects/moose/framework/build/unity_src/fvbcs_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1\ng++: error: unrecognized command line option '-fno-plt'\nmake: *** [/home/ahmad68/projects/moose/framework/build/unity_src/meshgenerators_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1\ng++: error: unrecognized command line option '-fno-plt'\nmake: *** [/home/ahmad68/projects/moose/framework/build/unity_src/relationshipmanagers_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1\ng++: error: unrecognized command line option '-fno-plt'\nmake: *** [/home/ahmad68/projects/moose/framework/build/unity_src/partitioner_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1\ng++: error: unrecognized command line option '-fno-plt'\nmake: *** [/home/ahmad68/projects/moose/framework/build/unity_src/vectorpostprocessors_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1\ng++: error: unrecognized command line option '-fno-plt'\nmake: *** [/home/ahmad68/projects/moose/framework/build/unity_src/problems_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1\ng++: error: unrecognized command line option '-fno-plt'\nmake: *** [/home/ahmad68/projects/moose/framework/build/unity_src/timesteppers_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1",
                          "url": "https://github.com/idaholab/moose/discussions/18245#discussioncomment-958357",
                          "updatedAt": "2021-07-02T19:49:35Z",
                          "publishedAt": "2021-07-02T19:49:35Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ahmad681"
                          },
                          "bodyText": "By the way, I am using linux operating system",
                          "url": "https://github.com/idaholab/moose/discussions/18245#discussioncomment-958438",
                          "updatedAt": "2023-08-07T15:19:52Z",
                          "publishedAt": "2021-07-02T20:22:21Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "did you remember to activate the moose conda envionment ? conda activate moose\nthis means a compiler flag passed is not understood by the compiler, so you are using the wrong compiller, not the one we provide with conda",
                          "url": "https://github.com/idaholab/moose/discussions/18245#discussioncomment-958459",
                          "updatedAt": "2023-08-07T15:19:52Z",
                          "publishedAt": "2021-07-02T20:29:29Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ahmad681"
                          },
                          "bodyText": "Yes, I did conda activate moose",
                          "url": "https://github.com/idaholab/moose/discussions/18245#discussioncomment-958460",
                          "updatedAt": "2023-08-07T15:19:52Z",
                          "publishedAt": "2021-07-02T20:30:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ahmad681"
                          },
                          "bodyText": "My groupmates use the same version of the code. They never had to update libmesh ever. I wonder why this happened to me also.\nAlso, I am using gcc 5.2.0 I thought any 5.15 and above is fine with conda.",
                          "url": "https://github.com/idaholab/moose/discussions/18245#discussioncomment-958472",
                          "updatedAt": "2023-08-07T15:19:52Z",
                          "publishedAt": "2021-07-02T20:36:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "5.2 should be ok.\nEveryone needs to update libmesh to use the version of moose. People do that through conda updates though, so it s pretty simple.\nwhat does echo $CXX; which mpicxx return? I suspect it's set to your system compiler, not the one in conda.\nIf you want to use your own system compiler instead of the one we provide in the conda environment, then you need to edit the environment variable CPP_FLAGS to remove the no-plt flag, because your (old) compiler does not support it.\nWe set that flag for the conda build because we assume people are using the compilers conda installs.",
                          "url": "https://github.com/idaholab/moose/discussions/18245#discussioncomment-958544",
                          "updatedAt": "2023-08-07T15:19:53Z",
                          "publishedAt": "2021-07-02T21:11:36Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Any news on this?",
                  "url": "https://github.com/idaholab/moose/discussions/18245#discussioncomment-1048064",
                  "updatedAt": "2022-08-23T14:33:49Z",
                  "publishedAt": "2021-07-25T19:53:52Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "ahmad681"
                          },
                          "bodyText": "Apparantely there was a problem with Conda installed on the cluster at my school. It seemed like it was compatible with gcc 4.8.5.\nSo I had to install miniconda and mpich because for some reason openmpi was not working properly. After installing the version miniconda and mpich specified on Moose website, it worked just fine.",
                          "url": "https://github.com/idaholab/moose/discussions/18245#discussioncomment-1051659",
                          "updatedAt": "2022-08-23T14:33:54Z",
                          "publishedAt": "2021-07-26T14:42:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Great thanks for letting us know!\nI'll add a little bit to the posting guidelines to reflect this",
                          "url": "https://github.com/idaholab/moose/discussions/18245#discussioncomment-1051702",
                          "updatedAt": "2022-08-23T14:33:54Z",
                          "publishedAt": "2021-07-26T14:52:25Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Orthotropic Thermal conductivity",
          "author": {
            "login": "makeclean"
          },
          "bodyText": "Hi folks, I've come across a problem where I'd like to have different themal conductivities in the x,y,z directions as a function of temperature. Is this currently possible?",
          "url": "https://github.com/idaholab/moose/discussions/18243",
          "updatedAt": "2022-09-21T06:07:25Z",
          "publishedAt": "2021-07-02T09:22:43Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "WilkAndy"
                  },
                  "bodyText": "AnisoHeatConduction allows you to define (k_xx, k_yy, k_zz), but not an arbitrary tensor.\nNot in PorousFlow, sorry.  We have anisotropic thermal conductivity that is a function of saturation.  You'd have to implement it yourself if you want to use PorousFlow.",
                  "url": "https://github.com/idaholab/moose/discussions/18243#discussioncomment-956037",
                  "updatedAt": "2022-09-21T06:07:26Z",
                  "publishedAt": "2021-07-02T09:59:09Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "hugary1995"
                  },
                  "bodyText": "Just replace the HeatConduction kernel with AnisotropicDiffusion. You need to define your tensor valued conduction coefficient.\nIf your orthotropic diffusivity is inhomogeneous, you need to use MatAnisoDiffusion and define your tensor valued conduction coefficient as a material property.",
                  "url": "https://github.com/idaholab/moose/discussions/18243#discussioncomment-956173",
                  "updatedAt": "2022-09-21T06:07:26Z",
                  "publishedAt": "2021-07-02T10:31:38Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "makeclean"
                  },
                  "bodyText": "Thats great, AnisoHeatConduction is exactly, what I wanted. Now, just to be awkward, I would like the k_xx, k_yy and k_zz to be functions. I did something like (where tcx etc are regular peicewiselinear functions.\n  [cfc_tile]\n    type = AnisoHeatConductionMaterial\n    block = 'bdcu-tile'\n    specific_heat_temperature_function = cp_dms704\n    thermal_conductivity_x = tcx\n    thermal_conductivity_y = tcy\n    thermal_conductivity_z = tcz\n    temp = temp\n  []\n\nIt doesnt seem to like that, so instead I'm going the pp route\n  [cfc_tile]\n    type = AnisoHeatConductionMaterial\n    block = 'bdcu-tile'\n    specific_heat_temperature_function = cp_dms704\n    thermal_conductivity_x_pp = tcx\n    thermal_conductivity_y_pp = tcy\n    thermal_conductivity_z_pp = tcz\n    temp = temp\n  []\n[]\n\nWhat isnt clear to me is in the postprocessor section\n[Postprocessors]\n  [./tcx]\n    type = FunctionValuePostprocessor\n    function = my_function\n    outputs = none\n    execute_on = 'initial timestep_end'\n  [../]\n\nWhat isn't clear to me, is how I pass the temperature to this function? Should I use something like\n[Functions]\n  [tc_z_dms704]\n    type = PiecewiseLinear\n    x = '313.15 393.15 493.15 593.15 693.15 793.15 893.15\n         993.15 1093.15 1193.15 1293.15 1393.15 1493.15\n         1593.15 1693.15'\n    y = '67.21 62.12 56.38 51.25 46.66 42.57 38.91 35.64\n         32.72 30.11 27.78 25.7 23.84 22.17 20.69'\n  []\n[]\n[Postprocessors]\n  [./tcx]\n    type = FunctionValuePostprocessor\n    function = tc_dms704\n    indirect_dependencies = get_temp\n    outputs = none\n    execute_on = 'initial timestep_end'\n  [../]\n  [get_temp]\n    type = ScalarVariable\n    variable = temp\n    execute_on = 'initial timestep_end'\n  [../]\n[]\n\nI.e. how on earth do I let the postprocessor know that I want to pass the temperature into the function as the argument f(temperature)?",
                  "url": "https://github.com/idaholab/moose/discussions/18243#discussioncomment-957441",
                  "updatedAt": "2022-09-21T06:07:26Z",
                  "publishedAt": "2021-07-02T15:40:30Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "Why do you take a harder route?",
                          "url": "https://github.com/idaholab/moose/discussions/18243#discussioncomment-958249",
                          "updatedAt": "2022-12-30T00:45:03Z",
                          "publishedAt": "2021-07-02T19:19:48Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "Is there an easier way?",
                          "url": "https://github.com/idaholab/moose/discussions/18243#discussioncomment-958326",
                          "updatedAt": "2022-12-30T00:45:03Z",
                          "publishedAt": "2021-07-02T19:40:57Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "If there is an easier route, I'd be very happy to take it :)",
                          "url": "https://github.com/idaholab/moose/discussions/18243#discussioncomment-964863",
                          "updatedAt": "2022-12-30T00:45:03Z",
                          "publishedAt": "2021-07-05T09:00:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "Sorry I'm late on this. As I mentioned above, you can use MatAnisoDiffusion for general anisotropic diffusion. You can define the anisotropic diffusivity matrix using GenericFunctionRanTwoTensor or whatever class you see fit.",
                          "url": "https://github.com/idaholab/moose/discussions/18243#discussioncomment-965483",
                          "updatedAt": "2022-12-30T00:44:52Z",
                          "publishedAt": "2021-07-05T11:08:54Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "Perfect, im trying that now. The question about the functions knowing that they refer to temperature, is that handled magically?",
                          "url": "https://github.com/idaholab/moose/discussions/18243#discussioncomment-965871",
                          "updatedAt": "2022-12-30T00:44:52Z",
                          "publishedAt": "2021-07-05T12:50:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "Sorry, Im really struggling with this, I feel there is something missing from my understanding.\n[Mesh]\n  type = FileMesh\n  file = '../mesh/bdu-tile.e'\n[]\n\n[Variables]\n  [temp]\n    initial_condition = 313.15\n  []\n[]\n\n[AuxVariables]\n  [temp_in_C]\n  []\n  [radiation_flux]\n  []\n[]\n\n[Kernels]\n  [isotropic-heat]\n    type = HeatConduction\n    variable = temp\n    block = 'heat-sink tie-rods tile-insert'\n  []\n  [anisotropic-heat]\n    type = MatAnisoDiffusion\n    variable = temp\n    diffusivity = 'conductivity'\n    block = 'bdcu-tile'\n  []\n  [heat-time]\n    type = HeatConductionTimeDerivative\n    variable = temp\n    density_name = density\n  []\n[]\n\n[AuxKernels]\n  [K_to_C]\n    type = ParsedAux\n    variable = 'temp_in_C'\n    function = 'temp-273.15'\n    args = 'temp'\n  []\n  [radiation]\n    type = ParsedAux\n    variable = radiation_flux\n    args = 'temp'\n    function = 'sigma*epsilon*(temp^4-start_t^4)'\n    constant_names = 'sigma epsilon start_t'\n    constant_expressions = '5.73e-8 0.9 313.15'\n  []\n[]\n\n[BCs]\n  [fixed-temp]\n    type = DirichletBC\n    variable = temp\n    boundary = 'fixed-temp'\n    value = 313.15\n  []\n  [radiation-leakage]\n    type = CoupledVarNeumannBC\n    variable = temp\n    boundary = 'heat-leakage-rear'\n    v = 'radiation_flux'\n  []\n  [radiation-source]\n    type = FunctionNeumannBC\n    variable = temp\n    boundary = 'heat-source'\n    function = 'gaussian-heat'\n  []\n[]\n\n[ThermalContact]\n  [TC_1]\n    emissivity_primary = 0.9\n    emissivity_secondary = 0.9\n    primary = tile_nut_contact_1\n    secondary = nut_tile_contact_1\n    quadrature = true\n    variable = temp\n    type = GapHeatTransfer\n  []\n  [TC_2]\n    emissivity_primary = 0.9\n    emissivity_secondary = 0.9\n    primary = tile_nut_contact_2\n    secondary = nut_tile_contact_2\n    quadrature = true\n    variable = temp\n    type = GapHeatTransfer\n  []\n  [TC_3]\n    emissivity_primary = 0.9\n    emissivity_secondary = 0.9\n    primary = tile_nut_contact_3\n    secondary = nut_tile_contact_3\n    quadrature = true\n    variable = temp\n    type = GapHeatTransfer\n  []\n  [TC_4]\n    emissivity_primary = 0.9\n    emissivity_secondary = 0.9\n    primary = tile_nut_contact_4\n    secondary = nut_tile_contact_4\n    quadrature = true\n    variable = temp\n    type = GapHeatTransfer\n  []\n  [TC_5]\n    emissivity_primary = 0.9\n    emissivity_secondary = 0.9\n    primary = tile_tie_contact_1\n    secondary = tie_tile_contact_1\n    quadrature = true\n    variable = temp\n    type = GapHeatTransfer\n  []\n  [TC_6]\n    emissivity_primary = 0.9\n    emissivity_secondary = 0.9\n    primary = tile_tie_contact_2\n    secondary = tie_tile_contact_2\n    quadrature = true\n    variable = temp\n    type = GapHeatTransfer\n  []\n[]\n\n[Functions]\n  [cp_dms704]\n    type = PiecewiseLinear\n    x = '313.15 393.15 493.15 593.15 693.15 793.15 893.15\n         993.15 1093.15 1193.15 1293.15 1393.15 1493.15\n         1593.15 1693.15'\n    y = '691.22 929.33 1168.8 1356.7 1504.1 1619.8 1710.5\n         1781.7 1837.6 1881.4 1915.8 1942.8 1964 1980.6\n         1993.6'\n  []\n  [tc_x_dms704]\n    type = PiecewiseLinear\n    x = '313.15 393.15 493.15 593.15 693.15 793.15 893.15\n         993.15 1093.15 1193.15 1293.15 1393.15 1493.15\n         1593.15 1693.15'\n    y = '314.14 290.11 263.04 238.92 217.44 198.3 181.25\n         166.07 152.54 140.49 129.76 120.2 111.68 104.09\n         97.33'\n  []\n  [tc_y_dms704]\n    type = PiecewiseLinear\n    x = '313.15 393.15 493.15 593.15 693.15 793.15 893.15\n         993.15 1093.15 1193.15 1293.15 1393.15 1493.15\n         1593.15 1693.15'\n    y = '314.14 290.11 263.04 238.92 217.44 198.3 181.25\n         166.07 152.54 140.49 129.76 120.2 111.68 104.09\n         97.33'\n  []\n  [tc_z_dms704]\n    type = PiecewiseLinear\n    x = '313.15 393.15 493.15 593.15 693.15 793.15 893.15\n         993.15 1093.15 1193.15 1293.15 1393.15 1493.15\n         1593.15 1693.15'\n    y = '67.21 62.12 56.38 51.25 46.66 42.57 38.91 35.64\n         32.72 30.11 27.78 25.7 23.84 22.17 20.69'\n  []\n\n  [density_ic]\n    type = PiecewiseLinear\n    x = '293.15 873.15'\n    y = '8221 8025'\n  []\n  [tc_ic]\n    type = PiecewiseLinear\n    x = '293.15 373.15 473.15 573.15 673.15 773.15 873.15'\n    y= '11.25 12.59 14.26 15.93 17.61 19.29 20.97'\n  []\n  [gaussian-heat]\n    type = ParsedFunction\n    value = 'p0'\n    vars = 'p0'\n    vals = '40e6'\n  []\n[]\n\n[Materials]\n  [heat-sink_density]\n    type = GenericConstantMaterial\n    prop_names = 'density'\n    block = 'heat-sink'\n    prop_values = '1.151e+006'\n  []\n  [heat-sink]\n    type = HeatConductionMaterial\n    block = 'heat-sink'\n    specific_heat = 509\n    thermal_conductivity = 0.9054\n  []\n  [inconel-density]\n    type = GenericFunctionMaterial\n    prop_names = 'density'\n    prop_values = 'density_ic'\n    block = 'tie-rods tile-insert'\n  []\n  [inconel]\n     type = HeatConductionMaterial\n     block = 'tie-rods tile-insert'\n     specific_heat = 460.0\n     temp = temp\n     thermal_conductivity_temperature_function = 'tc_ic'\n  []\n  [cfc-density]\n    type = GenericConstantMaterial\n    prop_names = 'density'\n    block = 'bdcu-tile'\n    prop_values = '1740'\n  []\n  [cfc-tile-specific-heat]\n    type = GenericFunctionMaterial\n    prop_names = 'specific_heat'\n    prop_values = 'cp_dms704'\n  []\n  [cfc-tile-conductivity]\n    type = GenericFunctionRankTwoTensor\n    tensor_name = 'cfc-tile-conductivity'\n    tensor_functions = 'tc_x_dms704 0 0 0 tc_y_dms704 0 0 0 tc_z_dms704'\n    outputs = all\n    block = 'bdcu-tile'\n  []\n[]\n\n[Preconditioning]\n  [./SMP]\n    type = SMP\n    full = true\n  [../]\n[]\n\n[Executioner]\n  automatic_scaling = true\n  solve_type = 'NEWTON'\n  type = Transient\n  line_search = none\n  nl_abs_tol = 1e-6\n  nl_rel_tol = 1e-5\n  l_tol = 1e-4\n  l_max_its = 100\n  nl_max_its = 10\n  dt = 0.1\n  num_steps = 30\n  petsc_options_iname = '-pc_type -ksp_gmres_restart'\n  petsc_options_value = 'ilu      101'\n[]\n\n[Outputs]\n  exodus = true\n[]\n\nIn the past I've used ADPiecewiseLinearInterpolationMaterial, and its clear how I pass temperature to determine the conductivity, in the case of this problem, its not clear (at least to me) how I tell the tensor it should use temperature as the x axis.\nAlso, doing what I think I should do, as above then I get\nBuilding mesh ............                                                                 [ 13.19 s]\nCaching mesh information ..........                                                        [ 11.01 s]\nThe following total 9 aux variables:\n  cfc-tile-conductivity_00\n  cfc-tile-conductivity_01\n  cfc-tile-conductivity_02\n  cfc-tile-conductivity_10\n  cfc-tile-conductivity_11\n  cfc-tile-conductivity_12\n  cfc-tile-conductivity_20\n  cfc-tile-conductivity_21\n  cfc-tile-conductivity_22\nare added for automatic output by MaterialOutputAction.\n\n\n*** ERROR ***\nMaterial has no property named: cfc-tile-conductivity\n\nStack frames: 19\n\nYou can see from my input that I have cfc-tile-conductivity defined, so again I think there is a gap in my understanding?",
                          "url": "https://github.com/idaholab/moose/discussions/18243#discussioncomment-966069",
                          "updatedAt": "2022-12-30T00:44:51Z",
                          "publishedAt": "2021-07-05T13:25:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "makeclean"
                          },
                          "bodyText": "@hugary1995 any advice?",
                          "url": "https://github.com/idaholab/moose/discussions/18243#discussioncomment-998679",
                          "updatedAt": "2022-12-30T00:45:44Z",
                          "publishedAt": "2021-07-13T12:48:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I would just create my own tensor material.\nI dont think there's a way to specify a function of temperature from the input file for this. ParsedFunctions will not let you define that dependence.\nI'm not sure what block is complaining about the missing material property.\nI think it may be the material output that is struggling. This would be a bug. Could you please remove the outputs = all from your tensor material property and see if the error disappears",
                          "url": "https://github.com/idaholab/moose/discussions/18243#discussioncomment-1048154",
                          "updatedAt": "2022-12-30T00:45:45Z",
                          "publishedAt": "2021-07-25T20:46:11Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "New MOOSE build on HPC - some tests failing",
          "author": {
            "login": "crb3874"
          },
          "bodyText": "Hello,\nI have set up a new installation of MOOSE on my university's HPC following the instructions at https://mooseframework.inl.gov/getting_started/installation/hpc_install_moose.html. I am able to compile the tests and run them, and something like 98% of them are passing with no issue, but a subset of them are failing. The failed tests all seem to be for similar reasons (CRASH) - when I look at their outputs, many of them exit with code 11 due to segfaults in one or more of the MPI processes. I've included the full console output from the tests, but at a glance the following tests are failing for me:\ndgkernels/ad_dg_diffusion.test ................................................................ FAILED (CRASH)\ndgkernels/2d_diffusion_dg.test ................................................................ FAILED (CRASH)\ndgkernels/adaptivity.dg_adaptivity ............................................................ FAILED (CRASH)\nbcs/dmg_periodic.check ....................................................... FAILED (EXPECTED ERROR MISSING)\nbcs/periodic.all_periodic_trans_test .......................................................... FAILED (CRASH)\nadaptivity/recompute_markers_during_cycles.test ............................................... FAILED (CRASH)\nadaptivity/cycles_per_step.test ............................................................... FAILED (CRASH)\npostprocessors/time_extreme_value.time_extreme_pps ............................................ FAILED (CRASH)\npostprocessors/nodal_extreme_value.nodal_extreme .............................................. FAILED (CRASH)\ncontrols/time_periods/dgkernels.test .......................................................... FAILED (CRASH)\nmesh/periodic_node_map.1D ..................................................................... FAILED (CRASH)\nmarkers/error_tolerance_marker.adapt_test ..................................................... FAILED (CRASH)\nmarkers/two_circle_marker.group/two_circle_marker_coarsen ..................................... FAILED (CRASH)\nmeshgenerators/distributed_rectilinear/dmg_displaced_mesh.pbc_adaptivity .... FAILED (EXPECTED OUTPUT MISSING)\nuserobjects/toggle_mesh_adaptivity.toggle_mesh_adaptivity/toggle_mesh_adaptivity_gaussian_ic .. FAILED (CRASH)\nbcs/periodic.auto_wrap_2d_test ................................................................ FAILED (CRASH)\nmesh/periodic_node_map.2D ..................................................................... FAILED (CRASH)\nmarkers/two_circle_marker.group/two_circle_marker_gaussian_ic ................................. FAILED (CRASH)\nbcs/periodic.auto_wrap_2d_test_non_generated .................................................. FAILED (CRASH)\nauxkernels/time_derivative.time_derivative_nl ................................................. FAILED (CRASH)\nmesh/periodic_node_map.3D ..................................................................... FAILED (CRASH)\nuserobjects/toggle_mesh_adaptivity.toggle_mesh_adaptivity/toggle_mesh_adaptivity_gaussian_ic_stop_time FAILED (CRASH)\ndgkernels/2d_diffusion_dg.no_mallocs_during_scaling ........................................... FAILED (CRASH)\nbcs/periodic.auto_wrap_3d_test ................................................................ FAILED (CRASH)\nmesh/named_entities.test_periodic_names ....................................................... FAILED (CRASH)\nbcs/periodic.orthogonal_pbc_on_square_test .................................................... FAILED (CRASH)\nbcs/dmg_periodic.check_one_step .................................................. [min_cpus=2] FAILED (CRASH)\ngeomsearch/patch_update_strategy.always-grid-sequencing ....................................... FAILED (CRASH)\nmortar/continuity-2d-non-conforming.sequencing-stateful-soln-continuity ....................... FAILED (CRASH)\nbcs/periodic.parallel_pbc_using_trans_test .................................................... FAILED (CRASH)\nbcs/periodic.testlevel1 ....................................................................... FAILED (CRASH)\nmaterials/stateful_prop.adaptivity/spatially_const ............................................ FAILED (CRASH)\noutputs/exodus.discontinuous .................................................................. FAILED (CRASH)\nbcs/periodic.testperiodic ..................................................................... FAILED (CRASH)\nbcs/periodic.testperiodic_dp ..................................................... [min_cpus=2] FAILED (CRASH)\nbcs/dmg_periodic.2d .............................................................. [min_cpus=2] FAILED (CRASH)\nmaterials/stateful_prop.adaptivity/spatially_varying .......................................... FAILED (CRASH)\nbcs/periodic.testtrapezoid .................................................................... FAILED (CRASH)\nbcs/dmg_periodic.3d .............................................................. [min_cpus=2] FAILED (CRASH)\nbcs/dmg_periodic.1d .............................................................. [min_cpus=2] FAILED (CRASH)\nbcs/periodic.testwedge ........................................................................ FAILED (CRASH)\nbcs/periodic.testwedgesys ..................................................................... FAILED (CRASH)\nbcs/periodic.auto_dir_repeated_id ............................................................. FAILED (CRASH)\nbcs/periodic.no_add_scalar .................................................................... FAILED (CRASH)\npartitioners/petsc_partitioner.ptscotch ........................ [min_cpus=4] FAILED (EXPECTED OUTPUT MISSING)\nmeshgenerators/distributed_rectilinear/dmg_displaced_mesh.pbc/adaptivity_nemesis . [min_cpus=4] FAILED (CRASH)\npartitioners/petsc_partitioner.ptscotch_weight_elment .......... [min_cpus=4] FAILED (EXPECTED OUTPUT MISSING)\npartitioners/petsc_partitioner.ptscotch_weight_side ............ [min_cpus=4] FAILED (EXPECTED OUTPUT MISSING)\npartitioners/petsc_partitioner.ptscotch_weight_both ............ [min_cpus=4] FAILED (EXPECTED OUTPUT MISSING)\nIn this case, 2583 of the tests passed before maximum failures was reached. Thank you for your help with this - the full console output from the tests is attached. Please let me know if any other information would help.\ntests_log.txt",
          "url": "https://github.com/idaholab/moose/discussions/17975",
          "updatedAt": "2022-08-02T05:49:06Z",
          "publishedAt": "2021-05-31T20:39:05Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nSomeone might know what is in common between all these tests. But if not,\n\nDo these tests pass when ran individually?\nIf not, could you please compile in debug mode and get us the backtrace?\n\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/17975#discussioncomment-809050",
                  "updatedAt": "2022-08-02T05:49:15Z",
                  "publishedAt": "2021-06-01T01:49:34Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "crb3874"
                          },
                          "bodyText": "Sorry for the extremely late reply - combination of vacation/starting an internship separate from this work. I've tried the two steps you suggested - when run individually, these tests still fail (with segfaults for the most part). However, when I compile and run in debug mode, they exit gracefully with no errors. This is strange to me, and I'm also I'm not sure how to get a useful backtrace without the error occurring again. Not sure where to go from here?",
                          "url": "https://github.com/idaholab/moose/discussions/17975#discussioncomment-899542",
                          "updatedAt": "2022-08-02T05:49:18Z",
                          "publishedAt": "2021-06-21T18:54:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "crb3874"
                          },
                          "bodyText": "I was able to get a backtrace by running one of the tests with moose_test-opt instead - I ran /tests/dgkernels/ad_dg_diffusion/2d_diffusion_ad_dg_test.i. The backtrace follows:\n #0  0x00002aaaaf731f9f in libMesh::GenericProjector<libMesh::OldSolutionValue<double, &(void libMesh::FEMContext::point_value<double>(unsigned int, libMesh::Point const&, double&, double) const)>, libMesh::OldSolutionValue<libMesh::VectorValue<double>, &(void libMesh::FEMContext::point_gradient<libMesh::VectorValue<double> >(unsigned int, libMesh::Point const&, libMesh::VectorValue<double>&, double) const)>, double, libMesh::VectorSetAction<double> >::SubProjector::construct_projection(std::vector<unsigned long, std::allocator<unsigned long> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, unsigned int, libMesh::Node const*, libMesh::FEGenericBase<double> const&) () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0 #1  0x00002aaaaf7315dd in libMesh::GenericProjector<libMesh::OldSolutionValue<double, &(void libMesh::FEMContext::point_value<double>(unsigned int, libMesh::Point const&, double&, double) const)>, libMesh::OldSolutionValue<libMesh::VectorValue<double>, &(void libMesh::FEMContext::point_gradient<libMesh::VectorValue<double> >(unsigned int, libMesh::Point const&, libMesh::VectorValue<double>&, double) const)>, double, libMesh::VectorSetAction<double> >::ProjectInteriors::operator()(libMesh::StoredRange<__gnu_cxx::__normal_iterator<libMesh::Elem const* const*, std::vector<libMesh::Elem const*, std::allocator<libMesh::Elem const*> > >, libMesh::Elem const*> const&) () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0 #2  0x00002aaaaf7301aa in void libMesh::Threads::parallel_reduce<libMesh::StoredRange<__gnu_cxx::__normal_iterator<libMesh::Elem const* const*, std::vector<libMesh::Elem const*, std::allocator<libMesh::Elem const*> > >, libMesh::Elem const*>, libMesh::GenericProjector<libMesh::OldSolutionValue<double, &(void libMesh::FEMContext::point_value<double>(unsigned int, libMesh::Point const&, double&, double) const)>, libMesh::OldSolutionValue<libMesh::VectorValue<double>, &(void libMesh::FEMContext::point_gradient<libMesh::VectorValue<double> >(unsigned int, libMesh::Point const&, libMesh::VectorValue<double>&, double) const)>, double, libMesh::VectorSetAction<double> >::ProjectInteriors>(libMesh::StoredRange<__gnu_cxx::__normal_iterator<libMesh::Elem const* const*, std::vector<libMesh::Elem const*, std::allocator<libMesh::Elem const*> > >, libMesh::Elem const*> const&, libMesh::GenericProjector<libMesh::OldSolutionValue<double, &(void libMesh::FEMContext::point_value<double>(unsigned int, libMesh::Point const&, double&, double) const)>, libMesh::OldSolutionValue<libMesh::VectorValue<double>, &(void libMesh::FEMContext::point_gradient<libMesh::VectorValue<double> >(unsigned int, libMesh::Point const&, libMesh::VectorValue<double>&, double) const)>, double, libMesh::VectorSetAction<double> >::ProjectInteriors&) () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0 #3  0x00002aaaaf727513 in libMesh::GenericProjector<libMesh::OldSolutionValue<double, &(void libMesh::FEMContext::point_value<double>(unsigned int, libMesh::Point const&, double&, double) const)>, libMesh::OldSolutionValue<libMesh::VectorValue<double>, &(void libMesh::FEMContext::point_gradient<libMesh::VectorValue<double> >(unsigned int, libMesh::Point const&, libMesh::VectorValue<double>&, double) const)>, double, libMesh::VectorSetAction<double> >::project(libMesh::StoredRange<libMesh::MeshBase::const_element_iterator, libMesh::Elem const*> const&) () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0 #4  0x00002aaaaf6ead0d in libMesh::System::project_vector(libMesh::NumericVector<double> const&, libMesh::NumericVector<double>&, int) const () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0 #5  0x00002aaaaf6e9ec1 in libMesh::System::project_vector(libMesh::NumericVector<double>&, int) const () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0 #6  0x00002aaaaf6bef3b in libMesh::System::restrict_vectors() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0 #7  0x00002aaaaf667c72 in libMesh::EquationSystems::reinit_solutions() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0 #8  0x00002aaaacdd8c20 in FEProblemBase::meshChangedHelper(bool) () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/framework/libmoose-opt.so.0 #9  0x00002aaaacdd870e in FEProblemBase::adaptMesh() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/framework/libmoose-opt.so.0 #10 0x00002aaaad919273 in Steady::execute() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/framework/libmoose-opt.so.0 #11 0x00002aaaadc91ed3 in MooseApp::executeExecutioner() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/framework/libmoose-opt.so.0 #12 0x00002aaaab89ce85 in MooseTestApp::executeExecutioner() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/test/lib/libmoose_test-opt.so.0 #13 0x00002aaaadc888b3 in MooseApp::run() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/framework/libmoose-opt.so.0 #14 0x00000000004043a0 in main ()",
                          "url": "https://github.com/idaholab/moose/discussions/17975#discussioncomment-899648",
                          "updatedAt": "2022-08-02T05:49:18Z",
                          "publishedAt": "2021-06-21T19:28:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "crb3874"
                          },
                          "bodyText": "I ran another one of the tests to obtain a backtrace - this time, ./tests/bcs/dmg_periodic/dmg_periodic_bc.i. I'm not sure what is common between these - I can get backtraces from more of them if needed.\n#0  0x00002aaaaeb3fc21 in libMesh::FEGenericBase::compute_periodic_constraints(libMesh::DofConstraints&, libMesh::DofMap&, libMesh::PeriodicBoundaries const&, libMesh::MeshBase const&, libMesh::PointLocatorBase const*, unsigned int, libMesh::Elem const*) () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0\n#1  0x00002aaaaed37ab1 in libMesh::FEInterface::compute_periodic_constraints(libMesh::DofConstraints&, libMesh::DofMap&, libMesh::PeriodicBoundaries const&, libMesh::MeshBase const&, libMesh::PointLocatorBase const*, unsigned int, libMesh::Elem const*) () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0\n#2  0x00002aaaae97954c in (anonymous namespace)::ComputeConstraints::operator()(libMesh::StoredRange<libMesh::MeshBase::const_element_iterator, libMesh::Elem const*> const&) const ()\nfrom /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0\n#3  0x00002aaaae978b42 in void libMesh::Threads::parallel_for<libMesh::StoredRange<libMesh::MeshBase::const_element_iterator, libMesh::Elem const*>, (anonymous namespace)::ComputeConstraints>(libMesh::StoredRange<libMesh::MeshBase::const_element_iterator, libMesh::Elem const*> const&, (anonymous namespace)::ComputeConstraints const&) ()\nfrom /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0\n#4  0x00002aaaae977b41 in libMesh::DofMap::create_dof_constraints(libMesh::MeshBase const&, double) ()\nfrom /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0\n#5  0x00002aaaaf6c4fdc in libMesh::System::reinit_constraints() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0\n#6  0x00002aaaaf6c4243 in libMesh::System::init_data() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0\n#7  0x00002aaaaf6c50fb in libMesh::System::init() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0\n#8  0x00002aaaaf66a5ce in libMesh::EquationSystems::init() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/scripts/../libmesh/installed/lib/libmesh_opt.so.0\n#9  0x00002aaaace17957 in FEProblemBase::init() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/framework/libmoose-opt.so.0\n#10 0x00002aaaad5edac6 in InitProblemAction::act() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/framework/libmoose-opt.so.0\n#11 0x00002aaaad53c318 in Action::timedAct() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/framework/libmoose-opt.so.0\n#12 0x00002aaaad53bf9c in ActionWarehouse::executeActionsWithAction(std::__cxx11::basic_string<char, std::char_traits, std::allocator > const&) ()\nfrom /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/framework/libmoose-opt.so.0\n#13 0x00002aaaad53b9b5 in ActionWarehouse::executeAllActions() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/framework/libmoose-opt.so.0\n#14 0x00002aaaadc91647 in MooseApp::runInputFile() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/framework/libmoose-opt.so.0\n#15 0x00002aaaadc8887a in MooseApp::run() () from /work2/07954/cbrennan/MOOSE_Shared/moose_proj/moose/framework/libmoose-opt.so.0\n#16 0x00000000004043a0 in main ()",
                          "url": "https://github.com/idaholab/moose/discussions/17975#discussioncomment-899703",
                          "updatedAt": "2022-08-02T05:49:46Z",
                          "publishedAt": "2021-06-21T19:50:06Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "These are linked to libmesh.\nDoes the devel build pass the tests?\nIt s very odd that opt would fail and dbg would pass.",
                          "url": "https://github.com/idaholab/moose/discussions/17975#discussioncomment-900022",
                          "updatedAt": "2022-08-02T05:49:49Z",
                          "publishedAt": "2021-06-21T21:39:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I would also rebuild everything (at least on the moose side, with make clobberall then make) then try again with opt",
                          "url": "https://github.com/idaholab/moose/discussions/17975#discussioncomment-917110",
                          "updatedAt": "2022-08-02T05:49:49Z",
                          "publishedAt": "2021-06-24T18:43:34Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Closing this.\nIf debug passes and opt fails, it's likely that an old library got linked, and that it needs to be rebuilt.\nmake clobberall OR commit everything then git clean -xfd will take care of that (the second better than the first).",
                          "url": "https://github.com/idaholab/moose/discussions/17975#discussioncomment-1048101",
                          "updatedAt": "2022-08-02T05:49:49Z",
                          "publishedAt": "2021-07-25T20:19:49Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "lower-dimensional meshes and VectorVariableComponentAux",
          "author": {
            "login": "WilkAndy"
          },
          "bodyText": "Hi everyone,\nI populate a CONSTANT MONOMIAL_VEC AuxVariable from an AuxKernel successfully.  However, zeroes are erroneously returned when i use a VectorVariableComponentAux to extract components of it when I have meshes with mixed-dimensional elements:\n\nfor 3D elements, everything is correct;\nfor 2D elements, the \"z\" component of the VectorVariableComponentAux is always zero, irrespective of the z value of the MONOMIAL_VEC, even if Mesh dimension and Spatial dimension are 3;\nfor 1D elements, the \"y\" and \"z\" components are zero.\n\nI'm not sure whether this is a bug, or deliberate, or whether I have to provide a certain flag or input-file command to retrieve the correct value.\na",
          "url": "https://github.com/idaholab/moose/discussions/17988",
          "updatedAt": "2023-02-14T10:45:12Z",
          "publishedAt": "2021-06-02T04:37:23Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@rwcarlsen will know.\nI m curious too",
                  "url": "https://github.com/idaholab/moose/discussions/17988#discussioncomment-900051",
                  "updatedAt": "2023-02-14T10:45:12Z",
                  "publishedAt": "2021-06-21T21:49:24Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "rwcarlsen"
                  },
                  "bodyText": "Upon first glance, this seems like a bug.  I'd need to dig more into the code to see if there are any design issues that prevent us from providing a more sensible result.",
                  "url": "https://github.com/idaholab/moose/discussions/17988#discussioncomment-916894",
                  "updatedAt": "2023-02-14T10:45:13Z",
                  "publishedAt": "2021-06-24T17:51:12Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Created an issue so we dont loose track of it. This could be consequential to some people",
                          "url": "https://github.com/idaholab/moose/discussions/17988#discussioncomment-1048090",
                          "updatedAt": "2023-02-14T10:45:14Z",
                          "publishedAt": "2021-07-25T20:10:53Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to load the Nemesis mesh group ?",
          "author": {
            "login": "mangerij"
          },
          "bodyText": "Say I have outputs from a single calculation\nout_PTOfilm_e25_T298K_E0_E0.e.N.M\nwhere N is number of total processors and M is the processor index. I want to load the group of these files as my mesh for a new calculation (same partitioning). Is this possible?\nAlong with file = out_PTOfilm_e25_T298K_E0_E0.e, I tried to add nemesis=true and skip_partition= true in the [Mesh] block and that does nothing.\nI also checked the test/tests/mesh/nemesis_test.i test and I have the same syntax to load the files.\nThe weird thing is that the error message says that it sees the files but for example,\n*** ERROR ***\nError opening ExodusII mesh file: /lcrc/project/Meso/projects/ferret/out_PTOfilm_e25_T298K_E0_E46.e.256.060\nthanks\nJohn",
          "url": "https://github.com/idaholab/moose/discussions/17137",
          "updatedAt": "2022-07-01T18:50:17Z",
          "publishedAt": "2021-02-25T12:49:40Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "PM-Hu"
                  },
                  "bodyText": "Hi\nI'm new to MOOSE, and also meet that error when I was intending to use the Exodus file generated by Trelis in Mesh block.\nThe chmod command may solve that error to change the ExodusII mesh file's [out_PTOfilm_e25_T298K_E0_E46.e.256.060] mode.\nhope that helps you\nHu",
                  "url": "https://github.com/idaholab/moose/discussions/17137#discussioncomment-424531",
                  "updatedAt": "2022-07-01T20:40:17Z",
                  "publishedAt": "2021-03-03T08:28:29Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Is this solved?",
                  "url": "https://github.com/idaholab/moose/discussions/17137#discussioncomment-491195",
                  "updatedAt": "2022-07-01T20:40:35Z",
                  "publishedAt": "2021-03-16T22:25:14Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I checked with the example moose/test/tests/meshgenerators/distributed_rectilinear/dmg_displaced_mesh there, loading modifying it to load the nemesis mesh group and it worked fine.\nClosing this",
                          "url": "https://github.com/idaholab/moose/discussions/17137#discussioncomment-1048080",
                          "updatedAt": "2022-07-01T20:40:36Z",
                          "publishedAt": "2021-07-25T20:05:05Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}