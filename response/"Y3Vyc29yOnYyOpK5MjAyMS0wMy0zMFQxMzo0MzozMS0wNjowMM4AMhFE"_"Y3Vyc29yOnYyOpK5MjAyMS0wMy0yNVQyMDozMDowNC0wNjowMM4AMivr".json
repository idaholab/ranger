{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyMS0wMy0yNVQyMDozMDowNC0wNjowMM4AMivr"
    },
    "edges": [
      {
        "node": {
          "title": "PorousFlow THM poroelastic eqs",
          "author": {
            "login": "AmandaLug"
          },
          "bodyText": "Hi all,\nI'm modeling a THM problem in PorousFlow and two questions came to my mind during the mechanical coupling and I could not make my mind around. I'm sorry if this is too simple and not MOOSE related...\n1.- In the earth_tide_fullsat.i model, FunctionDirichletBC are implemented to model strains. From the governing equations, Dirichlet BCs should be used for displacement and not strain, right?\n2.- The barometric_fully_confined.i model, works fine in that simple set up. However, if gravity is implemented, is not possible to obtain the barometric efficiency of the system anymore? I'm not sure which is the relation between effective stress and density.\nThanks!\nAmanda",
          "url": "https://github.com/idaholab/moose/discussions/17453",
          "updatedAt": "2022-09-16T11:13:46Z",
          "publishedAt": "2021-03-29T19:05:40Z",
          "category": {
            "name": "Q&A Modules: General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "WilkAndy"
                  },
                  "bodyText": "Hi @AmandaLug ,\nThanks for your questions.  They'll help future readers too!\n\nYou are correct, DirichletBC sets a condition on the displacement.  Because this particular model has only 1 element, this immediately sets the strain.  However, now imagine a real situation with complicated, heterogeneous geology.  We know that earth-tide software such as ETERNA-x will predict strains at all points within the earth, as a function of time.  The simplest thing to assume is that those strains are correct, on-average, throughout your complicated, heterogeneous geology.  At some points, the strains in your geology are a little larger than ETERNA-x, and at other points, the strains are a little smaller, but their mean is the same as the ETERNA-x prediction.  So, we use a suitable DirichletBC on the displacements to enforce the average strain throughout our model to be the same as the ETERNA-x prediction.  Given those DirichletBC, MOOSE will solve for the displacements (and strains) throughout our model, finding some strains are a little larger than average, some are a little smaller, but the mean strain is fixed by the BCs, to be equal to the ETERNA-x strains.  Make sense?\nYou can still compute barometric efficiency when you have gravity.  The setup is the same.  The atmospheric pressure applies a total stress to the top of your model: it applies a FunctionNeumannBC to the displacement variable, and a FunctionDirichletBC to your porepressure variable (which you would have in a realistic situation, but barometric_fully_confined.i does not have).  See atm_tides.i for an example.  Looking at Fig4 of the documentation, i estimate barometric efficiency to be around 0.4 (ratio of orange amplitude to blue amplitude).",
                  "url": "https://github.com/idaholab/moose/discussions/17453#discussioncomment-545719",
                  "updatedAt": "2022-09-16T11:13:46Z",
                  "publishedAt": "2021-03-29T21:23:34Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "AmandaLug"
                          },
                          "bodyText": "Thanks Andy for the great answers! I will need some time to process this.",
                          "url": "https://github.com/idaholab/moose/discussions/17453#discussioncomment-549594",
                          "updatedAt": "2022-09-16T11:13:46Z",
                          "publishedAt": "2021-03-30T18:03:04Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Instance UO inside Material",
          "author": {
            "login": "joe61vette"
          },
          "bodyText": "Hello:\nBeginner developer help needed here.  I want to instance a user object during the initialization of a material object.  To be specific, I have an ADMaterial \"SPWallDragMaterial\" that will produce a material property for wall drag.  I want the user to be able to select from a list of wall drag models.  So, I have a UO base class \"WallDragModel\", that has subclasses for the various models.\nIn SPWallDragMaterial.h, I have the declaration:\nconst WallDragModel & _wd;    // user object for wall drag model\nIn SPWallDragMaterial::validParams, there is a MooseEnum for the various models, along with the necessary input params that will need to be passed to the model UOs.  In the constructor for SPWallDragMaterial, I have:\nconst std::string uo_name = this->name() + \"_wall_drag\";\nstd::string uo_type = \"WallDragModel\";\nInputParameters params = WallDragModel::validParams();\nswitch (_wall_drag_model)\n{\ncase ASP::CONSTANT:\nuo_type = \"ConstantWallDrag\";\nparams.set(\"value\") = getParam(\"value\");\nbreak;\n..... other cases.....\n}\n_fe_problem.addUserObject(uo_type, uo_name, params);\n_wd = _fe_problem.getUserObject(uo_name);\nI included \"FEProblemBase.h\".  I get the following compile error for the last line of code above (along with the error that I did not initialize _wd):\nUsers/joe/projects/ASP/src/materials/SPWallDragMaterial.C:82:7: error: no viable overloaded '='\n_wd = _fe_problem.getUserObject(uo_name);\n/Users/joe/projects/ASP/build/header_symlinks/WallDragModel.h:5:7: note: candidate function (the implicit copy assignment operator) not viable: 'this' argument has type 'const WallDragModel', but method is not marked const\nclass WallDragModel : public GeneralUserObject\n\nWhat is the proper way to instance a UO inside of a Material class?  \n\nThanks for the help,\nJoe Kelly",
          "url": "https://github.com/idaholab/moose/discussions/17418",
          "updatedAt": "2022-06-22T09:09:28Z",
          "publishedAt": "2021-03-23T20:05:54Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "ngrilli"
                  },
                  "bodyText": "Dear Joe Kelly,\nThe way I use user objects inside materials is shown in this example:\nhttps://github.com/ngrilli/c_pfor_am/blob/main/src/materials/ComputeElasticityTensorCPGrain.C\nIn the class parameters you add:\nparams.addParam(\"read_prop_user_object\",\n\"The GrainPropertyReadFile \"\n\"GeneralUserObject to read element \"\n\"specific property values from file\");\nThen in the constructor:\n_read_prop_user_object(isParamValid(\"read_prop_user_object\")\n? &getUserObject(\"read_prop_user_object\")\n: nullptr),\nThen methods of the user object can be called as:\n_read_prop_user_object->getData(_current_elem, 0);\nIn the header file, there is:\nconst GrainPropertyReadFile * const _read_prop_user_object;\nIn this case you don't need the addUserObject function.\nBest Regards,\nNicol\u00f2 Grilli\nNational University of Singapore",
                  "url": "https://github.com/idaholab/moose/discussions/17418#discussioncomment-521638",
                  "updatedAt": "2022-06-22T09:09:28Z",
                  "publishedAt": "2021-03-24T08:22:30Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "ngrilli"
                          },
                          "bodyText": "@joe61vette",
                          "url": "https://github.com/idaholab/moose/discussions/17418#discussioncomment-521640",
                          "updatedAt": "2022-06-22T09:09:27Z",
                          "publishedAt": "2021-03-24T08:22:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "joe61vette"
                          },
                          "bodyText": "Dear Nicolo:\nThanks so much for your reply.  If I understand this correctly, the \"read_prop_user_object\" would have to already exist.  Namely be part of the input file.  Thus it would already be instanced and initialized.  The coding you provided would then \"use\" that UO.  Is that correct?\nWhat I am hoping to do is actually instance the UO inside the material.  So, no UO in the input model.  The input params would actually be read by the material and then passed to the UO when it is instanced.  I would like to follow this approach as a user convenience feature.\nThanks for the help,\nJoe",
                          "url": "https://github.com/idaholab/moose/discussions/17418#discussioncomment-525155",
                          "updatedAt": "2022-06-22T09:09:19Z",
                          "publishedAt": "2021-03-24T15:03:17Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "rwcarlsen"
                  },
                  "bodyText": "Would it be better to maybe just make all these things materials?  You could create a material for each drag model - e.g. FooDragModel, BarDragModel, etc.  Then you could have this material create properties with its own name plus a suffix.  The wall drag material itself would then take the name of the drag model material as a parameter - and then add the pre-coded suffixes to retrieve the appropriate values supplied by the drag model.  Something like this:\n[Materials]\n  [drag_model]\n    type = FooDragModel\n    # this material creates properties named \"drag_model_prop1\", \"drag_model_prop2\" - etc.\n  []\n  [wall_drag]\n    type = WallDrag\n    model = drag_model # uses this suffix to calc+retrieve properties \"drag_model_prop1\", \"drag_model_prop2\", etc.\n  []\n[]",
                  "url": "https://github.com/idaholab/moose/discussions/17418#discussioncomment-545125",
                  "updatedAt": "2022-06-22T09:09:14Z",
                  "publishedAt": "2021-03-29T18:50:04Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "joe61vette"
                          },
                          "bodyText": "Thanks Robert.  I think that would work.  I will need to look at it as I would like to instance the drag_model specific materials from the wall_drag material rather than have the user need to put them in the input model.  Wasn't sure if I needed an action for this or not.  In some of the drag_models, there is a significant amount of setup and data to store.  That is why I want them in their own objects.\nJoe",
                          "url": "https://github.com/idaholab/moose/discussions/17418#discussioncomment-548034",
                          "updatedAt": "2022-06-22T09:09:14Z",
                          "publishedAt": "2021-03-30T12:50:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "rwcarlsen"
                          },
                          "bodyText": "If you really want to streamline input file syntax, then an action that sets things up would be the way to go.  MOOSE really isn't designed for MooseObject/system subclasses (nearly all classes/objects in moose) to be created ouside of input files and/or actions.  Anything else you try will just be a bit wonky and won't be a \"first-class\" work-flow.  It's also generally good to try to stick with materials as often as reasonably doable over other objects (userobjects, etc.) - they offer some power/flexibility that can be quite convenient as your code evolves.",
                          "url": "https://github.com/idaholab/moose/discussions/17418#discussioncomment-548532",
                          "updatedAt": "2022-06-22T09:09:14Z",
                          "publishedAt": "2021-03-30T14:43:01Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Periodic BC on cubit generated, 3D mesh",
          "author": {
            "login": "bhollra"
          },
          "bodyText": "Hello,\nI am working on a steady state 3D solid heat conduction problem. I would like to implement a periodic boundary condition in my model, but adding the following into the [BCs] section of my input file causes the residuals to stagnate well before an appropriate convergence tolerance.\n\n[./Periodic]\n\t[./x]\n\t\tvariable = T_solid\n\t\tprimary = left\n\t\tsecondary = right\n\t\ttranslation = '1 0 0'\n\t[../]\n\t[./y]\n\t\tvariable = T_solid\n\t\tprimary = front\n\t\tsecondary = back\n\t\ttranslation = '0 1 0'\n\t[../]\n[../]\n\nTo demonstrate this, I have created a simpler version of my case where the issue is recreated. In a case where I remove the periodic BC, the solution has no trouble solving, as shown below:\n\nAny insight into any potential mistakes in my implementation or a more appropriate method for implementing a periodic BC for this case would be much appreciated.\nBrent\nAttached are the input file and mesh I am using.\nperiodic_bc.zip",
          "url": "https://github.com/idaholab/moose/discussions/17267",
          "updatedAt": "2022-10-20T05:23:43Z",
          "publishedAt": "2021-03-10T04:10:21Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "bhollra"
                  },
                  "bodyText": "It seems there was an error running the original files I uploaded with MOOSE (I was using SAM to run them with no issues). Attached are updated files, tested with MOOSE only, which reproduce the issue described above.\nperiodic_bc_1.zip",
                  "url": "https://github.com/idaholab/moose/discussions/17267#discussioncomment-465113",
                  "updatedAt": "2023-09-27T17:20:25Z",
                  "publishedAt": "2021-03-10T20:08:11Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Tagging @permcody @andrsd who should know a lot about periodic BCs",
                  "url": "https://github.com/idaholab/moose/discussions/17267#discussioncomment-495428",
                  "updatedAt": "2023-09-27T17:20:28Z",
                  "publishedAt": "2021-03-17T19:50:07Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "andrsd"
                  },
                  "bodyText": "Your problem is in your PETSc options. When I comment them out, I get this convergence:\n 0 Nonlinear |R| = 5.744566e+00\n      0 Linear |R| = 5.744566e+00\n      1 Linear |R| = 3.609398e-01\n      2 Linear |R| = 8.977029e-02\n      3 Linear |R| = 1.969694e-02\n      4 Linear |R| = 1.959308e-02\n      5 Linear |R| = 1.350552e-02\n      6 Linear |R| = 1.279056e-03\n      7 Linear |R| = 7.000715e-04\n      8 Linear |R| = 3.249532e-04\n      9 Linear |R| = 8.143903e-05\n     10 Linear |R| = 2.449361e-05\n 1 Nonlinear |R| = 2.449361e-05\n      0 Linear |R| = 2.449361e-05\n      1 Linear |R| = 2.279705e-05\n      2 Linear |R| = 2.255615e-05\n      3 Linear |R| = 2.128266e-05\n      4 Linear |R| = 1.389275e-05\n      5 Linear |R| = 4.483888e-06\n      6 Linear |R| = 9.006490e-07\n      7 Linear |R| = 3.828971e-07\n      8 Linear |R| = 1.659223e-07\n      9 Linear |R| = 7.817783e-08\n     10 Linear |R| = 4.299557e-08\n     11 Linear |R| = 2.047900e-08\n     12 Linear |R| = 8.901540e-09\n     13 Linear |R| = 4.517054e-09\n     14 Linear |R| = 1.658610e-09\n     15 Linear |R| = 7.478177e-10\n     16 Linear |R| = 3.790151e-10\n     17 Linear |R| = 9.637592e-11\n 2 Nonlinear |R| = 9.637587e-11\n Solve Converged!\n\nAnd this result:",
                  "url": "https://github.com/idaholab/moose/discussions/17267#discussioncomment-499167",
                  "updatedAt": "2023-09-27T17:20:32Z",
                  "publishedAt": "2021-03-18T15:56:47Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "bhollra"
                          },
                          "bodyText": "Thanks for your response. Removing the PETSc options did the trick for me!",
                          "url": "https://github.com/idaholab/moose/discussions/17267#discussioncomment-535090",
                          "updatedAt": "2023-09-27T17:20:36Z",
                          "publishedAt": "2021-03-26T16:35:54Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "mangerij"
                  },
                  "bodyText": "Very nice mesh... what sort of CUBIT options did you use to do this?\nI always have difficulty getting periodic nodes when using the tetrahedrons.",
                  "url": "https://github.com/idaholab/moose/discussions/17267#discussioncomment-544983",
                  "updatedAt": "2023-09-27T17:20:36Z",
                  "publishedAt": "2021-03-29T18:11:10Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "bhollra"
                          },
                          "bodyText": "I output the blocks using the \"hex8\" element type, but it looks like the actual output is a combination of hexahedral and wedge element types, so I don't have any experience using tetrahedrons with a periodic boundary.\nHere is the .jou file I used if you're interested.\ntest_mesh.txt",
                          "url": "https://github.com/idaholab/moose/discussions/17267#discussioncomment-545160",
                          "updatedAt": "2023-09-27T17:20:36Z",
                          "publishedAt": "2021-03-29T19:03:45Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Time dependent ConvectiveHeatFluxBC",
          "author": {
            "login": "makeclean"
          },
          "bodyText": "This may be a totally obvious question, but I would like to make the T_infinity value of an ADConvectiveHeatFluxBC or ConvectiveHeatFluxBC depend upon time, or at different timesteps different values of the T_infinity would be changeable. I've been able to have a functional description of the HTC, but not the T_infinity. Any suggestions?",
          "url": "https://github.com/idaholab/moose/discussions/17443",
          "updatedAt": "2022-06-21T23:35:06Z",
          "publishedAt": "2021-03-29T09:47:07Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "T_infinity is a material property ADConvectiveHeatFluxBC, same as the heat transfer coefficient, so it can be set using a GenericFunctionMaterial, which will use a function with the desired time dependence of the Tinf\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/17443#discussioncomment-544461",
                  "updatedAt": "2022-06-21T23:35:07Z",
                  "publishedAt": "2021-03-29T15:42:44Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Convergence issues due to huge differences in stiffness tensor",
          "author": {
            "login": "Bala-1005"
          },
          "bodyText": "Hello everyone,\nI am working on kks model for solidification with the two phases being solid and liquid. I have included the tensor mechanics module too similar to the one shown in the example \"/moose/modules/combined/examples/phase_field-mechanics/kks_mechanics_VTS.i\".\nThe stiffness of the liquid phase is several orders lower than the solid(~10^9). This has caused some convergence issues close to the boundary and when two solid parts of the domain interact. I am not sure how to alleviate this problem as this seems to be the only issue with the code. I have fiddled around with different boundary conditions but that has not helped at all.\nHow do I solve this issue?\nThanks,\nBala",
          "url": "https://github.com/idaholab/moose/discussions/16922",
          "updatedAt": "2022-06-11T08:58:44Z",
          "publishedAt": "2021-02-08T17:33:41Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "Have you tried using automatic_scaling? This can be enabled in the Executioner block with \"automatic_scaling = true\"",
                  "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-355553",
                  "updatedAt": "2022-06-11T08:58:45Z",
                  "publishedAt": "2021-02-10T06:48:51Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "Bala-1005"
                          },
                          "bodyText": "I did try that but it doesn't seem to help. I also tried to use ActivateElementsCoupled as instructed in #16715(This is a related issue where I wanted to activate tensor mechanics only for the solid and interface part but not the liquid phase). But it gives me the following error.\n\"A 'ActivateElementsCoupled' is not a registered object.\nIf you are trying to find this object in a dynamically linked library, make sure that\nthe library can be found either in your \"Problem/library_path\" parameter or in the\nMOOSE_LIBRARY_PATH environment variable.\"\nThanks,\nBala",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-356506",
                          "updatedAt": "2022-06-11T08:58:52Z",
                          "publishedAt": "2021-02-10T13:23:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Did you build your application with tensor_mechanics? It says what module is included in the Makefile",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-491470",
                          "updatedAt": "2022-06-11T08:59:00Z",
                          "publishedAt": "2021-03-17T00:34:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Bala-1005"
                          },
                          "bodyText": "Yes. I did build my application with tensor_mechanics. Besides, I had the physics wrong when I wanted to activate only the solid part of the domain for tensor mechanics. Right now I have applied tensor mechanics to the entire domain. Convergence is still an issue at the boundary.",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-491479",
                          "updatedAt": "2022-06-11T08:59:00Z",
                          "publishedAt": "2021-03-17T00:37:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "If you are using phase-field variable to indicate the phase domain, you should not use ActivateElementsCoupled. That will assign phase domain element-wise.\nIs your simulation running fine with two comparable stiffness tensors? Are you using a direct solver LU in the executioner block?",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-494244",
                          "updatedAt": "2022-06-11T09:00:12Z",
                          "publishedAt": "2021-03-17T15:21:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Bala-1005"
                          },
                          "bodyText": "Yes. The simulation runs fine with comparable stiffness tensors. I was able to solve the issue at the boundary by deactivating some of the boundary conditions. But when two solids meet, the simulation ceases to converge. I have attached an image\nat the point where the simulation stops.\n\nAlso, here is the executioner block.\n[Executioner]\ntype = Transient\nsolve_type = 'NEWTON'\npetsc_options_iname = '-pc_type -sub_pc_type -sub_pc_factor_shift_type'\npetsc_options_value = 'asm      ilu          nonzero'\nl_max_its = 30\nnl_max_its = 50\nnl_rel_tol = 1e-08\nautomatic_scaling = true\nend_time = 25\ndt = 0.008\n[./Adaptivity]\ninitial_adaptivity = 3 # Number of times mesh is adapted to initial condition\nrefine_fraction = 0.6 # Fraction of high error that will be refined\ncoarsen_fraction = 0.1 # Fraction of low error that will coarsened\nmax_h_level = 3 # Max number of refinements used, starting from initial mesh (before uniform refinement)\n[../]\n[]\nThanks,\nBala",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-495597",
                          "updatedAt": "2022-06-11T09:00:12Z",
                          "publishedAt": "2021-03-17T20:30:04Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Bala-1005"
                          },
                          "bodyText": "Hello @jiangwen84. I did try to run the simulation with LU solver. The issue of non convergence when two solid parts of the domain meet still persists. There is no issue when the stiffness matrices are comparable.\nThanks,\nBala",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-498837",
                          "updatedAt": "2022-06-11T09:00:46Z",
                          "publishedAt": "2021-03-18T14:56:52Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "It is a bit difficult to tell what will be issue. Looks like the convergence issue might due to your physics, i.e. some quantities become unphysical, and the large material contrast trigger this issue?\nAnyway, could you attach your output? It might be useful to take a look at the linear and nonlinear iterations behavior.",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-501697",
                          "updatedAt": "2022-06-11T09:00:46Z",
                          "publishedAt": "2021-03-19T05:32:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Bala-1005"
                          },
                          "bodyText": "I was able to slightly solve the issue by increasing the number of nonlinear iterations to 100. But only for several timesteps. The issue returned with convergence being the issue. The output file is very large and therefore I am attaching the part where it starts to cut timesteps and stop converging.\noutput.txt\nerror.txt\nThanks,\nBala",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-518258",
                          "updatedAt": "2022-06-11T09:00:46Z",
                          "publishedAt": "2021-03-23T15:10:33Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "Try to set solve_type = 'PJFNK'\nMaybe you want to loose your convergence tolerance a bit, i.e., nl_rel_tol = 1e-7 and nl_abs_tol=1e-7",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-518346",
                          "updatedAt": "2022-06-11T09:00:46Z",
                          "publishedAt": "2021-03-23T15:22:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Bala-1005"
                          },
                          "bodyText": "Thank you @jiangwen84 and everyone. I was able to solve the issue by increasing the nonlinear iterations number, loosening the tolerance a bit, and changing the boundary conditions from a displacement boundary condition to pressure boundary conditions. The simulation runs quite well now.\nOne final problem, I hope :),  is that there are a few memory issues. Any suggestions on how to solve these issues? Here is the error file. I used --malloc-dump while running the file.\nel2b_KHS.txt\nThanks,\nBala",
                          "url": "https://github.com/idaholab/moose/discussions/16922#discussioncomment-535529",
                          "updatedAt": "2022-06-11T09:00:50Z",
                          "publishedAt": "2021-03-26T18:22:52Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "MOOSE sub app freezing with no error message",
          "author": {
            "login": "s-dunnim"
          },
          "bodyText": "Hi, I'm using the Level Set module with Navier Stokes, and running a model involving a reinitialisation sub app. I'm getting a problem where at some point during the reinitialisation MOOSE will just stop, with no error message. If I run top in a separate terminal, I can see the processes are still running, but residuals are no longer being printed, and there are no more timesteps being added to the output file. This is happening at apparently random times; even with the same input file, it can happen a few timesteps in, or right towards the end.\nI can only recreate the problem in parallel, so used the lldb debugger in parallel to try and find the cause - it seems there is a problem in INSADTauMaterial which is being used by INSADStabilized3Eqn, that ends up in a divide by zero error. I've attached the backtrace information from the debugger here (interestingly the problem happened in the main app, not the sub app, when I was running the debugger... this may point to a completely separate issue, especially as the sub app itself doesn't use the Navier Stokes materials. However the freezing behaviour is the same - and if they aren't linked, how does one problem arise in the opt app and the other one in debug mode?). I've also attached the input files.\nDoes anyone know what could be causing this, and how I can prevent it? Thanks in advance.\nls_b.i.txt\nreinit_sub.i.txt",
          "url": "https://github.com/idaholab/moose/discussions/17438",
          "updatedAt": "2022-06-27T08:31:32Z",
          "publishedAt": "2021-03-26T19:46:35Z",
          "category": {
            "name": "Q&A Modules: Navier-Stokes"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "s-dunnim"
                  },
                  "bodyText": "I have found the solution (if not the cause) to the freezing problem in the sub app - something was going wrong with the SolutionTimeAdaptiveDT timestepper I was using, it works fine if I simply use a constant dt. This doesn't explain the debugger behaviour as far as I can tell, but I'm going to let that slide as it isn't affecting my model.",
                  "url": "https://github.com/idaholab/moose/discussions/17438#discussioncomment-543262",
                  "updatedAt": "2022-06-27T08:32:58Z",
                  "publishedAt": "2021-03-29T10:41:54Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Uniaxial tensile simulation of polycrystalline model based on crystal plasticity",
          "author": {
            "login": "PengWei97"
          },
          "bodyText": "Dear MOOSE experts,\nI recently wrote an input file based on the files prop_grain_read.i and crysp_user_object.i to simulate plastic uniaxial stretching of polycrystalline crystals, and established a two-dimensional model of 0.21*0.21mm. There are 200 grains in it. Stretching in the y-axis direction.\n284 time steps, time = 0.327438, stretching by 0.03%.\nBut the calculation is very slow. Is there any suggestion to speed up the calculation?\nThe content of the written input file and output terminal is as follows,\nprop_grain_read_cp.i\nprop_grain_read_cp.txt\nAny suggestions or recommendations to fix these problems would be greatly appreciated.\nThank you\nWei Peng",
          "url": "https://github.com/idaholab/moose/discussions/17423",
          "updatedAt": "2022-06-24T09:23:28Z",
          "publishedAt": "2021-03-24T15:22:42Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "jiangwen84"
                  },
                  "bodyText": "@PengWei97 I need to let you know that we are currently switching the UO-based CP (the one you used) to stress-update based CP. One on-going PR is #17405. The new version might give you a bit speedup.\nFor your current input file, I suggest you add maximum_substep_iteration = 4 in your FiniteStrainUObasedCP block. It will allow you to take larger dt. With sub stepping, each time step will take longer time, but it can speed up your simulation by taking less number of time steps.\n@sapitts @dewenyushu  any other thoughts?",
                  "url": "https://github.com/idaholab/moose/discussions/17423#discussioncomment-527866",
                  "updatedAt": "2022-06-24T09:23:32Z",
                  "publishedAt": "2021-03-25T03:53:45Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "dewenyushu"
                          },
                          "bodyText": "Hi @PengWei97 I agree with @jiangwen84 that using sub-stepping should allow larger dt thus speed up computation in general.\nFor the new stress-update based CP base class  (#17405). As Wen mentioned, this is a new implementation which will be more actively developed and maintained than the UO-based version in the future. With this new class, we do see some improvements in terms of the total linear/nonlinear iterations for several existing models (and some of them are improved a lot!). However, we are yet to quantify the improvement or examine the consistency of this improvement among all existing models.",
                          "url": "https://github.com/idaholab/moose/discussions/17423#discussioncomment-530003",
                          "updatedAt": "2022-06-24T09:23:34Z",
                          "publishedAt": "2021-03-25T14:41:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "PengWei97"
                          },
                          "bodyText": "Hi @jiangwen84 ,Thank you for your guidance.\nI added maximum_substep_iteration = 5 to theFiniteStrainUObasedCPblock according to your suggestion. But it is still calculated at the minimum incremental step set in the seventh step, and it took five and a half hours to calculate to \"Time Step 805, time = 0.897438, dt = 0.001\". Similarly, I used umat calculations on abaqus, and it took less than half an hour to complete the uniaxial stretching simulation. I think it must be an unreasonable setting in that place. The following is the input file. Do you have any suggestions for improvement?\n`[Mesh]\ntype = GeneratedMesh\ndim = 2\nelem_type = QUAD4\ndisplacements = 'disp_x disp_y'\nnx = 100\nny = 100\nxmax = 0.21\nymax = 0.21\n[]\n[Variables]\n[./disp_x]\nblock = 0\n[../]\n[./disp_y]\nblock = 0\n[../]\n[]\n[GlobalParams]\nvolumetric_locking_correction=true\n[]\n[AuxVariables]\n[./stress_yy]\norder = CONSTANT\nfamily = MONOMIAL\nblock = 0\n[../]\n[./e_yy]\norder = CONSTANT\nfamily = MONOMIAL\nblock = 0\n[../]\n[]\n[Functions]\n[./tdisp]\ntype = ParsedFunction\nvalue = 2e-4*t # Approximately 0.1% loading per second\n[../]\n[]\n[UserObjects]\n[./prop_read]\ntype = ElementPropertyReadFile\n# need read\nprop_file_name = 'input_file.txt' # euler angle\n# Enter file data as prop#1, prop#2, .., prop#nprop\nnprop = 3\nread_type = grain\nngrain = 200\n# rand_seed = 25346\nrve_type = periodic\n# Periodic or non-periodic grain distribution: Default is non-periodic\n[../]\n[]\n[AuxKernels]\n[./stress_yy]\ntype = RankTwoAux\nvariable = stress_yy\nrank_two_tensor = stress\nindex_j = 1\nindex_i = 1\nexecute_on = 'initial timestep_end'\nblock = 0\n[../]\n[./e_yy]\ntype = RankTwoAux\nvariable = e_yy\nrank_two_tensor = lage\nindex_j = 1\nindex_i = 1\nexecute_on = timestep_end\nblock = 0\n[../]\n[]\n[BCs]\n[./fix_x]\ntype = DirichletBC\nvariable = disp_x\nboundary = 'left'\nvalue = 0\n[../]\n[./fix_y]\ntype = DirichletBC\nvariable = disp_y\nboundary = 'bottom'\nvalue = 0\n[../]\n[./tdisp]\ntype = FunctionDirichletBC\nvariable = disp_y\nboundary = top\nfunction = tdisp\n[../]\n[]\n[UserObjects]\n[./slip_rate_gss]\ntype = CrystalPlasticitySlipRateGSS\nvariable_size = 12\nslip_sys_file_name = input_slip_sys.txt # slip system\nnum_slip_sys_flowrate_props = 2\nflowprops = '1 4 0.001 0.1 5 8 0.001 0.1 9 12 0.001 0.1'\nuo_state_var_name = state_var_gss\n[../]\n[./slip_resistance_gss]\ntype = CrystalPlasticitySlipResistanceGSS\nvariable_size = 12\nuo_state_var_name = state_var_gss\n[../]\n[./state_var_gss]\ntype = CrystalPlasticityStateVariable\nvariable_size = 12\ngroups = '0 4 8 12'\ngroup_values = '60.8 60.8 60.8'\nuo_state_var_evol_rate_comp_name = state_var_evol_rate_comp_gss\nscale_factor = 1.0\n[../]\n[./state_var_evol_rate_comp_gss]\ntype = CrystalPlasticityStateVarRateComponentGSS\nvariable_size = 12\nhprops = '1.0 541.5 109.8 2.5'\nuo_slip_rate_name = slip_rate_gss\nuo_state_var_name = state_var_gss\n[../]\n[]\n[Materials]\n[./crysp]\ntype = FiniteStrainUObasedCP\nstol = 1e-2\ntan_mod_type = exact\nuo_slip_rates = 'slip_rate_gss'\nuo_slip_resistances = 'slip_resistance_gss'\nuo_state_vars = 'state_var_gss'\nuo_state_var_evol_rate_comps = 'state_var_evol_rate_comp_gss'\nmaximum_substep_iteration = 5 # add for an advice of jiangwen84\n[../]\n[./strain]\ntype = ComputeFiniteStrain\ndisplacements = 'disp_x disp_y'\n[../]\n[./elasticity_tensor]\ntype = ComputeElasticityTensorCP\nC_ijkl = '1.684e5 1.214e5 1.214e5 1.684e5 1.214e5 1.684e5 0.754e5 0.754e5 0.754e5'\nfill_method = symmetric9\nread_prop_user_object = prop_read\n[../]\n[]\n[Postprocessors]\n[./stress_yy]\ntype = ElementAverageValue\nvariable = stress_yy\nblock = 'ANY_BLOCK_ID 0'\n[../]\n[./e_yy]\ntype = ElementAverageValue\nvariable = e_yy\nblock = 'ANY_BLOCK_ID 0'\n[../]\n[]\n[Preconditioning]\n[./smp]\ntype = SMP\nfull = true\n[../]\n[]\n[Executioner]\ntype = Transient\n#Preconditioned JFNK (default)\nsolve_type = 'PJFNK'\nl_max_its = 30\nl_tol = 1.0e-4\nnl_max_its = 20\nnl_rel_tol = 1.0e-9\nend_time = 1\ndtmin = 0.001\n[./TimeStepper]\ntype = IterationAdaptiveDT\ndt = 0.05\noptimal_iterations = 6\n[../]\n[]\n[Outputs]\nfile_base = prop_grain_read_out_xiu02\nexodus = true\ncsv = true\n[]\n[Kernels]\n[./TensorMechanics]\ndisplacements = 'disp_x disp_y'\nuse_displaced_mesh = true\n[../]\n[]\n`",
                          "url": "https://github.com/idaholab/moose/discussions/17423#discussioncomment-532930",
                          "updatedAt": "2022-06-24T09:23:35Z",
                          "publishedAt": "2021-03-26T07:07:49Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "PengWei97"
                          },
                          "bodyText": "Hi @PengWei97 I agree with @jiangwen84 that using sub-stepping should allow larger dt thus speed up computation in general.\nFor the new stress-update based CP base class (#17405). As Wen mentioned, this is a new implementation which will be more actively developed and maintained than the UO-based version in the future. With this new class, we do see some improvements in terms of the total linear/nonlinear iterations for several existing models (and some of them are improved a lot!). However, we are yet to quantify the improvement or examine the consistency of this improvement among all existing models.\n\nHi @dewenyushu,Thank you for your advice.\nI tried to clone the relevant *.C and *.h files in my development branch into the application I created, but an error was displayed when compiling, the error is as follows,\nerror_make.txt\nIs it because this class is still under development, so we can't use it now, or is it for other reasons?",
                          "url": "https://github.com/idaholab/moose/discussions/17423#discussioncomment-532965",
                          "updatedAt": "2022-06-24T09:23:36Z",
                          "publishedAt": "2021-03-26T07:21:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "jiangwen84"
                          },
                          "bodyText": "Try one more thing:\nadd\npetsc_options_iname = '-pc_type'\npetsc_options_value = 'lu'\nto [Executioner] block.",
                          "url": "https://github.com/idaholab/moose/discussions/17423#discussioncomment-535234",
                          "updatedAt": "2022-09-02T07:34:12Z",
                          "publishedAt": "2021-03-26T17:12:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "PengWei97"
                          },
                          "bodyText": "Try one more thing:\nadd\npetsc_options_iname = '-pc_type'\npetsc_options_value = 'lu'\nto [Executioner] block.\n\nhi @jiangwen84 \uff0cfollow your suggestion, this problem has been solved.\nThank you very much for your guidance.\nwei",
                          "url": "https://github.com/idaholab/moose/discussions/17423#discussioncomment-537754",
                          "updatedAt": "2022-09-02T07:34:12Z",
                          "publishedAt": "2021-03-27T14:29:20Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "dewenyushu"
                          },
                          "bodyText": "Hi @PengWei97 I agree with @jiangwen84 that using sub-stepping should allow larger dt thus speed up computation in general.\nFor the new stress-update based CP base class (#17405). As Wen mentioned, this is a new implementation which will be more actively developed and maintained than the UO-based version in the future. With this new class, we do see some improvements in terms of the total linear/nonlinear iterations for several existing models (and some of them are improved a lot!). However, we are yet to quantify the improvement or examine the consistency of this improvement among all existing models.\n\nHi @dewenyushu,Thank you for your advice.\nI tried to clone the relevant *.C and *.h files in my development branch into the application I created, but an error was displayed when compiling, the error is as follows,\nerror_make.txt\nIs it because this class is still under development, so we can't use it now, or is it for other reasons?\n\nThanks for trying this out! By looking at your log file, it seems like you will need to checkout the Kalidindi class (CrystalPlasticityKalidindiUpdate.C and .h) from my PR as well since I have made some changes for it being able to run with the new base class.\nThe new base class is something that is still in development right now and transforming all existing CP models to the new base class will require some more work. Currently, only the Kalidindi CP model works with this new base class. So I will not be surprised if there being some glitches when transforming your model without some changes. If you come across some new issues, please keep us updated.",
                          "url": "https://github.com/idaholab/moose/discussions/17423#discussioncomment-538290",
                          "updatedAt": "2022-09-02T07:34:11Z",
                          "publishedAt": "2021-03-27T16:58:38Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "PengWei97"
                          },
                          "bodyText": "Hi @PengWei97 I agree with @jiangwen84 that using sub-stepping should allow larger dt thus speed up computation in general.\nFor the new stress-update based CP base class (#17405). As Wen mentioned, this is a new implementation which will be more actively developed and maintained than the UO-based version in the future. With this new class, we do see some improvements in terms of the total linear/nonlinear iterations for several existing models (and some of them are improved a lot!). However, we are yet to quantify the improvement or examine the consistency of this improvement among all existing models.\n\nHi @dewenyushu,Thank you for your advice.\nI tried to clone the relevant *.C and *.h files in my development branch into the application I created, but an error was displayed when compiling, the error is as follows,\nerror_make.txt\nIs it because this class is still under development, so we can't use it now, or is it for other reasons?\n\nThanks for trying this out! By looking at your log file, it seems like you will need to checkout the Kalidindi class (CrystalPlasticityKalidindiUpdate.C and .h) from my PR as well since I have made some changes for it being able to run with the new base class.\nThe new base class is something that is still in development right now and transforming all existing CP models to the new base class will require some more work. Currently, only the Kalidindi CP model works with this new base class. So I will not be surprised if there being some glitches when transforming your model without some changes. If you come across some new issues, please keep us updated.\n\nThank you very much. I will try to modify it.",
                          "url": "https://github.com/idaholab/moose/discussions/17423#discussioncomment-539696",
                          "updatedAt": "2022-09-02T07:34:12Z",
                          "publishedAt": "2021-03-28T02:15:51Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "MooseDocs w/ Animal Codes",
          "author": {
            "login": "roskofnj"
          },
          "bodyText": "Each Animal code has it's own moosedoc.py which is used to start a document server on the localhost (127.0.0.1:8000). I would like to have all the Animal code document servers running at the same time. However I cannot launch multiple document servers on different ports - when the second doc server is launched it simply overwrites the first.\nIs there a way to start a document server that contains all documentation for all Animal codes, as well as RELAP7? It seems strange that I cannot start different servers on different ports.\nThanks very much for any help you're able to offer.",
          "url": "https://github.com/idaholab/moose/discussions/17436",
          "updatedAt": "2022-09-01T18:37:57Z",
          "publishedAt": "2021-03-26T15:08:47Z",
          "category": {
            "name": "Q&A Tools"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "cticenhour"
                  },
                  "bodyText": "You can change the port for the MooseDocs build command using the --port option. So, if I wanted to change my documentation website port to 9000, I would just run the following in my app's doc directory:\n./moosedocs.py build --port 9000 --serve\n\nand the built pages would be available on http://127.0.0.1:9000. To see all of the options for the build command, type ./moosedocs.py build -h in your doc directory.",
                  "url": "https://github.com/idaholab/moose/discussions/17436#discussioncomment-535177",
                  "updatedAt": "2022-09-01T18:37:57Z",
                  "publishedAt": "2021-03-26T16:55:59Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "roskofnj"
                          },
                          "bodyText": "This is exactly what I've trying to do. But any time I try to launch a separate instance it overwrites the first. For example:\n\nin the Griffin directory, I'll do ./moosedocs.py build --serve --port 8001 and then Griffin document server launches correctly at 127.0.0.1:8001\nin the Sockeye directory I'll do ./moosedocs.py build --serve --port 8002 and the Sockeye document server correctly launches at 127.0.0.1:8002. But then if I visit 127.0.0.1:8001 (which used to be a Griffin doc server) it now shows Sockeye.\n\nSo it seems like when I launch a second server it is always overwriting the first.",
                          "url": "https://github.com/idaholab/moose/discussions/17436#discussioncomment-535594",
                          "updatedAt": "2022-09-01T18:37:57Z",
                          "publishedAt": "2021-03-26T18:37:26Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "cticenhour"
                          },
                          "bodyText": "I deleted my previous comment, as I just got to test the --host alternative option (setting each documentation instance as a separate local IP) on a MacOS machine. Because both 127.x.x.x URLs are loopback interfaces (referring to the same machine), it will be overwritten regardless of the IP difference. @loganharbour's option is the proper answer here.",
                          "url": "https://github.com/idaholab/moose/discussions/17436#discussioncomment-535710",
                          "updatedAt": "2022-09-01T18:38:06Z",
                          "publishedAt": "2021-03-26T19:09:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "roskofnj"
                          },
                          "bodyText": "Thanks very much. I actually tried just what you were originally thinking.\nI way out of my league on this server stuff, so I really appreciate you both (@loganharbour ) for bearing with me on this.",
                          "url": "https://github.com/idaholab/moose/discussions/17436#discussioncomment-535779",
                          "updatedAt": "2022-09-01T18:38:06Z",
                          "publishedAt": "2021-03-26T19:25:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "loganharbour"
                          },
                          "bodyText": "Thanks for coming to us with questions! This information may be able to help someone in the future.\nI'd also like to add that in terms of documentation, if you have any suggestions to make the process any clearer to the user, we welcome issues on GitHub!",
                          "url": "https://github.com/idaholab/moose/discussions/17436#discussioncomment-535791",
                          "updatedAt": "2022-09-01T18:38:06Z",
                          "publishedAt": "2021-03-26T19:29:43Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "loganharbour"
                  },
                  "bodyText": "Note that while the serve option is the easiest way to get the documentation going, it isn't required to view the documentation. We suggest using --serve because (1) it is convenient, and (2) it auto updates from source so that you can make changes and see the changes almost real time.\nIf you would like to just generate the raw static site in one go (which you can also open with your browser by opening the html), you can generate with\n./moosedocs.py build --destination <directory>\n\nin which you fill <directory> with a directory where you want the site to be built into. Once built, you can traverse into said directory, start with the index.html page and browse as needed without having to start the local service to host the site. You could do this for all of the applications that you need documentation for, and put them in a convenient location for easy access.",
                  "url": "https://github.com/idaholab/moose/discussions/17436#discussioncomment-535196",
                  "updatedAt": "2022-09-01T18:38:08Z",
                  "publishedAt": "2021-03-26T17:01:12Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "roskofnj"
                          },
                          "bodyText": "Great, I appreciate this alternative approach. I'll give it a try. might be easier and less prone to issues than my original attempt. At this point I don't think I'll be doing too many changes to the documentation, so it should be sufficient.",
                          "url": "https://github.com/idaholab/moose/discussions/17436#discussioncomment-535601",
                          "updatedAt": "2022-09-01T18:38:08Z",
                          "publishedAt": "2021-03-26T18:39:20Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Question about explicit solve (TimeIntegrators\\ActuallyExplicitEuler)",
          "author": {
            "login": "jlinBE"
          },
          "bodyText": "Hi, I am testing explicit solves using moose\\modules\\tensor_mechanics\\examples\\coal_mining\\coarse.i\nAttached is an input file where I've turned off everything except the elastic behavior.\ncoarse_explicit.zip\nIf I use the ExplicitEuler solver, the sim runs, but convergence is much worse; if I use ActuallyExplicitEuler, I get an error message:\n[0]PETSC ERROR: --------------------- Error Message --------------------------------------------------------------\n[0]PETSC ERROR: Object is in wrong state\n[0]PETSC ERROR: Matrix is missing diagonal entry 0\n[0]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[0]PETSC ERROR: Petsc Release Version 3.13.3, unknown\n[0]PETSC ERROR: ../../tensor_mechanics-opt on a  named DELL-DBECK-2019 by moose Tue Nov  3 11:40:06 2020\n[0]PETSC ERROR: Configure options --COPTFLAGS=-O3 --CXXOPTFLAGS=-O3 --FOPTFLAGS=-O3 --with-x=0 --with-mpi=1 --with-ssl=0 --with-openmp=1 --with-debugging=0 --with-cxx-dialect=C++11 --with-shared-libraries=1 --download-mumps=1 --download-hypre=1 --download-metis=1 --download-slepc=1 --download-ptscotch=1 --download-parmetis=1 --download-scalapack=1 --download-superlu_dist=1 --with-fortran-bindings=0 --with-sowing=0 EOF --download-fblaslapack=1 AR=${PREFIX}/bin/x86_64-conda_cos6-linux-gnu-ar CC=mpicc CXX=mpicxx FC=mpifort FC=mpifort FC=mpifort CFLAGS=\"-march=nocona -mtune=haswell\" CXXFLAGS=\"-march=nocona -mtune=haswell\" LDFLAGS=\"-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/moose/miniconda3/envs/moose/lib -Wl,-rpath-link,/home/moose/miniconda3/envs/moose/lib -L/home/moose/miniconda3/envs/moose/lib\" --prefix=/home/moose/miniconda3/envs/moose\n[0]PETSC ERROR: #1 MatILUFactorSymbolic_SeqAIJ() line 1685 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/mat/impls/aij/seq/aijfact.c\n[0]PETSC ERROR: #2 MatILUFactorSymbolic() line 6620 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/mat/interface/matrix.c\n[0]PETSC ERROR: #3 PCSetUp_ILU() line 135 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/pc/impls/factor/ilu/ilu.c\n[0]PETSC ERROR: #4 PCSetUp() line 898 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/pc/interface/precon.c\n[0]PETSC ERROR: #5 KSPSetUp() line 376 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/ksp/interface/itfunc.c\n[0]PETSC ERROR: #6 PCSetUpOnBlocks_BJacobi_Singleblock() line 613 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/pc/impls/bjacobi/bjacobi.c\n[0]PETSC ERROR: #7 PCSetUpOnBlocks() line 927 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/pc/interface/precon.c\n[0]PETSC ERROR: #8 KSPSetUpOnBlocks() line 214 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/ksp/interface/itfunc.c\n[0]PETSC ERROR: #9 KSPSolve_Private() line 634 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/ksp/interface/itfunc.c\n[0]PETSC ERROR: #10 KSPSolve() line 853 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/ksp/interface/itfunc.c\napplication called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\n[unset]: write_line error; fd=-1 buf=:cmd=abort exitcode=1\nAny idea what the above means?  Also, a question about switching to explicit solve, in general: is it a drag-and-drop process where I simply add it into the input file?  Or am I supposed to change things elsewhere?",
          "url": "https://github.com/idaholab/moose/discussions/16052",
          "updatedAt": "2022-08-26T20:59:25Z",
          "publishedAt": "2020-11-03T00:53:38Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "@fdkong Could you take a look at this when you have some time.",
                  "url": "https://github.com/idaholab/moose/discussions/16052#discussioncomment-121486",
                  "updatedAt": "2022-09-13T07:58:00Z",
                  "publishedAt": "2020-11-04T19:50:25Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "Hi, I am testing explicit solves using moose\\modules\\tensor_mechanics\\examples\\coal_mining\\coarse.i\nAttached is an input file where I've turned off everything except the elastic behavior.\ncoarse_explicit.zip\nIf I use the ExplicitEuler solver, the sim runs, but convergence is much worse; if I use ActuallyExplicitEuler, I get an error message:\n[0]PETSC ERROR: --------------------- Error Message --------------------------------------------------------------\n[0]PETSC ERROR: Object is in wrong state\n[0]PETSC ERROR: Matrix is missing diagonal entry 0\n\nThe error means you might miss kernels for certain variables. Or you have a saddle point system. If you have a saddle point system, you need to explicitly add zeros for all variables to the diagonal of matrix.\nIf you could find a lot of examples about how to use ActuallyExplicitEuler in moose repo. If you are interested in ActuallyExplicitEuler, I would suggest you start from a simple example such as: ee-2d-linear.i .\n\n[0]PETSC ERROR: See https://www.mcs.anl.gov/petsc/documentation/faq.html for trouble shooting.\n[0]PETSC ERROR: Petsc Release Version 3.13.3, unknown\n[0]PETSC ERROR: ../../tensor_mechanics-opt on a named DELL-DBECK-2019 by moose Tue Nov 3 11:40:06 2020\n[0]PETSC ERROR: Configure options --COPTFLAGS=-O3 --CXXOPTFLAGS=-O3 --FOPTFLAGS=-O3 --with-x=0 --with-mpi=1 --with-ssl=0 --with-openmp=1 --with-debugging=0 --with-cxx-dialect=C++11 --with-shared-libraries=1 --download-mumps=1 --download-hypre=1 --download-metis=1 --download-slepc=1 --download-ptscotch=1 --download-parmetis=1 --download-scalapack=1 --download-superlu_dist=1 --with-fortran-bindings=0 --with-sowing=0 EOF --download-fblaslapack=1 AR=${PREFIX}/bin/x86_64-conda_cos6-linux-gnu-ar CC=mpicc CXX=mpicxx FC=mpifort FC=mpifort FC=mpifort CFLAGS=\"-march=nocona -mtune=haswell\" CXXFLAGS=\"-march=nocona -mtune=haswell\" LDFLAGS=\"-Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,-rpath,/home/moose/miniconda3/envs/moose/lib -Wl,-rpath-link,/home/moose/miniconda3/envs/moose/lib -L/home/moose/miniconda3/envs/moose/lib\" --prefix=/home/moose/miniconda3/envs/moose\n[0]PETSC ERROR: #1 MatILUFactorSymbolic_SeqAIJ() line 1685 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/mat/impls/aij/seq/aijfact.c\n[0]PETSC ERROR: #2 MatILUFactorSymbolic() line 6620 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/mat/interface/matrix.c\n[0]PETSC ERROR: #3 PCSetUp_ILU() line 135 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/pc/impls/factor/ilu/ilu.c\n[0]PETSC ERROR: #4 PCSetUp() line 898 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/pc/interface/precon.c\n[0]PETSC ERROR: #5 KSPSetUp() line 376 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/ksp/interface/itfunc.c\n[0]PETSC ERROR: #6 PCSetUpOnBlocks_BJacobi_Singleblock() line 613 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/pc/impls/bjacobi/bjacobi.c\n[0]PETSC ERROR: #7 PCSetUpOnBlocks() line 927 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/pc/interface/precon.c\n[0]PETSC ERROR: #8 KSPSetUpOnBlocks() line 214 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/ksp/interface/itfunc.c\n[0]PETSC ERROR: #9 KSPSolve_Private() line 634 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/ksp/interface/itfunc.c\n[0]PETSC ERROR: #10 KSPSolve() line 853 in /home/milljm/libs/miniconda3/conda-bld/moose-petsc_1597159363110/work/src/ksp/ksp/interface/itfunc.c\napplication called MPI_Abort(MPI_COMM_WORLD, 1) - process 0\n[unset]: write_line error; fd=-1 buf=:cmd=abort exitcode=1\nAny idea what the above means? Also, a question about switching to explicit solve, in general: is it a drag-and-drop process where I simply add it into the input file? Or am I supposed to change things elsewhere?\n\nThis is a simple block you need for triggering  explicit solve. The solver is still pretty new, and it is not popular as the implicit method in moose.  I will not be surprised  if there are some uncovered corner cases.\n  [./TimeIntegrator]\n    type = ActuallyExplicitEuler\n  [../]",
                          "url": "https://github.com/idaholab/moose/discussions/16052#discussioncomment-121521",
                          "updatedAt": "2022-09-13T07:58:01Z",
                          "publishedAt": "2020-11-04T20:58:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Traiwit"
                          },
                          "bodyText": "@fdkong\nI am having the same error, I see you mentioned\n\nThe error means you might miss kernels for certain variables. Or you have a saddle point system. If you have a saddle point system, you need to explicitly add zeros for all variables to the diagonal of matrix.\n\nHow do we actually add zeros to the matrix via an input file?\nI see the only difference I have between the example and my input file is the 'TimeDerivative' kernel, which I don't have, I instead update the material property (tensor mechanics) with respect to time.\nIt works fine if I run it using an implicit method.\nCould you please help with this? Thank you!\nKind regards,\nTraiwit\ntest_transient1.zip",
                          "url": "https://github.com/idaholab/moose/discussions/16052#discussioncomment-501473",
                          "updatedAt": "2022-09-13T07:58:03Z",
                          "publishedAt": "2021-03-19T03:40:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "hugary1995"
                          },
                          "bodyText": "@Traiwit Explicit time integration won't work without a mass matrix, i.e. how do you invert a zero matrix? Currently the Actually explicit euler integrator only works with InertialForce.",
                          "url": "https://github.com/idaholab/moose/discussions/16052#discussioncomment-534103",
                          "updatedAt": "2022-09-13T07:58:05Z",
                          "publishedAt": "2021-03-26T12:42:34Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "jlinBE"
                  },
                  "bodyText": "Thanks, I'll try it with simpler 2d cases and get back to you.",
                  "url": "https://github.com/idaholab/moose/discussions/16052#discussioncomment-124940",
                  "updatedAt": "2022-09-13T07:58:06Z",
                  "publishedAt": "2020-11-09T11:51:59Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Installing MOOSE on ARM64 architecture",
          "author": {
            "login": "shikhar413"
          },
          "bodyText": "Hello MOOSE team,\nI am a new MOOSE user and I am trying to build MOOSE on an ARM64 machine. I've had to rely on building most dependencies from source since many conda packages are built only on x86_64 machines. When compiling PETSC and libMesh by running ./scripts/update_and_rebuild_petsc.sh and ./scripts/update_and_rebuild_libmesh.sh, however, I run into the following messages:\nPETSC Compilation Error:\n\nlibMesh Compilation Error:\n\nThese errors occur while running make after configure has finished being called. I am running on ARM64 Ubuntu Server 20.04 and have installed MOOSE dependencies according to the instructions on https://mooseframework.inl.gov/getting_started/installation/manual_installation_gcc.html. I don't know how I can modify the PETSC and LibMesh install scripts to get around these issues, any guidance would be very much appreciated.",
          "url": "https://github.com/idaholab/moose/discussions/17416",
          "updatedAt": "2022-10-13T19:54:09Z",
          "publishedAt": "2021-03-23T15:56:47Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "aeslaughter"
                  },
                  "bodyText": "@dschwen You got MOOSE running on a Raspberry PI, correct? Do you have any insights?",
                  "url": "https://github.com/idaholab/moose/discussions/17416#discussioncomment-527792",
                  "updatedAt": "2023-07-31T17:51:54Z",
                  "publishedAt": "2021-03-25T03:15:44Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "shikhar413"
                  },
                  "bodyText": "The folks at Argonne MCS were able to lead me in the right direction wrt the PETSc installation. For some reason the Fortran compiler was unable to locate the shared libraries for mumps and manually setting the PETSc flag in scripts/update_and_rebuild_petsc.sh as FFLAGS=\"-fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument\" fixed the PETSc compilation issue.\nAdding the full commands to build MOOSE on ARM Ubuntu 20.04 for anyone else who might run into similar issues, happy to submit a PR to add these instructions to some documentation in the repo if there is interest as well. Might be good to also start testing builds on ARM architectures, especially to support installation on the new M1 Macs.\n#!/bin/sh\n\n# Install gcc, python3\nsudo apt install build-essential libssl-dev libffi-dev python3-dev manpages-dev\n\n# Install MPICH\nsudo apt install mpich\n\n# Install pip\nsudo apt install python3-pip\n\n# Install MOOSE dependencies\npip3 install coverage\npip3 install reportlab\npip3 install mako\npip3 install numpy\npip3 install scipy\npip3 install scikit-learn\nsudo apt install hdf5-tools\nsudo apt install hdf5-helpers\nsudo apt install python3-h5py\npip3 install scikit-image\npip3 install requests\nsudo apt install python3-vtk7\nsudo apt install python3-vtkplotter\npip3 install pyyaml\npip3 install matplotlib\npip3 install lxml\npip3 install pyflakes\npip3 install pandas\npip3 install mock\nsudo apt install python3-yaml\nsudo apt install python3-pyqt5\nsudo apt install swig\npip3 install --no-cache-dir pybtex livereload daemonlite pylint lxml pylatexenc anytree\n\n# Add locally installed binaries to path\necho \"export PATH=$HOME/.local/bin:$PATH\" >> ~/.bashrc\n\n# Set environment variables for MOOSE installation\necho \"export CC=mpicc\" >> ~/.bashrc\necho \"export CXX=mpicxx\" >> ~/.bashrc\necho \"export FC=mpif90\" >> ~/.bashrc\necho \"export F90=mpif90\" >> ~/.bashrc\n\n# Point default python environment to python3 (necessary for running MOOSE tests)\nsudo apt-get install python-is-python3\n\n# Install PETSC dependencies\nsudo apt install bison valgrind flex\n\n# Clone MOOSE repo\nmoose_dir=$HOME/projects\nmkdir -p $moose_dir\ncd $moose_dir\ngit clone https://github.com/idaholab/moose.git\ncd moose\ngit checkout master\n\n# Manually add flag for Fortran flags to PETSc installation script\nsed -i 's/hypre=1/hypre=1 --FFLAGS=\"-fPIC -Wall -ffree-line-length-0 -Wno-unused-dummy-argument\"/g' scripts/update_and_rebuild_petsc.sh\n\n# Compile PETSc\nunset PETSC_DIR PETSC_ARCH\n./scripts/update_and_rebuild_petsc.sh\ncd petsc\nmake PETSC_DIR=$moose_dir/moose/scripts/../petsc PETSC_ARCH=arch-moose check\n\n# Compile libMesh\ncd $moose_dir/moose\n./scripts/update_and_rebuild_libmesh.sh\n\n# Compile and test MOOSE\ncd $moose_dir/moose/test\nmake -j 4\n./run_tests -j 4",
                  "url": "https://github.com/idaholab/moose/discussions/17416#discussioncomment-532299",
                  "updatedAt": "2023-07-31T17:51:54Z",
                  "publishedAt": "2021-03-26T02:30:04Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}