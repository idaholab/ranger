{
  "discussions": {
    "pageInfo": {
      "hasNextPage": true,
      "endCursor": "Y3Vyc29yOnYyOpK5MjAyNS0wMi0xOFQxMTo1MDoxMi0wNzowMM4AeMtW"
    },
    "edges": [
      {
        "node": {
          "title": "Crystal Plasticity preconditioner and executioner",
          "author": {
            "login": "p-antonioni"
          },
          "bodyText": "Hi!\nI am performing some Crystal Plasticity simulations and I would have to run a simulation with  around some milions of dofs.\nEven for my test case, I saw that full Newton solver is faster than PJFNK (with Newton I have a quadratic convergence, with PJFNK superlinear). Maybe because I have not set it properly.\nDo you have some suggestions or experience with some solver/preconditioner to have good performances on such simulation? I mean for Newton and PJFNK\nThank you,\nPaolo\nThe set up I used for PJFNK:\n[Preconditioning]\n    [smp]\n    type = SMP\n    full = true\n    []\n[]\n\n[Executioner]\n  type = Transient\n  solve_type = 'PJFNK'\n  petsc_options_iname = '-pc_type -pc_asm_overlap -sub_pc_type -ksp_type -ksp_gmres_restart'\n  petsc_options_value = ' asm      2              lu            gmres     200'\n  nl_abs_tol = 1e-4\n ...\n[]\n\n\nNEWTON:\n[Preconditioning]\n  [smp]\n    type = SMP\n    full = true\n    petsc_options_iname = '-pc_type -pc_factor_mat_solver_package '\n    petsc_options_value = ' lu       mumps'\n  []\n[]\n\n[Executioner]\n  type = Transient\n  petsc_options_iname = '-pc_type -pc_asm_overlap -sub_pc_type -ksp_type -ksp_gmres_restart'\n  petsc_options_value = ' asm      2              lu            gmres     200'\n  solve_type = 'NEWTON'\n  use_pre_SMO_residual = false",
          "url": "https://github.com/idaholab/moose/discussions/29755",
          "updatedAt": "2025-02-21T15:45:41Z",
          "publishedAt": "2025-01-27T16:33:33Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@bwspenc",
                  "url": "https://github.com/idaholab/moose/discussions/29755#discussioncomment-11972290",
                  "updatedAt": "2025-01-27T16:36:19Z",
                  "publishedAt": "2025-01-27T16:36:18Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "p-antonioni"
                          },
                          "bodyText": "I made some tests. I saw that I am struggling to have quadratic convergence of the Full Newton method, the jacobian seems to be nice (I tested it following the procedure in the guide). I am using a sort of modified KalidindiUpdate in order to include HCP slip systems and isotropic hardening.\nCould it be to preconditioning? Do you have some suggestions?",
                          "url": "https://github.com/idaholab/moose/discussions/29755#discussioncomment-12054376",
                          "updatedAt": "2025-02-04T12:04:16Z",
                          "publishedAt": "2025-02-04T12:04:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "I made some tests. I saw that I am struggling to have quadratic convergence of the Full Newton method,\n\nWhat line_search are you using? It might be the default one failing. That is specified in the executioner.\nWe do expect quadratic convergence in the region of attraction when using Newton with a numerically correct Jacobian.\n\nCould it be to preconditioning? Do you have some suggestions?\n\nunlikely imo since you are using lu, equilvant to a direct solve on every linear iteration.\nHow many variables do you have?\nWhat preconditioning have you tried? Is the problem known to be elliptic or hyperbolic?",
                          "url": "https://github.com/idaholab/moose/discussions/29755#discussioncomment-12057026",
                          "updatedAt": "2025-02-04T16:22:11Z",
                          "publishedAt": "2025-02-04T16:22:11Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "p-antonioni"
                          },
                          "bodyText": "Hi @GiudGiud, thank you!\n\nWhat line_search are you using?\n\nI am using the basic one right now but is still very slow, i have around 20k Nonlinear dof and another 20k dof for the auxiliary system.\nHere is what I set in the simulation:\n[Executioner]\n  type = Transient\n  solve_type = 'NEWTON'\n  petsc_options_iname = '-pc_type -pc_hypre_type -pc_hypre_boomeramg_max_levels'\n  petsc_options_value = 'hypre boomeramg 10'\n  use_pre_SMO_residual = false\n  auto_preconditioning = true\n  line_search = BASIC\n\n  nl_abs_tol = 1e-4\n  #nl_rel_step_tol = 1e-2\n  dtmax = 0.5\n  #nl_rel_tol = 1e-2\n  dtmin = 1e-8\n  end_time = 10\n\n\nWhat preconditioning have you tried?\n\nI have tried asm and smp, I also tried to use the auto_preconditioning\n\nIs the problem known to be elliptic or hyperbolic?\n\nThe physics is mostly the same of the Kalidindi model (flow rule and hardening) with exception in the hardening i use some matrix operation to calculate the slip increment for each slip system mode. Should be parabolic/hyperbolic I guess.",
                          "url": "https://github.com/idaholab/moose/discussions/29755#discussioncomment-12058948",
                          "updatedAt": "2025-02-04T19:32:37Z",
                          "publishedAt": "2025-02-04T19:32:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "20k Nonlinear DOFs should be fast. You are right to solve it using LU (the direct solve) then, that will minimize the number of times you compute the residual / Jacobian, and it will minimize the number of material evaluations. Since you only use 20k dofs, I imagine the slow part is not the solver, but rather material properties?\nAre you using AD?\n\nwith exception in the hardening i use some matrix operation to calculate the slip increment for each slip system mode.\n\nAre your simulations significantly slower than the classic Kalindi model?",
                          "url": "https://github.com/idaholab/moose/discussions/29755#discussioncomment-12062545",
                          "updatedAt": "2025-02-05T04:33:05Z",
                          "publishedAt": "2025-02-05T04:33:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "p-antonioni"
                          },
                          "bodyText": "As far I understood, crystal plasticity does not support AD and at the time i did not use it.\nIf I restrict the model to simulate an FCC crystal with same slip systems and same parameters, it takes almost the same time (60.71s Kalidindi and 61.06s mine on this 20k dof mesh (20 micron cube) to simulate until 0.1s with a rate of 2.5e-4*t).\nNow since the simulation is bi-phasic (bcc (12 slip sys) - hcp (24 slip sys)), I expect to be slower but since i need to scale up I don't think that using the direct solver will be feasible.\nI do not know if I mess around something in the boundary conditions, i do not use any offset to define the nodesets.\n [Mesh]\n  [file]\n    type = FileMeshGenerator\n    file = 'micro.msh'\n  []\n\n  # Creating boundaries\n  [create_x0]\n    type = BoundingBoxNodeSetGenerator\n    input = file # input mesh file is the file we loaded for mesh --> n10-id1.msh\n    new_boundary = x0\n    # top_right and bottom_left are vertices of the bounding box\n    top_right = '0 0.021 0.021' # box 1,1,1\n    bottom_left = '0 0 0'\n  []\n  [create_y0]\n    type = BoundingBoxNodeSetGenerator\n    # Input mesh == the one at the previous step --> create_x0 beacuse we are adding boundaries and BoundingBoxNodeSetGereator modifies directly the mesh\n    input = create_x1\n    new_boundary = y0\n    # top_right and bottom_left are vertices of the bounding box\n    top_right = '0 0 0.02' #box 1,1,1\n    bottom_left = '0.02 0 0'\n  []\n  [create_z0]\n    type = BoundingBoxNodeSetGenerator\n    input = create_y1\n    new_boundary = z0\n    top_right = '0 0.02 0'\n    bottom_left = '0.02 0 0'\n  []\n  [create_z1]\n    type = BoundingBoxNodeSetGenerator\n    input = create_z0\n    new_boundary = z1\n    top_right = '0 0.02 0.02'\n    bottom_left = '0.02 0 0.02'\n  []\n[]\n\n[BCs]\n  [lock_z0_surface]\n    type = DirichletBC\n    variable = uz\n    boundary = z0\n    value = 0\n  []\n  [tdisp]\n    type = FunctionDirichletBC\n    variable = uz\n    boundary = z1\n    function = 2.5e-4*t\n  []\n  [symmy]\n    type = DirichletBC\n    variable = uy\n    boundary = y0\n    value = 0\n  []\n  [symmx]\n    type = DirichletBC\n    variable = ux\n    boundary = x0\n    value = 0\n  []\n\nIf it can help I can show the modifications I made for calculating hardening trough the interaction matrix.",
                          "url": "https://github.com/idaholab/moose/discussions/29755#discussioncomment-12070613",
                          "updatedAt": "2025-02-05T14:52:40Z",
                          "publishedAt": "2025-02-05T14:52:09Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@dewenyushu @jiangwen84 do you have any advice on performance and scaling for CP?",
                          "url": "https://github.com/idaholab/moose/discussions/29755#discussioncomment-12071420",
                          "updatedAt": "2025-02-05T15:55:12Z",
                          "publishedAt": "2025-02-05T15:55:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "sapitts"
                          },
                          "bodyText": "You are correct, the current crystal plasticity classes do not use AD.\nYou might consider running the HCP specific class, CrystalPlasticityHCPDislocationSlipBeyerleinUpdate for comparison with your modified Kalidindi model",
                          "url": "https://github.com/idaholab/moose/discussions/29755#discussioncomment-12074487",
                          "updatedAt": "2025-02-05T20:49:45Z",
                          "publishedAt": "2025-02-05T20:49:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "sapitts"
                          },
                          "bodyText": "The crystal plasticity models in MOOSE are unfortunately slow. In addition to the solver tolerances there are constitutive model tolerances one can adjust. See the discussion here for more information: #28534",
                          "url": "https://github.com/idaholab/moose/discussions/29755#discussioncomment-12074544",
                          "updatedAt": "2025-02-05T20:56:52Z",
                          "publishedAt": "2025-02-05T20:56:51Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "p-antonioni"
                          },
                          "bodyText": "Hi @sapitts,\nThanks for answering. I was looking at the CrystalPlasticityHCPDislocationSlipBayerleinUpdate, with the modifications I did, they look quite similar in some parts. Could I ask you why the equivalent slip increment is calculated in this way? It is like assuming that the slip system behavior does not depend on the current hardening state ?\n  for (const auto i : make_range(_number_slip_systems))\n  {\n    if (MooseUtils::absoluteFuzzyEqual(_tau[_qp][i], 0.0))\n      dslip_dtau[i] = 0.0;\n    else\n      dslip_dtau[i] = _slip_increment[_qp][i] /\n                      (_rate_sensitivity_exponent * std::abs(_tau[_qp][i])) * _substep_dt;\n  }\n}\n\nI would expect the derivative with respect to tau of the slip increment, something like this:\n  for (const auto i : make_range(_number_slip_systems))\n  {\n    if (MooseUtils::absoluteFuzzyEqual(_tau[_qp][i], 0.0))\n      dslip_dtau[i] = 0.0;\n    else\n      dslip_dtau[i] = _ao / _xm *\n                      std::pow(std::abs(_tau[_qp][i] / _slip_resistance[_qp][i]), 1.0 / _xm - 1.0) /\n                      _slip_resistance[_qp][i];\n  }\n\nIn general, there are some preconditioning/executioner settings that you found helpful when running on many dofs?",
                          "url": "https://github.com/idaholab/moose/discussions/29755#discussioncomment-12082039",
                          "updatedAt": "2025-02-06T13:35:08Z",
                          "publishedAt": "2025-02-06T13:35:07Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "sapitts"
                          },
                          "bodyText": "Hi @p-antonioni,\nIn case you haven't seen this, the HCP version of the derivative simply uses the definition of the slip increment to reduce the computational load. I've written out the intermediate steps in the image below, starting with the derivative as you copied from the Kalidindi model and ending with the HCP model, excluding the time step at the end\n\nI've had the most luck with using the SMP preconditioner and asm petsc option as in the regression tests. I've heard others have had better luck with using hyper boomerang. Generally I find I get more performance improvements by changing the constitutive model tolerances, since most of the compute time for my runs is in the material class.\nHope this helps,",
                          "url": "https://github.com/idaholab/moose/discussions/29755#discussioncomment-12240358",
                          "updatedAt": "2025-02-18T18:08:58Z",
                          "publishedAt": "2025-02-18T18:07:37Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Libmesh directory empty in my MOOSE download",
          "author": {
            "login": "Am-Americium"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n I have consulted the Posting Guidelines.\n I have searched the Discussions Forum and MOOSE Framework Troubleshooting and have not found what I was looking for\n Q&A Getting Started is the most appropriate category for my question (trouble installing, beginner user, ...)\n\nIssue or question about MOOSE\nI have followed the steps for downloading MOOSE on the INL HPC Cluster.\nAfter attempting to run make -j in the phase field directory, I get the error attached. After looking in my moose/libmesh directory, I notice it's empty.\nThis may be a silly issue on my part, but did I miss a step upon downloading MOOSE that would update and fill that directory? If so, what is that step?\n(Optional) code in question / simulation log / errors\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/libtool: No such file or directory\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/libtool: No such file or directory\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\nWARNING: Failed to initialize PreMake for version checking; this may be ignored but may suggest an environment issue.\nTraceback (most recent call last):\n  File \"/scratch/longaide2/projects/moose/framework/../scripts/premake.py\", line 20, in __init__\n    from versioner import Versioner\n  File \"/ibmstor/scratch/longaide2/projects/moose/scripts/versioner.py\", line 25, in <module>\n    import jinja2\nModuleNotFoundError: No module named 'jinja2'\n\nTraceback (most recent call last):\n  File \"/scratch/longaide2/projects/moose/framework/../scripts/premake.py\", line 261, in <module>\n    PreMake().check()\n  File \"/scratch/longaide2/projects/moose/framework/../scripts/premake.py\", line 27, in __init__\n    self.versioner_meta = Versioner().version_meta()\nUnboundLocalError: local variable 'Versioner' referenced before assignment\nmake: [/scratch/longaide2/projects/moose/framework/moose.mk:399: prebuild] Error 1 (ignored)\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\nCompiling C++ (in opt mode) /scratch/longaide2/projects/moose/framework/build/unity_src/convergence_Unity.C...\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\nCompiling C++ (in opt mode) /scratch/longaide2/projects/moose/framework/build/unity_src/hdgkernels_Unity.C...\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/libtool: No such file or directory\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\nmake: *** [/scratch/longaide2/projects/moose/framework/build.mk:151: /scratch/longaide2/projects/moose/framework/build/unity_src/convergence_Unity..opt.lo] Error 127\nmake: *** Waiting for unfinished jobs....\n/bin/sh: /scratch/longaide2/projects/moose/libmesh/installed/libtool: No such file or directory\nmake: *** [/scratch/longaide2/projects/moose/framework/build.mk:151: /scratch/longaide2/projects/moose/framework/build/unity_src/hd\ngkernels_Unity..opt.lo] Error 127\nEncountering Errors? Please include diagnostic output\nNo response",
          "url": "https://github.com/idaholab/moose/discussions/29877",
          "updatedAt": "2025-02-21T14:03:19Z",
          "publishedAt": "2025-02-13T17:49:41Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nPlease follow the instructions here\nhttps://mooseframework.inl.gov/getting_started/installation/hpc_install_moose.html\non HPC you use a script from the instructionrs to get the libmesh and petsc folders to populate",
                  "url": "https://github.com/idaholab/moose/discussions/29877#discussioncomment-12191445",
                  "updatedAt": "2025-02-13T18:24:42Z",
                  "publishedAt": "2025-02-13T18:24:41Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "jinja2 will need to be installed separately. using conda or pip for example",
                          "url": "https://github.com/idaholab/moose/discussions/29877#discussioncomment-12191453",
                          "updatedAt": "2025-02-13T18:25:06Z",
                          "publishedAt": "2025-02-13T18:25:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Am-Americium"
                          },
                          "bodyText": "When I run the first PETc step, it exits with the following error:\nThe environmental variable PETSC_DIR=/opt/petsc MUST be the current directory\n/scratch/longaide2/projects/moose/petsc\n\nFile \"/scratch/longaide2/projects/moose/petsc/config/configure.py\", line 461, in petsc_configure\nframework.configure(out = sys.stdout)\nFile \"/scratch/longaide2/projects/moose/petsc/config/BuildSystem/config/framework.py\", line 1460, in configure\nself.processChildren()\nFile \"/scratch/longaide2/projects/moose/petsc/config/BuildSystem/config/framework.py\", line 1448, in processChildren\nself.serialEvaluation(self.childGraph)\nFile \"/scratch/longaide2/projects/moose/petsc/config/BuildSystem/config/framework.py\", line 1423, in serialEvaluation\nchild.configure()\nFile \"/scratch/longaide2/projects/moose/petsc/config/PETSc/options/petscdir.py\", line 76, in configure\nself.executeTest(self.configureDirectories)\nFile \"/scratch/longaide2/projects/moose/petsc/config/BuildSystem/config/base.py\", line 138, in executeTest\nret = test(*args,**kargs)\n^^^^^^^^^^^^^^^^^^^\nFile \"/scratch/longaide2/projects/moose/petsc/config/PETSc/options/petscdir.py\", line 46, in configureDirectories\nraise RuntimeError('{0} PETSC_DIR={1} MUST be the current directory {2}'.format(msg1, self.dir, os.getcwd()))",
                          "url": "https://github.com/idaholab/moose/discussions/29877#discussioncomment-12191686",
                          "updatedAt": "2025-02-13T18:49:43Z",
                          "publishedAt": "2025-02-13T18:49:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "The environmental variable PETSC_DIR=/opt/petsc MUST be the current directory\n\nIs your environment setting this? Do you have petsc installed in a module for example?",
                          "url": "https://github.com/idaholab/moose/discussions/29877#discussioncomment-12192083",
                          "updatedAt": "2025-02-13T19:40:18Z",
                          "publishedAt": "2025-02-13T19:40:18Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Am-Americium"
                          },
                          "bodyText": "This is returned in the sawtooth HPC environment",
                          "url": "https://github.com/idaholab/moose/discussions/29877#discussioncomment-12192139",
                          "updatedAt": "2025-02-13T19:47:59Z",
                          "publishedAt": "2025-02-13T19:47:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "if you are on sawtooth use these instructions\nhttps://mooseframework.inl.gov/getting_started/installation/inl_hpc_install_moose.html",
                          "url": "https://github.com/idaholab/moose/discussions/29877#discussioncomment-12192152",
                          "updatedAt": "2025-02-13T19:49:07Z",
                          "publishedAt": "2025-02-13T19:49:05Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Am-Americium"
                          },
                          "bodyText": "I used those instructions and am still receiving this error. The ./run_tests returned with 0 failed tests as well.",
                          "url": "https://github.com/idaholab/moose/discussions/29877#discussioncomment-12192219",
                          "updatedAt": "2025-02-13T19:57:08Z",
                          "publishedAt": "2025-02-13T19:57:08Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "you can address this error\nexport PETSC_DIR=\"path to your petsc folder under moose\"",
                          "url": "https://github.com/idaholab/moose/discussions/29877#discussioncomment-12192269",
                          "updatedAt": "2025-02-13T20:02:50Z",
                          "publishedAt": "2025-02-13T20:02:50Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Am-Americium"
                          },
                          "bodyText": "should I do this for each directory? like libmesh too?",
                          "url": "https://github.com/idaholab/moose/discussions/29877#discussioncomment-12192284",
                          "updatedAt": "2025-02-13T20:04:26Z",
                          "publishedAt": "2025-02-13T20:04:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "Am-Americium"
                          },
                          "bodyText": "after running the above command, the update_and_rebuild petc command and am experiencing the same error\nsorry, this is a weird issue to have. Would you maybe suggest uninstalling moose and redownloading?",
                          "url": "https://github.com/idaholab/moose/discussions/29877#discussioncomment-12192718",
                          "updatedAt": "2025-02-13T20:53:19Z",
                          "publishedAt": "2025-02-13T20:53:19Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "If you are following the INL HPC instructions, there is no need to build PETSc or libMesh. You will be operating in a container that has these already built.\nWhat you probably need to do, is clean out the previous failed build attempt.\ncd /scratch/longaide2/projects/moose\ngit clean -xfd; git submodule foreach --recursive git clean -xfd\nNext, following the instructions here: https://mooseframework.inl.gov/getting_started/installation/inl_hpc_install_moose.html\nyou will end up doing:\nmodule load use.moose versioner\ncd /scratch/longaide2/projects/moose\nscripts/versioner.py moose-dev\nThis return a hash, which basically forms the naming suffix of the module you need to load. As an example:\nmodule load use.moose moose-dev-openmpi/5e9be02\n                                        ^^^^^^^ being what was returned in the previous step\nOnce this final module is loading, you should be able to run make.",
                          "url": "https://github.com/idaholab/moose/discussions/29877#discussioncomment-12201436",
                          "updatedAt": "2025-02-14T14:23:31Z",
                          "publishedAt": "2025-02-14T14:23:30Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "MaterialADConverter recognizing RankTwoTensor as double",
          "author": {
            "login": "richmondodufisan"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n Q&A General is the most appropriate section for my question\n I have consulted the posting Guidelines on the Discussions front page\n I have searched the Discussions forum and my question has not been asked before\n I have searched the MOOSE website and the documentation does not answer my question\n I have formatted my post following the posting guidelines (screenshots as a last resort, triple back quotes around pasted text)\n\nQuestion\nI am trying to convert a non-AD Material Property to an AD one. Specifically, the deformation gradient calculated by ComputeLagrangianStrain. In a simple 1D problem (more details here: #29895), I am attempting to use non-AD quantities computed by ComputeLagrangianStrain to develop AD Materials that will be used by ADStressDivergenceTensors, i.e the old system.\nThe block equation for my Jacobian matrix should look like this:\n\nHowever, with ../../purple-opt -i incompressible_1D_test.i  -snes_view -ksp_view -vec_view -ksp_view_mat to print out the full Jacobian, it looks something like this:\nrow 0: (0, 1.)  (21, 0.)  (22, 0.)\nrow 1: (1, 0.)  (21, 0.166667)  (22, 2.22045e-16)  (23, -0.166667)\nrow 2: (2, 0.)  (21, 0.666667)  (22, -0.666667)\nrow 3: (3, 0.)  (22, 0.166667)  (23, 0.)  (24, -0.166667)\nrow 4: (4, 0.)  (22, 0.666667)  (23, -0.666667)\nrow 5: (5, 0.)  (23, 0.166667)  (24, 0.)  (25, -0.166667)\nrow 6: (6, 0.)  (23, 0.666667)  (24, -0.666667)\nrow 7: (7, 0.)  (24, 0.166667)  (25, -1.11022e-16)  (26, -0.166667)\nrow 8: (8, 0.)  (24, 0.666667)  (25, -0.666667)\nrow 9: (9, 0.)  (25, 0.166667)  (26, 2.22045e-16)  (27, -0.166667)\nrow 10: (10, 0.)  (25, 0.666667)  (26, -0.666667)\nrow 11: (11, 0.)  (26, 0.166667)  (27, 0.)  (28, -0.166667)\nrow 12: (12, 0.)  (26, 0.666667)  (27, -0.666667)\nrow 13: (13, 0.)  (27, 0.166667)  (28, -2.22045e-16)  (29, -0.166667)\nrow 14: (14, 0.)  (27, 0.666667)  (28, -0.666667)\nrow 15: (15, 0.)  (28, 0.166667)  (29, 2.22045e-16)  (30, -0.166667)\nrow 16: (16, 0.)  (28, 0.666667)  (29, -0.666667)\nrow 17: (17, 0.)  (29, 0.166667)  (30, -0.0462963)  (31, -0.185185)\nrow 18: (18, 0.)  (29, 0.666667)  (30, -0.666667)\nrow 19: (19, 1.)  (30, 0.)  (31, 0.)\nrow 20: (20, 0.)  (30, 0.793651)  (31, -0.396825)\nrow 21: (21, 0.)\nrow 22: (22, 0.)\nrow 23: (23, 0.)\nrow 24: (24, 0.)\nrow 25: (25, 0.)\nrow 26: (26, 0.)\nrow 27: (27, 0.)\nrow 28: (28, 0.)\nrow 29: (29, 0.)\nrow 30: (30, 0.)\nrow 31: (31, 1.)\n\nWhich tells me that it is unable to find the Jacobian terms dependent on the displacement/deformation gradient, which I believe is due to ComputeLagrangianStrain being non-AD. I was wondering if MaterialADConverter could get around this, so I did:\n  [ad_convert]\n    type = MaterialADConverter\n\treg_props_in = 'deformation_gradient'\n\tad_props_out = 'ad_deformation_gradient'\n  []\n\nBut I got the error:\n*** ERROR ***\n/home/richmond98/projects/purple/Cornea_Hyperelastic_NeoHookean/stress_strain_tests/incompressible_1D_test_MaterialAD_issue.i:50.3:\nThe following error occurred in the MaterialBase 'ad_convert' of type MaterialADConverter.\n\nThe requested non-AD material property 'deformation_gradient' of type 'double'\nis already retrieved or declared as a non-AD property of type 'RankTwoTensorTempl<double>'.\n\nWhich is strange, since deformation_gradient is a Tensor. Here is the full input file:\nhttps://github.com/richmondodufisan/purple/blob/main/Cornea_Hyperelastic_NeoHookean/stress_strain_tests/incompressible_1D_test_MaterialAD_issue.i\nIf I am able to get this conversion to work then it'll enable me to use quantities calculated with the new and improved ComputeLagrangianStrain with Dynamics/the old ADStressDivergenceTensors",
          "url": "https://github.com/idaholab/moose/discussions/29923",
          "updatedAt": "2025-02-20T20:56:48Z",
          "publishedAt": "2025-02-20T08:16:26Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThat material is for Real/ADReal properties. Not for tensors.\nYou will need to add support to that material for tensors if you want to use it",
                  "url": "https://github.com/idaholab/moose/discussions/29923#discussioncomment-12268101",
                  "updatedAt": "2025-02-20T20:25:22Z",
                  "publishedAt": "2025-02-20T20:25:21Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "Looking through the source code to implement it, I found that it already has been implemented as RankTwoTensorMaterialADConverter.\nIt didn't work though, my Jacobian is still  the same as before, i.e dR_u/du and dR_p/du (both of which depend on the deformation gradient) are zero:\nrow 0: (0, 1.)  (21, 0.)  (22, 0.)\nrow 1: (1, 0.)  (21, 0.166667)  (22, 2.22045e-16)  (23, -0.166667)\nrow 2: (2, 0.)  (21, 0.666667)  (22, -0.666667)\nrow 3: (3, 0.)  (22, 0.166667)  (23, 0.)  (24, -0.166667)\nrow 4: (4, 0.)  (22, 0.666667)  (23, -0.666667)\nrow 5: (5, 0.)  (23, 0.166667)  (24, 0.)  (25, -0.166667)\nrow 6: (6, 0.)  (23, 0.666667)  (24, -0.666667)\nrow 7: (7, 0.)  (24, 0.166667)  (25, -1.11022e-16)  (26, -0.166667)\nrow 8: (8, 0.)  (24, 0.666667)  (25, -0.666667)\nrow 9: (9, 0.)  (25, 0.166667)  (26, 2.22045e-16)  (27, -0.166667)\nrow 10: (10, 0.)  (25, 0.666667)  (26, -0.666667)\nrow 11: (11, 0.)  (26, 0.166667)  (27, 0.)  (28, -0.166667)\nrow 12: (12, 0.)  (26, 0.666667)  (27, -0.666667)\nrow 13: (13, 0.)  (27, 0.166667)  (28, -2.22045e-16)  (29, -0.166667)\nrow 14: (14, 0.)  (27, 0.666667)  (28, -0.666667)\nrow 15: (15, 0.)  (28, 0.166667)  (29, 2.22045e-16)  (30, -0.166667)\nrow 16: (16, 0.)  (28, 0.666667)  (29, -0.666667)\nrow 17: (17, 0.)  (29, 0.166667)  (30, -0.0462963)  (31, -0.185185)\nrow 18: (18, 0.)  (29, 0.666667)  (30, -0.666667)\nrow 19: (19, 1.)  (30, 0.)  (31, 0.)\nrow 20: (20, 0.)  (30, 0.793651)  (31, -0.396825)\nrow 21: (21, 0.)\nrow 22: (22, 0.)\nrow 23: (23, 0.)\nrow 24: (24, 0.)\nrow 25: (25, 0.)\nrow 26: (26, 0.)\nrow 27: (27, 0.)\nrow 28: (28, 0.)\nrow 29: (29, 0.)\nrow 30: (30, 0.)\nrow 31: (31, 1.)\n\nBut the exact same problem with the new system (dR_u/du (i.e dR_u/dF for large deformation) is implemented exactly, dR_p/dp is supposed to be zero, but other entries dR_u/dp and dR_p/du are not computed due to the lack of AD functionality) gives this Jacobian:\nrow 0: (0, 1.)  (1, 0.)  (2, 0.)\nrow 1: (0, 4.44444e+06)  (1, 6.22222e+07)  (2, -3.55556e+07)  (3, 4.44444e+06)  (4, -3.55556e+07)\nrow 2: (0, -3.55556e+07)  (1, -3.55556e+07)  (2, 7.11111e+07)\nrow 3: (1, 4.44444e+06)  (3, 6.22222e+07)  (4, -3.55556e+07)  (5, 4.44444e+06)  (6, -3.55556e+07)\nrow 4: (1, -3.55556e+07)  (3, -3.55556e+07)  (4, 7.11111e+07)\nrow 5: (3, 4.44444e+06)  (5, 6.22222e+07)  (6, -3.55556e+07)  (7, 4.44444e+06)  (8, -3.55556e+07)\nrow 6: (3, -3.55556e+07)  (5, -3.55556e+07)  (6, 7.11111e+07)\nrow 7: (5, 4.44444e+06)  (7, 6.22222e+07)  (8, -3.55556e+07)  (9, 4.44444e+06)  (10, -3.55556e+07)\nrow 8: (5, -3.55556e+07)  (7, -3.55556e+07)  (8, 7.11111e+07)\nrow 9: (7, 4.44444e+06)  (9, 6.22222e+07)  (10, -3.55556e+07)  (11, 4.44444e+06)  (12, -3.55556e+07)\nrow 10: (7, -3.55556e+07)  (9, -3.55556e+07)  (10, 7.11111e+07)\nrow 11: (9, 4.44444e+06)  (11, 6.22222e+07)  (12, -3.55556e+07)  (13, 4.44444e+06)  (14, -3.55556e+07)\nrow 12: (9, -3.55556e+07)  (11, -3.55556e+07)  (12, 7.11111e+07)\nrow 13: (11, 4.44444e+06)  (13, 6.22222e+07)  (14, -3.55556e+07)  (15, 4.44444e+06)  (16, -3.55556e+07)\nrow 14: (11, -3.55556e+07)  (13, -3.55556e+07)  (14, 7.11111e+07)\nrow 15: (13, 4.44444e+06)  (15, 6.22222e+07)  (16, -3.55556e+07)  (17, 4.44444e+06)  (18, -3.55556e+07)\nrow 16: (13, -3.55556e+07)  (15, -3.55556e+07)  (16, 7.11111e+07)\nrow 17: (15, 4.44444e+06)  (17, 6.71626e+07)  (18, -3.55556e+07)  (19, 5.01226e+06)  (20, -4.10637e+07)\nrow 18: (15, -3.55556e+07)  (17, -3.55556e+07)  (18, 7.11111e+07)\nrow 19: (17, 0.)  (19, 1.)  (20, 0.)\nrow 20: (17, -4.10637e+07)  (19, -2.01276e+07)  (20, 6.11913e+07)\nrow 21: (21, 0.)\nrow 22: (22, 0.)\nrow 23: (23, 0.)\nrow 24: (24, 0.)\nrow 25: (25, 0.)\nrow 26: (26, 0.)\nrow 27: (27, 0.)\nrow 28: (28, 0.)\nrow 29: (29, 0.)\nrow 30: (30, 0.)\nrow 31: (31, 1.)\n\nconfirming for sure that the issue with my model is the lack of AD functionality in the new system and/or the inability to create an AD version of the deformation gradient to use with the old system.",
                          "url": "https://github.com/idaholab/moose/discussions/29923#discussioncomment-12268380",
                          "updatedAt": "2025-02-20T20:59:56Z",
                          "publishedAt": "2025-02-20T20:56:03Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "I'll mark this as answered since the question was on AD conversion. RankTwoTensorMaterialADConverter does that for RankTwoTensors.",
                          "url": "https://github.com/idaholab/moose/discussions/29923#discussioncomment-12268384",
                          "updatedAt": "2025-02-20T20:56:46Z",
                          "publishedAt": "2025-02-20T20:56:45Z",
                          "isAnswer": true
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Install MOOSE in HPC cluster",
          "author": {
            "login": "sidharthsarmah"
          },
          "bodyText": "Hi @GiudGiud , I am trying to run jobs in HPC cluster using MOOSE, I followed these steps for condo https://mooseframework.inl.gov/getting_started/installation/conda.html but encountered some error to install MOOSE in it using https://mooseframework.inl.gov/getting_started/installation/hpc_install_moose.html, please suggest.\n(moose) [sidharth@narval2 scripts]$ export MOOSE_JOBS=6 METHODS=opt\n(moose) [sidharth@narval2 scripts]$ ./update_and_rebuild_petsc.sh\n/home/sidharth/projects/def-jsong/sidharth/projects/moose/scripts\nSubmodule 'petsc' (https://gitlab.com/petsc/petsc.git) registered for path 'petsc'\nCloning into '/lustre06/project/6002274/sidharth/projects/moose/petsc'...\nSubmodule path 'petsc': checked out '1580af9fcba34756b20ac647dbc5659db3fdd05b'\nINFO: Checking for HDF5...\nINFO: HDF5 installation location was set using HDF5_DIR=/home/sidharth/miniforge/envs/moose\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nThe version of PETSc you are using is out-of-date, we recommend updating to the new release\n Available Version: 3.20.4   Installed Version: 3.20.3\nhttps://petsc.org/release/download/\n+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n=============================================================================================\n                         Configuring PETSc to compile on your system\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Found environment variable: CC=mpicc. Ignoring it! Use \"./configure CC=$CC\" if you really\n  want to use this value\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Found environment variable: CXX=mpicxx. Ignoring it! Use \"./configure CXX=$CXX\" if you\n  really want to use this value\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Found environment variable: FC=mpif90. Ignoring it! Use \"./configure FC=$FC\" if you\n  really want to use this value\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Found environment variable: F77=mpif77. Ignoring it! Use \"./configure F77=$F77\" if you\n  really want to use this value\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Found environment variable: F90=mpif90. Ignoring it! Use \"./configure F90=$F90\" if you\n  really want to use this value\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Found environment variable: CFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC\n  -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem\n  /home/sidharth/miniforge/envs/moose/include. Ignoring it! Use \"./configure\n  CFLAGS=$CFLAGS\" if you really want to use this value\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Found environment variable: CXXFLAGS=-fvisibility-inlines-hidden  -fmessage-length=0\n  -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2\n  -ffunction-sections -pipe -isystem /home/sidharth/miniforge/envs/moose/include\n  -std=c++17. Ignoring it! Use \"./configure CXXFLAGS=$CXXFLAGS\" if you really want to use\n  this value\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Found environment variable: FFLAGS=-march=nocona -mtune=haswell -ftree-vectorize -fPIC\n  -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem\n  /home/sidharth/miniforge/envs/moose/include. Ignoring it! Use \"./configure\n  FFLAGS=$FFLAGS\" if you really want to use this value\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Found environment variable:\n  CPP=/home/sidharth/miniforge/envs/moose/bin/x86_64-conda-linux-gnu-cpp. Ignoring it! Use\n  \"./configure CPP=$CPP\" if you really want to use this value\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Found environment variable: CPPFLAGS=-DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem\n  /home/sidharth/miniforge/envs/moose/include. Ignoring it! Use \"./configure\n  CPPFLAGS=$CPPFLAGS\" if you really want to use this value\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Found environment variable: LDFLAGS=-Wl,-O2 -Wl,--sort-common -Wl,--as-needed\n  -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections\n  -Wl,--allow-shlib-undefined -Wl,-rpath,/home/sidharth/miniforge/envs/moose/lib\n  -Wl,-rpath-link,/home/sidharth/miniforge/envs/moose/lib\n  -L/home/sidharth/miniforge/envs/moose/lib. Ignoring it! Use \"./configure\n  LDFLAGS=$LDFLAGS\" if you really want to use this value\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Found environment variable:\n  AR=/home/sidharth/miniforge/envs/moose/bin/x86_64-conda-linux-gnu-ar. Ignoring it! Use\n  \"./configure AR=$AR\" if you really want to use this value\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Found environment variable:\n  RANLIB=/home/sidharth/miniforge/envs/moose/bin/x86_64-conda-linux-gnu-ranlib. Ignoring\n  it! Use \"./configure RANLIB=$RANLIB\" if you really want to use this value\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Using default C optimization flags \"-g -O\". You might consider manually setting optimal\n  optimization flags for your system with COPTFLAGS=\"optimization flags\" see\n  config/examples/arch-*-opt.py for examples\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Using default Cxx optimization flags \"-g -O\". You might consider manually setting optimal\n  optimization flags for your system with CXXOPTFLAGS=\"optimization flags\" see\n  config/examples/arch-*-opt.py for examples\n=============================================================================================\n=============================================================================================\n                                     ***** WARNING *****\n  Using default FC optimization flags \"-g -O\". You might consider manually setting optimal\n  optimization flags for your system with FOPTFLAGS=\"optimization flags\" see\n  config/examples/arch-*-opt.py for examples\n=============================================================================================\nTESTING: configureMPI2 from config.packages.MPI(config/BuildSystem/config/packages/MPI.py:396)\n*********************************************************************************************\n           UNABLE to CONFIGURE with GIVEN OPTIONS (see configure.log for details):\n---------------------------------------------------------------------------------------------\n  PETSc requires some of the MPI-2.0 (1997), MPI-2.1 (2008) functions - they are not\n  available with the specified MPI library\n*********************************************************************************************\nThere was an error. Exiting...\n(moose) [sidharth@narval2 scripts]$",
          "url": "https://github.com/idaholab/moose/discussions/26787",
          "updatedAt": "2025-02-20T17:55:39Z",
          "publishedAt": "2024-02-10T05:22:12Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "your mpi seems too old?\nwhat does the diagnostics script in moose/scripts return?",
                  "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8425417",
                  "updatedAt": "2024-02-10T05:45:04Z",
                  "publishedAt": "2024-02-10T05:45:04Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "sidharthsarmah"
                          },
                          "bodyText": "HI, FYI",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8425436",
                          "updatedAt": "2024-02-11T16:33:36Z",
                          "publishedAt": "2024-02-10T05:51:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "COMPILER icc:\nicc (ICC) 19.1.1.217 20200306\nCopyright (C) 1985-2020 Intel Corporation.  All rights reserved.\n\nintel compilers are not supported. You ll need to follow the installation page more closely",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8425441",
                          "updatedAt": "2024-02-10T05:53:25Z",
                          "publishedAt": "2024-02-10T05:53:25Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "sidharthsarmah"
                          },
                          "bodyText": "Hi @GiudGiud , any suggestions? do I need to change the cluster?",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8425479",
                          "updatedAt": "2024-02-10T06:03:17Z",
                          "publishedAt": "2024-02-10T06:03:16Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "you ll need to find a different compiler. Maybe there are modules on your cluster? what does module avail return?",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8425497",
                          "updatedAt": "2024-02-10T06:07:42Z",
                          "publishedAt": "2024-02-10T06:07:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "sidharthsarmah"
                          },
                          "bodyText": "FYI",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8425509",
                          "updatedAt": "2024-02-11T16:34:17Z",
                          "publishedAt": "2024-02-10T06:10:45Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "well first unload intel/2020.1.217\nbut it s not enough. You need a mpi module that uses gcc and I dont see one in the list of modules.\nmaybe ask your cluster administators for one?",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8425519",
                          "updatedAt": "2024-02-10T06:17:56Z",
                          "publishedAt": "2024-02-10T06:17:55Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "sidharthsarmah"
                          },
                          "bodyText": "Hi @GiudGiud , I unload using module unload intel/2020.1.217 and will get back once I ask the administrators for the installations.",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8425571",
                          "updatedAt": "2024-02-10T06:37:12Z",
                          "publishedAt": "2024-02-10T06:37:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "sidharthsarmah"
                          },
                          "bodyText": "Hi @GiudGiud, they suggest that I use std end:\nhttps://docs.alliancecan.ca/wiki/Standard_software_environments\nThese are the lines which I used along with the outcome:\n1.\n[name@server ~]$ module load StdEnv/2023\n\n\n\n\nexport MOOSE_JOBS=6 METHODS=opt\n./update_and_rebuild_petsc.sh\n...\n====================================\n/cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/bin/gmake --no-print-directory -f makefile PETSC_ARCH=arch-moose PETSC_DIR=/home/sidharth/projects/def-jsong/sidharth/projects/moose/scripts/../petsc SLEPC_DIR=/lustre06/project/6002274/sidharth/projects/moose/petsc/arch-moose/externalpackages/git.slepc install-builtafterslepc\n/cvmfs/soft.computecanada.ca/gentoo/2023/x86-64-v3/usr/bin/gmake --no-print-directory -f makefile PETSC_ARCH=arch-moose PETSC_DIR=/home/sidharth/projects/def-jsong/sidharth/projects/moose/scripts/../petsc SLEPC_DIR=/lustre06/project/6002274/sidharth/projects/moose/petsc/arch-moose/externalpackages/git.slepc slepc4py-install\ngmake[6]: Nothing to be done for 'slepc4py-install'.\ngmake[2]: Leaving directory '/lustre06/project/6002274/sidharth/projects/moose/petsc'\n=========================================\nNow to check if the libraries are working do:\nmake PETSC_DIR=/home/sidharth/projects/def-jsong/sidharth/projects/moose/scripts/../petsc PETSC_ARCH=arch-moose check\n=========================================\n\n\n\n\n./update_and_rebuild_libmesh.sh\n...\n---------------------------------------------\n-- Done configuring core library features ---\n---------------------------------------------\n---------------------------------------------\n----- Configuring for optional packages -----\n---------------------------------------------\nchecking for built-in XDR support... no\nchecking for XDR support in /usr/include/tirpc... no\nconfigure: error: *** XDR was not found, but --enable-xdr-required was specified.\nRunning make -j 6...\nmake: *** No targets specified and no makefile found.  Stop.\n\n\n\n\n./update_and_rebuild_wasp.sh\n...\n-- Installing: /home/sidharth/projects/def-jsong/sidharth/projects/moose/scripts/../framework/contrib/wasp/install/include/wasplsp/ThreadConnection.h\n-- Installing: /home/sidharth/projects/def-jsong/sidharth/projects/moose/scripts/../framework/contrib/wasp/install/include/wasplsp/WaspServer.h\n-- Installing: /home/sidharth/projects/def-jsong/sidharth/projects/moose/scripts/../framework/contrib/wasp/install/include/wasplsp/WaspServer.i.h\n\n\n\n\nConda activate moose\nmake -j 6\n...\nmake: *** [/home/sidharth/projects/def-jsong/sidharth/projects/moose/framework/build.mk:150: /home/sidharth/projects/def-jsong/sidharth/projects/moose/framework/build/unity_src/transfers_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1\nmake: *** [/home/sidharth/projects/def-jsong/sidharth/projects/moose/framework/build.mk:150: /home/sidharth/projects/def-jsong/sidharth/projects/moose/framework/build/unity_src/meshgenerators_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1\nmake: *** [/home/sidharth/projects/def-jsong/sidharth/projects/moose/framework/build.mk:150: /home/sidharth/projects/def-jsong/sidharth/projects/moose/framework/build/unity_src/actions_Unity.x86_64-conda-linux-gnu.opt.lo] Error 1\n\n\n\n\n(moose) [sidharth@narval2 test]$ ./run_tests -j 6\n...\n<frozen importlib._bootstrap>:241: RuntimeWarning: compile time version 3.10 of module 'hit' does not match runtime version 3.11\nSegmentation fault (core dumped)\n(moose) [sidharth@narval2 test]$",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8447200",
                          "updatedAt": "2024-02-12T21:53:11Z",
                          "publishedAt": "2024-02-12T21:48:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "Step 3:\nAsk your cluster admins if they can install the necessary package. Reference: #22659\nStep 5:\nBecause you are using the System's provided compile suite (module load StdEnv/2023), you cannot use Conda. By trying to use both, you will run into conflicting scenarios (module load StdEnv/2023 provides a compiler stack, and conda activate will provide another completely different compiler stack).\nYou must continue to use the same compiler stack throughout the entire installation and usage of MOOSE.\nIf it turns out your HPC Cluster admins do not wish to install libtirpc-devel, you always have the option of using only Conda. But if you use Conda, you can no longer use modules. To make this clear, module load is akin to conda activate. They do the same thing: Attempt to manage your environment. You cannot/should not/it is dangerous unless you know what you're doing/ use both.",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8447579",
                          "updatedAt": "2024-02-12T22:48:52Z",
                          "publishedAt": "2024-02-12T22:48:23Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "sidharthsarmah"
                          },
                          "bodyText": "HI @milljm , @GiudGiud  :\nThe admin suggested me the following:\nplease try to install it locally with these instructions given by my colleague in a previous ticket :\n\nmodule load scipy-stack/2023b\nmodule load gcc/9.3.0\nmodule load petsc/3.17.1 boost/1.80.0 eigen/3.4.0 glpk/5.0 wasp/3.1.4\n\ngit clone --depth 1 --branch 2023-11-08 https://can01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fidaholab%2Fmoose.git&data=05%7C02%7Csidharth.sarmah%40mail.mcgill.ca%7C24d040bc9d6f47c4d6d108dc3186a880%7Ccd31967152e74a68afa9fcf8f89f09ea%7C0%7C0%7C638439701616227442%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&sdata=8IJ%2BzC2s1THteBQyUqam3peKZSDK5I%2FSL6hmeXepPwo%3D&reserved=0\ncd moose/scripts\nexport MOOSE_JOBS=6 METHODS=opt\nmodule load gcc/9.3.0 petsc/3.17.1 boost/1.80.0 eigen/3.4.0 glpk/5.0\nexport PETSC_DIR=$EBROOTPETSC\nexport WASP_DIR=$EBROOTWASP\nexport EIGEN_INC=$EBROOTEIGEN/include\nexport GLPK_DIR=$EBROOTGLPK\n\nCPPFLAGS=\"$CPPFLAGS -I$EBROOTGENTOO/include/tirpc \" ./update_and_rebuild_libmesh.sh\n\ncd ../test\nmake\n\nOnce I did it, it was okay but then when I tried, ./run_tests. I got this error message please suggest.\nmisc/check_error.aux_kernel_with_var ......................................... FAILED (EXPECTED ERROR MISSING)\nmisc/check_error.bad_kernel_test ............................................. FAILED (EXPECTED ERROR MISSING)\nmisc/check_error.function_file_test3 ......................................... FAILED (EXPECTED ERROR MISSING)\nmisc/check_error.incomplete_kernel_variable_coverage_test .................... FAILED (EXPECTED ERROR MISSING)\nmisc/check_error.scalar_kernel_with_var ...................................... FAILED (EXPECTED ERROR MISSING)\nmisc/check_error.bad_kernel_var_test ......................................... FAILED (EXPECTED ERROR MISSING)\n--------------------------------------------------------------------------------------------------------------\nRan 52 tests in 83.0 seconds. Average test time 0.9 seconds, maximum test time 1.0 seconds.\n2 passed, 250 skipped, 0 pending, 50 FAILED\nMAX FAILURES REACHED\n[sidharth@beluga2 test]$",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8536350",
                          "updatedAt": "2024-02-20T23:30:03Z",
                          "publishedAt": "2024-02-20T23:28:27Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "adityakavalur"
                  },
                  "bodyText": "@sidharthsarmah and @milljm just to add my two cents in case its useful.\n\nIf you are trying to unload stickied modules module --force unload foo should do the trick, similarly module purge does not unload stickied modules you need to add the force flag for that as well, module --force purge, however that seems a bit excessive to me. The former will allow you to get to cleaner environments in a meaningful manner.\nI would suggest trying without the StdEnv module and simply the base gcc, often times vendors/HPC admin will add a whole bunch of compiler flags and compiler wrappers on the base version through these modules.",
                  "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8677706",
                  "updatedAt": "2024-03-05T09:33:52Z",
                  "publishedAt": "2024-03-05T09:33:51Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "That is what I think is going on as well. I've tried to get the cluster admins involved, and they have invited @sidharthsarmah to contact them with issues involving building on their cluster, and I think that is the correct direction to go at this time.\nWe have no support for Intel compilers, yet this environment seems to still be setting Intel Math Kernel libraries. Whether or not this is actually affecting the build process... I don't know.\nThe cluster is operating on Gentoo. Which by itself is fine, but the packages are not being maintained by portage. But instead by easybuild. In my most humble opinion, this is a little odd. As it defeats the purpose of the highly customizable/optimized builds provided by portage. Personally I think this is why 'building' things on this cluster is proving difficult.",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8682805",
                          "updatedAt": "2024-03-05T16:37:01Z",
                          "publishedAt": "2024-03-05T16:37:00Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "sidharthsarmah"
                          },
                          "bodyText": "Hi, @GiudGiud @milljm could you please suggest what do I do next? Thanks.",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8683251",
                          "updatedAt": "2024-03-05T17:08:59Z",
                          "publishedAt": "2024-03-05T17:08:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "At this point, I would ask a cluster admin to try and follow our Getting Started instructions while on B\u00e9luga. There is something special needed that we're unable to detect.",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8683366",
                          "updatedAt": "2024-03-05T17:17:48Z",
                          "publishedAt": "2024-03-05T17:17:47Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "bartoldeman"
                          },
                          "bodyText": "I'm an analyst for Beluga. We have Gentoo as a compatibility layer because the same modules are used on different OSes, it's the same approach as EESSI (http://www.eessi.io/docs/overview/)\nIn the end it wasn't so hard, but you need to load the petsc module instead of trying to build it yourself.\nmodule load StdEnv/2023 \nmodule load python scipy-stack petsc slepc vtk parmetis wasp eigen boost\nexport WASP_DIR=$EBROOTWASP\nexport MOOSE_JOBS=6 METHODS=opt\ngit clone --depth 1 --branch 2024-02-12 https://github.com/idaholab/moose.git moose-2024-02-12\ncd moose-2024-02-12/scripts/\n./update_and_rebuild_libmesh.sh --with-xdr-include=$EBROOTGENTOO/include/tirpc\ncd ../test\nmake -j6\n./run_tests -j6\n\nthis passed all tests except a few:\nmisc/signal_handler.test_signal .................................................... FAILED (TESTER EXCEPTION)\nmisc/signal_handler.test_signal_parallel .............................. [min_cpus=3] FAILED (TESTER EXCEPTION)\nkernels/bad_scaling_scalar_kernels.poorly-conditioned-field-scalar-system ... FAILED (EXPECTED OUTPUT MISSING)\nparser/hit_error.error ....................................................... FAILED (EXPECTED ERROR MISSING)\nkernels/simple_transient_diffusion.cant-solve-poorly-scaled ................. FAILED (EXPECTED OUTPUT MISSING)\nkernels/simple_transient_diffusion.cant-solve-large-transient-changes ........ FAILED (EXPECTED ERROR MISSING)\nusability.diagnostic ......................................................... FAILED (EXPECTED ERROR MISSING)\npartitioners/petsc_partitioner.party ............................................ [min_cpus=4] FAILED (ERRMSG)\n--------------------------------------------------------------------------------------------------------------\nRan 4037 tests in 1277.9 seconds. Average test time 1.4 seconds, maximum test time 47.5 seconds.\n4029 passed, 212 skipped, 0 pending, 8 FAILED",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8724962",
                          "updatedAt": "2024-03-08T21:26:47Z",
                          "publishedAt": "2024-03-08T21:26:46Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "sidharthsarmah"
                          },
                          "bodyText": "Hi @milljm @bartoldeman ,\nThis work but when I tried to make the tensor mechanics-opt to run my input file, I got this error:\n[sidharth@beluga3 tensor_mechanics]$ make -j 6\n\n***ERROR***\nWASP does not seem to be available.\nMake sure to either run scripts/update_and_rebuild_wasp.sh in your MOOSE directory,\nor set WASP_DIR to a valid WASP install\n\nDo you think I need to install petsc and wasp as well, as suggested it need not be except we have wasp and petsc as modules already, so no need to install them another time as well)? @bartoldeman",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8737906",
                          "updatedAt": "2024-03-11T17:25:00Z",
                          "publishedAt": "2024-03-10T20:15:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Can you follow the exact instrucitons that @bartoldeman pasted here and attach the console log so we can see what it outputs.\nAs a reminder:\nmodule load StdEnv/2023 \nmodule load python scipy-stack petsc slepc vtk parmetis wasp eigen boost\nexport WASP_DIR=$EBROOTWASP\nexport MOOSE_JOBS=6 METHODS=opt\ngit clone --depth 1 --branch 2024-02-12 https://github.com/idaholab/moose.git moose-2024-02-12\ncd moose-2024-02-12/scripts/\n./update_and_rebuild_libmesh.sh --with-xdr-include=$EBROOTGENTOO/include/tirpc\ncd ../test\nmake -j6\n./run_tests -j6",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8738519",
                          "updatedAt": "2024-03-10T22:22:54Z",
                          "publishedAt": "2024-03-10T22:22:53Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "bartoldeman"
                          },
                          "bodyText": "Hello @sidharthsarmah\nyou need to load the wasp module included in (module load python scipy-stack petsc slepc vtk parmetis wasp eigen boost) and set export WASP_DIR=$EBROOTWASP for WASP_DIR to be set correctly. I suspect you forgot one or both of those commands before make -j6.\nmodule avail merely lists the modules you can load, not the ones that are loaded, that would be module list.",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8744344",
                          "updatedAt": "2024-03-11T11:05:31Z",
                          "publishedAt": "2024-03-11T11:05:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "milljm"
                          },
                          "bodyText": "@bartoldeman thank you for looking into this!\nIn the event a user needs to build PETSc themselves, what are the \"special tricks\" in order to do so on this cluster? I am asking only because we usually see pre-built versions of PETSc (or libMesh, etc) available as modules, they do not contain every contrib/configure option MOOSE based applications require.\nAlso note: I do not think the 8 failures you received are due to any sort of missing contribs in PETSc. I am genuinely interested in knowing so that I may provide helpful tips in the future.",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8744737",
                          "updatedAt": "2024-03-11T11:44:57Z",
                          "publishedAt": "2024-03-11T11:44:56Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "sidharthsarmah"
                          },
                          "bodyText": "Hi @bartoldeman\nI did followed these commands:\nmodule load StdEnv/2023 \nmodule load python scipy-stack petsc slepc vtk parmetis wasp eigen boost\nexport WASP_DIR=$EBROOTWASP\nexport MOOSE_JOBS=6 METHODS=opt\ngit clone --depth 1 --branch 2024-02-12 https://github.com/idaholab/moose.git moose-2024-02-12\ncd moose-2024-02-12/scripts/\n./update_and_rebuild_libmesh.sh --with-xdr-include=$EBROOTGENTOO/include/tirpc\ncd ../test\nmake -j6\n./run_tests -j6\n\n\nDo I need to do it again before making the tensor mechanics app?\nmodule load python scipy-stack petsc slepc vtk parmetis wasp eigen boost \nexport WASP_DIR=$EBROOTWASP",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8744840",
                          "updatedAt": "2024-03-11T11:55:47Z",
                          "publishedAt": "2024-03-11T11:54:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "bartoldeman"
                          },
                          "bodyText": "@sidharthsarmah yes you need to run those two commands every time.",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8745481",
                          "updatedAt": "2024-03-11T12:48:13Z",
                          "publishedAt": "2024-03-11T12:48:12Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "sidharthsarmah"
                  },
                  "bodyText": "Hi @GiudGiud , I am getting this error when I try to make -j6 in my local laptop. Should I run update_and_rebuild_libmesh.sh in scripts folder.\nError:\n(base) sidharthsarmah@Sidharths-MacBook-Air tensor_mechanics % make -j 6\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/libtool: No such file or directory\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/libtool: No such file or directory\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\n\n***WARNING***\nYour libmesh is out of date.\nYou need to run update_and_rebuild_libmesh.sh in the scripts directory.\n\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\nCompiling C++ (in opt mode) /Users/sidharthsarmah/MOOSE/projects/moose/framework/build/unity_src/src_Unity.C...\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\nCompiling C++ (in opt mode) /Users/sidharthsarmah/MOOSE/projects/moose/framework/build/unity_src/linesearches_Unity.C...\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/libtool: No such file or directory\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/contrib/bin/libmesh-config: No such file or directory\nmake: *** [/Users/sidharthsarmah/MOOSE/projects/moose/framework/build/unity_src/src_Unity..opt.lo] Error 127\nmake: *** Waiting for unfinished jobs....\n/bin/sh: /Users/sidharthsarmah/MOOSE/projects/moose/libmesh/installed/libtool: No such file or directory\nmake: *** [/Users/sidharthsarmah/MOOSE/projects/moose/framework/build/unity_src/linesearches_Unity..opt.lo] Error 127\n(base) sidharthsarmah@Sidharths-MacBook-Air tensor_mechanics %",
                  "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8832965",
                  "updatedAt": "2024-03-18T20:45:13Z",
                  "publishedAt": "2024-03-18T20:44:25Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "you forgot to activate the moose conda environment on your local laptop",
                          "url": "https://github.com/idaholab/moose/discussions/26787#discussioncomment-8832977",
                          "updatedAt": "2024-03-18T20:45:40Z",
                          "publishedAt": "2024-03-18T20:45:39Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "GPUs and/or CUDA",
          "author": {
            "login": "aladshaw3"
          },
          "bodyText": "I know that PETSc has some methods to utilize GPUs and was wondering if MOOSE makes use of GPUs?\nIf so, how would users invoke GPUs for scaling on hybrid computer systems?\nAnd if not, are there any future plans for making MOOSE scalable on hybrid systems (CPUs and GPUs)?\nThanks.",
          "url": "https://github.com/idaholab/moose/discussions/21152",
          "updatedAt": "2025-02-20T13:44:06Z",
          "publishedAt": "2022-05-26T15:19:54Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nThere are plans. Though not sure if we have something laid out somewhere to share.\n@fdkong can call Petsc for MOOSE on GPU.\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/21152#discussioncomment-2828438",
                  "updatedAt": "2022-05-26T15:23:15Z",
                  "publishedAt": "2022-05-26T15:22:36Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              },
              {
                "node": {
                  "author": {
                    "login": "fdkong"
                  },
                  "bodyText": "I know that PETSc has some methods to utilize GPUs and was wondering if MOOSE makes use of GPUs?\n\nMOOSE can now seamlessly use GPUs for algebra operations (linear solvers, preconditioners) via PETSc. It will significantly speed up the simulation if it is expensive on the solvers, which is valid for many applications.\nWe form matrix and vector on CPU and move that to GPU to solve\nIn the future, we plan to explore the approaches to move more calculations on GPU. It will also depend on the resources.",
                  "url": "https://github.com/idaholab/moose/discussions/21152#discussioncomment-2828492",
                  "updatedAt": "2022-05-26T15:32:11Z",
                  "publishedAt": "2022-05-26T15:30:52Z",
                  "isAnswer": true,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "aladshaw3"
                          },
                          "bodyText": "Thanks for the information. As a follow up, I assume that only certain PETSc operations (specific linear solvers or preconditioners) utilize GPUs. Is there a comprehensive list somewhere so I know which solvers to invoke? Thanks.",
                          "url": "https://github.com/idaholab/moose/discussions/21152#discussioncomment-2828560",
                          "updatedAt": "2022-05-26T15:40:43Z",
                          "publishedAt": "2022-05-26T15:40:40Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "This was a great question. We (PETSc team) will work on a table for that. In general, AMG and other iterative methods work pretty well on GPUs and direct solvers such as LU, CC do not play well with GPUs because of data dependency. In theory, at the algebra side, the GPU support is almost  about Vec and Mat. Here is a roadmap at PETSc side https://petsc.org/release/overview/gpu_roadmap/\nFor practice,  you could start with a simple example and then use \"-snes_view\" to check what solvers are you using and use \"-log_view\" to check what operations are carried out on GPU/CPU. By doing that, you will have enough information to understand the simulation.",
                          "url": "https://github.com/idaholab/moose/discussions/21152#discussioncomment-2828721",
                          "updatedAt": "2022-05-26T16:04:44Z",
                          "publishedAt": "2022-05-26T16:04:43Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "mangerij"
                          },
                          "bodyText": "This is a fantastic development!",
                          "url": "https://github.com/idaholab/moose/discussions/21152#discussioncomment-2833757",
                          "updatedAt": "2022-05-27T11:51:25Z",
                          "publishedAt": "2022-05-27T11:51:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ajdubas"
                          },
                          "bodyText": "Good morning,\nI was wondering if you could shed a little more light on how to get MOOSE to solve GPU?  So far I have built PETSc with CUDA and have tried passing in various -dm_mat_type and -dm_vec_type flags to my MOOSE app to utilise the GPU solvers in PETSc but so far to no avail; -log_view shows no data transfer or GPU FLOPS.  Do you have a guide to what I need to do to solve on GPU?  Or any thoughts on what I might be missing?\nMany Thanks in Advance,\nAleks",
                          "url": "https://github.com/idaholab/moose/discussions/21152#discussioncomment-5098480",
                          "updatedAt": "2023-02-24T10:10:30Z",
                          "publishedAt": "2023-02-24T10:10:30Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@fdkong can you please share your branch where you were doing that?",
                          "url": "https://github.com/idaholab/moose/discussions/21152#discussioncomment-5102326",
                          "updatedAt": "2023-02-24T16:33:10Z",
                          "publishedAt": "2023-02-24T16:33:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "fdkong"
                          },
                          "bodyText": "All GPU related stuffs have been merged into libMesh and MOOSE. You might use -mat_type and -vec_type instead of -dm_mat_type and -dm_vec_type to take GPU matrix and vector.\nIn MOOSE, we do not really support DM.  BTW,  if you still have issue, please post the result of \"-snes_view\" and \"-log_view\" to us. Otherwise, we are simply blind.\nThanks for asking",
                          "url": "https://github.com/idaholab/moose/discussions/21152#discussioncomment-5102539",
                          "updatedAt": "2023-02-24T16:56:52Z",
                          "publishedAt": "2023-02-24T16:56:08Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ajdubas"
                          },
                          "bodyText": "Ok, my initial approach was to use -mat_type and -vec_type of aijcuda and cuda respectively; are these the right types to be using?\nCheers,\nAleks",
                          "url": "https://github.com/idaholab/moose/discussions/21152#discussioncomment-5123606",
                          "updatedAt": "2023-02-27T14:41:42Z",
                          "publishedAt": "2023-02-27T14:41:42Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ajdubas"
                          },
                          "bodyText": "An update: I have managed to confirm that the compiled PETSc is GPU enabled with the flags -mat_type seqaijcusparse -vec_type seqcuda passed directly to PETSc.  However, when I try the same with a MOOSE problem it does not work neither as part of the [Executioner] block nor as direct command line inputs.  Where do I go from here?\nAll the best,\nAleks",
                          "url": "https://github.com/idaholab/moose/discussions/21152#discussioncomment-5290682",
                          "updatedAt": "2023-03-13T12:15:37Z",
                          "publishedAt": "2023-03-13T12:15:36Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "Hi,\nCould you please attach the run log with with  the result of \"-snes_view\" and \"-log_view\" passed to Petsc\nthanks\nGuillaume",
                          "url": "https://github.com/idaholab/moose/discussions/21152#discussioncomment-5292292",
                          "updatedAt": "2023-03-13T14:26:24Z",
                          "publishedAt": "2023-03-13T14:26:24Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "ajdubas"
                          },
                          "bodyText": "Is there an example input file or command line invocation for running on GPU?",
                          "url": "https://github.com/idaholab/moose/discussions/21152#discussioncomment-5300525",
                          "updatedAt": "2023-03-14T08:38:28Z",
                          "publishedAt": "2023-03-14T08:38:27Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Will there be automatic differentiation (AD) versions of the new solid mechanics system?",
          "author": {
            "login": "richmondodufisan"
          },
          "bodyText": "I am trying to implement a couple of custom models and as they increase in complexity, manually calculating the Jacobians is becoming a problem... I am facing convergence issues that I am 99% sure caused by something in my Jacobian. For example I had a Jacobian with 7 terms (lots of coupled variables) and I finally figured out that the issue was a switched index (i,j). Which took so long to find given how complex the expression is.\nAlternatively, is there a way I can develop stress objects with the new system and have them work with the AD functionality in the old system? Otherwise I am thinking I will have to completely re-implement my models with the old system.\nAnd on the old system, how can I get the deformation gradient from computeFiniteStrain?\nADDITIONAL DETAILS\nThe problem is incompressible hyperelasticity, where the strain energy functional is defined as:\n\nWhere p is a Lagrangian Multiplier to enforce incompressibility, i.e J = 1. Taking variations wrt C and p respectively, we have the PK2 Stress:\n\nAnd the residual for the new p variable:\n\nI implemented these in my app, and input files for simple uniaxial stress/strain tests are here:\nhttps://github.com/richmondodufisan/purple/tree/main/Cornea_Hyperelastic_NeoHookean/stress_strain_tests\nHowever, for the incompressible case, the solution is not converging, with this error:\n    Linear solve did not converge due to DIVERGED_PC_FAILED iterations 0\n                   PC failed due to FACTOR_NUMERIC_ZEROPIVOT\n  Nonlinear solve did not converge due to DIVERGED_FNORM_NAN iterations 0\n\nSuggesting that the matrix is singular. This makes sense, because by my understanding, TotalLagrangianStressDivergence has an exact Jacobian for the case where pressure is not coupled (i.e only has diagonal terms). To test my hypothesis, I implemented the same problem, but this time with a penalty term instead of an extra field, i.e:\n\nWhere kappa is a really large number (used the bulk modulus for nu = 0.4999), and I got great convergence, with J = 1 everywhere. I manually calculated an exact Jacobian for S (i.e dS/dE) as required by the new system.\n\nIn summary\nWith the current non-AD TotalLagrangianStressDivergence, if I ever want to implement anything with an additional field that couples to it, I would need to manually calculate the Jacobian as well, and create a new TotalLagrangianStressDivergence object with the new Jacobian that includes the off-diagonal contributions from my new field.\nWith an ADTotalLagrangianStressDivergence object, I wouldn't need to do that.\n\nP.S, I was also wondering if I could create a wrapper to use ComputeLagrangianStrain with the old system. I actually just attempted to do that, and used ComputeLagrangianStrain with my wrapper object & ADStressDivergenceTensors, but my matrix is still singular so I'm not sure if I made the wrapper wrong or that coupling doesn't work (edit: it doesn't, see #29923). (wrapper input file: https://github.com/richmondodufisan/purple/blob/main/Cornea_Hyperelastic_NeoHookean/stress_strain_tests/incompressible_1D_test.i)\nIt would be nice to have a wrapper to couple \"new\" to \"old\"- this would be the best case scenario because by my understanding there is currently no Dynamics object implemented in the new system. And I plan to run some dynamic simulations.\nAlternatively, if I plan to run everything on the old system (and recreate my objects in the old system), how can I get the deformation gradient from ComputeFiniteStrain?",
          "url": "https://github.com/idaholab/moose/discussions/29895",
          "updatedAt": "2025-02-20T11:46:27Z",
          "publishedAt": "2025-02-17T01:00:34Z",
          "category": {
            "name": "Q&A Modules: Solid mechanics"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "@hugary1995 @reverendbedford",
                  "url": "https://github.com/idaholab/moose/discussions/29895#discussioncomment-12220653",
                  "updatedAt": "2025-02-17T06:29:51Z",
                  "publishedAt": "2025-02-17T06:29:50Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@bwspenc @vanwdani",
                          "url": "https://github.com/idaholab/moose/discussions/29895#discussioncomment-12256037",
                          "updatedAt": "2025-02-19T23:19:32Z",
                          "publishedAt": "2025-02-19T23:19:31Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "Just added more details",
                          "url": "https://github.com/idaholab/moose/discussions/29895#discussioncomment-12256275",
                          "updatedAt": "2025-02-19T23:53:33Z",
                          "publishedAt": "2025-02-19T23:53:32Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "ElementIntegralVariablePostprocessor does not seem to be integrating a variable",
          "author": {
            "login": "richmondodufisan"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n Q&A General is the most appropriate section for my question\n I have consulted the posting Guidelines on the Discussions front page\n I have searched the Discussions forum and my question has not been asked before\n I have searched the MOOSE website and the documentation does not answer my question\n I have formatted my post following the posting guidelines (screenshots as a last resort, triple back quotes around pasted text)\n\nQuestion\nHi,\nI'm running an incompressibility problem. My coupled equations are the momentum balance, captured by TotalLagrangianStressDivergence where the PK2 stress is defined as:\n\nand the residual for the coupled equation is:\n\nTo get around the\nLinear solve did not converge due to DIVERGED_PC_FAILED iterations 0\n                   PC failed due to FACTOR_NUMERIC_ZEROPIVOT\n  Nonlinear solve did not converge due to DIVERGED_FNORM_NAN iterations 0\n\nerror caused by the singular matrix due to \"p\" being underconstrained, I followed the steps here:\nhttps://mooseframework.inl.gov/source/kernels/ScalarLagrangeMultiplier.html\nto add a zero-mean-pressure constraint, i.e\nTo do this, I used ElementIntegralVariablePostprocessor to get the volume integral of p. However, the FACTOR_NUMERIC_ZEROPIVOT error still remains. I wanted to see if p was actually being coupled properly and/or if the integral was actually integrating anything, so I set a random initial condition, p = 1.\nI then added print statements to both my stress material and my p (pressure) kernel. For example,\n// std::cout << \"Initial Pressure at qp \" << _qp << \" is \" << p << std::endl;\ngot me:\n...\nInitial Pressure at qp 4 is (1,{(66765,0.25), (66766,0.25), (67173,0.25), (67229,0.25)})\nInitial Pressure at qp 4 is (1,{(66765,0.25), (66766,0.25), (67173,0.25), (67229,0.25)})\nInitial Pressure at qp 4 is (1,{(66765,0.25), (66766,0.25), (67173,0.25), (67229,0.25)})\nInitial Pressure at qp 4 is (1,{(66765,0.25), (66766,0.25), (67173,0.25), (67229,0.25)})\nInitial Pressure at qp 5 is (1,{(66765,0.443649), (66766,0.0563508), (67173,0.0563508), (67229,0.443649)})\nInitial Pressure at qp 5 is (1,{(66765,0.443649), (66766,0.0563508), (67173,0.0563508), (67229,0.443649)})\nInitial Pressure at qp 5 is (1,{(66765,0.443649), (66766,0.0563508), (67173,0.0563508), (67229,0.443649)})\n..\n\nSo I know the pressure is being assigned by the IC correctly. But then, when I set ElementIntegralVariablePostprocessor to perform it's integral at timestep_begin, that is,\n  [pressure_integral]\n    type = ElementIntegralVariablePostprocessor\n    variable = pressure\n\texecute_on = timestep_begin\n  []\n\nOn my initial timestep I see:\nTime Step 0, time = 0\n\nPostprocessor Values:\n+----------------+----------------+-------------------+----------------+\n| time           | displace_x     | pressure_integral | react_x        |\n+----------------+----------------+-------------------+----------------+\n|   0.000000e+00 |   0.000000e+00 |      0.000000e+00 |   0.000000e+00 |\n+----------------+----------------+-------------------+----------------+\n\n\nScalar Variable Values:\n+----------------+----------------+\n| time           | lambda         |\n+----------------+----------------+\n|   0.000000e+00 |   0.000000e+00 |\n+----------------+----------------+\n\n\nTime Step 1, time = 0.0008, dt = 0.0008\n\nPerforming automatic scaling calculation\n\n 0 Nonlinear |R| = 4.595748e-03\n    Linear solve did not converge due to DIVERGED_PC_FAILED iterations 0\n                   PC failed due to FACTOR_NUMERIC_ZEROPIVOT\n  Nonlinear solve did not converge due to DIVERGED_FNORM_NAN iterations 0\n Solve Did NOT Converge!\nAborting as solve did not converge\n\nThis is not what I expect to see, I expect pressure_integral to have a non-zero value if I understand how that postprocessor works correctly, and as explained in (https://mooseframework.inl.gov/source/kernels/ScalarLagrangeMultiplier.html).\nDo you have any idea why this is the case? The problem is a simple axial tensile test, and I have implemented the exact same problem using a penalty instead of an extra field (i.e p = 0.5 * kappa * ln(J)), (where kappa is a large number) successfully with exact Jacobians, and it ran fine and enforced incompressibility perfectly well.\nThe full input file is here (i.e incompressible_neo_hookean.i): https://github.com/richmondodufisan/purple/tree/main/Cornea_Hyperelastic_NeoHookean/stress_strain_tests",
          "url": "https://github.com/idaholab/moose/discussions/29917",
          "updatedAt": "2025-02-20T00:01:42Z",
          "publishedAt": "2025-02-19T10:15:27Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "execute_on = 'initial timestep_begin'\n\nto see the output in the table, being executed on INITIAL",
                  "url": "https://github.com/idaholab/moose/discussions/29917#discussioncomment-12251315",
                  "updatedAt": "2025-02-19T15:33:04Z",
                  "publishedAt": "2025-02-19T15:07:49Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "That worked, I got a numeric value, thanks.\nAnd I figured out the reason for the zeropivot error... the Jacobians computed in TotalLagrangianStressDivergence do not include the contribution from the additional field, p. Hence why the matrix is singular. I could try to manually calculate the Jacobians but that would be a lot. Especially as incompressibility is just the first phase of the project, I plan to couple some other things too.\nThis makes sense why my weak-incompressibility object converged fine, there is no additional field and instead of p as an additional field I just have a penalty term. So the StressDivergence Jacobian remains unchanged. Hence why I was able to get a nice J = 1 (incompressibility) enforced everywhere:\n\nWhich brings me back to #29895 ... the new system is really great because it has familiar quantities computed by the strain object (deformation gradient, cauchy-green tensors, etc) that make development easier. But without Automatic Differentiation, implementing any coupled physics that require an additional field would mean I need to always create a new StressDivergence object with new jacobians.\nThat means I would need to revert to the old system (ComputeFiniteStrain), but it doesn't give me the deformation gradient. Or is there a way to get the deformation gradient from it? I looked through the source and couldn't figure out how.",
                          "url": "https://github.com/idaholab/moose/discussions/29917#discussioncomment-12255465",
                          "updatedAt": "2025-02-19T22:04:50Z",
                          "publishedAt": "2025-02-19T21:52:10Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "Also, Dynamics (which I plan to use) is built based on the old objects... is it possible to create some kind of wrapper that would allow me to use stress calculated by objects based on the new system onto the old one?\nFor example, use TotalLagrangianStrain with large_kinematics set to true, calculate a stress, and then use a wrapper to send the calculated stress to the old system?",
                          "url": "https://github.com/idaholab/moose/discussions/29917#discussioncomment-12255687",
                          "updatedAt": "2025-02-19T22:25:34Z",
                          "publishedAt": "2025-02-19T22:25:33Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "you might want to add these details to #29895",
                          "url": "https://github.com/idaholab/moose/discussions/29917#discussioncomment-12256035",
                          "updatedAt": "2025-02-19T23:19:16Z",
                          "publishedAt": "2025-02-19T23:19:15Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "richmondodufisan"
                          },
                          "bodyText": "I just added more details to the main post of #29895. I also attempted just now to create a wrapper object to convert stress calculated with the new system to the old system. But I still got the singular error. All details are in the main post of #29895.\nI'll mark this is answered since this post was about getting the volume integral. By the way,  does\nexecute_on = 'initial timestep_begin'\nmean it will only calculate the integral at the beginning of the timestep on the initial timsetp t = 0? Or will it still calculate it at all timesteps?",
                          "url": "https://github.com/idaholab/moose/discussions/29917#discussioncomment-12256298",
                          "updatedAt": "2025-02-19T23:56:45Z",
                          "publishedAt": "2025-02-19T23:56:44Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "it will compute it effectively twice at the begining of time step 0 then every other time step",
                          "url": "https://github.com/idaholab/moose/discussions/29917#discussioncomment-12256314",
                          "updatedAt": "2025-02-19T23:59:03Z",
                          "publishedAt": "2025-02-19T23:59:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "execute_on = 'initial timestep_end' is more common",
                          "url": "https://github.com/idaholab/moose/discussions/29917#discussioncomment-12256316",
                          "updatedAt": "2025-02-19T23:59:14Z",
                          "publishedAt": "2025-02-19T23:59:13Z",
                          "isAnswer": true
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Installing RACCOON",
          "author": {
            "login": "sskalati"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n I have consulted the Posting Guidelines.\n I have searched the Discussions Forum and MOOSE Framework Troubleshooting and have not found what I was looking for\n Q&A Getting Started is the most appropriate category for my question (trouble installing, beginner user, ...)\n\nIssue or question about MOOSE\nHi\nI am trying to use the RACCOON app for my simulation and have been following the instructions provided on the website. However, when I run the following command:\ngit submodule update --init --recursive\nI encounter the following error:\nSkipping submodule '../../../../moose/'\nSkipping submodule '../../../../moose/'\nSkipping submodule '../../../../moose/'\nSkipping submodule '../../../../moose/'\nSkipping submodule '../../../../moose/'\nSkipping submodule '../../../../moose/'\nSkipping submodule '../../../../moose/'\nSkipping submodule '../../../../moose/'\nSkipping submodule '../../../../moose/'\nCloning into '/home/kalati/projects/raccoon/moose/petsc'...\nfatal: unable to access 'https://gitlab.com/petsc/petsc.git/': The requested URL returned error: 403\nfatal: clone of 'https://gitlab.com/petsc/petsc.git' into submodule path '/home/kalati/projects/raccoon/moose/petsc' failed\nFailed to clone 'petsc'. Retry scheduled\nCloning into '/home/kalati/projects/raccoon/moose/petsc'...\nfatal: unable to access 'https://gitlab.com/petsc/petsc.git/': The requested URL returned error: 403\nfatal: clone of 'https://gitlab.com/petsc/petsc.git' into submodule path '/home/kalati/projects/raccoon/moose/petsc' failed\nFailed to clone 'petsc' a second time, aborting\nfatal: Failed to recurse into submodule path 'moose'\nIt seems that the provided link to the PETSc repository is either incorrect or inaccessible. I would greatly appreciate any guidance or a fix for this issue.\nThank you for your time and help.\n(Optional) code in question / simulation log / errors\nNo response\nEncountering Errors? Please include diagnostic output\nNo response",
          "url": "https://github.com/idaholab/moose/discussions/29916",
          "updatedAt": "2025-02-19T14:50:49Z",
          "publishedAt": "2025-02-19T08:30:13Z",
          "category": {
            "name": "Q&A Getting Started"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nIs your machine behind a corporate firewall?",
                  "url": "https://github.com/idaholab/moose/discussions/29916#discussioncomment-12251125",
                  "updatedAt": "2025-02-19T14:50:51Z",
                  "publishedAt": "2025-02-19T14:50:49Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": []
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "How to turn off Jacobian check?",
          "author": {
            "login": "PEI0214"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n Q&A General is the most appropriate section for my question\n I have consulted the posting Guidelines on the Discussions front page\n I have searched the Discussions forum and my question has not been asked before\n I have searched the MOOSE website and the documentation does not answer my question\n I have formatted my post following the posting guidelines (screenshots as a last resort, triple back quotes around pasted text)\n\nQuestion\nHello.\nThere are some degenerate elements in my grid. There was an error in the calculation.\nElem Information\n   id()=4769, unique_id()=33014, subdomain_id()=20, processor_id()=3\n   type()=HEX8\n   dim()=3\n   n_nodes()=8\n   mapping=LAGRANGE_MAP\n    0  Node id()=5709, processor_id()=3, Point=(x,y,z)=(   173.4,     -234,   2803.8)\n    DoFs=\n    1  Node id()=5711, processor_id()=3, Point=(x,y,z)=(   173.4,  -216.45,   2806.9)\n    DoFs=\n    2  Node id()=5711, processor_id()=3, Point=(x,y,z)=(   173.4,  -216.45,   2806.9)\n    DoFs=\n    3  Node id()=5710, processor_id()=3, Point=(x,y,z)=(   173.4,     -231,   2804.3)\n    DoFs=\n    4  Node id()=4945, processor_id()=3, Point=(x,y,z)=(   188.4,     -234,   2789.7)\n    DoFs=\n    5  Node id()=4947, processor_id()=3, Point=(x,y,z)=(   188.4,  -216.45,     2793)\n    DoFs=\n    6  Node id()=4947, processor_id()=3, Point=(x,y,z)=(   188.4,  -216.45,     2793)\n    DoFs=\n    7  Node id()=5712, processor_id()=3, Point=(x,y,z)=(   188.4,     -231,   2790.3)\n    DoFs=\n   n_sides()=6\n    neighbor(0)=nullptr\n    neighbor(1)=nullptr\n    neighbor(2)=nullptr\n    neighbor(3)=5415\n    neighbor(4)=4771\n    neighbor(5)=4772\n   hmin()=0, hmax()=28.7896\n   volume()=0.401469\n   active()=1, ancestor()=0, subactive()=0, has_children()=0\n   parent()=nullptr\n   level()=0, p_level()=0\n   refinement_flag()=DO_NOTHING\n   p_refinement_flag()=DO_NOTHING\n   DoFs=(1/0/71352) (1/1/71353) (1/2/71354) (1/3/71355) (1/4/71356) (1/5/71357) (1/6/71358) (1/7/71359) (1/8/71360) \nlibMesh terminating:\nERROR: negative Jacobian -0.414386 at point (x,y,z)=(  176.57, -229.791,  2801.57) in element 4769\n\nIs there a good way to solve this? Can I turn off Jacobian's check?",
          "url": "https://github.com/idaholab/moose/discussions/29712",
          "updatedAt": "2025-02-18T20:22:14Z",
          "publishedAt": "2025-01-20T15:19:18Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\nNo you have to fix your mesh. Turning off the check would just lead to a failed solve\nGuillaume",
                  "url": "https://github.com/idaholab/moose/discussions/29712#discussioncomment-11891898",
                  "updatedAt": "2025-01-20T15:26:35Z",
                  "publishedAt": "2025-01-20T15:26:34Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "@PEI0214 can you please share your mesh? i d like to have an example of the Jacobian check being useful",
                          "url": "https://github.com/idaholab/moose/discussions/29712#discussioncomment-12241840",
                          "updatedAt": "2025-02-18T20:22:16Z",
                          "publishedAt": "2025-02-18T20:22:14Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      },
      {
        "node": {
          "title": "Defining vector-valued current densities, conservation laws, and a Joule heating term",
          "author": {
            "login": "JLB577"
          },
          "bodyText": "Check these boxes if you have followed the posting rules.\n\n Q&A General is the most appropriate section for my question\n I have consulted the posting Guidelines on the Discussions front page\n I have searched the Discussions forum and my question has not been asked before\n I have searched the MOOSE website and the documentation does not answer my question\n I have formatted my post following the posting guidelines (screenshots as a last resort, triple back quotes around pasted text)\n\nQuestion\nI'm a beginner to MOOSE, and I'm struggling to understand how to build an input file that encompasses my use case.\nI'm attempting to solve a set of four coupled PDEs relating to semiconductor physics in four variables: psi (electrical potential), n (electron number density), p (hole number density), and T (temperature). I have questions about how to set up these PDEs in a MOOSE input file. My equations look like:\n\n\nPoisson's equation for psi. (I've already successfully set this one up, so I don't have any questions on it.)\n\n\nA conservation law for n: partial n / partial t + div.J_n = 0. Here \"div.J_n\" means the divergence of the electron number density current J_n (a vector field), which is in turn a function of n, T, the gradient of n, the gradient of psi, the gradient of T, and various parameters.\n\n\nA conservation law for p: partial p / partial t + div.J_p = 0. Here \"div.J_p\" means the divergence of the hole number density current J_p (a vector field), which is in turn a function of p, T, the gradient of p, the gradient of psi, the gradient of T, and various parameters.\n\n\nThe thermal equation: rhoc_ppartial T/partial t - E.J + (other terms I already know how to implement) = 0. Here \"E.J\" is the Joule heating term, i.e. the dot product between the electric field vector E = -grad psi and the total current density J = q(J_p - J_n). (rho, c_p, and q are fixed parameters.)\n\n\nI already know how to invoke the appropriate kernels to create the time derivative terms in the PDEs above, as well as to create a few other terms I didn't list explicitly. My questions are:\n\n\nHow do I define the vector fields J_n and J_p with their somewhat complex dependence on the independent variables and their gradients?\n\n\nWhat kernel (if any) do I use to include the terms div.J_n and div.J_p in their respective conservation laws?\n\n\nHow do I define the electric field E = -grad psi vector field for use in the Joule heating term?\n\n\nHow do I define the total current density J = q(J_p - J_n) vector field for use in the Joule heating term?\n\n\nWhat kernel (if any) do I use to include the Joule heating term -E.J in my thermal PDE? Note: I'm aware of the existence of the \"JouleHeatingSource\" kernel. However, it does not have a general enough form for my purposes. It assumes Ohm's law J = sigma E holds, which doesn't apply in my situation. It also appears to have the wrong sign.\n\n\nPlease let me know if I can clarify any of the above, or if there is any other information I should provide. Thanks for any and all help.",
          "url": "https://github.com/idaholab/moose/discussions/29809",
          "updatedAt": "2025-02-18T18:50:12Z",
          "publishedAt": "2025-02-04T18:54:48Z",
          "category": {
            "name": "Q&A General"
          },
          "comments": {
            "edges": [
              {
                "node": {
                  "author": {
                    "login": "GiudGiud"
                  },
                  "bodyText": "Hello\n\nHow do I define the vector fields J_n and J_p with their somewhat complex dependence on the independent variables and their gradients?\n\nThere are two options:\n\neither define a new material property with an ADRealVectorValue type that computes J_n / J_p. This will let you look at the value of the vector field. Then create a new kernel that computes div (J). This will likely mean that material property will also need to define dJ/dx, dJ/dy etc as properties\ndirectly implement div J_n in a kernel. This is the simplest, but you wont be able to look at J_n unless you create an auxkernel that computes it\n\n\nWhat kernel (if any) do I use to include the terms div.J_n and div.J_p in their respective conservation laws?\n\nThere are quite a few divergence kernels implemented in the MOOSE modules, but I think you may want to code your own. You can look at the Navier Stokes conservation of mass for an example. This one does not use AD. I recommend you use AD\nhttps://mooseframework.inl.gov/source/kernels/INSMass.html\n\nHow do I define the electric field E = -grad psi vector field for use in the Joule heating term?\n\nSince this field depends on a nonlinear variable, you should use a kernel to add this Joule heating term. You can use the Coupleable API in a kernel to retrieve the gradient of a field\nhttps://mooseframework.inl.gov/source/interfaces/Coupleable.html\n\nHow do I define the total current density J = q(J_p - J_n) vector field for use in the Joule heating term?\n\nnew kernel probably unless you find one which fits you already\n\nWhat kernel (if any) do I use to include the Joule heating term -E.J in my thermal PDE? Note: I'm aware of the existence of the \"JouleHeatingSource\" kernel. However, it does not have a general enough form for my purposes. It assumes Ohm's law J = sigma E holds, which doesn't apply in my situation.\n\nProbably make a new kernel then. We wont have an additional JouleHeatingSource kernel.\n\nIt also appears to have the wrong sign.\n\nAre you sure? Kernels compute a residual which is on the left hand side always.",
                  "url": "https://github.com/idaholab/moose/discussions/29809#discussioncomment-12062597",
                  "updatedAt": "2025-02-05T04:43:54Z",
                  "publishedAt": "2025-02-05T04:43:54Z",
                  "isAnswer": false,
                  "replies": {
                    "edges": [
                      {
                        "node": {
                          "author": {
                            "login": "JLB577"
                          },
                          "bodyText": "Thanks for your response. I think I didn't properly communicate how much of a beginner I am. I really have no idea what I'm doing or what most of the terminology means.\n\n\nHow do I define the vector fields J_n and J_p with their somewhat complex dependence on the independent variables and their gradients?\n\n\n\nThere are two options:\neither define a new material property with an ADRealVectorValue type that computes J_n / J_p. This will let you look at the value of the vector field.\n\nBut how do I do that? What is the actual input file syntax? This is where I'm getting hung up. I haven't been able to find any documentation with examples that shows how to use ADRealVectorValue.\n\nThen create a new kernel that computes div (J).\n\nWhat do you mean when you say \"create a new kernel\"? Do you mean to invoke an existing kernel or to write my own kernel? If the latter, I have no idea how to do that.\n\nThis will likely mean that material property will also need to define dJ/dx, dJ/dy etc as properties\n\nI don't understand what this means. How and where do I define dJ/dx, dJ/dy etc as properties? Can you point me to somewhere in the docs where the syntax for that is explained with an example?\n\ndirectly implement div J_n in a kernel. This is the simplest, but you wont be able to look at J_n unless you create an auxkernel that computes it\n\nI don't know what you mean by \"directly implement div J_n in a kernel\". Does this mean to write my own kernel? How do I create an auxkernel? When you say \"implement\" or \"create\" I don't know whether you mean to invoke something in the input file that already exists or to write my own.\n\n\nWhat kernel (if any) do I use to include the terms div.J_n and div.J_p in their respective conservation laws?\n\n\n\nThere are quite a few divergence kernels implemented in the MOOSE modules\n\nWhat are some examples?\n\n, but I think you may want to code your own. You can look at the Navier Stokes conservation of mass for an example. This one does not use AD. I recommend you use AD https://mooseframework.inl.gov/source/kernels/INSMass.html\n\nI don't see where in that example the divergence of a vector field is taken.\n\n\nHow do I define the electric field E = -grad psi vector field for use in the Joule heating term?\n\n\n\nSince this field depends on a nonlinear variable, you should use a kernel to add this Joule heating term. You can use the Coupleable API in a kernel to retrieve the gradient of a field https://mooseframework.inl.gov/source/interfaces/Coupleable.html\n\nIf I'm understanding you correctly, the only way to take a gradient of an existing variable is to write my own kernel? Is there really no way to do that within the framework of a MOOSE input file's syntax?\n\n\nHow do I define the total current density J = q(J_p - J_n) vector field for use in the Joule heating term?\n\n\n\nnew kernel probably unless you find one which fits you already\n\n\n\nWhat kernel (if any) do I use to include the Joule heating term -E.J in my thermal PDE? Note: I'm aware of the existence of the \"JouleHeatingSource\" kernel. However, it does not have a general enough form for my purposes. It assumes Ohm's law J = sigma E holds, which doesn't apply in my situation.\n\n\n\nProbably make a new kernel then. We wont have an additional JouleHeatingSource kernel.\n\nWould you say that it's typical for most applications that users write their own kernels instead of using built-in MOOSE kernels?\n\n\nIt also appears to have the wrong sign.\n\n\n\nAre you sure? Kernels compute a residual which is on the left hand side always.\n\nI think so. If I have a thermal PDE of the form dT/dt = E.J, and then I put the E.J onto the left hand side, I get: dT/dt - E.J = 0. The E.J has a minus sign in front of it. The documentation for the JouleHeatingSource kernel (here: https://mooseframework.inl.gov/source/kernels/JouleHeatingSource.html) shows it without a minus sign. Is there some additional minus sign floating around that cancels it?\nThanks!",
                          "url": "https://github.com/idaholab/moose/discussions/29809#discussioncomment-12240552",
                          "updatedAt": "2025-02-18T18:24:00Z",
                          "publishedAt": "2025-02-18T18:23:59Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "hey\nyou will need to follow the tutorial\nhttps://mooseframework.inl.gov/getting_started/examples_and_tutorials/index.html\n\nI haven't been able to find any documentation with examples that shows how to use ADRealVectorValue.\n\nthis is in the code, when you code a new material\n\nDo you mean to invoke an existing kernel or to write my own kernel? If the latter, I have no idea how to do that.\n\nthe latter, the tutorial will teach you\n\nHow and where do I define dJ/dx, dJ/dy etc as properties?\n\nthese are additional material properties. the tutorial willl show you how to define material properties\n\nDoes this mean to write my own kernel?  How do I create an auxkernel? When you say \"implement\" or \"create\" I don't know whether you mean to invoke something in the input file that already exists or to write my own.\n\nyes. This is all coding work that the tutorial will help with.\n\nWhat are some examples?\n\nuse grep / find in the repo to find examples, in the code\n\nIf I'm understanding you correctly, the only way to take a gradient of an existing variable is to write my own kernel? Is there really no way to do that within the framework of a MOOSE input file's syntax?\n\nthere are plenty of examples but I dont think they apply well to what you are trying to do so I recommend coding here\n\nWould you say that it's typical for most applications that users write their own kernels instead of using built-in MOOSE kernels?\n\nUnless there is a module that is targeted at solving the equation you are solving, you need to be coding kernels. The general form kernels do not cover every single equation\nYour problem involves EM terms, but our EM module is focused on waves at the moment (check back in a year for additional developments related to magnet coupled EM-thermomechanics).",
                          "url": "https://github.com/idaholab/moose/discussions/29809#discussioncomment-12240647",
                          "updatedAt": "2025-02-18T18:31:28Z",
                          "publishedAt": "2025-02-18T18:31:27Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "JLB577"
                          },
                          "bodyText": "OK, thanks for your help! I'll go through the tutorial again. By the way, I find that some of the links in the tutorial that point to the wiki are 404.",
                          "url": "https://github.com/idaholab/moose/discussions/29809#discussioncomment-12240815",
                          "updatedAt": "2025-02-18T18:45:58Z",
                          "publishedAt": "2025-02-18T18:45:58Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "GiudGiud"
                          },
                          "bodyText": "if you post them here I can fix them! thanks",
                          "url": "https://github.com/idaholab/moose/discussions/29809#discussioncomment-12240823",
                          "updatedAt": "2025-02-18T18:47:03Z",
                          "publishedAt": "2025-02-18T18:47:02Z",
                          "isAnswer": false
                        }
                      },
                      {
                        "node": {
                          "author": {
                            "login": "JLB577"
                          },
                          "bodyText": "Well, for example, at https://mooseframework.inl.gov/getting_started/examples_and_tutorials/examples/ex01_inputfile.html at the bottom of the page, the links \"Finite Element Methods\" and \"The Anatomy of a MOOSE Object\" are both 404. I'll let you know if I find any more.",
                          "url": "https://github.com/idaholab/moose/discussions/29809#discussioncomment-12240857",
                          "updatedAt": "2025-02-18T18:50:12Z",
                          "publishedAt": "2025-02-18T18:50:12Z",
                          "isAnswer": false
                        }
                      }
                    ]
                  }
                }
              }
            ]
          }
        }
      }
    ]
  }
}